Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
{'1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], '2': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], '3': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], '4': [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], '5': [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], '6': [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], '7': [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], '8': [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], '9': [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], '10': [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]}
{'model_lr': 0.0001, 'momentum': 0, 'model_weight_decay': 5e-05, 'schedule': [30, 60, 80], 'model_type': 'resnet', 'model_name': 'resnet18', 'model_weights': None, 'out_dim': {'1': 10, '2': 10, '3': 10, '4': 10, '5': 10, '6': 10, '7': 10, '8': 10, '9': 10, '10': 10}, 'model_optimizer': 'Adam', 'print_freq': 10, 'gpu': True, 'with_head': False, 'reset_model_opt': True, 'reg_coef': 100.0, 'head_lr': 0.001, 'svd_lr': 5e-05, 'bn_lr': 0.0005, 'svd_thres': 10.0, 'gamma': 0.5, 'dataset_name': 'CIFAR100_10_10'}
#param of model:11218340
Task order: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']
====================== 1 =======================
Epoch:0
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	3.3624 (3.3624)	0.0086 (0.0086)	2.446 (2.446)	3.12 (3.12)
[10/157]	0.0545 (0.3568)	0.0311 (0.0308)	2.151 (2.218)	28.12 (20.17)
[20/157]	0.0530 (0.2123)	0.0309 (0.0308)	1.828 (2.138)	31.25 (22.32)
[30/157]	0.0529 (0.1610)	0.0313 (0.0309)	1.958 (2.057)	25.00 (26.01)
[40/157]	0.0537 (0.1348)	0.0315 (0.0310)	2.123 (2.010)	31.25 (27.90)
[50/157]	0.0538 (0.1188)	0.0312 (0.0311)	1.764 (1.957)	40.62 (30.09)
[60/157]	0.0528 (0.1081)	0.0313 (0.0311)	1.716 (1.946)	53.12 (31.10)
[70/157]	0.0572 (0.1005)	0.0319 (0.0311)	1.775 (1.925)	43.75 (32.13)
[80/157]	0.0532 (0.0947)	0.0314 (0.0311)	1.931 (1.893)	40.62 (33.68)
[90/157]	0.0555 (0.0902)	0.0316 (0.0311)	1.608 (1.880)	37.50 (34.17)
[100/157]	0.0540 (0.0866)	0.0316 (0.0311)	1.928 (1.856)	40.62 (35.46)
[110/157]	0.0542 (0.0836)	0.0315 (0.0311)	1.270 (1.822)	68.75 (36.60)
[120/157]	0.0533 (0.0811)	0.0311 (0.0311)	1.518 (1.799)	37.50 (37.40)
[130/157]	0.0536 (0.0790)	0.0316 (0.0312)	1.484 (1.780)	46.88 (37.83)
[140/157]	0.0542 (0.0772)	0.0318 (0.0312)	1.321 (1.766)	56.25 (38.36)
[150/157]	0.0539 (0.0757)	0.0317 (0.0312)	1.793 (1.758)	34.38 (38.60)
[156/157]	0.0428 (0.0748)	0.0266 (0.0311)	1.947 (1.746)	25.00 (38.98)
 * Train Acc 38.980
 * Val Acc 46.700, Total time 0.50
 * Val loss 1.652, Total time 0.00
Epoch:1
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0304 (0.0304)	0.0064 (0.0064)	1.642 (1.642)	46.88 (46.88)
[10/157]	0.0549 (0.0519)	0.0322 (0.0292)	1.325 (1.475)	53.12 (49.15)
[20/157]	0.0545 (0.0528)	0.0321 (0.0303)	1.096 (1.471)	75.00 (49.55)
[30/157]	0.0552 (0.0532)	0.0322 (0.0307)	1.432 (1.423)	46.88 (50.71)
[40/157]	0.0547 (0.0534)	0.0321 (0.0310)	1.221 (1.408)	59.38 (51.75)
[50/157]	0.0544 (0.0535)	0.0318 (0.0311)	1.349 (1.437)	56.25 (51.04)
[60/157]	0.0547 (0.0536)	0.0320 (0.0312)	1.209 (1.431)	53.12 (51.49)
[70/157]	0.0541 (0.0537)	0.0317 (0.0312)	1.400 (1.424)	40.62 (51.50)
[80/157]	0.0542 (0.0538)	0.0319 (0.0313)	1.386 (1.413)	59.38 (52.04)
[90/157]	0.0545 (0.0538)	0.0321 (0.0313)	1.519 (1.408)	62.50 (52.27)
[100/157]	0.0544 (0.0539)	0.0318 (0.0314)	1.186 (1.410)	62.50 (52.44)
[110/157]	0.0549 (0.0539)	0.0315 (0.0314)	1.408 (1.407)	53.12 (52.51)
[120/157]	0.0552 (0.0539)	0.0322 (0.0314)	1.435 (1.395)	40.62 (52.79)
[130/157]	0.0551 (0.0540)	0.0318 (0.0314)	1.264 (1.382)	62.50 (53.20)
[140/157]	0.0551 (0.0540)	0.0321 (0.0314)	0.890 (1.373)	71.88 (53.12)
[150/157]	0.0548 (0.0540)	0.0321 (0.0314)	0.967 (1.357)	68.75 (53.70)
[156/157]	0.0413 (0.0539)	0.0270 (0.0314)	1.163 (1.352)	62.50 (53.92)
 * Train Acc 53.920
 * Val Acc 58.700, Total time 0.51
 * Val loss 1.306, Total time 0.00
Epoch:2
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0300 (0.0300)	0.0069 (0.0069)	1.057 (1.057)	65.62 (65.62)
[10/157]	0.0567 (0.0525)	0.0323 (0.0295)	0.906 (1.198)	78.12 (61.08)
[20/157]	0.0550 (0.0534)	0.0319 (0.0305)	1.340 (1.205)	53.12 (60.57)
[30/157]	0.0558 (0.0539)	0.0321 (0.0309)	1.255 (1.197)	59.38 (60.99)
[40/157]	0.0551 (0.0540)	0.0317 (0.0311)	1.016 (1.189)	65.62 (61.43)
[50/157]	0.0559 (0.0542)	0.0322 (0.0313)	1.446 (1.185)	59.38 (61.76)
[60/157]	0.0556 (0.0543)	0.0319 (0.0314)	1.113 (1.173)	68.75 (61.99)
[70/157]	0.0562 (0.0543)	0.0322 (0.0314)	1.146 (1.164)	68.75 (62.10)
[80/157]	0.0560 (0.0544)	0.0328 (0.0315)	0.928 (1.154)	62.50 (62.42)
[90/157]	0.0567 (0.0544)	0.0326 (0.0315)	1.189 (1.154)	53.12 (62.26)
[100/157]	0.0558 (0.0545)	0.0322 (0.0316)	0.962 (1.147)	65.62 (62.10)
[110/157]	0.0560 (0.0545)	0.0319 (0.0316)	1.182 (1.153)	62.50 (61.85)
[120/157]	0.0564 (0.0545)	0.0320 (0.0316)	0.932 (1.145)	71.88 (61.96)
[130/157]	0.0568 (0.0546)	0.0324 (0.0316)	1.343 (1.155)	46.88 (61.55)
[140/157]	0.0564 (0.0546)	0.0320 (0.0316)	0.815 (1.155)	78.12 (61.46)
[150/157]	0.0566 (0.0546)	0.0319 (0.0316)	1.071 (1.154)	59.38 (61.57)
[156/157]	0.0421 (0.0546)	0.0273 (0.0316)	1.154 (1.157)	75.00 (61.44)
 * Train Acc 61.440
 * Val Acc 64.000, Total time 0.51
 * Val loss 1.047, Total time 0.00
Epoch:3
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0304 (0.0304)	0.0073 (0.0073)	1.437 (1.437)	37.50 (37.50)
[10/157]	0.0574 (0.0532)	0.0325 (0.0301)	1.249 (1.097)	56.25 (62.50)
[20/157]	0.0566 (0.0540)	0.0322 (0.0309)	0.884 (1.052)	75.00 (64.43)
[30/157]	0.0571 (0.0543)	0.0323 (0.0312)	1.295 (1.037)	53.12 (65.22)
[40/157]	0.0575 (0.0546)	0.0315 (0.0313)	0.957 (1.009)	68.75 (66.23)
[50/157]	0.0563 (0.0546)	0.0321 (0.0314)	1.259 (1.012)	59.38 (65.75)
[60/157]	0.0570 (0.0547)	0.0319 (0.0315)	0.671 (1.024)	81.25 (65.62)
[70/157]	0.0559 (0.0548)	0.0321 (0.0316)	0.841 (1.009)	65.62 (65.80)
[80/157]	0.0573 (0.0548)	0.0325 (0.0316)	0.851 (0.996)	71.88 (66.51)
[90/157]	0.0565 (0.0548)	0.0322 (0.0316)	0.953 (0.996)	65.62 (66.55)
[100/157]	0.0608 (0.0549)	0.0320 (0.0317)	1.027 (1.002)	53.12 (66.34)
[110/157]	0.0566 (0.0549)	0.0319 (0.0317)	1.246 (0.997)	68.75 (66.64)
[120/157]	0.0563 (0.0549)	0.0322 (0.0317)	0.877 (0.988)	75.00 (66.99)
[130/157]	0.0572 (0.0550)	0.0320 (0.0317)	1.704 (0.994)	34.38 (66.72)
[140/157]	0.0559 (0.0549)	0.0322 (0.0317)	0.835 (0.998)	68.75 (66.62)
[150/157]	0.0574 (0.0550)	0.0319 (0.0317)	1.115 (1.000)	59.38 (66.58)
[156/157]	0.0426 (0.0549)	0.0277 (0.0317)	0.757 (0.996)	87.50 (66.62)
 * Train Acc 66.620
 * Val Acc 64.500, Total time 0.51
 * Val loss 1.032, Total time 0.00
Epoch:4
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0330 (0.0330)	0.0093 (0.0093)	0.874 (0.874)	68.75 (68.75)
[10/157]	0.0569 (0.0533)	0.0324 (0.0302)	1.041 (0.865)	68.75 (68.75)
[20/157]	0.0564 (0.0542)	0.0323 (0.0311)	0.665 (0.865)	84.38 (69.05)
[30/157]	0.0575 (0.0547)	0.0326 (0.0315)	1.120 (0.873)	65.62 (70.06)
[40/157]	0.0574 (0.0548)	0.0325 (0.0316)	1.425 (0.878)	46.88 (69.66)
[50/157]	0.0568 (0.0550)	0.0325 (0.0317)	0.542 (0.880)	84.38 (70.16)
[60/157]	0.0570 (0.0551)	0.0325 (0.0318)	0.897 (0.883)	68.75 (70.13)
[70/157]	0.0578 (0.0551)	0.0325 (0.0319)	0.638 (0.890)	78.12 (70.29)
[80/157]	0.0603 (0.0552)	0.0323 (0.0319)	1.015 (0.892)	68.75 (70.29)
[90/157]	0.0575 (0.0553)	0.0319 (0.0319)	1.324 (0.897)	62.50 (69.92)
[100/157]	0.0584 (0.0553)	0.0324 (0.0319)	0.967 (0.898)	65.62 (69.80)
[110/157]	0.0578 (0.0554)	0.0323 (0.0319)	0.834 (0.891)	75.00 (70.05)
[120/157]	0.0600 (0.0554)	0.0342 (0.0319)	0.869 (0.887)	68.75 (70.38)
[130/157]	0.0577 (0.0555)	0.0320 (0.0319)	0.961 (0.883)	75.00 (70.54)
[140/157]	0.0570 (0.0555)	0.0322 (0.0319)	1.086 (0.892)	59.38 (70.06)
[150/157]	0.0573 (0.0555)	0.0320 (0.0319)	0.856 (0.885)	65.62 (70.12)
[156/157]	0.0437 (0.0555)	0.0278 (0.0319)	0.977 (0.887)	75.00 (70.00)
 * Train Acc 70.000
 * Val Acc 74.000, Total time 0.52
 * Val loss 0.795, Total time 0.00
Epoch:5
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0312 (0.0312)	0.0075 (0.0075)	0.572 (0.572)	78.12 (78.12)
[10/157]	0.0571 (0.0540)	0.0323 (0.0300)	0.857 (0.899)	62.50 (67.33)
[20/157]	0.0572 (0.0547)	0.0324 (0.0309)	1.257 (0.876)	65.62 (69.05)
[30/157]	0.0570 (0.0551)	0.0325 (0.0313)	0.663 (0.842)	81.25 (70.56)
[40/157]	0.0573 (0.0553)	0.0324 (0.0315)	0.694 (0.824)	75.00 (71.11)
[50/157]	0.0569 (0.0555)	0.0324 (0.0316)	0.772 (0.820)	75.00 (71.63)
[60/157]	0.0591 (0.0556)	0.0340 (0.0317)	1.427 (0.820)	53.12 (71.57)
[70/157]	0.0571 (0.0557)	0.0325 (0.0318)	0.759 (0.820)	75.00 (71.48)
[80/157]	0.0583 (0.0558)	0.0327 (0.0318)	0.790 (0.816)	75.00 (71.84)
[90/157]	0.0579 (0.0558)	0.0328 (0.0319)	0.862 (0.809)	75.00 (72.42)
[100/157]	0.0580 (0.0559)	0.0325 (0.0319)	0.959 (0.810)	75.00 (72.46)
[110/157]	0.0584 (0.0559)	0.0332 (0.0320)	0.790 (0.810)	71.88 (72.52)
[120/157]	0.0581 (0.0559)	0.0331 (0.0320)	0.589 (0.805)	78.12 (72.65)
[130/157]	0.0571 (0.0560)	0.0326 (0.0320)	0.671 (0.796)	78.12 (73.00)
[140/157]	0.0582 (0.0560)	0.0321 (0.0320)	1.128 (0.801)	71.88 (72.72)
[150/157]	0.0572 (0.0560)	0.0324 (0.0320)	1.115 (0.801)	68.75 (72.89)
[156/157]	0.0440 (0.0560)	0.0271 (0.0320)	0.957 (0.801)	75.00 (72.94)
 * Train Acc 72.940
 * Val Acc 72.400, Total time 0.53
 * Val loss 0.897, Total time 0.00
Epoch:6
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0326 (0.0326)	0.0074 (0.0074)	0.554 (0.554)	81.25 (81.25)
[10/157]	0.0566 (0.0545)	0.0319 (0.0301)	0.728 (0.737)	78.12 (76.14)
[20/157]	0.0571 (0.0555)	0.0316 (0.0310)	0.792 (0.725)	78.12 (76.34)
[30/157]	0.0582 (0.0559)	0.0325 (0.0314)	0.757 (0.723)	75.00 (76.01)
[40/157]	0.0575 (0.0561)	0.0323 (0.0314)	0.400 (0.712)	87.50 (75.91)
[50/157]	0.0577 (0.0562)	0.0325 (0.0315)	0.444 (0.700)	81.25 (76.04)
[60/157]	0.0578 (0.0562)	0.0328 (0.0317)	0.968 (0.693)	62.50 (76.43)
[70/157]	0.0572 (0.0563)	0.0329 (0.0317)	0.789 (0.708)	75.00 (75.97)
[80/157]	0.0569 (0.0563)	0.0324 (0.0318)	0.698 (0.710)	71.88 (76.00)
[90/157]	0.0580 (0.0563)	0.0328 (0.0319)	0.590 (0.711)	81.25 (76.13)
[100/157]	0.0574 (0.0564)	0.0328 (0.0320)	0.700 (0.721)	75.00 (75.74)
[110/157]	0.0576 (0.0564)	0.0329 (0.0320)	0.925 (0.719)	68.75 (75.70)
[120/157]	0.0586 (0.0564)	0.0336 (0.0321)	0.552 (0.732)	75.00 (75.34)
[130/157]	0.0578 (0.0565)	0.0330 (0.0321)	0.846 (0.739)	68.75 (74.95)
[140/157]	0.0575 (0.0565)	0.0326 (0.0321)	0.621 (0.741)	81.25 (74.84)
[150/157]	0.0580 (0.0565)	0.0332 (0.0322)	0.607 (0.739)	78.12 (74.94)
[156/157]	0.0440 (0.0564)	0.0284 (0.0321)	0.533 (0.737)	75.00 (75.08)
 * Train Acc 75.080
 * Val Acc 77.300, Total time 0.53
 * Val loss 0.688, Total time 0.00
Epoch:7
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0338 (0.0338)	0.0076 (0.0076)	0.524 (0.524)	84.38 (84.38)
[10/157]	0.0586 (0.0550)	0.0337 (0.0304)	0.662 (0.587)	68.75 (78.41)
[20/157]	0.0581 (0.0557)	0.0330 (0.0314)	0.725 (0.620)	78.12 (77.08)
[30/157]	0.0578 (0.0561)	0.0334 (0.0317)	0.506 (0.604)	84.38 (78.63)
[40/157]	0.0583 (0.0562)	0.0324 (0.0319)	0.725 (0.604)	78.12 (78.89)
[50/157]	0.0580 (0.0563)	0.0335 (0.0321)	0.645 (0.610)	78.12 (78.74)
[60/157]	0.0604 (0.0564)	0.0358 (0.0322)	0.486 (0.610)	81.25 (79.05)
[70/157]	0.0570 (0.0565)	0.0325 (0.0323)	0.621 (0.620)	75.00 (78.83)
[80/157]	0.0580 (0.0565)	0.0331 (0.0323)	0.646 (0.633)	75.00 (78.51)
[90/157]	0.0578 (0.0566)	0.0331 (0.0324)	0.557 (0.631)	81.25 (78.40)
[100/157]	0.0583 (0.0566)	0.0334 (0.0324)	0.585 (0.630)	81.25 (78.47)
[110/157]	0.0582 (0.0566)	0.0333 (0.0324)	0.804 (0.641)	75.00 (78.07)
[120/157]	0.0576 (0.0567)	0.0332 (0.0325)	0.639 (0.648)	68.75 (77.79)
[130/157]	0.0570 (0.0567)	0.0332 (0.0325)	0.903 (0.653)	75.00 (77.62)
[140/157]	0.0619 (0.0568)	0.0353 (0.0325)	1.074 (0.658)	65.62 (77.46)
[150/157]	0.0589 (0.0568)	0.0343 (0.0325)	0.614 (0.660)	81.25 (77.40)
[156/157]	0.0444 (0.0567)	0.0280 (0.0325)	1.274 (0.658)	50.00 (77.44)
 * Train Acc 77.440
 * Val Acc 76.100, Total time 0.54
 * Val loss 0.710, Total time 0.00
Epoch:8
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0321 (0.0321)	0.0076 (0.0076)	0.806 (0.806)	75.00 (75.00)
[10/157]	0.0599 (0.0553)	0.0343 (0.0309)	0.662 (0.661)	78.12 (75.00)
[20/157]	0.0582 (0.0562)	0.0332 (0.0317)	0.781 (0.670)	75.00 (76.04)
[30/157]	0.0585 (0.0567)	0.0333 (0.0320)	1.023 (0.682)	68.75 (75.60)
[40/157]	0.0591 (0.0568)	0.0335 (0.0322)	0.396 (0.690)	81.25 (75.53)
[50/157]	0.0588 (0.0569)	0.0335 (0.0323)	0.692 (0.677)	71.88 (76.04)
[60/157]	0.0593 (0.0570)	0.0343 (0.0324)	0.484 (0.660)	81.25 (76.79)
[70/157]	0.0597 (0.0571)	0.0342 (0.0325)	0.858 (0.655)	71.88 (77.16)
[80/157]	0.0601 (0.0571)	0.0335 (0.0326)	0.591 (0.648)	81.25 (77.31)
[90/157]	0.0589 (0.0572)	0.0338 (0.0326)	0.735 (0.641)	84.38 (77.71)
[100/157]	0.0580 (0.0572)	0.0335 (0.0326)	0.492 (0.635)	84.38 (78.12)
[110/157]	0.0587 (0.0572)	0.0332 (0.0326)	0.908 (0.632)	78.12 (78.43)
[120/157]	0.0589 (0.0572)	0.0332 (0.0326)	0.275 (0.628)	93.75 (78.69)
[130/157]	0.0583 (0.0572)	0.0335 (0.0326)	0.428 (0.625)	81.25 (78.72)
[140/157]	0.0580 (0.0572)	0.0331 (0.0326)	0.647 (0.627)	84.38 (78.79)
[150/157]	0.0589 (0.0572)	0.0335 (0.0327)	1.028 (0.635)	65.62 (78.46)
[156/157]	0.0445 (0.0571)	0.0281 (0.0326)	1.936 (0.637)	50.00 (78.52)
 * Train Acc 78.520
 * Val Acc 73.000, Total time 0.54
 * Val loss 0.832, Total time 0.00
Epoch:9
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0315 (0.0315)	0.0076 (0.0076)	0.664 (0.664)	78.12 (78.12)
[10/157]	0.0595 (0.0553)	0.0344 (0.0309)	0.464 (0.639)	84.38 (76.42)
[20/157]	0.0581 (0.0564)	0.0329 (0.0317)	0.620 (0.636)	75.00 (77.38)
[30/157]	0.0585 (0.0568)	0.0332 (0.0321)	0.457 (0.616)	81.25 (79.03)
[40/157]	0.0593 (0.0570)	0.0339 (0.0322)	0.516 (0.631)	78.12 (78.73)
[50/157]	0.0586 (0.0571)	0.0336 (0.0324)	0.331 (0.622)	90.62 (79.41)
[60/157]	0.0595 (0.0572)	0.0347 (0.0325)	0.543 (0.613)	84.38 (79.71)
[70/157]	0.0597 (0.0573)	0.0344 (0.0326)	0.967 (0.604)	68.75 (80.06)
[80/157]	0.0597 (0.0573)	0.0342 (0.0326)	0.499 (0.602)	81.25 (79.82)
[90/157]	0.0593 (0.0574)	0.0336 (0.0326)	0.532 (0.601)	87.50 (79.98)
[100/157]	0.0590 (0.0575)	0.0332 (0.0327)	0.480 (0.592)	81.25 (80.26)
[110/157]	0.0592 (0.0575)	0.0338 (0.0327)	0.613 (0.586)	71.88 (80.35)
[120/157]	0.0582 (0.0576)	0.0330 (0.0326)	0.275 (0.577)	90.62 (80.53)
[130/157]	0.0610 (0.0576)	0.0352 (0.0326)	0.670 (0.580)	81.25 (80.42)
[140/157]	0.0602 (0.0576)	0.0342 (0.0326)	0.382 (0.579)	81.25 (80.43)
[150/157]	0.0597 (0.0576)	0.0342 (0.0327)	0.373 (0.580)	90.62 (80.42)
[156/157]	0.0444 (0.0575)	0.0279 (0.0326)	0.685 (0.584)	75.00 (80.28)
 * Train Acc 80.280
 * Val Acc 79.400, Total time 0.54
 * Val loss 0.632, Total time 0.00
Epoch:10
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0325 (0.0325)	0.0080 (0.0080)	0.352 (0.352)	87.50 (87.50)
[10/157]	0.0603 (0.0556)	0.0347 (0.0309)	0.621 (0.524)	71.88 (81.25)
[20/157]	0.0606 (0.0565)	0.0353 (0.0319)	0.635 (0.542)	71.88 (81.10)
[30/157]	0.0587 (0.0570)	0.0339 (0.0321)	0.222 (0.516)	96.88 (82.36)
[40/157]	0.0593 (0.0571)	0.0343 (0.0324)	0.419 (0.502)	90.62 (82.77)
[50/157]	0.0607 (0.0573)	0.0353 (0.0326)	0.739 (0.510)	71.88 (82.35)
[60/157]	0.0602 (0.0573)	0.0351 (0.0327)	0.462 (0.494)	87.50 (83.04)
[70/157]	0.0599 (0.0574)	0.0348 (0.0328)	0.427 (0.504)	84.38 (82.66)
[80/157]	0.0603 (0.0574)	0.0349 (0.0329)	0.571 (0.504)	81.25 (82.75)
[90/157]	0.0608 (0.0575)	0.0344 (0.0329)	0.785 (0.501)	75.00 (82.83)
[100/157]	0.0608 (0.0575)	0.0350 (0.0329)	0.788 (0.504)	78.12 (82.92)
[110/157]	0.0597 (0.0576)	0.0337 (0.0329)	0.946 (0.517)	81.25 (82.40)
[120/157]	0.0602 (0.0576)	0.0344 (0.0329)	0.489 (0.519)	78.12 (82.36)
[130/157]	0.0600 (0.0576)	0.0348 (0.0330)	0.787 (0.529)	78.12 (82.08)
[140/157]	0.0592 (0.0576)	0.0343 (0.0330)	0.519 (0.532)	78.12 (81.96)
[150/157]	0.0594 (0.0576)	0.0346 (0.0330)	0.303 (0.528)	90.62 (82.10)
[156/157]	0.0443 (0.0575)	0.0278 (0.0329)	0.881 (0.527)	62.50 (82.04)
 * Train Acc 82.040
 * Val Acc 79.100, Total time 0.55
 * Val loss 0.665, Total time 0.00
Epoch:11
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0322 (0.0322)	0.0077 (0.0077)	0.525 (0.525)	87.50 (87.50)
[10/157]	0.0595 (0.0556)	0.0347 (0.0310)	0.385 (0.508)	84.38 (82.39)
[20/157]	0.0588 (0.0564)	0.0338 (0.0319)	0.604 (0.475)	78.12 (83.33)
[30/157]	0.0599 (0.0569)	0.0346 (0.0324)	0.175 (0.465)	93.75 (83.47)
[40/157]	0.0593 (0.0570)	0.0340 (0.0325)	0.262 (0.470)	96.88 (83.23)
[50/157]	0.0594 (0.0572)	0.0342 (0.0326)	0.602 (0.465)	75.00 (83.64)
[60/157]	0.0604 (0.0573)	0.0345 (0.0327)	0.534 (0.467)	81.25 (83.50)
[70/157]	0.0605 (0.0574)	0.0342 (0.0328)	0.578 (0.477)	75.00 (83.36)
[80/157]	0.0609 (0.0575)	0.0351 (0.0328)	0.333 (0.473)	90.62 (83.18)
[90/157]	0.0607 (0.0575)	0.0345 (0.0328)	0.309 (0.471)	90.62 (83.17)
[100/157]	0.0592 (0.0575)	0.0336 (0.0328)	0.439 (0.475)	84.38 (82.77)
[110/157]	0.0597 (0.0576)	0.0343 (0.0328)	0.509 (0.478)	78.12 (82.85)
[120/157]	0.0586 (0.0576)	0.0332 (0.0328)	0.542 (0.483)	84.38 (82.83)
[130/157]	0.0588 (0.0576)	0.0335 (0.0328)	0.423 (0.482)	87.50 (83.09)
[140/157]	0.0588 (0.0576)	0.0340 (0.0329)	0.335 (0.481)	87.50 (83.33)
[150/157]	0.0588 (0.0576)	0.0339 (0.0328)	0.380 (0.479)	90.62 (83.34)
[156/157]	0.0443 (0.0576)	0.0277 (0.0328)	0.113 (0.479)	100.00 (83.36)
 * Train Acc 83.360
 * Val Acc 79.100, Total time 0.55
 * Val loss 0.635, Total time 0.00
Epoch:12
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0333 (0.0333)	0.0083 (0.0083)	0.438 (0.438)	84.38 (84.38)
[10/157]	0.0593 (0.0559)	0.0341 (0.0312)	0.157 (0.468)	96.88 (87.50)
[20/157]	0.0596 (0.0568)	0.0344 (0.0321)	0.369 (0.424)	93.75 (87.05)
[30/157]	0.0606 (0.0573)	0.0350 (0.0326)	0.432 (0.421)	84.38 (85.89)
[40/157]	0.0598 (0.0574)	0.0340 (0.0327)	0.463 (0.429)	84.38 (86.13)
[50/157]	0.0597 (0.0575)	0.0343 (0.0328)	0.569 (0.431)	81.25 (85.85)
[60/157]	0.0601 (0.0576)	0.0347 (0.0328)	0.457 (0.438)	78.12 (85.14)
[70/157]	0.0608 (0.0577)	0.0348 (0.0329)	0.291 (0.442)	84.38 (85.21)
[80/157]	0.0620 (0.0578)	0.0349 (0.0329)	0.339 (0.439)	81.25 (84.99)
[90/157]	0.0596 (0.0578)	0.0343 (0.0329)	0.574 (0.440)	81.25 (84.79)
[100/157]	0.0595 (0.0578)	0.0341 (0.0329)	0.285 (0.436)	87.50 (84.84)
[110/157]	0.0600 (0.0579)	0.0348 (0.0329)	0.675 (0.429)	81.25 (85.08)
[120/157]	0.0600 (0.0579)	0.0343 (0.0329)	0.536 (0.439)	87.50 (84.92)
[130/157]	0.0598 (0.0579)	0.0342 (0.0329)	0.439 (0.434)	84.38 (85.07)
[140/157]	0.0606 (0.0580)	0.0348 (0.0329)	0.548 (0.434)	81.25 (84.91)
[150/157]	0.0612 (0.0580)	0.0349 (0.0329)	0.541 (0.435)	81.25 (84.89)
[156/157]	0.0453 (0.0579)	0.0283 (0.0329)	0.972 (0.437)	62.50 (84.88)
 * Train Acc 84.880
 * Val Acc 79.600, Total time 0.55
 * Val loss 0.669, Total time 0.00
Epoch:13
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0329 (0.0329)	0.0083 (0.0083)	0.605 (0.605)	78.12 (78.12)
[10/157]	0.0596 (0.0563)	0.0340 (0.0312)	0.280 (0.476)	90.62 (82.39)
[20/157]	0.0598 (0.0571)	0.0340 (0.0321)	0.328 (0.460)	87.50 (82.74)
[30/157]	0.0609 (0.0575)	0.0349 (0.0325)	0.587 (0.460)	81.25 (83.77)
[40/157]	0.0618 (0.0577)	0.0354 (0.0327)	0.551 (0.454)	68.75 (83.84)
[50/157]	0.0605 (0.0578)	0.0349 (0.0327)	0.537 (0.438)	84.38 (84.50)
[60/157]	0.0592 (0.0579)	0.0344 (0.0328)	0.444 (0.438)	75.00 (84.32)
[70/157]	0.0596 (0.0579)	0.0346 (0.0329)	0.406 (0.432)	84.38 (84.77)
[80/157]	0.0600 (0.0580)	0.0344 (0.0329)	0.134 (0.431)	93.75 (84.95)
[90/157]	0.0596 (0.0580)	0.0340 (0.0329)	0.401 (0.436)	84.38 (84.75)
[100/157]	0.0600 (0.0580)	0.0344 (0.0330)	0.627 (0.434)	71.88 (84.59)
[110/157]	0.0605 (0.0580)	0.0345 (0.0330)	0.205 (0.432)	90.62 (84.63)
[120/157]	0.0605 (0.0581)	0.0351 (0.0330)	0.468 (0.431)	87.50 (84.81)
[130/157]	0.0599 (0.0581)	0.0351 (0.0330)	0.204 (0.438)	90.62 (84.78)
[140/157]	0.0602 (0.0581)	0.0347 (0.0330)	0.368 (0.440)	87.50 (84.80)
[150/157]	0.0597 (0.0581)	0.0336 (0.0330)	0.448 (0.444)	84.38 (84.60)
[156/157]	0.0442 (0.0581)	0.0273 (0.0330)	0.858 (0.442)	62.50 (84.62)
 * Train Acc 84.620
 * Val Acc 81.500, Total time 0.55
 * Val loss 0.603, Total time 0.00
Epoch:14
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0333 (0.0333)	0.0080 (0.0080)	0.290 (0.290)	93.75 (93.75)
[10/157]	0.0606 (0.0563)	0.0342 (0.0313)	0.126 (0.360)	93.75 (88.92)
[20/157]	0.0604 (0.0572)	0.0348 (0.0322)	0.176 (0.370)	93.75 (88.69)
[30/157]	0.0591 (0.0576)	0.0341 (0.0326)	0.445 (0.352)	87.50 (88.81)
[40/157]	0.0593 (0.0578)	0.0346 (0.0327)	0.591 (0.389)	75.00 (87.50)
[50/157]	0.0595 (0.0579)	0.0341 (0.0329)	0.409 (0.402)	84.38 (86.89)
[60/157]	0.0604 (0.0580)	0.0344 (0.0329)	0.362 (0.393)	81.25 (87.35)
[70/157]	0.0601 (0.0580)	0.0344 (0.0330)	0.432 (0.398)	87.50 (87.10)
[80/157]	0.0593 (0.0581)	0.0342 (0.0330)	0.317 (0.398)	90.62 (87.11)
[90/157]	0.0598 (0.0581)	0.0342 (0.0330)	0.486 (0.404)	87.50 (86.57)
[100/157]	0.0592 (0.0582)	0.0335 (0.0330)	0.189 (0.402)	90.62 (86.51)
[110/157]	0.0616 (0.0582)	0.0355 (0.0331)	0.402 (0.405)	90.62 (86.29)
[120/157]	0.0591 (0.0582)	0.0347 (0.0331)	0.508 (0.399)	78.12 (86.31)
[130/157]	0.0589 (0.0582)	0.0337 (0.0331)	0.327 (0.397)	84.38 (86.40)
[140/157]	0.0613 (0.0583)	0.0352 (0.0331)	0.344 (0.396)	90.62 (86.48)
[150/157]	0.0613 (0.0583)	0.0344 (0.0331)	0.219 (0.396)	93.75 (86.47)
[156/157]	0.0450 (0.0582)	0.0279 (0.0331)	0.527 (0.398)	87.50 (86.36)
 * Train Acc 86.360
 * Val Acc 80.300, Total time 0.55
 * Val loss 0.629, Total time 0.00
Epoch:15
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0329 (0.0329)	0.0082 (0.0082)	0.503 (0.503)	78.12 (78.12)
[10/157]	0.0590 (0.0567)	0.0337 (0.0309)	0.265 (0.392)	90.62 (86.36)
[20/157]	0.0610 (0.0575)	0.0356 (0.0319)	0.182 (0.399)	93.75 (86.31)
[30/157]	0.0609 (0.0579)	0.0346 (0.0322)	0.468 (0.411)	78.12 (86.39)
[40/157]	0.0599 (0.0580)	0.0345 (0.0324)	0.235 (0.415)	87.50 (86.36)
[50/157]	0.0598 (0.0581)	0.0341 (0.0326)	0.152 (0.417)	93.75 (86.27)
[60/157]	0.0616 (0.0582)	0.0346 (0.0327)	0.417 (0.413)	90.62 (86.12)
[70/157]	0.0595 (0.0583)	0.0338 (0.0328)	0.165 (0.413)	96.88 (86.09)
[80/157]	0.0598 (0.0583)	0.0340 (0.0328)	0.228 (0.412)	93.75 (86.00)
[90/157]	0.0608 (0.0584)	0.0341 (0.0328)	0.434 (0.410)	87.50 (86.02)
[100/157]	0.0603 (0.0584)	0.0343 (0.0329)	0.380 (0.402)	84.38 (86.23)
[110/157]	0.0598 (0.0584)	0.0338 (0.0329)	0.201 (0.397)	93.75 (86.46)
[120/157]	0.0609 (0.0584)	0.0342 (0.0329)	0.172 (0.393)	96.88 (86.49)
[130/157]	0.0594 (0.0585)	0.0340 (0.0329)	0.605 (0.393)	87.50 (86.52)
[140/157]	0.0599 (0.0585)	0.0342 (0.0330)	0.289 (0.395)	90.62 (86.52)
[150/157]	0.0608 (0.0585)	0.0345 (0.0330)	0.309 (0.396)	93.75 (86.71)
[156/157]	0.0443 (0.0584)	0.0276 (0.0329)	0.520 (0.396)	87.50 (86.72)
 * Train Acc 86.720
 * Val Acc 81.800, Total time 0.55
 * Val loss 0.626, Total time 0.00
Epoch:16
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0340 (0.0340)	0.0080 (0.0080)	0.170 (0.170)	93.75 (93.75)
[10/157]	0.0607 (0.0562)	0.0343 (0.0311)	0.487 (0.341)	84.38 (86.65)
[20/157]	0.0611 (0.0572)	0.0352 (0.0321)	0.391 (0.360)	81.25 (86.01)
[30/157]	0.0599 (0.0578)	0.0342 (0.0324)	0.211 (0.355)	90.62 (86.19)
[40/157]	0.0611 (0.0579)	0.0344 (0.0326)	0.242 (0.333)	93.75 (87.27)
[50/157]	0.0600 (0.0581)	0.0342 (0.0327)	0.246 (0.327)	90.62 (87.81)
[60/157]	0.0614 (0.0582)	0.0348 (0.0328)	0.295 (0.333)	90.62 (87.96)
[70/157]	0.0604 (0.0582)	0.0341 (0.0329)	0.386 (0.336)	81.25 (87.94)
[80/157]	0.0598 (0.0583)	0.0344 (0.0329)	0.173 (0.331)	90.62 (88.16)
[90/157]	0.0611 (0.0584)	0.0343 (0.0329)	0.316 (0.334)	87.50 (88.22)
[100/157]	0.0590 (0.0584)	0.0341 (0.0330)	0.413 (0.335)	87.50 (88.15)
[110/157]	0.0593 (0.0584)	0.0333 (0.0330)	0.192 (0.345)	96.88 (87.81)
[120/157]	0.0605 (0.0585)	0.0337 (0.0330)	0.195 (0.338)	93.75 (88.04)
[130/157]	0.0594 (0.0585)	0.0336 (0.0330)	0.313 (0.337)	84.38 (88.17)
[140/157]	0.0611 (0.0586)	0.0342 (0.0330)	0.495 (0.337)	84.38 (87.99)
[150/157]	0.0591 (0.0586)	0.0334 (0.0330)	0.298 (0.340)	81.25 (87.87)
[156/157]	0.0456 (0.0585)	0.0279 (0.0330)	0.572 (0.341)	87.50 (87.90)
 * Train Acc 87.900
 * Val Acc 83.600, Total time 0.56
 * Val loss 0.588, Total time 0.00
Epoch:17
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0341 (0.0341)	0.0085 (0.0085)	0.300 (0.300)	87.50 (87.50)
[10/157]	0.0595 (0.0568)	0.0337 (0.0309)	0.317 (0.320)	96.88 (90.06)
[20/157]	0.0623 (0.0578)	0.0349 (0.0320)	0.297 (0.321)	90.62 (89.88)
[30/157]	0.0596 (0.0581)	0.0343 (0.0324)	0.114 (0.296)	96.88 (91.03)
[40/157]	0.0618 (0.0584)	0.0340 (0.0326)	0.592 (0.295)	84.38 (90.55)
[50/157]	0.0595 (0.0584)	0.0339 (0.0327)	0.285 (0.299)	81.25 (89.89)
[60/157]	0.0607 (0.0586)	0.0338 (0.0328)	0.247 (0.305)	90.62 (89.65)
[70/157]	0.0608 (0.0586)	0.0344 (0.0328)	0.234 (0.307)	90.62 (89.66)
[80/157]	0.0611 (0.0587)	0.0345 (0.0329)	0.252 (0.311)	90.62 (89.39)
[90/157]	0.0603 (0.0587)	0.0343 (0.0330)	0.169 (0.314)	93.75 (89.25)
[100/157]	0.0605 (0.0588)	0.0341 (0.0330)	0.153 (0.315)	93.75 (89.20)
[110/157]	0.0593 (0.0588)	0.0336 (0.0330)	0.251 (0.318)	93.75 (89.19)
[120/157]	0.0608 (0.0588)	0.0342 (0.0330)	0.398 (0.322)	81.25 (88.89)
[130/157]	0.0603 (0.0588)	0.0338 (0.0330)	0.176 (0.328)	93.75 (88.79)
[140/157]	0.0610 (0.0588)	0.0346 (0.0331)	0.549 (0.327)	81.25 (88.72)
[150/157]	0.0599 (0.0589)	0.0335 (0.0331)	0.215 (0.325)	93.75 (88.76)
[156/157]	0.0455 (0.0588)	0.0277 (0.0330)	0.168 (0.325)	100.00 (88.74)
 * Train Acc 88.740
 * Val Acc 84.900, Total time 0.56
 * Val loss 0.522, Total time 0.00
Epoch:18
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0338 (0.0338)	0.0086 (0.0086)	0.190 (0.190)	96.88 (96.88)
[10/157]	0.0607 (0.0570)	0.0336 (0.0310)	0.291 (0.255)	84.38 (90.34)
[20/157]	0.0600 (0.0578)	0.0339 (0.0319)	0.276 (0.265)	87.50 (90.03)
[30/157]	0.0599 (0.0583)	0.0330 (0.0322)	0.341 (0.283)	84.38 (89.52)
[40/157]	0.0602 (0.0583)	0.0343 (0.0325)	0.410 (0.289)	81.25 (89.63)
[50/157]	0.0597 (0.0585)	0.0350 (0.0326)	0.257 (0.294)	90.62 (89.40)
[60/157]	0.0618 (0.0585)	0.0351 (0.0328)	0.444 (0.295)	87.50 (89.55)
[70/157]	0.0606 (0.0586)	0.0348 (0.0328)	0.234 (0.296)	93.75 (89.44)
[80/157]	0.0612 (0.0586)	0.0342 (0.0329)	0.264 (0.288)	93.75 (89.70)
[90/157]	0.0599 (0.0586)	0.0340 (0.0329)	0.249 (0.292)	90.62 (89.59)
[100/157]	0.0604 (0.0587)	0.0339 (0.0330)	0.344 (0.291)	90.62 (89.70)
[110/157]	0.0599 (0.0587)	0.0340 (0.0330)	0.128 (0.290)	96.88 (89.70)
[120/157]	0.0612 (0.0587)	0.0341 (0.0330)	0.509 (0.304)	75.00 (89.18)
[130/157]	0.0596 (0.0587)	0.0338 (0.0330)	0.526 (0.306)	81.25 (89.19)
[140/157]	0.0610 (0.0588)	0.0343 (0.0331)	0.479 (0.311)	84.38 (89.10)
[150/157]	0.0596 (0.0588)	0.0341 (0.0331)	0.433 (0.317)	84.38 (88.95)
[156/157]	0.0458 (0.0587)	0.0282 (0.0331)	0.332 (0.316)	75.00 (89.04)
 * Train Acc 89.040
 * Val Acc 81.800, Total time 0.56
 * Val loss 0.633, Total time 0.00
Epoch:19
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0337 (0.0337)	0.0085 (0.0085)	0.359 (0.359)	84.38 (84.38)
[10/157]	0.0599 (0.0568)	0.0343 (0.0314)	0.230 (0.324)	90.62 (88.92)
[20/157]	0.0604 (0.0578)	0.0344 (0.0323)	0.379 (0.325)	90.62 (89.14)
[30/157]	0.0605 (0.0582)	0.0345 (0.0327)	0.204 (0.314)	90.62 (89.01)
[40/157]	0.0604 (0.0584)	0.0337 (0.0328)	0.221 (0.309)	90.62 (89.56)
[50/157]	0.0602 (0.0585)	0.0344 (0.0329)	0.173 (0.297)	93.75 (89.64)
[60/157]	0.0615 (0.0587)	0.0343 (0.0330)	0.221 (0.288)	87.50 (89.81)
[70/157]	0.0597 (0.0587)	0.0340 (0.0330)	0.267 (0.293)	87.50 (89.52)
[80/157]	0.0603 (0.0588)	0.0338 (0.0331)	0.176 (0.294)	90.62 (89.58)
[90/157]	0.0593 (0.0588)	0.0336 (0.0331)	0.082 (0.286)	96.88 (89.73)
[100/157]	0.0601 (0.0588)	0.0338 (0.0331)	0.229 (0.284)	93.75 (89.85)
[110/157]	0.0601 (0.0588)	0.0341 (0.0331)	0.272 (0.280)	87.50 (90.09)
[120/157]	0.0592 (0.0588)	0.0329 (0.0331)	0.452 (0.286)	87.50 (89.93)
[130/157]	0.0599 (0.0589)	0.0339 (0.0331)	0.223 (0.285)	87.50 (90.00)
[140/157]	0.0609 (0.0589)	0.0346 (0.0332)	0.481 (0.284)	84.38 (90.09)
[150/157]	0.0615 (0.0589)	0.0344 (0.0332)	0.309 (0.287)	84.38 (89.84)
[156/157]	0.0459 (0.0588)	0.0275 (0.0331)	0.334 (0.287)	75.00 (89.90)
 * Train Acc 89.900
 * Val Acc 81.500, Total time 0.56
 * Val loss 0.626, Total time 0.00
Epoch:20
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0333 (0.0333)	0.0082 (0.0082)	0.248 (0.248)	93.75 (93.75)
[10/157]	0.0592 (0.0570)	0.0327 (0.0315)	0.251 (0.333)	90.62 (88.92)
[20/157]	0.0605 (0.0580)	0.0344 (0.0322)	0.172 (0.290)	96.88 (90.18)
[30/157]	0.0605 (0.0583)	0.0341 (0.0326)	0.342 (0.277)	90.62 (90.12)
[40/157]	0.0603 (0.0585)	0.0340 (0.0327)	0.202 (0.269)	90.62 (90.32)
[50/157]	0.0605 (0.0586)	0.0338 (0.0328)	0.276 (0.271)	87.50 (90.38)
[60/157]	0.0593 (0.0587)	0.0330 (0.0328)	0.172 (0.285)	90.62 (90.01)
[70/157]	0.0591 (0.0587)	0.0335 (0.0329)	0.205 (0.284)	93.75 (90.10)
[80/157]	0.0577 (0.0588)	0.0326 (0.0329)	0.176 (0.280)	93.75 (90.12)
[90/157]	0.0607 (0.0589)	0.0345 (0.0330)	0.405 (0.294)	90.62 (89.59)
[100/157]	0.0618 (0.0589)	0.0351 (0.0330)	0.120 (0.288)	96.88 (89.91)
[110/157]	0.0593 (0.0589)	0.0324 (0.0330)	0.261 (0.293)	90.62 (89.78)
[120/157]	0.0607 (0.0590)	0.0346 (0.0331)	0.240 (0.291)	90.62 (89.90)
[130/157]	0.0604 (0.0590)	0.0346 (0.0331)	0.547 (0.294)	81.25 (89.96)
[140/157]	0.0623 (0.0590)	0.0341 (0.0331)	0.238 (0.293)	93.75 (90.09)
[150/157]	0.0603 (0.0591)	0.0338 (0.0331)	0.302 (0.289)	90.62 (90.25)
[156/157]	0.0460 (0.0590)	0.0276 (0.0330)	0.743 (0.290)	62.50 (90.20)
 * Train Acc 90.200
 * Val Acc 82.900, Total time 0.57
 * Val loss 0.659, Total time 0.00
Epoch:21
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0347 (0.0347)	0.0087 (0.0087)	0.100 (0.100)	96.88 (96.88)
[10/157]	0.0604 (0.0571)	0.0339 (0.0313)	0.195 (0.255)	93.75 (91.76)
[20/157]	0.0611 (0.0581)	0.0339 (0.0322)	0.181 (0.253)	93.75 (91.37)
[30/157]	0.0608 (0.0586)	0.0339 (0.0325)	0.191 (0.234)	90.62 (91.63)
[40/157]	0.0622 (0.0587)	0.0354 (0.0327)	0.184 (0.235)	96.88 (91.92)
[50/157]	0.0610 (0.0589)	0.0344 (0.0329)	0.174 (0.235)	90.62 (91.97)
[60/157]	0.0605 (0.0590)	0.0332 (0.0329)	0.114 (0.231)	96.88 (92.06)
[70/157]	0.0605 (0.0591)	0.0340 (0.0330)	0.165 (0.232)	93.75 (92.17)
[80/157]	0.0615 (0.0591)	0.0353 (0.0331)	0.113 (0.231)	96.88 (92.32)
[90/157]	0.0614 (0.0591)	0.0340 (0.0331)	0.362 (0.234)	90.62 (92.20)
[100/157]	0.0604 (0.0592)	0.0337 (0.0331)	0.201 (0.232)	93.75 (92.26)
[110/157]	0.0585 (0.0591)	0.0325 (0.0331)	0.361 (0.233)	84.38 (92.23)
[120/157]	0.0609 (0.0592)	0.0340 (0.0331)	0.082 (0.232)	96.88 (92.15)
[130/157]	0.0605 (0.0592)	0.0337 (0.0331)	0.293 (0.234)	90.62 (92.15)
[140/157]	0.0610 (0.0592)	0.0349 (0.0331)	0.088 (0.234)	96.88 (92.20)
[150/157]	0.0602 (0.0592)	0.0336 (0.0331)	0.226 (0.236)	90.62 (92.09)
[156/157]	0.0459 (0.0591)	0.0279 (0.0331)	0.927 (0.241)	62.50 (91.92)
 * Train Acc 91.920
 * Val Acc 81.600, Total time 0.57
 * Val loss 0.680, Total time 0.00
Epoch:22
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0347 (0.0347)	0.0087 (0.0087)	0.492 (0.492)	84.38 (84.38)
[10/157]	0.0595 (0.0574)	0.0327 (0.0309)	0.151 (0.268)	93.75 (92.05)
[20/157]	0.0604 (0.0584)	0.0345 (0.0320)	0.278 (0.276)	87.50 (91.22)
[30/157]	0.0616 (0.0587)	0.0348 (0.0325)	0.481 (0.302)	84.38 (89.42)
[40/157]	0.0615 (0.0589)	0.0344 (0.0327)	0.282 (0.298)	90.62 (89.86)
[50/157]	0.0610 (0.0590)	0.0338 (0.0328)	0.169 (0.289)	93.75 (90.07)
[60/157]	0.0604 (0.0591)	0.0344 (0.0328)	0.287 (0.268)	87.50 (90.62)
[70/157]	0.0603 (0.0591)	0.0334 (0.0329)	0.503 (0.268)	81.25 (90.54)
[80/157]	0.0597 (0.0591)	0.0334 (0.0329)	0.227 (0.267)	90.62 (90.66)
[90/157]	0.0595 (0.0591)	0.0332 (0.0329)	0.210 (0.264)	90.62 (90.66)
[100/157]	0.0618 (0.0591)	0.0341 (0.0330)	0.443 (0.268)	75.00 (90.53)
[110/157]	0.0580 (0.0592)	0.0316 (0.0329)	0.191 (0.266)	96.88 (90.74)
[120/157]	0.0616 (0.0592)	0.0348 (0.0330)	0.111 (0.262)	100.00 (90.96)
[130/157]	0.0611 (0.0592)	0.0339 (0.0330)	0.133 (0.256)	96.88 (91.15)
[140/157]	0.0584 (0.0592)	0.0326 (0.0330)	0.140 (0.251)	93.75 (91.27)
[150/157]	0.0598 (0.0592)	0.0337 (0.0330)	0.544 (0.250)	90.62 (91.39)
[156/157]	0.0464 (0.0591)	0.0286 (0.0329)	0.010 (0.250)	100.00 (91.38)
 * Train Acc 91.380
 * Val Acc 84.500, Total time 0.57
 * Val loss 0.549, Total time 0.00
Epoch:23
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0346 (0.0346)	0.0096 (0.0096)	0.100 (0.100)	96.88 (96.88)
[10/157]	0.0605 (0.0575)	0.0339 (0.0316)	0.285 (0.194)	90.62 (94.32)
[20/157]	0.0607 (0.0583)	0.0339 (0.0322)	0.212 (0.206)	90.62 (93.30)
[30/157]	0.0601 (0.0586)	0.0344 (0.0326)	0.079 (0.215)	100.00 (92.94)
[40/157]	0.0619 (0.0588)	0.0349 (0.0328)	0.132 (0.203)	96.88 (93.29)
[50/157]	0.0590 (0.0590)	0.0320 (0.0329)	0.325 (0.207)	90.62 (93.01)
[60/157]	0.0599 (0.0591)	0.0330 (0.0328)	0.347 (0.207)	87.50 (93.03)
[70/157]	0.0606 (0.0592)	0.0343 (0.0329)	0.224 (0.204)	90.62 (92.87)
[80/157]	0.0595 (0.0592)	0.0330 (0.0329)	0.191 (0.203)	93.75 (93.06)
[90/157]	0.0613 (0.0592)	0.0340 (0.0329)	0.059 (0.210)	100.00 (92.62)
[100/157]	0.0610 (0.0592)	0.0346 (0.0330)	0.175 (0.217)	93.75 (92.39)
[110/157]	0.0609 (0.0592)	0.0347 (0.0330)	0.240 (0.216)	93.75 (92.48)
[120/157]	0.0618 (0.0593)	0.0339 (0.0331)	0.186 (0.216)	96.88 (92.46)
[130/157]	0.0605 (0.0593)	0.0337 (0.0331)	0.097 (0.216)	100.00 (92.46)
[140/157]	0.0608 (0.0593)	0.0345 (0.0331)	0.382 (0.223)	87.50 (92.29)
[150/157]	0.0596 (0.0593)	0.0330 (0.0331)	0.196 (0.222)	93.75 (92.32)
[156/157]	0.0461 (0.0592)	0.0272 (0.0331)	0.699 (0.222)	62.50 (92.30)
 * Train Acc 92.300
 * Val Acc 83.500, Total time 0.57
 * Val loss 0.591, Total time 0.00
Epoch:24
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0355 (0.0355)	0.0087 (0.0087)	0.571 (0.571)	84.38 (84.38)
[10/157]	0.0581 (0.0571)	0.0318 (0.0310)	0.331 (0.265)	87.50 (90.62)
[20/157]	0.0616 (0.0584)	0.0345 (0.0320)	0.212 (0.234)	87.50 (91.37)
[30/157]	0.0613 (0.0590)	0.0338 (0.0324)	0.379 (0.241)	93.75 (91.63)
[40/157]	0.0614 (0.0591)	0.0345 (0.0326)	0.150 (0.234)	93.75 (91.77)
[50/157]	0.0623 (0.0592)	0.0348 (0.0328)	0.122 (0.221)	96.88 (92.16)
[60/157]	0.0612 (0.0593)	0.0346 (0.0329)	0.108 (0.221)	93.75 (92.01)
[70/157]	0.0593 (0.0593)	0.0328 (0.0330)	0.225 (0.218)	93.75 (92.25)
[80/157]	0.0618 (0.0594)	0.0345 (0.0330)	0.231 (0.217)	90.62 (92.28)
[90/157]	0.0617 (0.0594)	0.0339 (0.0330)	0.101 (0.216)	96.88 (92.24)
[100/157]	0.0618 (0.0594)	0.0351 (0.0330)	0.423 (0.224)	93.75 (92.02)
[110/157]	0.0608 (0.0594)	0.0346 (0.0331)	0.376 (0.224)	84.38 (91.86)
[120/157]	0.0611 (0.0594)	0.0341 (0.0331)	0.460 (0.225)	81.25 (91.97)
[130/157]	0.0614 (0.0595)	0.0344 (0.0332)	0.173 (0.219)	93.75 (92.13)
[140/157]	0.0627 (0.0595)	0.0345 (0.0332)	0.298 (0.216)	90.62 (92.24)
[150/157]	0.0598 (0.0595)	0.0332 (0.0332)	0.160 (0.212)	96.88 (92.38)
[156/157]	0.0470 (0.0594)	0.0265 (0.0331)	0.103 (0.209)	100.00 (92.56)
 * Train Acc 92.560
 * Val Acc 85.000, Total time 0.58
 * Val loss 0.574, Total time 0.00
Epoch:25
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0352 (0.0352)	0.0090 (0.0090)	0.204 (0.204)	90.62 (90.62)
[10/157]	0.0612 (0.0576)	0.0349 (0.0318)	0.243 (0.152)	90.62 (94.03)
[20/157]	0.0606 (0.0589)	0.0342 (0.0330)	0.179 (0.150)	93.75 (94.49)
[30/157]	0.0612 (0.0604)	0.0348 (0.0343)	0.219 (0.168)	90.62 (93.65)
[40/157]	0.0615 (0.0602)	0.0351 (0.0342)	0.058 (0.167)	100.00 (93.67)
[50/157]	0.0605 (0.0602)	0.0340 (0.0342)	0.436 (0.183)	84.38 (93.32)
[60/157]	0.0606 (0.0602)	0.0340 (0.0343)	0.126 (0.184)	93.75 (93.24)
[70/157]	0.0612 (0.0602)	0.0345 (0.0343)	0.206 (0.188)	90.62 (93.05)
[80/157]	0.0622 (0.0602)	0.0348 (0.0342)	0.400 (0.190)	84.38 (93.17)
[90/157]	0.0611 (0.0602)	0.0339 (0.0342)	0.320 (0.193)	93.75 (93.30)
[100/157]	0.0613 (0.0602)	0.0341 (0.0341)	0.076 (0.200)	96.88 (93.01)
[110/157]	0.0617 (0.0602)	0.0348 (0.0341)	0.137 (0.203)	96.88 (93.02)
[120/157]	0.0613 (0.0602)	0.0351 (0.0342)	0.464 (0.205)	81.25 (92.85)
[130/157]	0.0600 (0.0602)	0.0338 (0.0341)	0.340 (0.209)	87.50 (92.82)
[140/157]	0.0610 (0.0602)	0.0340 (0.0341)	0.066 (0.212)	96.88 (92.71)
[150/157]	0.0607 (0.0602)	0.0333 (0.0341)	0.366 (0.216)	90.62 (92.55)
[156/157]	0.0472 (0.0601)	0.0290 (0.0341)	0.066 (0.215)	100.00 (92.58)
 * Train Acc 92.580
 * Val Acc 84.400, Total time 0.59
 * Val loss 0.564, Total time 0.00
Epoch:26
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0345 (0.0345)	0.0080 (0.0080)	0.121 (0.121)	93.75 (93.75)
[10/157]	0.0597 (0.0577)	0.0335 (0.0322)	0.252 (0.213)	90.62 (91.76)
[20/157]	0.0599 (0.0589)	0.0337 (0.0332)	0.075 (0.207)	96.88 (92.86)
[30/157]	0.0601 (0.0591)	0.0337 (0.0334)	0.039 (0.214)	100.00 (92.54)
[40/157]	0.0601 (0.0593)	0.0337 (0.0336)	0.162 (0.200)	93.75 (92.99)
[50/157]	0.0602 (0.0594)	0.0337 (0.0337)	0.094 (0.200)	96.88 (93.01)
[60/157]	0.0609 (0.0595)	0.0347 (0.0338)	0.216 (0.200)	93.75 (93.08)
[70/157]	0.0650 (0.0601)	0.0372 (0.0341)	0.240 (0.194)	87.50 (93.22)
[80/157]	0.0647 (0.0606)	0.0368 (0.0345)	0.207 (0.185)	87.50 (93.56)
[90/157]	0.0643 (0.0610)	0.0363 (0.0347)	0.143 (0.183)	96.88 (93.68)
[100/157]	0.0602 (0.0612)	0.0332 (0.0348)	0.320 (0.187)	87.50 (93.50)
[110/157]	0.0617 (0.0610)	0.0352 (0.0347)	0.118 (0.191)	93.75 (93.36)
[120/157]	0.0618 (0.0610)	0.0342 (0.0347)	0.225 (0.188)	90.62 (93.47)
[130/157]	0.0610 (0.0610)	0.0346 (0.0347)	0.154 (0.188)	96.88 (93.44)
[140/157]	0.0625 (0.0610)	0.0354 (0.0346)	0.395 (0.189)	87.50 (93.37)
[150/157]	0.0624 (0.0609)	0.0346 (0.0346)	0.228 (0.187)	93.75 (93.48)
[156/157]	0.0452 (0.0608)	0.0270 (0.0346)	0.618 (0.190)	75.00 (93.42)
 * Train Acc 93.420
 * Val Acc 81.000, Total time 0.59
 * Val loss 0.762, Total time 0.00
Epoch:27
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0351 (0.0351)	0.0086 (0.0086)	0.241 (0.241)	81.25 (81.25)
[10/157]	0.0618 (0.0582)	0.0348 (0.0322)	0.313 (0.207)	90.62 (92.33)
[20/157]	0.0619 (0.0593)	0.0347 (0.0332)	0.114 (0.188)	96.88 (93.45)
[30/157]	0.0613 (0.0599)	0.0348 (0.0337)	0.214 (0.192)	93.75 (93.25)
[40/157]	0.0610 (0.0600)	0.0346 (0.0338)	0.471 (0.190)	81.25 (93.45)
[50/157]	0.0611 (0.0602)	0.0342 (0.0339)	0.063 (0.180)	100.00 (94.00)
[60/157]	0.0626 (0.0603)	0.0350 (0.0340)	0.134 (0.184)	96.88 (93.75)
[70/157]	0.0611 (0.0603)	0.0347 (0.0340)	0.243 (0.188)	90.62 (93.49)
[80/157]	0.0619 (0.0604)	0.0343 (0.0340)	0.126 (0.184)	93.75 (93.63)
[90/157]	0.0613 (0.0604)	0.0346 (0.0340)	0.155 (0.176)	90.62 (93.85)
[100/157]	0.0612 (0.0604)	0.0345 (0.0341)	0.385 (0.182)	87.50 (93.75)
[110/157]	0.0612 (0.0604)	0.0345 (0.0341)	0.085 (0.181)	100.00 (93.81)
[120/157]	0.0604 (0.0604)	0.0337 (0.0341)	0.325 (0.182)	84.38 (93.78)
[130/157]	0.0610 (0.0604)	0.0344 (0.0341)	0.290 (0.182)	90.62 (93.75)
[140/157]	0.0610 (0.0604)	0.0345 (0.0342)	0.232 (0.184)	90.62 (93.75)
[150/157]	0.0606 (0.0604)	0.0339 (0.0342)	0.133 (0.188)	96.88 (93.69)
[156/157]	0.0463 (0.0603)	0.0281 (0.0341)	0.303 (0.186)	87.50 (93.74)
 * Train Acc 93.740
 * Val Acc 83.100, Total time 0.59
 * Val loss 0.641, Total time 0.00
Epoch:28
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0356 (0.0356)	0.0087 (0.0087)	0.157 (0.157)	93.75 (93.75)
[10/157]	0.0605 (0.0582)	0.0336 (0.0318)	0.078 (0.179)	100.00 (95.17)
[20/157]	0.0623 (0.0594)	0.0355 (0.0330)	0.217 (0.189)	90.62 (93.30)
[30/157]	0.0589 (0.0598)	0.0320 (0.0332)	0.194 (0.187)	93.75 (93.45)
[40/157]	0.0618 (0.0600)	0.0346 (0.0335)	0.156 (0.187)	96.88 (93.52)
[50/157]	0.0617 (0.0601)	0.0347 (0.0337)	0.137 (0.177)	96.88 (93.87)
[60/157]	0.0621 (0.0602)	0.0351 (0.0338)	0.067 (0.187)	96.88 (93.44)
[70/157]	0.0684 (0.0609)	0.0390 (0.0343)	0.089 (0.181)	96.88 (93.57)
[80/157]	0.0599 (0.0617)	0.0335 (0.0348)	0.249 (0.185)	93.75 (93.40)
[90/157]	0.0615 (0.0615)	0.0349 (0.0346)	0.182 (0.184)	93.75 (93.48)
[100/157]	0.0781 (0.0621)	0.0458 (0.0351)	0.411 (0.186)	84.38 (93.41)
[110/157]	0.0627 (0.0627)	0.0344 (0.0355)	0.226 (0.185)	93.75 (93.41)
[120/157]	0.0625 (0.0626)	0.0353 (0.0355)	0.203 (0.187)	90.62 (93.26)
[130/157]	0.0619 (0.0626)	0.0350 (0.0355)	0.050 (0.187)	100.00 (93.32)
[140/157]	0.0622 (0.0626)	0.0341 (0.0354)	0.062 (0.185)	96.88 (93.37)
[150/157]	0.0624 (0.0626)	0.0345 (0.0354)	0.126 (0.182)	96.88 (93.50)
[156/157]	0.0476 (0.0625)	0.0287 (0.0354)	0.094 (0.180)	100.00 (93.58)
 * Train Acc 93.580
 * Val Acc 83.900, Total time 0.61
 * Val loss 0.606, Total time 0.00
Epoch:29
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0358 (0.0358)	0.0098 (0.0098)	0.096 (0.096)	96.88 (96.88)
[10/157]	0.0631 (0.0604)	0.0354 (0.0337)	0.189 (0.148)	93.75 (94.60)
[20/157]	0.0623 (0.0613)	0.0345 (0.0344)	0.118 (0.143)	93.75 (94.20)
[30/157]	0.0615 (0.0618)	0.0343 (0.0348)	0.194 (0.142)	93.75 (94.46)
[40/157]	0.0615 (0.0620)	0.0342 (0.0349)	0.075 (0.139)	96.88 (94.74)
[50/157]	0.0613 (0.0620)	0.0341 (0.0350)	0.081 (0.132)	96.88 (95.04)
[60/157]	0.0622 (0.0620)	0.0347 (0.0351)	0.088 (0.128)	100.00 (95.18)
[70/157]	0.0626 (0.0621)	0.0354 (0.0351)	0.138 (0.131)	93.75 (95.16)
[80/157]	0.0623 (0.0621)	0.0352 (0.0352)	0.251 (0.137)	93.75 (94.98)
[90/157]	0.0624 (0.0621)	0.0353 (0.0352)	0.432 (0.142)	84.38 (94.92)
[100/157]	0.0624 (0.0622)	0.0351 (0.0352)	0.156 (0.140)	87.50 (94.86)
[110/157]	0.0626 (0.0622)	0.0353 (0.0352)	0.155 (0.139)	90.62 (94.88)
[120/157]	0.0630 (0.0622)	0.0346 (0.0352)	0.082 (0.139)	96.88 (94.91)
[130/157]	0.0618 (0.0622)	0.0349 (0.0352)	0.091 (0.142)	96.88 (94.87)
[140/157]	0.0631 (0.0622)	0.0356 (0.0352)	0.342 (0.147)	87.50 (94.66)
[150/157]	0.0625 (0.0622)	0.0350 (0.0352)	0.362 (0.150)	87.50 (94.64)
[156/157]	0.0475 (0.0621)	0.0294 (0.0352)	0.154 (0.150)	100.00 (94.54)
 * Train Acc 94.540
 * Val Acc 83.400, Total time 0.60
 * Val loss 0.655, Total time 0.00
Epoch:30
LR: 0.0001
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0353 (0.0353)	0.0087 (0.0087)	0.077 (0.077)	96.88 (96.88)
[10/157]	0.0620 (0.0597)	0.0348 (0.0329)	0.186 (0.146)	93.75 (95.17)
[20/157]	0.0619 (0.0609)	0.0352 (0.0341)	0.109 (0.133)	93.75 (95.24)
[30/157]	0.0630 (0.0614)	0.0354 (0.0345)	0.127 (0.117)	93.75 (95.77)
[40/157]	0.0628 (0.0616)	0.0354 (0.0347)	0.077 (0.115)	96.88 (95.43)
[50/157]	0.0618 (0.0618)	0.0346 (0.0349)	0.098 (0.110)	96.88 (95.77)
[60/157]	0.0618 (0.0619)	0.0345 (0.0350)	0.150 (0.104)	96.88 (96.16)
[70/157]	0.0621 (0.0619)	0.0352 (0.0350)	0.088 (0.106)	96.88 (96.13)
[80/157]	0.0622 (0.0620)	0.0354 (0.0350)	0.034 (0.107)	100.00 (96.06)
[90/157]	0.0623 (0.0621)	0.0344 (0.0351)	0.022 (0.106)	100.00 (96.15)
[100/157]	0.0626 (0.0621)	0.0352 (0.0351)	0.013 (0.105)	100.00 (96.16)
[110/157]	0.0616 (0.0621)	0.0344 (0.0351)	0.165 (0.103)	87.50 (96.14)
[120/157]	0.0614 (0.0621)	0.0344 (0.0351)	0.108 (0.102)	96.88 (96.26)
[130/157]	0.0612 (0.0621)	0.0346 (0.0351)	0.056 (0.099)	100.00 (96.42)
[140/157]	0.0618 (0.0621)	0.0347 (0.0352)	0.071 (0.098)	96.88 (96.48)
[150/157]	0.0628 (0.0622)	0.0352 (0.0352)	0.028 (0.097)	100.00 (96.54)
[156/157]	0.0481 (0.0621)	0.0296 (0.0352)	0.025 (0.095)	100.00 (96.58)
 * Train Acc 96.580
 * Val Acc 86.600, Total time 0.60
 * Val loss 0.492, Total time 0.00
Epoch:31
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0360 (0.0360)	0.0091 (0.0091)	0.043 (0.043)	100.00 (100.00)
[10/157]	0.0616 (0.0598)	0.0349 (0.0330)	0.061 (0.067)	96.88 (96.88)
[20/157]	0.0621 (0.0611)	0.0352 (0.0342)	0.030 (0.066)	100.00 (97.77)
[30/157]	0.0624 (0.0615)	0.0344 (0.0346)	0.058 (0.075)	100.00 (97.18)
[40/157]	0.0617 (0.0618)	0.0346 (0.0348)	0.104 (0.071)	96.88 (97.56)
[50/157]	0.0618 (0.0619)	0.0347 (0.0349)	0.080 (0.072)	96.88 (97.67)
[60/157]	0.0621 (0.0620)	0.0342 (0.0350)	0.068 (0.069)	96.88 (97.69)
[70/157]	0.0623 (0.0620)	0.0356 (0.0350)	0.279 (0.074)	90.62 (97.54)
[80/157]	0.0635 (0.0621)	0.0355 (0.0351)	0.025 (0.073)	100.00 (97.57)
[90/157]	0.0608 (0.0621)	0.0333 (0.0350)	0.035 (0.072)	96.88 (97.56)
[100/157]	0.0621 (0.0621)	0.0352 (0.0351)	0.146 (0.072)	96.88 (97.62)
[110/157]	0.0630 (0.0621)	0.0356 (0.0351)	0.190 (0.070)	93.75 (97.69)
[120/157]	0.0625 (0.0622)	0.0348 (0.0351)	0.044 (0.070)	100.00 (97.70)
[130/157]	0.0619 (0.0622)	0.0345 (0.0351)	0.023 (0.069)	100.00 (97.69)
[140/157]	0.0633 (0.0622)	0.0357 (0.0352)	0.067 (0.068)	100.00 (97.72)
[150/157]	0.0615 (0.0622)	0.0336 (0.0352)	0.009 (0.067)	100.00 (97.74)
[156/157]	0.0482 (0.0621)	0.0289 (0.0351)	0.434 (0.067)	75.00 (97.72)
 * Train Acc 97.720
 * Val Acc 87.000, Total time 0.61
 * Val loss 0.488, Total time 0.00
Epoch:32
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0361 (0.0361)	0.0093 (0.0093)	0.044 (0.044)	100.00 (100.00)
[10/157]	0.0631 (0.0598)	0.0353 (0.0328)	0.012 (0.087)	100.00 (98.58)
[20/157]	0.0630 (0.0611)	0.0352 (0.0341)	0.023 (0.074)	100.00 (98.36)
[30/157]	0.0634 (0.0616)	0.0352 (0.0344)	0.041 (0.067)	96.88 (98.39)
[40/157]	0.0621 (0.0617)	0.0345 (0.0346)	0.030 (0.067)	100.00 (98.17)
[50/157]	0.0625 (0.0619)	0.0348 (0.0347)	0.052 (0.070)	100.00 (98.04)
[60/157]	0.0627 (0.0620)	0.0352 (0.0348)	0.029 (0.068)	100.00 (98.16)
[70/157]	0.0628 (0.0621)	0.0346 (0.0349)	0.086 (0.069)	100.00 (98.15)
[80/157]	0.0625 (0.0622)	0.0348 (0.0350)	0.031 (0.071)	100.00 (98.03)
[90/157]	0.0622 (0.0622)	0.0346 (0.0350)	0.016 (0.069)	100.00 (98.04)
[100/157]	0.0622 (0.0622)	0.0348 (0.0350)	0.156 (0.068)	96.88 (98.11)
[110/157]	0.0624 (0.0623)	0.0347 (0.0351)	0.019 (0.067)	100.00 (98.20)
[120/157]	0.0618 (0.0623)	0.0350 (0.0351)	0.026 (0.066)	100.00 (98.17)
[130/157]	0.0628 (0.0623)	0.0353 (0.0351)	0.110 (0.067)	93.75 (98.09)
[140/157]	0.0623 (0.0623)	0.0342 (0.0351)	0.047 (0.067)	100.00 (98.03)
[150/157]	0.0624 (0.0623)	0.0347 (0.0351)	0.092 (0.066)	96.88 (98.05)
[156/157]	0.0474 (0.0623)	0.0289 (0.0351)	0.924 (0.066)	75.00 (98.08)
 * Train Acc 98.080
 * Val Acc 87.500, Total time 0.60
 * Val loss 0.516, Total time 0.00
Epoch:33
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0360 (0.0360)	0.0088 (0.0088)	0.024 (0.024)	100.00 (100.00)
[10/157]	0.0623 (0.0598)	0.0347 (0.0329)	0.081 (0.081)	96.88 (97.73)
[20/157]	0.0615 (0.0612)	0.0343 (0.0341)	0.102 (0.088)	96.88 (97.47)
[30/157]	0.0628 (0.0616)	0.0353 (0.0345)	0.171 (0.085)	93.75 (97.28)
[40/157]	0.0618 (0.0618)	0.0349 (0.0347)	0.106 (0.089)	96.88 (97.10)
[50/157]	0.0620 (0.0618)	0.0344 (0.0348)	0.023 (0.085)	100.00 (97.00)
[60/157]	0.0618 (0.0619)	0.0341 (0.0348)	0.030 (0.078)	100.00 (97.34)
[70/157]	0.0632 (0.0621)	0.0352 (0.0349)	0.066 (0.079)	100.00 (97.36)
[80/157]	0.0632 (0.0621)	0.0350 (0.0350)	0.147 (0.077)	93.75 (97.45)
[90/157]	0.0621 (0.0622)	0.0347 (0.0350)	0.044 (0.076)	96.88 (97.32)
[100/157]	0.0614 (0.0622)	0.0342 (0.0350)	0.103 (0.075)	96.88 (97.46)
[110/157]	0.0622 (0.0622)	0.0350 (0.0350)	0.036 (0.073)	100.00 (97.52)
[120/157]	0.0626 (0.0622)	0.0347 (0.0351)	0.054 (0.070)	96.88 (97.62)
[130/157]	0.0627 (0.0623)	0.0347 (0.0351)	0.020 (0.072)	100.00 (97.57)
[140/157]	0.0618 (0.0623)	0.0349 (0.0351)	0.058 (0.072)	93.75 (97.54)
[150/157]	0.0624 (0.0623)	0.0349 (0.0351)	0.219 (0.072)	87.50 (97.50)
[156/157]	0.0484 (0.0622)	0.0296 (0.0351)	0.012 (0.072)	100.00 (97.50)
 * Train Acc 97.500
 * Val Acc 86.500, Total time 0.60
 * Val loss 0.540, Total time 0.00
Epoch:34
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0361 (0.0361)	0.0088 (0.0088)	0.008 (0.008)	100.00 (100.00)
[10/157]	0.0627 (0.0602)	0.0352 (0.0331)	0.006 (0.037)	100.00 (99.15)
[20/157]	0.0611 (0.0611)	0.0343 (0.0341)	0.032 (0.034)	100.00 (98.96)
[30/157]	0.0620 (0.0614)	0.0348 (0.0345)	0.014 (0.035)	100.00 (98.79)
[40/157]	0.0617 (0.0616)	0.0345 (0.0345)	0.150 (0.043)	96.88 (98.55)
[50/157]	0.0615 (0.0617)	0.0349 (0.0346)	0.037 (0.051)	100.00 (98.47)
[60/157]	0.0619 (0.0618)	0.0346 (0.0347)	0.078 (0.050)	96.88 (98.41)
[70/157]	0.0603 (0.0619)	0.0337 (0.0348)	0.026 (0.048)	100.00 (98.46)
[80/157]	0.0613 (0.0619)	0.0340 (0.0349)	0.043 (0.049)	100.00 (98.53)
[90/157]	0.0636 (0.0620)	0.0353 (0.0349)	0.012 (0.049)	100.00 (98.49)
[100/157]	0.0622 (0.0620)	0.0346 (0.0349)	0.061 (0.049)	96.88 (98.42)
[110/157]	0.0620 (0.0621)	0.0349 (0.0349)	0.055 (0.049)	96.88 (98.42)
[120/157]	0.0624 (0.0621)	0.0350 (0.0349)	0.187 (0.051)	93.75 (98.37)
[130/157]	0.0625 (0.0621)	0.0350 (0.0350)	0.097 (0.050)	96.88 (98.43)
[140/157]	0.0630 (0.0621)	0.0353 (0.0350)	0.067 (0.052)	100.00 (98.36)
[150/157]	0.0623 (0.0621)	0.0348 (0.0350)	0.052 (0.053)	96.88 (98.28)
[156/157]	0.0484 (0.0620)	0.0299 (0.0349)	0.044 (0.054)	100.00 (98.26)
 * Train Acc 98.260
 * Val Acc 86.600, Total time 0.60
 * Val loss 0.581, Total time 0.00
Epoch:35
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0360 (0.0360)	0.0088 (0.0088)	0.070 (0.070)	96.88 (96.88)
[10/157]	0.0619 (0.0596)	0.0352 (0.0323)	0.074 (0.064)	96.88 (98.30)
[20/157]	0.0623 (0.0609)	0.0351 (0.0337)	0.047 (0.057)	100.00 (98.21)
[30/157]	0.0621 (0.0613)	0.0352 (0.0343)	0.026 (0.054)	100.00 (98.39)
[40/157]	0.0621 (0.0617)	0.0352 (0.0346)	0.015 (0.056)	100.00 (98.25)
[50/157]	0.0622 (0.0618)	0.0347 (0.0347)	0.070 (0.054)	96.88 (98.35)
[60/157]	0.0638 (0.0619)	0.0356 (0.0348)	0.187 (0.054)	93.75 (98.36)
[70/157]	0.0630 (0.0621)	0.0349 (0.0349)	0.009 (0.051)	100.00 (98.46)
[80/157]	0.0628 (0.0621)	0.0351 (0.0350)	0.265 (0.052)	87.50 (98.34)
[90/157]	0.0622 (0.0621)	0.0348 (0.0350)	0.025 (0.051)	100.00 (98.35)
[100/157]	0.0621 (0.0622)	0.0344 (0.0350)	0.020 (0.048)	100.00 (98.42)
[110/157]	0.0628 (0.0622)	0.0355 (0.0350)	0.021 (0.046)	100.00 (98.51)
[120/157]	0.0621 (0.0622)	0.0344 (0.0350)	0.014 (0.047)	100.00 (98.42)
[130/157]	0.0630 (0.0622)	0.0353 (0.0351)	0.123 (0.047)	93.75 (98.38)
[140/157]	0.0624 (0.0623)	0.0347 (0.0351)	0.015 (0.046)	100.00 (98.43)
[150/157]	0.0628 (0.0623)	0.0348 (0.0351)	0.019 (0.045)	100.00 (98.47)
[156/157]	0.0473 (0.0622)	0.0286 (0.0351)	0.350 (0.046)	87.50 (98.46)
 * Train Acc 98.460
 * Val Acc 86.700, Total time 0.60
 * Val loss 0.560, Total time 0.00
Epoch:36
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0366 (0.0366)	0.0091 (0.0091)	0.008 (0.008)	100.00 (100.00)
[10/157]	0.0614 (0.0599)	0.0336 (0.0327)	0.056 (0.075)	100.00 (98.30)
[20/157]	0.0624 (0.0612)	0.0354 (0.0340)	0.065 (0.086)	96.88 (97.62)
[30/157]	0.0627 (0.0617)	0.0355 (0.0344)	0.125 (0.080)	96.88 (97.68)
[40/157]	0.0629 (0.0620)	0.0351 (0.0347)	0.013 (0.072)	100.00 (97.87)
[50/157]	0.0626 (0.0621)	0.0355 (0.0349)	0.105 (0.072)	96.88 (97.86)
[60/157]	0.0624 (0.0621)	0.0350 (0.0349)	0.047 (0.070)	96.88 (97.64)
[70/157]	0.0630 (0.0622)	0.0357 (0.0350)	0.014 (0.071)	100.00 (97.58)
[80/157]	0.0619 (0.0622)	0.0345 (0.0350)	0.032 (0.070)	100.00 (97.72)
[90/157]	0.0604 (0.0623)	0.0329 (0.0351)	0.024 (0.072)	100.00 (97.63)
[100/157]	0.0622 (0.0623)	0.0351 (0.0351)	0.053 (0.072)	96.88 (97.56)
[110/157]	0.0621 (0.0623)	0.0346 (0.0351)	0.067 (0.072)	96.88 (97.55)
[120/157]	0.0625 (0.0624)	0.0351 (0.0351)	0.046 (0.071)	96.88 (97.57)
[130/157]	0.0630 (0.0624)	0.0352 (0.0351)	0.104 (0.071)	96.88 (97.57)
[140/157]	0.0629 (0.0624)	0.0354 (0.0352)	0.053 (0.072)	96.88 (97.50)
[150/157]	0.0613 (0.0624)	0.0346 (0.0352)	0.091 (0.070)	96.88 (97.52)
[156/157]	0.0469 (0.0623)	0.0285 (0.0351)	0.085 (0.072)	100.00 (97.50)
 * Train Acc 97.500
 * Val Acc 87.000, Total time 0.61
 * Val loss 0.551, Total time 0.00
Epoch:37
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0371 (0.0371)	0.0092 (0.0092)	0.020 (0.020)	100.00 (100.00)
[10/157]	0.0618 (0.0598)	0.0339 (0.0327)	0.018 (0.037)	100.00 (99.15)
[20/157]	0.0621 (0.0611)	0.0349 (0.0339)	0.012 (0.035)	100.00 (99.26)
[30/157]	0.0617 (0.0615)	0.0343 (0.0344)	0.018 (0.038)	100.00 (99.09)
[40/157]	0.0621 (0.0617)	0.0347 (0.0346)	0.008 (0.036)	100.00 (99.09)
[50/157]	0.0629 (0.0619)	0.0356 (0.0347)	0.013 (0.035)	100.00 (99.08)
[60/157]	0.0629 (0.0621)	0.0355 (0.0348)	0.024 (0.036)	100.00 (98.87)
[70/157]	0.0624 (0.0622)	0.0347 (0.0350)	0.009 (0.038)	100.00 (98.77)
[80/157]	0.0619 (0.0622)	0.0343 (0.0350)	0.120 (0.038)	96.88 (98.84)
[90/157]	0.0620 (0.0622)	0.0350 (0.0350)	0.013 (0.038)	100.00 (98.80)
[100/157]	0.0619 (0.0622)	0.0349 (0.0350)	0.062 (0.038)	96.88 (98.76)
[110/157]	0.0615 (0.0622)	0.0344 (0.0351)	0.078 (0.038)	96.88 (98.76)
[120/157]	0.0633 (0.0622)	0.0354 (0.0351)	0.056 (0.038)	96.88 (98.68)
[130/157]	0.0625 (0.0622)	0.0344 (0.0351)	0.023 (0.038)	100.00 (98.71)
[140/157]	0.0616 (0.0622)	0.0336 (0.0351)	0.002 (0.037)	100.00 (98.71)
[150/157]	0.0625 (0.0623)	0.0350 (0.0351)	0.023 (0.037)	100.00 (98.72)
[156/157]	0.0480 (0.0622)	0.0294 (0.0351)	0.073 (0.037)	100.00 (98.74)
 * Train Acc 98.740
 * Val Acc 85.600, Total time 0.61
 * Val loss 0.564, Total time 0.00
Epoch:38
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0359 (0.0359)	0.0093 (0.0093)	0.099 (0.099)	96.88 (96.88)
[10/157]	0.0610 (0.0598)	0.0347 (0.0325)	0.028 (0.038)	100.00 (98.86)
[20/157]	0.0635 (0.0612)	0.0356 (0.0339)	0.067 (0.037)	96.88 (98.96)
[30/157]	0.0624 (0.0616)	0.0348 (0.0344)	0.125 (0.037)	93.75 (98.99)
[40/157]	0.0626 (0.0618)	0.0350 (0.0346)	0.179 (0.043)	93.75 (98.63)
[50/157]	0.0633 (0.0620)	0.0356 (0.0348)	0.005 (0.039)	100.00 (98.77)
[60/157]	0.0618 (0.0620)	0.0347 (0.0348)	0.034 (0.040)	100.00 (98.77)
[70/157]	0.0626 (0.0621)	0.0351 (0.0349)	0.042 (0.040)	100.00 (98.81)
[80/157]	0.0617 (0.0621)	0.0346 (0.0350)	0.008 (0.040)	100.00 (98.73)
[90/157]	0.0620 (0.0621)	0.0352 (0.0350)	0.024 (0.040)	100.00 (98.76)
[100/157]	0.0617 (0.0622)	0.0348 (0.0351)	0.109 (0.042)	96.88 (98.73)
[110/157]	0.0624 (0.0622)	0.0349 (0.0351)	0.004 (0.041)	100.00 (98.70)
[120/157]	0.0624 (0.0622)	0.0351 (0.0351)	0.033 (0.040)	100.00 (98.76)
[130/157]	0.0595 (0.0623)	0.0322 (0.0351)	0.035 (0.041)	100.00 (98.76)
[140/157]	0.0637 (0.0623)	0.0354 (0.0352)	0.038 (0.041)	100.00 (98.71)
[150/157]	0.0618 (0.0623)	0.0344 (0.0352)	0.041 (0.041)	100.00 (98.76)
[156/157]	0.0476 (0.0623)	0.0293 (0.0352)	0.088 (0.040)	100.00 (98.76)
 * Train Acc 98.760
 * Val Acc 87.900, Total time 0.60
 * Val loss 0.541, Total time 0.00
Epoch:39
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0367 (0.0367)	0.0088 (0.0088)	0.007 (0.007)	100.00 (100.00)
[10/157]	0.0630 (0.0600)	0.0356 (0.0330)	0.042 (0.018)	100.00 (99.72)
[20/157]	0.0623 (0.0611)	0.0354 (0.0341)	0.091 (0.030)	96.88 (99.11)
[30/157]	0.0633 (0.0616)	0.0355 (0.0346)	0.090 (0.035)	96.88 (98.79)
[40/157]	0.0618 (0.0619)	0.0351 (0.0348)	0.049 (0.038)	100.00 (98.78)
[50/157]	0.0629 (0.0620)	0.0352 (0.0349)	0.010 (0.041)	100.00 (98.65)
[60/157]	0.0626 (0.0622)	0.0351 (0.0351)	0.030 (0.045)	100.00 (98.51)
[70/157]	0.0615 (0.0622)	0.0338 (0.0351)	0.042 (0.048)	100.00 (98.42)
[80/157]	0.0619 (0.0622)	0.0345 (0.0351)	0.132 (0.048)	96.88 (98.46)
[90/157]	0.0609 (0.0622)	0.0342 (0.0351)	0.031 (0.053)	100.00 (98.28)
[100/157]	0.0626 (0.0623)	0.0347 (0.0351)	0.066 (0.052)	93.75 (98.27)
[110/157]	0.0617 (0.0623)	0.0346 (0.0351)	0.045 (0.056)	96.88 (98.20)
[120/157]	0.0638 (0.0624)	0.0348 (0.0351)	0.048 (0.056)	96.88 (98.14)
[130/157]	0.0641 (0.0625)	0.0360 (0.0351)	0.031 (0.056)	96.88 (98.12)
[140/157]	0.0662 (0.0626)	0.0361 (0.0352)	0.010 (0.054)	100.00 (98.20)
[150/157]	0.0640 (0.0626)	0.0351 (0.0352)	0.117 (0.054)	96.88 (98.18)
[156/157]	0.0480 (0.0625)	0.0288 (0.0351)	0.523 (0.057)	87.50 (98.10)
 * Train Acc 98.100
 * Val Acc 86.500, Total time 0.61
 * Val loss 0.580, Total time 0.00
Epoch:40
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0370 (0.0370)	0.0092 (0.0092)	0.036 (0.036)	100.00 (100.00)
[10/157]	0.0657 (0.0609)	0.0361 (0.0330)	0.102 (0.047)	96.88 (98.30)
[20/157]	0.0777 (0.0643)	0.0445 (0.0356)	0.128 (0.051)	90.62 (97.77)
[30/157]	0.0740 (0.0684)	0.0444 (0.0388)	0.031 (0.050)	100.00 (98.08)
[40/157]	0.0770 (0.0673)	0.0433 (0.0379)	0.075 (0.059)	93.75 (97.79)
[50/157]	0.0758 (0.0696)	0.0437 (0.0395)	0.175 (0.058)	93.75 (97.79)
[60/157]	0.0698 (0.0693)	0.0386 (0.0393)	0.068 (0.058)	93.75 (97.69)
[70/157]	0.0680 (0.0689)	0.0378 (0.0391)	0.099 (0.055)	96.88 (97.89)
[80/157]	0.0682 (0.0687)	0.0376 (0.0390)	0.055 (0.053)	96.88 (97.96)
[90/157]	0.0652 (0.0684)	0.0376 (0.0389)	0.020 (0.053)	100.00 (97.97)
[100/157]	0.0687 (0.0682)	0.0390 (0.0388)	0.104 (0.053)	96.88 (98.05)
[110/157]	0.0690 (0.0681)	0.0375 (0.0387)	0.020 (0.051)	100.00 (98.06)
[120/157]	0.0689 (0.0679)	0.0382 (0.0387)	0.031 (0.052)	100.00 (98.06)
[130/157]	0.0683 (0.0678)	0.0380 (0.0386)	0.006 (0.051)	100.00 (98.09)
[140/157]	0.0686 (0.0677)	0.0376 (0.0385)	0.013 (0.049)	100.00 (98.18)
[150/157]	0.0685 (0.0676)	0.0381 (0.0385)	0.039 (0.048)	100.00 (98.30)
[156/157]	0.0508 (0.0675)	0.0320 (0.0384)	0.005 (0.047)	100.00 (98.32)
 * Train Acc 98.320
 * Val Acc 87.100, Total time 0.63
 * Val loss 0.517, Total time 0.00
Epoch:41
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0392 (0.0392)	0.0091 (0.0091)	0.057 (0.057)	96.88 (96.88)
[10/157]	0.0699 (0.0646)	0.0360 (0.0354)	0.011 (0.038)	100.00 (99.43)
[20/157]	0.0677 (0.0652)	0.0384 (0.0365)	0.002 (0.030)	100.00 (99.40)
[30/157]	0.0676 (0.0654)	0.0382 (0.0369)	0.032 (0.031)	96.88 (99.29)
[40/157]	0.0692 (0.0656)	0.0389 (0.0372)	0.015 (0.029)	100.00 (99.24)
[50/157]	0.0681 (0.0657)	0.0383 (0.0374)	0.062 (0.038)	96.88 (98.77)
[60/157]	0.0676 (0.0658)	0.0374 (0.0374)	0.164 (0.040)	93.75 (98.72)
[70/157]	0.0674 (0.0659)	0.0381 (0.0376)	0.051 (0.047)	96.88 (98.42)
[80/157]	0.0695 (0.0660)	0.0392 (0.0376)	0.073 (0.050)	96.88 (98.34)
[90/157]	0.0594 (0.0656)	0.0323 (0.0373)	0.065 (0.049)	96.88 (98.35)
[100/157]	0.0624 (0.0651)	0.0354 (0.0370)	0.015 (0.051)	100.00 (98.27)
[110/157]	0.0623 (0.0649)	0.0349 (0.0369)	0.114 (0.051)	96.88 (98.23)
[120/157]	0.0624 (0.0647)	0.0346 (0.0367)	0.046 (0.052)	96.88 (98.17)
[130/157]	0.0614 (0.0645)	0.0339 (0.0366)	0.026 (0.052)	100.00 (98.19)
[140/157]	0.0615 (0.0644)	0.0346 (0.0365)	0.070 (0.052)	96.88 (98.20)
[150/157]	0.0622 (0.0642)	0.0345 (0.0364)	0.041 (0.052)	96.88 (98.20)
[156/157]	0.0476 (0.0641)	0.0288 (0.0363)	0.451 (0.052)	75.00 (98.16)
 * Train Acc 98.160
 * Val Acc 86.400, Total time 0.60
 * Val loss 0.597, Total time 0.00
Epoch:42
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0359 (0.0359)	0.0091 (0.0091)	0.037 (0.037)	100.00 (100.00)
[10/157]	0.0633 (0.0598)	0.0349 (0.0324)	0.182 (0.102)	96.88 (97.16)
[20/157]	0.0628 (0.0610)	0.0352 (0.0337)	0.004 (0.084)	100.00 (97.47)
[30/157]	0.0616 (0.0614)	0.0342 (0.0341)	0.010 (0.083)	100.00 (96.98)
[40/157]	0.0618 (0.0616)	0.0343 (0.0344)	0.160 (0.083)	96.88 (97.03)
[50/157]	0.0618 (0.0618)	0.0343 (0.0345)	0.020 (0.079)	100.00 (97.12)
[60/157]	0.0621 (0.0619)	0.0351 (0.0346)	0.139 (0.073)	93.75 (97.34)
[70/157]	0.0625 (0.0619)	0.0347 (0.0347)	0.007 (0.070)	100.00 (97.36)
[80/157]	0.0630 (0.0620)	0.0348 (0.0347)	0.005 (0.069)	100.00 (97.42)
[90/157]	0.0738 (0.0627)	0.0428 (0.0352)	0.136 (0.068)	93.75 (97.39)
[100/157]	0.0611 (0.0629)	0.0336 (0.0354)	0.010 (0.066)	100.00 (97.56)
[110/157]	0.0646 (0.0629)	0.0366 (0.0354)	0.070 (0.063)	96.88 (97.69)
[120/157]	0.0643 (0.0629)	0.0359 (0.0354)	0.042 (0.063)	96.88 (97.70)
[130/157]	0.0641 (0.0630)	0.0358 (0.0355)	0.032 (0.066)	100.00 (97.61)
[140/157]	0.0643 (0.0631)	0.0359 (0.0355)	0.042 (0.065)	100.00 (97.65)
[150/157]	0.0647 (0.0631)	0.0356 (0.0355)	0.011 (0.064)	100.00 (97.72)
[156/157]	0.0484 (0.0630)	0.0295 (0.0355)	0.307 (0.065)	87.50 (97.64)
 * Train Acc 97.640
 * Val Acc 87.800, Total time 0.61
 * Val loss 0.567, Total time 0.00
Epoch:43
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0367 (0.0367)	0.0092 (0.0092)	0.018 (0.018)	100.00 (100.00)
[10/157]	0.0654 (0.0614)	0.0363 (0.0338)	0.044 (0.048)	96.88 (98.30)
[20/157]	0.0647 (0.0626)	0.0361 (0.0350)	0.006 (0.070)	100.00 (97.47)
[30/157]	0.0644 (0.0630)	0.0355 (0.0353)	0.030 (0.078)	96.88 (97.28)
[40/157]	0.0635 (0.0632)	0.0353 (0.0355)	0.098 (0.079)	96.88 (97.18)
[50/157]	0.0643 (0.0633)	0.0361 (0.0356)	0.004 (0.072)	100.00 (97.37)
[60/157]	0.0645 (0.0633)	0.0361 (0.0357)	0.025 (0.072)	100.00 (97.34)
[70/157]	0.0674 (0.0634)	0.0363 (0.0357)	0.169 (0.074)	93.75 (97.32)
[80/157]	0.0639 (0.0633)	0.0364 (0.0357)	0.138 (0.070)	93.75 (97.45)
[90/157]	0.0641 (0.0633)	0.0356 (0.0358)	0.032 (0.070)	100.00 (97.49)
[100/157]	0.0639 (0.0633)	0.0358 (0.0358)	0.037 (0.066)	100.00 (97.65)
[110/157]	0.0648 (0.0634)	0.0362 (0.0358)	0.068 (0.064)	96.88 (97.75)
[120/157]	0.0651 (0.0634)	0.0366 (0.0359)	0.012 (0.063)	100.00 (97.73)
[130/157]	0.0647 (0.0634)	0.0364 (0.0359)	0.015 (0.061)	100.00 (97.83)
[140/157]	0.0656 (0.0635)	0.0365 (0.0359)	0.007 (0.060)	100.00 (97.85)
[150/157]	0.0633 (0.0635)	0.0356 (0.0359)	0.005 (0.061)	100.00 (97.89)
[156/157]	0.0486 (0.0634)	0.0296 (0.0359)	0.037 (0.060)	100.00 (97.92)
 * Train Acc 97.920
 * Val Acc 85.800, Total time 0.61
 * Val loss 0.620, Total time 0.00
Epoch:44
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0361 (0.0361)	0.0090 (0.0090)	0.047 (0.047)	100.00 (100.00)
[10/157]	0.0653 (0.0609)	0.0362 (0.0333)	0.004 (0.026)	100.00 (99.72)
[20/157]	0.0654 (0.0622)	0.0362 (0.0344)	0.053 (0.030)	96.88 (99.40)
[30/157]	0.0646 (0.0626)	0.0363 (0.0349)	0.044 (0.037)	96.88 (98.79)
[40/157]	0.0645 (0.0629)	0.0358 (0.0352)	0.014 (0.034)	100.00 (98.93)
[50/157]	0.0644 (0.0631)	0.0357 (0.0353)	0.044 (0.035)	96.88 (99.02)
[60/157]	0.0651 (0.0632)	0.0361 (0.0355)	0.047 (0.034)	96.88 (99.08)
[70/157]	0.0640 (0.0632)	0.0359 (0.0355)	0.022 (0.033)	100.00 (99.12)
[80/157]	0.0648 (0.0633)	0.0361 (0.0356)	0.051 (0.033)	100.00 (99.15)
[90/157]	0.0645 (0.0633)	0.0365 (0.0357)	0.016 (0.034)	100.00 (99.14)
[100/157]	0.0648 (0.0634)	0.0359 (0.0357)	0.062 (0.037)	96.88 (99.01)
[110/157]	0.0657 (0.0634)	0.0362 (0.0357)	0.012 (0.037)	100.00 (98.99)
[120/157]	0.0659 (0.0634)	0.0370 (0.0357)	0.062 (0.037)	96.88 (98.94)
[130/157]	0.0655 (0.0634)	0.0363 (0.0357)	0.049 (0.037)	100.00 (98.95)
[140/157]	0.0631 (0.0634)	0.0349 (0.0357)	0.025 (0.037)	100.00 (98.91)
[150/157]	0.0656 (0.0635)	0.0356 (0.0357)	0.006 (0.036)	100.00 (98.92)
[156/157]	0.0497 (0.0634)	0.0299 (0.0357)	0.219 (0.036)	87.50 (98.92)
 * Train Acc 98.920
 * Val Acc 85.200, Total time 0.61
 * Val loss 0.681, Total time 0.00
Epoch:45
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0373 (0.0373)	0.0091 (0.0091)	0.007 (0.007)	100.00 (100.00)
[10/157]	0.0656 (0.0613)	0.0368 (0.0337)	0.005 (0.053)	100.00 (98.58)
[20/157]	0.0651 (0.0624)	0.0363 (0.0348)	0.011 (0.059)	100.00 (98.21)
[30/157]	0.0631 (0.0628)	0.0347 (0.0352)	0.004 (0.055)	100.00 (98.29)
[40/157]	0.0629 (0.0629)	0.0356 (0.0353)	0.054 (0.049)	96.88 (98.48)
[50/157]	0.0644 (0.0631)	0.0356 (0.0354)	0.002 (0.048)	100.00 (98.47)
[60/157]	0.0645 (0.0632)	0.0360 (0.0356)	0.048 (0.048)	100.00 (98.46)
[70/157]	0.0631 (0.0632)	0.0350 (0.0356)	0.108 (0.047)	93.75 (98.46)
[80/157]	0.0643 (0.0633)	0.0351 (0.0356)	0.124 (0.050)	96.88 (98.38)
[90/157]	0.0639 (0.0634)	0.0353 (0.0356)	0.028 (0.048)	96.88 (98.42)
[100/157]	0.0635 (0.0633)	0.0352 (0.0357)	0.023 (0.053)	100.00 (98.30)
[110/157]	0.0638 (0.0634)	0.0354 (0.0357)	0.012 (0.051)	100.00 (98.40)
[120/157]	0.0638 (0.0635)	0.0344 (0.0357)	0.006 (0.053)	100.00 (98.35)
[130/157]	0.0651 (0.0635)	0.0367 (0.0357)	0.016 (0.052)	100.00 (98.33)
[140/157]	0.0637 (0.0634)	0.0355 (0.0357)	0.001 (0.050)	100.00 (98.40)
[150/157]	0.0656 (0.0634)	0.0354 (0.0357)	0.073 (0.049)	96.88 (98.43)
[156/157]	0.0483 (0.0634)	0.0286 (0.0357)	0.024 (0.049)	100.00 (98.40)
 * Train Acc 98.400
 * Val Acc 86.100, Total time 0.61
 * Val loss 0.601, Total time 0.00
Epoch:46
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0374 (0.0374)	0.0089 (0.0089)	0.017 (0.017)	100.00 (100.00)
[10/157]	0.0646 (0.0606)	0.0362 (0.0332)	0.072 (0.035)	96.88 (98.58)
[20/157]	0.0638 (0.0618)	0.0359 (0.0345)	0.017 (0.027)	100.00 (99.26)
[30/157]	0.0630 (0.0623)	0.0355 (0.0350)	0.033 (0.037)	100.00 (98.99)
[40/157]	0.0640 (0.0626)	0.0359 (0.0353)	0.034 (0.035)	96.88 (99.01)
[50/157]	0.0642 (0.0629)	0.0358 (0.0355)	0.003 (0.035)	100.00 (99.02)
[60/157]	0.0631 (0.0629)	0.0356 (0.0356)	0.009 (0.035)	100.00 (99.08)
[70/157]	0.0636 (0.0630)	0.0357 (0.0356)	0.032 (0.036)	100.00 (98.99)
[80/157]	0.0642 (0.0631)	0.0358 (0.0357)	0.151 (0.038)	93.75 (98.96)
[90/157]	0.0649 (0.0631)	0.0364 (0.0357)	0.136 (0.042)	93.75 (98.70)
[100/157]	0.0649 (0.0632)	0.0362 (0.0358)	0.127 (0.042)	96.88 (98.70)
[110/157]	0.0639 (0.0633)	0.0354 (0.0358)	0.016 (0.043)	100.00 (98.70)
[120/157]	0.0645 (0.0633)	0.0358 (0.0358)	0.016 (0.043)	100.00 (98.63)
[130/157]	0.0655 (0.0633)	0.0368 (0.0359)	0.005 (0.043)	100.00 (98.64)
[140/157]	0.0642 (0.0633)	0.0363 (0.0359)	0.065 (0.045)	96.88 (98.54)
[150/157]	0.0638 (0.0634)	0.0361 (0.0359)	0.195 (0.046)	93.75 (98.51)
[156/157]	0.0490 (0.0633)	0.0304 (0.0359)	1.324 (0.047)	87.50 (98.50)
 * Train Acc 98.500
 * Val Acc 85.800, Total time 0.61
 * Val loss 0.720, Total time 0.00
Epoch:47
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0366 (0.0366)	0.0089 (0.0089)	0.014 (0.014)	100.00 (100.00)
[10/157]	0.0661 (0.0613)	0.0371 (0.0338)	0.016 (0.074)	100.00 (97.73)
[20/157]	0.0656 (0.0626)	0.0369 (0.0351)	0.083 (0.065)	96.88 (97.92)
[30/157]	0.0648 (0.0630)	0.0363 (0.0353)	0.163 (0.058)	96.88 (98.19)
[40/157]	0.0657 (0.0633)	0.0369 (0.0355)	0.151 (0.062)	96.88 (97.87)
[50/157]	0.0633 (0.0633)	0.0352 (0.0355)	0.045 (0.058)	100.00 (98.04)
[60/157]	0.0647 (0.0634)	0.0362 (0.0356)	0.172 (0.061)	93.75 (97.85)
[70/157]	0.0643 (0.0634)	0.0358 (0.0357)	0.243 (0.063)	93.75 (97.71)
[80/157]	0.0652 (0.0634)	0.0359 (0.0357)	0.010 (0.063)	100.00 (97.76)
[90/157]	0.0643 (0.0635)	0.0351 (0.0358)	0.020 (0.064)	100.00 (97.70)
[100/157]	0.0633 (0.0636)	0.0343 (0.0359)	0.017 (0.066)	100.00 (97.65)
[110/157]	0.0795 (0.0645)	0.0461 (0.0365)	0.058 (0.065)	96.88 (97.69)
[120/157]	0.0678 (0.0647)	0.0376 (0.0367)	0.137 (0.063)	96.88 (97.78)
[130/157]	0.0671 (0.0648)	0.0378 (0.0368)	0.057 (0.062)	96.88 (97.78)
[140/157]	0.0660 (0.0649)	0.0363 (0.0368)	0.061 (0.063)	93.75 (97.74)
[150/157]	0.0678 (0.0650)	0.0380 (0.0369)	0.060 (0.063)	96.88 (97.72)
[156/157]	0.0504 (0.0649)	0.0309 (0.0369)	0.846 (0.063)	75.00 (97.72)
 * Train Acc 97.720
 * Val Acc 86.000, Total time 0.63
 * Val loss 0.682, Total time 0.00
Epoch:48
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0409 (0.0409)	0.0092 (0.0092)	0.047 (0.047)	96.88 (96.88)
[10/157]	0.0673 (0.0632)	0.0373 (0.0348)	0.242 (0.061)	93.75 (98.01)
[20/157]	0.0680 (0.0644)	0.0369 (0.0360)	0.046 (0.093)	96.88 (97.02)
[30/157]	0.0679 (0.0649)	0.0374 (0.0365)	0.016 (0.089)	100.00 (97.28)
[40/157]	0.0665 (0.0651)	0.0369 (0.0368)	0.047 (0.098)	100.00 (96.95)
[50/157]	0.0666 (0.0652)	0.0373 (0.0370)	0.019 (0.087)	100.00 (97.43)
[60/157]	0.0671 (0.0653)	0.0375 (0.0371)	0.291 (0.082)	90.62 (97.54)
[70/157]	0.0666 (0.0653)	0.0378 (0.0371)	0.007 (0.077)	100.00 (97.76)
[80/157]	0.0682 (0.0654)	0.0383 (0.0372)	0.002 (0.074)	100.00 (97.80)
[90/157]	0.0675 (0.0655)	0.0378 (0.0373)	0.015 (0.071)	100.00 (97.80)
[100/157]	0.0684 (0.0656)	0.0384 (0.0373)	0.012 (0.067)	100.00 (97.90)
[110/157]	0.0664 (0.0656)	0.0374 (0.0374)	0.023 (0.063)	100.00 (98.06)
[120/157]	0.0673 (0.0657)	0.0377 (0.0374)	0.009 (0.060)	100.00 (98.14)
[130/157]	0.0670 (0.0657)	0.0381 (0.0374)	0.048 (0.061)	96.88 (98.12)
[140/157]	0.0668 (0.0657)	0.0370 (0.0374)	0.038 (0.060)	100.00 (98.14)
[150/157]	0.0676 (0.0657)	0.0380 (0.0375)	0.030 (0.058)	100.00 (98.22)
[156/157]	0.0496 (0.0656)	0.0306 (0.0374)	0.014 (0.058)	100.00 (98.22)
 * Train Acc 98.220
 * Val Acc 86.400, Total time 0.63
 * Val loss 0.625, Total time 0.00
Epoch:49
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0383 (0.0383)	0.0091 (0.0091)	0.015 (0.015)	100.00 (100.00)
[10/157]	0.0661 (0.0633)	0.0369 (0.0350)	0.058 (0.036)	100.00 (98.86)
[20/157]	0.0664 (0.0645)	0.0368 (0.0362)	0.001 (0.034)	100.00 (98.96)
[30/157]	0.0676 (0.0649)	0.0374 (0.0367)	0.011 (0.031)	100.00 (98.99)
[40/157]	0.0666 (0.0651)	0.0369 (0.0369)	0.014 (0.033)	100.00 (98.86)
[50/157]	0.0672 (0.0652)	0.0373 (0.0371)	0.046 (0.034)	96.88 (98.77)
[60/157]	0.0675 (0.0653)	0.0379 (0.0371)	0.023 (0.036)	100.00 (98.72)
[70/157]	0.0594 (0.0649)	0.0327 (0.0369)	0.019 (0.037)	100.00 (98.77)
[80/157]	0.0626 (0.0646)	0.0349 (0.0367)	0.027 (0.035)	100.00 (98.88)
[90/157]	0.0644 (0.0645)	0.0355 (0.0366)	0.095 (0.036)	96.88 (98.87)
[100/157]	0.0650 (0.0644)	0.0360 (0.0365)	0.009 (0.035)	100.00 (98.86)
[110/157]	0.0640 (0.0643)	0.0359 (0.0365)	0.002 (0.033)	100.00 (98.93)
[120/157]	0.0645 (0.0643)	0.0365 (0.0364)	0.032 (0.033)	100.00 (98.94)
[130/157]	0.0644 (0.0642)	0.0354 (0.0364)	0.030 (0.035)	100.00 (98.83)
[140/157]	0.0623 (0.0641)	0.0340 (0.0364)	0.066 (0.036)	96.88 (98.78)
[150/157]	0.0628 (0.0641)	0.0355 (0.0363)	0.075 (0.037)	96.88 (98.74)
[156/157]	0.0495 (0.0640)	0.0302 (0.0362)	0.010 (0.037)	100.00 (98.76)
 * Train Acc 98.760
 * Val Acc 85.800, Total time 0.61
 * Val loss 0.668, Total time 0.00
Epoch:50
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0368 (0.0368)	0.0090 (0.0090)	0.002 (0.002)	100.00 (100.00)
[10/157]	0.0649 (0.0608)	0.0364 (0.0334)	0.038 (0.049)	96.88 (98.30)
[20/157]	0.0638 (0.0621)	0.0359 (0.0346)	0.191 (0.050)	93.75 (98.36)
[30/157]	0.0637 (0.0626)	0.0355 (0.0351)	0.020 (0.049)	100.00 (98.59)
[40/157]	0.0650 (0.0629)	0.0359 (0.0353)	0.008 (0.043)	100.00 (98.86)
[50/157]	0.0643 (0.0630)	0.0358 (0.0354)	0.108 (0.045)	93.75 (98.65)
[60/157]	0.0631 (0.0631)	0.0356 (0.0355)	0.005 (0.043)	100.00 (98.67)
[70/157]	0.0634 (0.0631)	0.0357 (0.0355)	0.025 (0.041)	100.00 (98.77)
[80/157]	0.0651 (0.0632)	0.0362 (0.0356)	0.032 (0.040)	100.00 (98.80)
[90/157]	0.0654 (0.0632)	0.0365 (0.0356)	0.027 (0.039)	100.00 (98.80)
[100/157]	0.0638 (0.0632)	0.0352 (0.0356)	0.041 (0.038)	100.00 (98.82)
[110/157]	0.0645 (0.0633)	0.0358 (0.0357)	0.053 (0.038)	96.88 (98.85)
[120/157]	0.0638 (0.0633)	0.0358 (0.0357)	0.056 (0.040)	96.88 (98.76)
[130/157]	0.0647 (0.0633)	0.0358 (0.0357)	0.012 (0.040)	100.00 (98.74)
[140/157]	0.0642 (0.0633)	0.0358 (0.0357)	0.034 (0.040)	96.88 (98.71)
[150/157]	0.0642 (0.0633)	0.0361 (0.0357)	0.006 (0.040)	100.00 (98.74)
[156/157]	0.0483 (0.0633)	0.0295 (0.0357)	0.002 (0.040)	100.00 (98.72)
 * Train Acc 98.720
 * Val Acc 87.200, Total time 0.62
 * Val loss 0.605, Total time 0.00
Epoch:51
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0356 (0.0356)	0.0089 (0.0089)	0.010 (0.010)	100.00 (100.00)
[10/157]	0.0655 (0.0614)	0.0357 (0.0337)	0.003 (0.025)	100.00 (98.86)
[20/157]	0.0648 (0.0625)	0.0359 (0.0348)	0.006 (0.029)	100.00 (98.51)
[30/157]	0.0650 (0.0628)	0.0364 (0.0351)	0.006 (0.026)	100.00 (98.69)
[40/157]	0.0643 (0.0630)	0.0361 (0.0353)	0.012 (0.025)	100.00 (98.86)
[50/157]	0.0640 (0.0630)	0.0355 (0.0354)	0.002 (0.027)	100.00 (98.77)
[60/157]	0.0644 (0.0631)	0.0355 (0.0355)	0.004 (0.024)	100.00 (98.98)
[70/157]	0.0645 (0.0631)	0.0366 (0.0355)	0.009 (0.023)	100.00 (99.03)
[80/157]	0.0639 (0.0632)	0.0357 (0.0355)	0.091 (0.024)	96.88 (99.07)
[90/157]	0.0632 (0.0632)	0.0351 (0.0356)	0.022 (0.023)	100.00 (99.00)
[100/157]	0.0667 (0.0633)	0.0357 (0.0356)	0.013 (0.022)	100.00 (99.07)
[110/157]	0.0635 (0.0633)	0.0348 (0.0356)	0.006 (0.021)	100.00 (99.16)
[120/157]	0.0666 (0.0633)	0.0359 (0.0356)	0.007 (0.021)	100.00 (99.20)
[130/157]	0.0640 (0.0633)	0.0361 (0.0356)	0.000 (0.022)	100.00 (99.14)
[140/157]	0.0635 (0.0633)	0.0349 (0.0356)	0.019 (0.021)	100.00 (99.20)
[150/157]	0.0646 (0.0633)	0.0361 (0.0356)	0.009 (0.023)	100.00 (99.13)
[156/157]	0.0487 (0.0633)	0.0297 (0.0356)	0.305 (0.023)	75.00 (99.08)
 * Train Acc 99.080
 * Val Acc 86.400, Total time 0.61
 * Val loss 0.612, Total time 0.00
Epoch:52
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0357 (0.0357)	0.0090 (0.0090)	0.052 (0.052)	96.88 (96.88)
[10/157]	0.0634 (0.0609)	0.0347 (0.0334)	0.178 (0.060)	93.75 (98.30)
[20/157]	0.0659 (0.0621)	0.0367 (0.0347)	0.014 (0.078)	100.00 (97.47)
[30/157]	0.0667 (0.0627)	0.0361 (0.0352)	0.014 (0.063)	100.00 (97.88)
[40/157]	0.0637 (0.0629)	0.0357 (0.0354)	0.007 (0.061)	100.00 (98.02)
[50/157]	0.0659 (0.0631)	0.0355 (0.0354)	0.095 (0.057)	96.88 (98.16)
[60/157]	0.0638 (0.0632)	0.0346 (0.0355)	0.040 (0.054)	100.00 (98.26)
[70/157]	0.0634 (0.0633)	0.0365 (0.0356)	0.007 (0.050)	100.00 (98.42)
[80/157]	0.0657 (0.0633)	0.0365 (0.0356)	0.015 (0.048)	100.00 (98.50)
[90/157]	0.0646 (0.0633)	0.0364 (0.0357)	0.019 (0.048)	100.00 (98.49)
[100/157]	0.0634 (0.0633)	0.0355 (0.0357)	0.005 (0.050)	100.00 (98.39)
[110/157]	0.0643 (0.0633)	0.0360 (0.0357)	0.011 (0.048)	100.00 (98.42)
[120/157]	0.0640 (0.0633)	0.0352 (0.0357)	0.040 (0.048)	96.88 (98.40)
[130/157]	0.0647 (0.0633)	0.0362 (0.0358)	0.014 (0.048)	100.00 (98.40)
[140/157]	0.0649 (0.0633)	0.0363 (0.0358)	0.057 (0.049)	96.88 (98.34)
[150/157]	0.0647 (0.0633)	0.0356 (0.0358)	0.040 (0.050)	96.88 (98.32)
[156/157]	0.0487 (0.0633)	0.0299 (0.0357)	0.062 (0.050)	100.00 (98.36)
 * Train Acc 98.360
 * Val Acc 84.900, Total time 0.61
 * Val loss 0.727, Total time 0.00
Epoch:53
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0365 (0.0365)	0.0089 (0.0089)	0.004 (0.004)	100.00 (100.00)
[10/157]	0.0782 (0.0697)	0.0447 (0.0393)	0.095 (0.040)	96.88 (98.58)
[20/157]	0.0647 (0.0694)	0.0359 (0.0394)	0.021 (0.059)	100.00 (98.21)
[30/157]	0.0631 (0.0673)	0.0355 (0.0382)	0.018 (0.064)	100.00 (98.19)
[40/157]	0.0634 (0.0663)	0.0358 (0.0375)	0.057 (0.063)	96.88 (98.09)
[50/157]	0.0729 (0.0658)	0.0392 (0.0372)	0.008 (0.064)	100.00 (98.10)
[60/157]	0.0802 (0.0679)	0.0464 (0.0387)	0.009 (0.066)	100.00 (98.00)
[70/157]	0.0641 (0.0683)	0.0351 (0.0390)	0.032 (0.062)	100.00 (98.20)
[80/157]	0.0615 (0.0675)	0.0347 (0.0385)	0.058 (0.061)	96.88 (98.15)
[90/157]	0.0619 (0.0669)	0.0345 (0.0381)	0.044 (0.058)	96.88 (98.21)
[100/157]	0.0625 (0.0665)	0.0349 (0.0378)	0.044 (0.056)	96.88 (98.24)
[110/157]	0.0616 (0.0661)	0.0340 (0.0376)	0.037 (0.055)	96.88 (98.25)
[120/157]	0.0715 (0.0666)	0.0411 (0.0378)	0.030 (0.056)	100.00 (98.11)
[130/157]	0.0659 (0.0664)	0.0368 (0.0377)	0.003 (0.055)	100.00 (98.09)
[140/157]	0.0654 (0.0663)	0.0368 (0.0377)	0.198 (0.057)	90.62 (97.98)
[150/157]	0.0660 (0.0663)	0.0367 (0.0377)	0.004 (0.056)	100.00 (97.99)
[156/157]	0.0508 (0.0662)	0.0318 (0.0376)	0.066 (0.056)	100.00 (98.02)
 * Train Acc 98.020
 * Val Acc 85.400, Total time 0.63
 * Val loss 0.709, Total time 0.00
Epoch:54
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0385 (0.0385)	0.0098 (0.0098)	0.009 (0.009)	100.00 (100.00)
[10/157]	0.0665 (0.0630)	0.0370 (0.0346)	0.007 (0.054)	100.00 (98.86)
[20/157]	0.0670 (0.0643)	0.0374 (0.0360)	0.014 (0.046)	100.00 (98.81)
[30/157]	0.0659 (0.0647)	0.0369 (0.0364)	0.004 (0.037)	100.00 (99.09)
[40/157]	0.0658 (0.0648)	0.0365 (0.0367)	0.023 (0.035)	100.00 (99.09)
[50/157]	0.0660 (0.0649)	0.0367 (0.0369)	0.012 (0.033)	100.00 (99.14)
[60/157]	0.0672 (0.0650)	0.0373 (0.0370)	0.016 (0.032)	100.00 (99.08)
[70/157]	0.0655 (0.0650)	0.0369 (0.0370)	0.121 (0.033)	96.88 (99.03)
[80/157]	0.0650 (0.0651)	0.0363 (0.0371)	0.060 (0.036)	96.88 (98.92)
[90/157]	0.0669 (0.0651)	0.0373 (0.0372)	0.050 (0.037)	96.88 (98.83)
[100/157]	0.0661 (0.0651)	0.0374 (0.0372)	0.021 (0.039)	100.00 (98.79)
[110/157]	0.0654 (0.0652)	0.0368 (0.0372)	0.076 (0.039)	96.88 (98.73)
[120/157]	0.0678 (0.0652)	0.0374 (0.0372)	0.039 (0.037)	96.88 (98.79)
[130/157]	0.0677 (0.0652)	0.0379 (0.0372)	0.032 (0.037)	100.00 (98.76)
[140/157]	0.0656 (0.0652)	0.0368 (0.0372)	0.075 (0.038)	96.88 (98.74)
[150/157]	0.0683 (0.0652)	0.0382 (0.0373)	0.003 (0.037)	100.00 (98.70)
[156/157]	0.0515 (0.0651)	0.0324 (0.0372)	0.068 (0.037)	100.00 (98.72)
 * Train Acc 98.720
 * Val Acc 86.700, Total time 0.63
 * Val loss 0.684, Total time 0.00
Epoch:55
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0384 (0.0384)	0.0088 (0.0088)	0.173 (0.173)	96.88 (96.88)
[10/157]	0.0672 (0.0631)	0.0380 (0.0349)	0.004 (0.048)	100.00 (98.30)
[20/157]	0.0663 (0.0643)	0.0376 (0.0362)	0.016 (0.041)	100.00 (98.51)
[30/157]	0.0647 (0.0647)	0.0353 (0.0366)	0.006 (0.034)	100.00 (98.89)
[40/157]	0.0685 (0.0649)	0.0375 (0.0368)	0.005 (0.032)	100.00 (98.93)
[50/157]	0.0706 (0.0650)	0.0387 (0.0370)	0.012 (0.030)	100.00 (99.02)
[60/157]	0.0658 (0.0651)	0.0374 (0.0371)	0.012 (0.027)	100.00 (99.18)
[70/157]	0.0659 (0.0652)	0.0372 (0.0371)	0.010 (0.025)	100.00 (99.16)
[80/157]	0.0671 (0.0652)	0.0376 (0.0372)	0.007 (0.024)	100.00 (99.23)
[90/157]	0.0674 (0.0652)	0.0378 (0.0372)	0.007 (0.022)	100.00 (99.31)
[100/157]	0.0666 (0.0652)	0.0376 (0.0372)	0.002 (0.023)	100.00 (99.26)
[110/157]	0.0664 (0.0653)	0.0368 (0.0373)	0.013 (0.023)	100.00 (99.24)
[120/157]	0.0672 (0.0653)	0.0378 (0.0373)	0.004 (0.022)	100.00 (99.28)
[130/157]	0.0663 (0.0653)	0.0376 (0.0373)	0.014 (0.023)	100.00 (99.28)
[140/157]	0.0652 (0.0653)	0.0368 (0.0373)	0.024 (0.023)	96.88 (99.27)
[150/157]	0.0661 (0.0652)	0.0370 (0.0373)	0.008 (0.024)	100.00 (99.21)
[156/157]	0.0501 (0.0652)	0.0312 (0.0373)	0.386 (0.024)	75.00 (99.16)
 * Train Acc 99.160
 * Val Acc 87.000, Total time 0.63
 * Val loss 0.686, Total time 0.00
Epoch:56
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0361 (0.0361)	0.0087 (0.0087)	0.002 (0.002)	100.00 (100.00)
[10/157]	0.0668 (0.0625)	0.0370 (0.0347)	0.177 (0.063)	93.75 (98.01)
[20/157]	0.0669 (0.0640)	0.0369 (0.0361)	0.005 (0.060)	100.00 (97.77)
[30/157]	0.0665 (0.0644)	0.0373 (0.0365)	0.021 (0.061)	100.00 (97.78)
[40/157]	0.0666 (0.0647)	0.0372 (0.0367)	0.053 (0.063)	96.88 (97.64)
[50/157]	0.0671 (0.0650)	0.0368 (0.0369)	0.012 (0.068)	100.00 (97.86)
[60/157]	0.0667 (0.0651)	0.0370 (0.0369)	0.046 (0.068)	96.88 (97.85)
[70/157]	0.0675 (0.0652)	0.0376 (0.0370)	0.138 (0.071)	96.88 (97.76)
[80/157]	0.0675 (0.0653)	0.0374 (0.0371)	0.051 (0.066)	96.88 (97.88)
[90/157]	0.0664 (0.0653)	0.0369 (0.0372)	0.006 (0.065)	100.00 (97.94)
[100/157]	0.0667 (0.0653)	0.0371 (0.0372)	0.131 (0.063)	96.88 (97.99)
[110/157]	0.0662 (0.0653)	0.0366 (0.0372)	0.055 (0.065)	96.88 (97.83)
[120/157]	0.0665 (0.0654)	0.0370 (0.0372)	0.123 (0.065)	96.88 (97.75)
[130/157]	0.0678 (0.0654)	0.0376 (0.0372)	0.015 (0.066)	100.00 (97.83)
[140/157]	0.0662 (0.0654)	0.0368 (0.0372)	0.002 (0.066)	100.00 (97.81)
[150/157]	0.0670 (0.0654)	0.0374 (0.0373)	0.056 (0.068)	96.88 (97.79)
[156/157]	0.0510 (0.0653)	0.0315 (0.0372)	0.458 (0.069)	75.00 (97.68)
 * Train Acc 97.680
 * Val Acc 85.300, Total time 0.63
 * Val loss 0.780, Total time 0.00
Epoch:57
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0347 (0.0347)	0.0079 (0.0079)	0.016 (0.016)	100.00 (100.00)
[10/157]	0.0669 (0.0624)	0.0372 (0.0346)	0.303 (0.060)	87.50 (98.01)
[20/157]	0.0670 (0.0639)	0.0375 (0.0360)	0.006 (0.073)	100.00 (97.32)
[30/157]	0.0713 (0.0646)	0.0411 (0.0366)	0.017 (0.065)	100.00 (97.58)
[40/157]	0.0661 (0.0648)	0.0368 (0.0368)	0.014 (0.057)	100.00 (98.02)
[50/157]	0.0661 (0.0649)	0.0367 (0.0368)	0.210 (0.058)	96.88 (98.04)
[60/157]	0.0665 (0.0650)	0.0372 (0.0369)	0.020 (0.054)	100.00 (98.16)
[70/157]	0.0657 (0.0650)	0.0369 (0.0370)	0.114 (0.059)	96.88 (98.06)
[80/157]	0.0661 (0.0651)	0.0372 (0.0371)	0.003 (0.056)	100.00 (98.11)
[90/157]	0.0747 (0.0653)	0.0444 (0.0372)	0.045 (0.052)	100.00 (98.25)
[100/157]	0.0653 (0.0653)	0.0371 (0.0372)	0.037 (0.053)	100.00 (98.24)
[110/157]	0.0661 (0.0653)	0.0373 (0.0373)	0.029 (0.054)	100.00 (98.25)
[120/157]	0.0664 (0.0653)	0.0363 (0.0372)	0.015 (0.052)	100.00 (98.27)
[130/157]	0.0678 (0.0653)	0.0380 (0.0372)	0.023 (0.053)	100.00 (98.23)
[140/157]	0.0660 (0.0653)	0.0371 (0.0372)	0.017 (0.052)	100.00 (98.27)
[150/157]	0.0688 (0.0653)	0.0380 (0.0372)	0.006 (0.053)	100.00 (98.24)
[156/157]	0.0505 (0.0652)	0.0318 (0.0372)	0.005 (0.052)	100.00 (98.26)
 * Train Acc 98.260
 * Val Acc 85.700, Total time 0.62
 * Val loss 0.696, Total time 0.00
Epoch:58
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0393 (0.0393)	0.0089 (0.0089)	0.034 (0.034)	96.88 (96.88)
[10/157]	0.0651 (0.0627)	0.0360 (0.0345)	0.058 (0.035)	93.75 (98.30)
[20/157]	0.0692 (0.0643)	0.0374 (0.0360)	0.007 (0.051)	100.00 (98.07)
[30/157]	0.0677 (0.0647)	0.0374 (0.0364)	0.087 (0.041)	100.00 (98.59)
[40/157]	0.0655 (0.0650)	0.0364 (0.0366)	0.014 (0.039)	100.00 (98.63)
[50/157]	0.0680 (0.0651)	0.0373 (0.0367)	0.016 (0.041)	100.00 (98.59)
[60/157]	0.0655 (0.0651)	0.0365 (0.0367)	0.060 (0.042)	93.75 (98.46)
[70/157]	0.0682 (0.0651)	0.0382 (0.0368)	0.004 (0.040)	100.00 (98.64)
[80/157]	0.0670 (0.0652)	0.0374 (0.0369)	0.013 (0.040)	100.00 (98.65)
[90/157]	0.0669 (0.0652)	0.0374 (0.0370)	0.002 (0.039)	100.00 (98.63)
[100/157]	0.0665 (0.0652)	0.0374 (0.0370)	0.029 (0.037)	100.00 (98.70)
[110/157]	0.0679 (0.0653)	0.0389 (0.0371)	0.048 (0.037)	96.88 (98.70)
[120/157]	0.0677 (0.0653)	0.0372 (0.0371)	0.040 (0.036)	100.00 (98.76)
[130/157]	0.0669 (0.0653)	0.0370 (0.0371)	0.027 (0.036)	100.00 (98.71)
[140/157]	0.0663 (0.0653)	0.0370 (0.0371)	0.005 (0.035)	100.00 (98.74)
[150/157]	0.0649 (0.0653)	0.0355 (0.0371)	0.030 (0.034)	96.88 (98.78)
[156/157]	0.0500 (0.0652)	0.0312 (0.0371)	0.002 (0.034)	100.00 (98.76)
 * Train Acc 98.760
 * Val Acc 86.400, Total time 0.63
 * Val loss 0.690, Total time 0.00
Epoch:59
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0384 (0.0384)	0.0088 (0.0088)	0.001 (0.001)	100.00 (100.00)
[10/157]	0.0640 (0.0624)	0.0345 (0.0342)	0.050 (0.020)	96.88 (99.72)
[20/157]	0.0669 (0.0639)	0.0378 (0.0358)	0.005 (0.018)	100.00 (99.55)
[30/157]	0.0664 (0.0644)	0.0372 (0.0363)	0.025 (0.022)	100.00 (99.60)
[40/157]	0.0676 (0.0648)	0.0378 (0.0367)	0.009 (0.022)	100.00 (99.54)
[50/157]	0.0685 (0.0650)	0.0386 (0.0369)	0.010 (0.027)	100.00 (99.39)
[60/157]	0.0684 (0.0651)	0.0389 (0.0370)	0.003 (0.024)	100.00 (99.44)
[70/157]	0.0666 (0.0652)	0.0371 (0.0371)	0.002 (0.025)	100.00 (99.43)
[80/157]	0.0662 (0.0652)	0.0374 (0.0371)	0.005 (0.027)	100.00 (99.42)
[90/157]	0.0663 (0.0653)	0.0368 (0.0372)	0.002 (0.025)	100.00 (99.48)
[100/157]	0.0660 (0.0653)	0.0361 (0.0372)	0.023 (0.024)	100.00 (99.47)
[110/157]	0.0663 (0.0654)	0.0367 (0.0372)	0.014 (0.024)	100.00 (99.47)
[120/157]	0.0682 (0.0654)	0.0381 (0.0372)	0.022 (0.024)	100.00 (99.48)
[130/157]	0.0674 (0.0654)	0.0377 (0.0372)	0.002 (0.025)	100.00 (99.40)
[140/157]	0.0666 (0.0654)	0.0370 (0.0372)	0.057 (0.025)	96.88 (99.40)
[150/157]	0.0680 (0.0654)	0.0370 (0.0372)	0.025 (0.025)	100.00 (99.38)
[156/157]	0.0510 (0.0653)	0.0319 (0.0372)	0.030 (0.025)	100.00 (99.38)
 * Train Acc 99.380
 * Val Acc 87.800, Total time 0.62
 * Val loss 0.630, Total time 0.00
Epoch:60
LR: 5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0354 (0.0354)	0.0084 (0.0084)	0.008 (0.008)	100.00 (100.00)
[10/157]	0.0658 (0.0623)	0.0368 (0.0348)	0.017 (0.034)	100.00 (98.86)
[20/157]	0.0657 (0.0636)	0.0376 (0.0360)	0.002 (0.035)	100.00 (98.81)
[30/157]	0.0672 (0.0642)	0.0374 (0.0365)	0.056 (0.034)	96.88 (98.79)
[40/157]	0.0659 (0.0644)	0.0372 (0.0367)	0.160 (0.033)	96.88 (98.86)
[50/157]	0.0669 (0.0646)	0.0359 (0.0368)	0.011 (0.028)	100.00 (99.08)
[60/157]	0.0692 (0.0647)	0.0377 (0.0369)	0.048 (0.026)	96.88 (99.13)
[70/157]	0.0738 (0.0649)	0.0442 (0.0370)	0.012 (0.024)	100.00 (99.21)
[80/157]	0.0659 (0.0649)	0.0371 (0.0371)	0.002 (0.023)	100.00 (99.27)
[90/157]	0.0684 (0.0650)	0.0375 (0.0371)	0.001 (0.022)	100.00 (99.31)
[100/157]	0.0662 (0.0651)	0.0368 (0.0372)	0.052 (0.020)	96.88 (99.35)
[110/157]	0.0666 (0.0651)	0.0377 (0.0372)	0.043 (0.019)	100.00 (99.41)
[120/157]	0.0658 (0.0651)	0.0372 (0.0372)	0.004 (0.018)	100.00 (99.46)
[130/157]	0.0667 (0.0651)	0.0366 (0.0372)	0.042 (0.018)	96.88 (99.48)
[140/157]	0.0664 (0.0651)	0.0370 (0.0372)	0.005 (0.017)	100.00 (99.49)
[150/157]	0.0664 (0.0651)	0.0364 (0.0372)	0.001 (0.016)	100.00 (99.50)
[156/157]	0.0505 (0.0650)	0.0316 (0.0372)	0.001 (0.016)	100.00 (99.50)
 * Train Acc 99.500
 * Val Acc 86.500, Total time 0.63
 * Val loss 0.634, Total time 0.00
Epoch:61
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0378 (0.0378)	0.0089 (0.0089)	0.045 (0.045)	96.88 (96.88)
[10/157]	0.0675 (0.0629)	0.0379 (0.0348)	0.030 (0.016)	100.00 (99.15)
[20/157]	0.0651 (0.0638)	0.0365 (0.0359)	0.002 (0.024)	100.00 (98.96)
[30/157]	0.0657 (0.0644)	0.0367 (0.0362)	0.023 (0.021)	100.00 (99.19)
[40/157]	0.0679 (0.0646)	0.0378 (0.0365)	0.009 (0.018)	100.00 (99.39)
[50/157]	0.0672 (0.0648)	0.0374 (0.0367)	0.019 (0.018)	100.00 (99.45)
[60/157]	0.0658 (0.0648)	0.0362 (0.0367)	0.003 (0.019)	100.00 (99.44)
[70/157]	0.0650 (0.0649)	0.0362 (0.0368)	0.002 (0.017)	100.00 (99.52)
[80/157]	0.0691 (0.0650)	0.0386 (0.0369)	0.003 (0.017)	100.00 (99.50)
[90/157]	0.0666 (0.0650)	0.0376 (0.0370)	0.020 (0.017)	100.00 (99.52)
[100/157]	0.0656 (0.0651)	0.0362 (0.0370)	0.003 (0.016)	100.00 (99.54)
[110/157]	0.0659 (0.0651)	0.0367 (0.0371)	0.003 (0.016)	100.00 (99.58)
[120/157]	0.0682 (0.0652)	0.0375 (0.0371)	0.018 (0.015)	100.00 (99.61)
[130/157]	0.0662 (0.0652)	0.0371 (0.0371)	0.003 (0.015)	100.00 (99.62)
[140/157]	0.0690 (0.0652)	0.0372 (0.0371)	0.020 (0.015)	100.00 (99.58)
[150/157]	0.0662 (0.0653)	0.0370 (0.0371)	0.023 (0.015)	100.00 (99.57)
[156/157]	0.0501 (0.0652)	0.0309 (0.0371)	0.061 (0.015)	100.00 (99.56)
 * Train Acc 99.560
 * Val Acc 87.400, Total time 0.62
 * Val loss 0.600, Total time 0.00
Epoch:62
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0373 (0.0373)	0.0093 (0.0093)	0.001 (0.001)	100.00 (100.00)
[10/157]	0.0665 (0.0629)	0.0367 (0.0347)	0.015 (0.010)	100.00 (99.72)
[20/157]	0.0657 (0.0640)	0.0371 (0.0359)	0.001 (0.008)	100.00 (99.85)
[30/157]	0.0667 (0.0645)	0.0374 (0.0364)	0.014 (0.008)	100.00 (99.90)
[40/157]	0.0697 (0.0647)	0.0376 (0.0367)	0.019 (0.009)	100.00 (99.77)
[50/157]	0.0677 (0.0648)	0.0368 (0.0367)	0.024 (0.010)	100.00 (99.75)
[60/157]	0.0671 (0.0648)	0.0377 (0.0367)	0.020 (0.011)	100.00 (99.69)
[70/157]	0.0665 (0.0648)	0.0374 (0.0368)	0.003 (0.011)	100.00 (99.69)
[80/157]	0.0673 (0.0649)	0.0375 (0.0369)	0.001 (0.013)	100.00 (99.61)
[90/157]	0.0656 (0.0649)	0.0370 (0.0369)	0.000 (0.012)	100.00 (99.66)
[100/157]	0.0671 (0.0650)	0.0363 (0.0369)	0.002 (0.012)	100.00 (99.60)
[110/157]	0.0666 (0.0650)	0.0370 (0.0370)	0.006 (0.013)	100.00 (99.58)
[120/157]	0.0675 (0.0651)	0.0373 (0.0370)	0.002 (0.014)	100.00 (99.56)
[130/157]	0.0676 (0.0651)	0.0383 (0.0370)	0.001 (0.015)	100.00 (99.57)
[140/157]	0.0646 (0.0651)	0.0361 (0.0371)	0.028 (0.014)	100.00 (99.60)
[150/157]	0.0658 (0.0651)	0.0363 (0.0371)	0.001 (0.014)	100.00 (99.61)
[156/157]	0.0499 (0.0651)	0.0308 (0.0370)	0.387 (0.015)	87.50 (99.60)
 * Train Acc 99.600
 * Val Acc 87.500, Total time 0.63
 * Val loss 0.605, Total time 0.00
Epoch:63
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0399 (0.0399)	0.0090 (0.0090)	0.055 (0.055)	96.88 (96.88)
[10/157]	0.0676 (0.0629)	0.0375 (0.0346)	0.007 (0.011)	100.00 (99.72)
[20/157]	0.0671 (0.0641)	0.0373 (0.0360)	0.008 (0.012)	100.00 (99.70)
[30/157]	0.0654 (0.0646)	0.0362 (0.0365)	0.021 (0.019)	100.00 (99.50)
[40/157]	0.0662 (0.0648)	0.0368 (0.0367)	0.001 (0.016)	100.00 (99.62)
[50/157]	0.0668 (0.0650)	0.0368 (0.0367)	0.004 (0.015)	100.00 (99.63)
[60/157]	0.0647 (0.0650)	0.0363 (0.0367)	0.005 (0.014)	100.00 (99.64)
[70/157]	0.0662 (0.0650)	0.0369 (0.0368)	0.052 (0.014)	96.88 (99.60)
[80/157]	0.0666 (0.0651)	0.0367 (0.0369)	0.041 (0.013)	96.88 (99.61)
[90/157]	0.0682 (0.0651)	0.0372 (0.0370)	0.008 (0.013)	100.00 (99.59)
[100/157]	0.0681 (0.0652)	0.0369 (0.0370)	0.005 (0.013)	100.00 (99.60)
[110/157]	0.0684 (0.0652)	0.0377 (0.0370)	0.038 (0.013)	100.00 (99.61)
[120/157]	0.0687 (0.0652)	0.0378 (0.0370)	0.025 (0.012)	100.00 (99.61)
[130/157]	0.0658 (0.0652)	0.0370 (0.0370)	0.009 (0.012)	100.00 (99.62)
[140/157]	0.0662 (0.0652)	0.0367 (0.0371)	0.002 (0.012)	100.00 (99.65)
[150/157]	0.0664 (0.0653)	0.0371 (0.0371)	0.004 (0.012)	100.00 (99.67)
[156/157]	0.0513 (0.0652)	0.0327 (0.0371)	0.328 (0.012)	87.50 (99.66)
 * Train Acc 99.660
 * Val Acc 87.500, Total time 0.63
 * Val loss 0.598, Total time 0.00
Epoch:64
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0374 (0.0374)	0.0088 (0.0088)	0.001 (0.001)	100.00 (100.00)
[10/157]	0.0670 (0.0629)	0.0371 (0.0347)	0.005 (0.007)	100.00 (100.00)
[20/157]	0.0672 (0.0643)	0.0378 (0.0361)	0.006 (0.017)	100.00 (99.70)
[30/157]	0.0660 (0.0647)	0.0368 (0.0365)	0.002 (0.016)	100.00 (99.80)
[40/157]	0.0669 (0.0649)	0.0371 (0.0367)	0.001 (0.016)	100.00 (99.62)
[50/157]	0.0666 (0.0650)	0.0365 (0.0368)	0.001 (0.015)	100.00 (99.63)
[60/157]	0.0665 (0.0650)	0.0370 (0.0369)	0.002 (0.015)	100.00 (99.64)
[70/157]	0.0667 (0.0651)	0.0370 (0.0369)	0.004 (0.014)	100.00 (99.65)
[80/157]	0.0666 (0.0651)	0.0359 (0.0370)	0.011 (0.014)	100.00 (99.65)
[90/157]	0.0659 (0.0652)	0.0373 (0.0370)	0.005 (0.013)	100.00 (99.66)
[100/157]	0.0661 (0.0652)	0.0364 (0.0370)	0.015 (0.013)	100.00 (99.69)
[110/157]	0.0670 (0.0652)	0.0374 (0.0370)	0.001 (0.012)	100.00 (99.69)
[120/157]	0.0678 (0.0652)	0.0379 (0.0371)	0.000 (0.012)	100.00 (99.72)
[130/157]	0.0665 (0.0653)	0.0370 (0.0371)	0.006 (0.012)	100.00 (99.71)
[140/157]	0.0658 (0.0653)	0.0366 (0.0371)	0.003 (0.012)	100.00 (99.71)
[150/157]	0.0663 (0.0653)	0.0369 (0.0371)	0.002 (0.012)	100.00 (99.71)
[156/157]	0.0512 (0.0652)	0.0319 (0.0371)	0.238 (0.013)	87.50 (99.66)
 * Train Acc 99.660
 * Val Acc 88.200, Total time 0.63
 * Val loss 0.621, Total time 0.00
Epoch:65
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0388 (0.0388)	0.0091 (0.0091)	0.004 (0.004)	100.00 (100.00)
[10/157]	0.0674 (0.0629)	0.0377 (0.0347)	0.002 (0.007)	100.00 (99.72)
[20/157]	0.0680 (0.0643)	0.0378 (0.0361)	0.014 (0.013)	100.00 (99.70)
[30/157]	0.0668 (0.0647)	0.0379 (0.0365)	0.003 (0.012)	100.00 (99.80)
[40/157]	0.0672 (0.0649)	0.0372 (0.0367)	0.002 (0.012)	100.00 (99.77)
[50/157]	0.0659 (0.0651)	0.0366 (0.0369)	0.003 (0.012)	100.00 (99.69)
[60/157]	0.0681 (0.0651)	0.0374 (0.0370)	0.017 (0.012)	100.00 (99.64)
[70/157]	0.0678 (0.0652)	0.0374 (0.0370)	0.001 (0.012)	100.00 (99.69)
[80/157]	0.0665 (0.0652)	0.0372 (0.0371)	0.033 (0.011)	96.88 (99.69)
[90/157]	0.0663 (0.0653)	0.0381 (0.0371)	0.003 (0.011)	100.00 (99.69)
[100/157]	0.0666 (0.0653)	0.0373 (0.0372)	0.006 (0.011)	100.00 (99.69)
[110/157]	0.0668 (0.0653)	0.0373 (0.0372)	0.001 (0.011)	100.00 (99.69)
[120/157]	0.0668 (0.0653)	0.0372 (0.0372)	0.004 (0.011)	100.00 (99.69)
[130/157]	0.0663 (0.0653)	0.0366 (0.0372)	0.003 (0.011)	100.00 (99.67)
[140/157]	0.0671 (0.0653)	0.0374 (0.0372)	0.007 (0.011)	100.00 (99.67)
[150/157]	0.0660 (0.0653)	0.0373 (0.0373)	0.000 (0.011)	100.00 (99.69)
[156/157]	0.0507 (0.0652)	0.0318 (0.0372)	0.002 (0.011)	100.00 (99.70)
 * Train Acc 99.700
 * Val Acc 88.900, Total time 0.63
 * Val loss 0.582, Total time 0.00
Epoch:66
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0363 (0.0363)	0.0091 (0.0091)	0.000 (0.000)	100.00 (100.00)
[10/157]	0.0674 (0.0630)	0.0375 (0.0350)	0.001 (0.005)	100.00 (99.72)
[20/157]	0.0678 (0.0642)	0.0375 (0.0361)	0.004 (0.013)	100.00 (99.55)
[30/157]	0.0677 (0.0646)	0.0378 (0.0365)	0.000 (0.009)	100.00 (99.70)
[40/157]	0.0668 (0.0649)	0.0380 (0.0367)	0.003 (0.013)	100.00 (99.47)
[50/157]	0.0672 (0.0649)	0.0369 (0.0368)	0.001 (0.013)	100.00 (99.51)
[60/157]	0.0673 (0.0650)	0.0371 (0.0369)	0.006 (0.011)	100.00 (99.59)
[70/157]	0.0661 (0.0650)	0.0373 (0.0370)	0.002 (0.011)	100.00 (99.60)
[80/157]	0.0668 (0.0651)	0.0371 (0.0370)	0.003 (0.011)	100.00 (99.61)
[90/157]	0.0676 (0.0651)	0.0383 (0.0371)	0.000 (0.010)	100.00 (99.66)
[100/157]	0.0662 (0.0651)	0.0378 (0.0371)	0.004 (0.010)	100.00 (99.66)
[110/157]	0.0634 (0.0652)	0.0352 (0.0372)	0.136 (0.011)	96.88 (99.66)
[120/157]	0.0660 (0.0651)	0.0366 (0.0372)	0.058 (0.011)	96.88 (99.66)
[130/157]	0.0655 (0.0652)	0.0370 (0.0372)	0.001 (0.011)	100.00 (99.69)
[140/157]	0.0667 (0.0652)	0.0378 (0.0372)	0.008 (0.011)	100.00 (99.67)
[150/157]	0.0669 (0.0652)	0.0372 (0.0372)	0.089 (0.012)	93.75 (99.63)
[156/157]	0.0502 (0.0651)	0.0310 (0.0372)	0.004 (0.012)	100.00 (99.62)
 * Train Acc 99.620
 * Val Acc 87.900, Total time 0.62
 * Val loss 0.643, Total time 0.00
Epoch:67
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0374 (0.0374)	0.0089 (0.0089)	0.001 (0.001)	100.00 (100.00)
[10/157]	0.0658 (0.0625)	0.0364 (0.0346)	0.021 (0.018)	100.00 (99.43)
[20/157]	0.0678 (0.0637)	0.0371 (0.0358)	0.001 (0.013)	100.00 (99.70)
[30/157]	0.0664 (0.0641)	0.0373 (0.0362)	0.000 (0.011)	100.00 (99.70)
[40/157]	0.0677 (0.0645)	0.0368 (0.0366)	0.002 (0.009)	100.00 (99.77)
[50/157]	0.0677 (0.0646)	0.0370 (0.0367)	0.004 (0.008)	100.00 (99.82)
[60/157]	0.0656 (0.0647)	0.0367 (0.0367)	0.021 (0.008)	100.00 (99.80)
[70/157]	0.0660 (0.0648)	0.0365 (0.0369)	0.011 (0.008)	100.00 (99.82)
[80/157]	0.0667 (0.0649)	0.0371 (0.0369)	0.004 (0.009)	100.00 (99.81)
[90/157]	0.0667 (0.0650)	0.0373 (0.0369)	0.004 (0.009)	100.00 (99.83)
[100/157]	0.0658 (0.0650)	0.0369 (0.0370)	0.006 (0.009)	100.00 (99.81)
[110/157]	0.0655 (0.0650)	0.0364 (0.0370)	0.006 (0.010)	100.00 (99.75)
[120/157]	0.0666 (0.0650)	0.0372 (0.0370)	0.002 (0.010)	100.00 (99.74)
[130/157]	0.0663 (0.0651)	0.0363 (0.0371)	0.003 (0.011)	100.00 (99.71)
[140/157]	0.0658 (0.0651)	0.0363 (0.0371)	0.038 (0.012)	96.88 (99.69)
[150/157]	0.0671 (0.0651)	0.0376 (0.0371)	0.002 (0.011)	100.00 (99.71)
[156/157]	0.0503 (0.0650)	0.0311 (0.0370)	0.000 (0.011)	100.00 (99.70)
 * Train Acc 99.700
 * Val Acc 88.300, Total time 0.59
 * Val loss 0.636, Total time 0.00
Epoch:68
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0350 (0.0350)	0.0087 (0.0087)	0.003 (0.003)	100.00 (100.00)
[10/157]	0.0635 (0.0596)	0.0351 (0.0327)	0.009 (0.021)	100.00 (99.43)
[20/157]	0.0616 (0.0607)	0.0343 (0.0338)	0.003 (0.013)	100.00 (99.70)
[30/157]	0.0616 (0.0612)	0.0339 (0.0342)	0.002 (0.011)	100.00 (99.70)
[40/157]	0.0613 (0.0614)	0.0341 (0.0343)	0.004 (0.014)	100.00 (99.62)
[50/157]	0.0625 (0.0615)	0.0351 (0.0345)	0.002 (0.012)	100.00 (99.69)
[60/157]	0.0622 (0.0617)	0.0347 (0.0346)	0.014 (0.012)	100.00 (99.69)
[70/157]	0.0617 (0.0617)	0.0346 (0.0346)	0.026 (0.011)	96.88 (99.69)
[80/157]	0.0620 (0.0618)	0.0350 (0.0347)	0.000 (0.011)	100.00 (99.69)
[90/157]	0.0625 (0.0618)	0.0351 (0.0347)	0.011 (0.010)	100.00 (99.73)
[100/157]	0.0661 (0.0619)	0.0364 (0.0348)	0.003 (0.010)	100.00 (99.72)
[110/157]	0.0680 (0.0624)	0.0385 (0.0351)	0.004 (0.010)	100.00 (99.75)
[120/157]	0.0710 (0.0628)	0.0404 (0.0354)	0.001 (0.009)	100.00 (99.77)
[130/157]	0.0706 (0.0632)	0.0397 (0.0356)	0.005 (0.009)	100.00 (99.79)
[140/157]	0.0689 (0.0634)	0.0383 (0.0358)	0.001 (0.009)	100.00 (99.78)
[150/157]	0.0708 (0.0637)	0.0392 (0.0360)	0.004 (0.009)	100.00 (99.77)
[156/157]	0.0517 (0.0637)	0.0333 (0.0361)	0.226 (0.010)	75.00 (99.72)
 * Train Acc 99.720
 * Val Acc 87.900, Total time 0.60
 * Val loss 0.617, Total time 0.00
Epoch:69
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0350 (0.0350)	0.0085 (0.0085)	0.002 (0.002)	100.00 (100.00)
[10/157]	0.0782 (0.0630)	0.0452 (0.0350)	0.019 (0.012)	100.00 (99.72)
[20/157]	0.0634 (0.0680)	0.0347 (0.0389)	0.000 (0.016)	100.00 (99.70)
[30/157]	0.0621 (0.0659)	0.0350 (0.0375)	0.002 (0.015)	100.00 (99.70)
[40/157]	0.0615 (0.0648)	0.0347 (0.0368)	0.001 (0.013)	100.00 (99.70)
[50/157]	0.0807 (0.0660)	0.0468 (0.0376)	0.012 (0.017)	100.00 (99.51)
[60/157]	0.0638 (0.0661)	0.0343 (0.0377)	0.001 (0.016)	100.00 (99.49)
[70/157]	0.0628 (0.0656)	0.0351 (0.0373)	0.001 (0.014)	100.00 (99.52)
[80/157]	0.0620 (0.0652)	0.0350 (0.0370)	0.003 (0.013)	100.00 (99.54)
[90/157]	0.0617 (0.0649)	0.0343 (0.0368)	0.003 (0.015)	100.00 (99.52)
[100/157]	0.0617 (0.0646)	0.0341 (0.0367)	0.001 (0.014)	100.00 (99.54)
[110/157]	0.0627 (0.0644)	0.0348 (0.0365)	0.030 (0.014)	100.00 (99.52)
[120/157]	0.0630 (0.0642)	0.0344 (0.0364)	0.002 (0.015)	100.00 (99.48)
[130/157]	0.0616 (0.0641)	0.0345 (0.0362)	0.005 (0.015)	100.00 (99.52)
[140/157]	0.0722 (0.0646)	0.0402 (0.0365)	0.001 (0.014)	100.00 (99.56)
[150/157]	0.0678 (0.0646)	0.0369 (0.0365)	0.002 (0.014)	100.00 (99.54)
[156/157]	0.0572 (0.0648)	0.0381 (0.0367)	0.020 (0.014)	100.00 (99.56)
 * Train Acc 99.560
 * Val Acc 87.600, Total time 0.60
 * Val loss 0.603, Total time 0.00
Epoch:70
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0353 (0.0353)	0.0091 (0.0091)	0.015 (0.015)	100.00 (100.00)
[10/157]	0.0688 (0.0617)	0.0393 (0.0343)	0.004 (0.007)	100.00 (100.00)
[20/157]	0.0700 (0.0649)	0.0402 (0.0368)	0.002 (0.007)	100.00 (100.00)
[30/157]	0.0702 (0.0662)	0.0398 (0.0377)	0.002 (0.006)	100.00 (100.00)
[40/157]	0.0723 (0.0667)	0.0396 (0.0381)	0.002 (0.008)	100.00 (99.85)
[50/157]	0.0692 (0.0670)	0.0402 (0.0383)	0.027 (0.010)	100.00 (99.75)
[60/157]	0.0722 (0.0673)	0.0403 (0.0385)	0.005 (0.010)	100.00 (99.69)
[70/157]	0.0583 (0.0665)	0.0326 (0.0380)	0.019 (0.013)	100.00 (99.65)
[80/157]	0.0823 (0.0668)	0.0477 (0.0382)	0.008 (0.012)	100.00 (99.65)
[90/157]	0.0678 (0.0668)	0.0375 (0.0382)	0.004 (0.015)	100.00 (99.52)
[100/157]	0.0687 (0.0668)	0.0388 (0.0382)	0.002 (0.014)	100.00 (99.54)
[110/157]	0.0677 (0.0667)	0.0382 (0.0381)	0.011 (0.014)	100.00 (99.55)
[120/157]	0.0684 (0.0667)	0.0383 (0.0381)	0.001 (0.013)	100.00 (99.59)
[130/157]	0.0675 (0.0666)	0.0377 (0.0380)	0.008 (0.012)	100.00 (99.62)
[140/157]	0.0673 (0.0666)	0.0378 (0.0380)	0.013 (0.012)	100.00 (99.60)
[150/157]	0.0678 (0.0665)	0.0377 (0.0380)	0.015 (0.012)	100.00 (99.63)
[156/157]	0.0518 (0.0664)	0.0325 (0.0380)	0.016 (0.012)	100.00 (99.64)
 * Train Acc 99.640
 * Val Acc 88.500, Total time 0.63
 * Val loss 0.605, Total time 0.00
Epoch:71
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0384 (0.0384)	0.0090 (0.0090)	0.009 (0.009)	100.00 (100.00)
[10/157]	0.0665 (0.0636)	0.0377 (0.0348)	0.021 (0.005)	100.00 (100.00)
[20/157]	0.0659 (0.0647)	0.0369 (0.0362)	0.030 (0.007)	100.00 (100.00)
[30/157]	0.0660 (0.0651)	0.0369 (0.0367)	0.001 (0.006)	100.00 (100.00)
[40/157]	0.0665 (0.0653)	0.0369 (0.0369)	0.006 (0.006)	100.00 (99.92)
[50/157]	0.0669 (0.0654)	0.0370 (0.0371)	0.003 (0.006)	100.00 (99.88)
[60/157]	0.0657 (0.0654)	0.0364 (0.0372)	0.006 (0.007)	100.00 (99.85)
[70/157]	0.0678 (0.0655)	0.0374 (0.0372)	0.004 (0.007)	100.00 (99.82)
[80/157]	0.0599 (0.0650)	0.0325 (0.0369)	0.002 (0.008)	100.00 (99.77)
[90/157]	0.0686 (0.0646)	0.0370 (0.0365)	0.029 (0.009)	96.88 (99.73)
[100/157]	0.0615 (0.0655)	0.0331 (0.0371)	0.001 (0.010)	100.00 (99.69)
[110/157]	0.0616 (0.0650)	0.0346 (0.0369)	0.006 (0.011)	100.00 (99.63)
[120/157]	0.0767 (0.0653)	0.0446 (0.0370)	0.006 (0.011)	100.00 (99.64)
[130/157]	0.0621 (0.0654)	0.0339 (0.0372)	0.007 (0.011)	100.00 (99.67)
[140/157]	0.0641 (0.0651)	0.0368 (0.0370)	0.001 (0.011)	100.00 (99.65)
[150/157]	0.0662 (0.0652)	0.0367 (0.0370)	0.001 (0.013)	100.00 (99.63)
[156/157]	0.0511 (0.0651)	0.0322 (0.0370)	0.025 (0.013)	100.00 (99.62)
 * Train Acc 99.620
 * Val Acc 88.900, Total time 0.63
 * Val loss 0.564, Total time 0.00
Epoch:72
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0380 (0.0380)	0.0078 (0.0078)	0.002 (0.002)	100.00 (100.00)
[10/157]	0.0693 (0.0635)	0.0390 (0.0354)	0.007 (0.016)	100.00 (99.72)
[20/157]	0.0683 (0.0648)	0.0381 (0.0365)	0.008 (0.017)	100.00 (99.55)
[30/157]	0.0682 (0.0653)	0.0379 (0.0370)	0.002 (0.014)	100.00 (99.60)
[40/157]	0.0685 (0.0655)	0.0387 (0.0372)	0.001 (0.012)	100.00 (99.70)
[50/157]	0.0670 (0.0657)	0.0376 (0.0374)	0.001 (0.011)	100.00 (99.75)
[60/157]	0.0595 (0.0651)	0.0331 (0.0370)	0.031 (0.011)	96.88 (99.74)
[70/157]	0.0736 (0.0655)	0.0425 (0.0372)	0.001 (0.010)	100.00 (99.74)
[80/157]	0.0634 (0.0657)	0.0347 (0.0374)	0.020 (0.010)	100.00 (99.77)
[90/157]	0.0785 (0.0655)	0.0452 (0.0373)	0.049 (0.011)	96.88 (99.66)
[100/157]	0.0695 (0.0661)	0.0387 (0.0377)	0.002 (0.010)	100.00 (99.69)
[110/157]	0.0678 (0.0662)	0.0381 (0.0377)	0.032 (0.012)	96.88 (99.63)
[120/157]	0.0681 (0.0662)	0.0387 (0.0378)	0.013 (0.013)	100.00 (99.61)
[130/157]	0.0688 (0.0663)	0.0391 (0.0378)	0.008 (0.013)	100.00 (99.64)
[140/157]	0.0642 (0.0663)	0.0359 (0.0378)	0.008 (0.013)	100.00 (99.67)
[150/157]	0.0799 (0.0663)	0.0463 (0.0379)	0.000 (0.012)	100.00 (99.67)
[156/157]	0.0472 (0.0664)	0.0281 (0.0380)	0.824 (0.014)	87.50 (99.64)
 * Train Acc 99.640
 * Val Acc 89.100, Total time 0.59
 * Val loss 0.538, Total time 0.00
Epoch:73
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0355 (0.0355)	0.0086 (0.0086)	0.001 (0.001)	100.00 (100.00)
[10/157]	0.0661 (0.0602)	0.0370 (0.0330)	0.004 (0.013)	100.00 (99.72)
[20/157]	0.0722 (0.0627)	0.0383 (0.0351)	0.046 (0.015)	100.00 (99.55)
[30/157]	0.0696 (0.0634)	0.0399 (0.0358)	0.001 (0.024)	100.00 (99.40)
[40/157]	0.0666 (0.0639)	0.0361 (0.0361)	0.005 (0.023)	100.00 (99.39)
[50/157]	0.0660 (0.0642)	0.0366 (0.0364)	0.026 (0.022)	100.00 (99.39)
[60/157]	0.0661 (0.0644)	0.0371 (0.0365)	0.001 (0.021)	100.00 (99.39)
[70/157]	0.0656 (0.0645)	0.0367 (0.0366)	0.001 (0.020)	100.00 (99.38)
[80/157]	0.0692 (0.0646)	0.0373 (0.0367)	0.004 (0.018)	100.00 (99.46)
[90/157]	0.0688 (0.0647)	0.0384 (0.0368)	0.196 (0.020)	93.75 (99.42)
[100/157]	0.0675 (0.0648)	0.0376 (0.0368)	0.000 (0.019)	100.00 (99.44)
[110/157]	0.0681 (0.0648)	0.0359 (0.0368)	0.128 (0.018)	96.88 (99.47)
[120/157]	0.0707 (0.0649)	0.0400 (0.0369)	0.002 (0.018)	100.00 (99.48)
[130/157]	0.0664 (0.0649)	0.0371 (0.0369)	0.002 (0.017)	100.00 (99.52)
[140/157]	0.0654 (0.0649)	0.0357 (0.0369)	0.025 (0.016)	100.00 (99.56)
[150/157]	0.0669 (0.0650)	0.0369 (0.0369)	0.000 (0.016)	100.00 (99.57)
[156/157]	0.0505 (0.0649)	0.0313 (0.0369)	0.191 (0.016)	87.50 (99.54)
 * Train Acc 99.540
 * Val Acc 88.100, Total time 0.62
 * Val loss 0.600, Total time 0.00
Epoch:74
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0393 (0.0393)	0.0092 (0.0092)	0.001 (0.001)	100.00 (100.00)
[10/157]	0.0674 (0.0631)	0.0376 (0.0348)	0.003 (0.007)	100.00 (99.72)
[20/157]	0.0669 (0.0642)	0.0369 (0.0359)	0.004 (0.019)	100.00 (99.26)
[30/157]	0.0680 (0.0647)	0.0372 (0.0364)	0.005 (0.020)	100.00 (99.29)
[40/157]	0.0674 (0.0648)	0.0375 (0.0366)	0.007 (0.021)	100.00 (99.24)
[50/157]	0.0643 (0.0649)	0.0361 (0.0367)	0.078 (0.021)	96.88 (99.26)
[60/157]	0.0666 (0.0649)	0.0369 (0.0368)	0.012 (0.019)	100.00 (99.23)
[70/157]	0.0656 (0.0650)	0.0368 (0.0368)	0.008 (0.019)	100.00 (99.25)
[80/157]	0.0649 (0.0650)	0.0360 (0.0369)	0.003 (0.018)	100.00 (99.34)
[90/157]	0.0671 (0.0651)	0.0376 (0.0370)	0.033 (0.017)	96.88 (99.38)
[100/157]	0.0662 (0.0651)	0.0374 (0.0370)	0.014 (0.016)	100.00 (99.44)
[110/157]	0.0655 (0.0651)	0.0364 (0.0370)	0.002 (0.016)	100.00 (99.47)
[120/157]	0.0673 (0.0651)	0.0374 (0.0370)	0.002 (0.015)	100.00 (99.51)
[130/157]	0.0657 (0.0651)	0.0369 (0.0370)	0.001 (0.014)	100.00 (99.55)
[140/157]	0.0652 (0.0651)	0.0363 (0.0370)	0.002 (0.014)	100.00 (99.53)
[150/157]	0.0674 (0.0652)	0.0377 (0.0371)	0.005 (0.013)	100.00 (99.57)
[156/157]	0.0499 (0.0651)	0.0306 (0.0370)	0.008 (0.013)	100.00 (99.56)
 * Train Acc 99.560
 * Val Acc 88.000, Total time 0.63
 * Val loss 0.598, Total time 0.00
Epoch:75
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0391 (0.0391)	0.0091 (0.0091)	0.016 (0.016)	100.00 (100.00)
[10/157]	0.0668 (0.0625)	0.0375 (0.0345)	0.012 (0.005)	100.00 (100.00)
[20/157]	0.0666 (0.0638)	0.0370 (0.0358)	0.043 (0.006)	96.88 (99.85)
[30/157]	0.0659 (0.0642)	0.0371 (0.0362)	0.001 (0.006)	100.00 (99.80)
[40/157]	0.0676 (0.0645)	0.0370 (0.0365)	0.005 (0.006)	100.00 (99.85)
[50/157]	0.0657 (0.0646)	0.0373 (0.0366)	0.001 (0.007)	100.00 (99.88)
[60/157]	0.0655 (0.0647)	0.0363 (0.0368)	0.001 (0.007)	100.00 (99.85)
[70/157]	0.0672 (0.0648)	0.0378 (0.0368)	0.003 (0.008)	100.00 (99.78)
[80/157]	0.0671 (0.0649)	0.0374 (0.0369)	0.002 (0.007)	100.00 (99.81)
[90/157]	0.0654 (0.0649)	0.0361 (0.0369)	0.037 (0.008)	96.88 (99.73)
[100/157]	0.0672 (0.0650)	0.0368 (0.0370)	0.001 (0.009)	100.00 (99.69)
[110/157]	0.0695 (0.0650)	0.0370 (0.0370)	0.043 (0.009)	96.88 (99.69)
[120/157]	0.0652 (0.0650)	0.0363 (0.0370)	0.004 (0.009)	100.00 (99.69)
[130/157]	0.0664 (0.0651)	0.0365 (0.0370)	0.001 (0.010)	100.00 (99.69)
[140/157]	0.0683 (0.0651)	0.0387 (0.0370)	0.001 (0.010)	100.00 (99.67)
[150/157]	0.0665 (0.0651)	0.0368 (0.0370)	0.000 (0.011)	100.00 (99.67)
[156/157]	0.0505 (0.0650)	0.0316 (0.0370)	0.023 (0.011)	100.00 (99.66)
 * Train Acc 99.660
 * Val Acc 87.400, Total time 0.62
 * Val loss 0.633, Total time 0.00
Epoch:76
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0359 (0.0359)	0.0085 (0.0085)	0.001 (0.001)	100.00 (100.00)
[10/157]	0.0690 (0.0624)	0.0371 (0.0347)	0.002 (0.006)	100.00 (99.72)
[20/157]	0.0665 (0.0637)	0.0371 (0.0357)	0.003 (0.006)	100.00 (99.70)
[30/157]	0.0666 (0.0642)	0.0362 (0.0361)	0.001 (0.008)	100.00 (99.60)
[40/157]	0.0659 (0.0645)	0.0365 (0.0364)	0.007 (0.010)	100.00 (99.54)
[50/157]	0.0655 (0.0646)	0.0374 (0.0366)	0.001 (0.009)	100.00 (99.63)
[60/157]	0.0670 (0.0646)	0.0367 (0.0367)	0.002 (0.009)	100.00 (99.69)
[70/157]	0.0687 (0.0647)	0.0372 (0.0366)	0.006 (0.008)	100.00 (99.74)
[80/157]	0.0670 (0.0648)	0.0368 (0.0367)	0.001 (0.008)	100.00 (99.77)
[90/157]	0.0673 (0.0648)	0.0372 (0.0368)	0.003 (0.008)	100.00 (99.79)
[100/157]	0.0653 (0.0649)	0.0357 (0.0367)	0.005 (0.007)	100.00 (99.81)
[110/157]	0.0669 (0.0649)	0.0370 (0.0368)	0.002 (0.007)	100.00 (99.83)
[120/157]	0.0666 (0.0649)	0.0371 (0.0368)	0.004 (0.007)	100.00 (99.85)
[130/157]	0.0676 (0.0650)	0.0381 (0.0368)	0.001 (0.007)	100.00 (99.86)
[140/157]	0.0670 (0.0650)	0.0371 (0.0369)	0.001 (0.007)	100.00 (99.87)
[150/157]	0.0662 (0.0650)	0.0361 (0.0369)	0.006 (0.007)	100.00 (99.88)
[156/157]	0.0494 (0.0649)	0.0304 (0.0368)	0.001 (0.007)	100.00 (99.88)
 * Train Acc 99.880
 * Val Acc 88.900, Total time 0.62
 * Val loss 0.582, Total time 0.00
Epoch:77
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0377 (0.0377)	0.0090 (0.0090)	0.007 (0.007)	100.00 (100.00)
[10/157]	0.0646 (0.0622)	0.0360 (0.0342)	0.007 (0.009)	100.00 (99.72)
[20/157]	0.0669 (0.0635)	0.0373 (0.0356)	0.111 (0.013)	93.75 (99.40)
[30/157]	0.0668 (0.0639)	0.0374 (0.0361)	0.002 (0.012)	100.00 (99.50)
[40/157]	0.0673 (0.0643)	0.0379 (0.0364)	0.001 (0.010)	100.00 (99.62)
[50/157]	0.0657 (0.0645)	0.0367 (0.0365)	0.001 (0.010)	100.00 (99.63)
[60/157]	0.0658 (0.0646)	0.0367 (0.0366)	0.005 (0.009)	100.00 (99.64)
[70/157]	0.0662 (0.0646)	0.0367 (0.0367)	0.000 (0.009)	100.00 (99.60)
[80/157]	0.0662 (0.0647)	0.0367 (0.0368)	0.004 (0.009)	100.00 (99.61)
[90/157]	0.0669 (0.0648)	0.0378 (0.0368)	0.060 (0.010)	96.88 (99.59)
[100/157]	0.0666 (0.0649)	0.0367 (0.0369)	0.006 (0.010)	100.00 (99.60)
[110/157]	0.0664 (0.0649)	0.0373 (0.0369)	0.003 (0.011)	100.00 (99.58)
[120/157]	0.0673 (0.0649)	0.0378 (0.0369)	0.056 (0.011)	96.88 (99.56)
[130/157]	0.0664 (0.0649)	0.0368 (0.0370)	0.032 (0.010)	100.00 (99.57)
[140/157]	0.0667 (0.0649)	0.0374 (0.0370)	0.004 (0.011)	100.00 (99.53)
[150/157]	0.0651 (0.0649)	0.0367 (0.0370)	0.003 (0.011)	100.00 (99.57)
[156/157]	0.0501 (0.0648)	0.0311 (0.0369)	0.284 (0.011)	87.50 (99.56)
 * Train Acc 99.560
 * Val Acc 88.600, Total time 0.63
 * Val loss 0.610, Total time 0.00
Epoch:78
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0389 (0.0389)	0.0090 (0.0090)	0.000 (0.000)	100.00 (100.00)
[10/157]	0.0659 (0.0625)	0.0370 (0.0345)	0.002 (0.007)	100.00 (99.43)
[20/157]	0.0662 (0.0637)	0.0370 (0.0358)	0.016 (0.008)	100.00 (99.55)
[30/157]	0.0657 (0.0642)	0.0367 (0.0363)	0.019 (0.015)	100.00 (99.40)
[40/157]	0.0656 (0.0644)	0.0367 (0.0366)	0.001 (0.014)	100.00 (99.47)
[50/157]	0.0664 (0.0646)	0.0371 (0.0367)	0.040 (0.013)	96.88 (99.45)
[60/157]	0.0662 (0.0647)	0.0371 (0.0369)	0.062 (0.013)	96.88 (99.44)
[70/157]	0.0665 (0.0648)	0.0370 (0.0369)	0.027 (0.013)	96.88 (99.47)
[80/157]	0.0687 (0.0649)	0.0379 (0.0370)	0.035 (0.015)	96.88 (99.42)
[90/157]	0.0672 (0.0649)	0.0368 (0.0370)	0.001 (0.014)	100.00 (99.48)
[100/157]	0.0682 (0.0649)	0.0378 (0.0370)	0.001 (0.015)	100.00 (99.41)
[110/157]	0.0689 (0.0649)	0.0396 (0.0370)	0.005 (0.014)	100.00 (99.47)
[120/157]	0.0671 (0.0650)	0.0375 (0.0370)	0.008 (0.015)	100.00 (99.46)
[130/157]	0.0653 (0.0649)	0.0368 (0.0370)	0.001 (0.014)	100.00 (99.48)
[140/157]	0.0663 (0.0650)	0.0366 (0.0370)	0.003 (0.014)	100.00 (99.47)
[150/157]	0.0652 (0.0650)	0.0364 (0.0370)	0.012 (0.014)	100.00 (99.50)
[156/157]	0.0489 (0.0649)	0.0299 (0.0370)	0.011 (0.015)	100.00 (99.46)
 * Train Acc 99.460
 * Val Acc 88.700, Total time 0.62
 * Val loss 0.665, Total time 0.00
Epoch:79
LR: 2.5e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0364 (0.0364)	0.0090 (0.0090)	0.001 (0.001)	100.00 (100.00)
[10/157]	0.0657 (0.0625)	0.0365 (0.0347)	0.001 (0.011)	100.00 (99.43)
[20/157]	0.0664 (0.0638)	0.0374 (0.0360)	0.008 (0.008)	100.00 (99.70)
[30/157]	0.0670 (0.0642)	0.0376 (0.0364)	0.005 (0.015)	100.00 (99.60)
[40/157]	0.0656 (0.0644)	0.0365 (0.0365)	0.007 (0.013)	100.00 (99.70)
[50/157]	0.0673 (0.0646)	0.0378 (0.0367)	0.000 (0.014)	100.00 (99.63)
[60/157]	0.0663 (0.0647)	0.0371 (0.0368)	0.012 (0.013)	100.00 (99.64)
[70/157]	0.0656 (0.0648)	0.0360 (0.0368)	0.001 (0.014)	100.00 (99.60)
[80/157]	0.0681 (0.0649)	0.0377 (0.0369)	0.004 (0.013)	100.00 (99.65)
[90/157]	0.0663 (0.0648)	0.0372 (0.0369)	0.000 (0.014)	100.00 (99.62)
[100/157]	0.0663 (0.0648)	0.0368 (0.0369)	0.014 (0.015)	100.00 (99.60)
[110/157]	0.0680 (0.0649)	0.0378 (0.0370)	0.000 (0.015)	100.00 (99.61)
[120/157]	0.0665 (0.0649)	0.0369 (0.0370)	0.023 (0.015)	100.00 (99.61)
[130/157]	0.0681 (0.0649)	0.0369 (0.0370)	0.041 (0.015)	96.88 (99.57)
[140/157]	0.0655 (0.0649)	0.0364 (0.0370)	0.036 (0.014)	96.88 (99.56)
[150/157]	0.0674 (0.0650)	0.0373 (0.0370)	0.009 (0.014)	100.00 (99.59)
[156/157]	0.0509 (0.0649)	0.0314 (0.0370)	0.057 (0.013)	100.00 (99.60)
 * Train Acc 99.600
 * Val Acc 88.200, Total time 0.63
 * Val loss 0.608, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 5/27; cond: 337601.8125, radio:4.2250321712344885e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0041,  0.1075, -0.1582],
          [-0.1382, -0.0747,  0.0503],
          [ 0.0053,  0.1572, -0.0150]],

         [[ 0.0548, -0.0551, -0.0375],
          [-0.1811, -0.1278, -0.0789],
          [ 0.0137,  0.0788,  0.1180]],

         [[-0.1300, -0.0835,  0.0679],
          [ 0.1597, -0.0407,  0.1443],
          [-0.0282,  0.0217,  0.1758]]],


        [[[-0.1679, -0.1051, -0.0260],
          [-0.0668,  0.1739, -0.1110],
          [-0.0790, -0.1354, -0.1712]],

         [[-0.1062,  0.1790,  0.1033],
          [ 0.0971,  0.0173, -0.0889],
          [ 0.0342, -0.1827, -0.1368]],

         [[-0.0979,  0.1281,  0.1211],
          [-0.0829, -0.0015,  0.1286],
          [ 0.1914,  0.0734,  0.0265]]],


        [[[ 0.1331, -0.1137,  0.0366],
          [-0.1474, -0.1348, -0.0991],
          [ 0.0868,  0.0755, -0.1127]],

         [[ 0.0590,  0.1012, -0.0280],
          [ 0.0060,  0.0401,  0.1155],
          [ 0.1802, -0.1534, -0.0732]],

         [[ 0.0777,  0.1566,  0.1638],
          [ 0.1680,  0.0344, -0.1713],
          [ 0.0120, -0.1272, -0.1842]]],


        ...,


        [[[ 0.0681, -0.0763, -0.1205],
          [ 0.0766,  0.0334, -0.1412],
          [-0.1325,  0.0946,  0.1446]],

         [[ 0.0184, -0.0979,  0.1006],
          [ 0.0362, -0.1678, -0.1245],
          [ 0.0340,  0.1233, -0.0630]],

         [[ 0.0808, -0.1303,  0.1817],
          [ 0.1269,  0.0269,  0.0763],
          [ 0.0458, -0.1186, -0.0130]]],


        [[[ 0.0627, -0.0409,  0.1927],
          [-0.0629, -0.1656, -0.0868],
          [-0.1709,  0.1689, -0.0842]],

         [[-0.1291, -0.0210, -0.1256],
          [-0.0926,  0.1033, -0.1425],
          [ 0.0978,  0.1511,  0.0007]],

         [[ 0.0848, -0.1122,  0.1495],
          [ 0.1867,  0.0454,  0.1724],
          [-0.1951, -0.0197,  0.0319]]],


        [[[-0.1669,  0.0944, -0.0724],
          [-0.0708, -0.1544,  0.0819],
          [-0.1407, -0.0459,  0.0853]],

         [[ 0.1452,  0.0467, -0.0712],
          [-0.0837,  0.1103,  0.0390],
          [ 0.1597,  0.0671,  0.1517]],

         [[-0.1559,  0.1529, -0.0939],
          [-0.0552, -0.0043, -0.0430],
          [ 0.0334,  0.1608, -0.1262]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([5.0048e+05, 4.8654e+04, 4.8278e+04, 3.4790e+04, 1.8212e+04, 1.4725e+04,
        3.2970e+03, 1.8882e+03, 6.3512e+02, 6.3027e+02, 5.9582e+02, 5.6322e+02,
        2.0891e+02, 1.5541e+02, 1.4189e+02, 9.7099e+01, 8.6260e+01, 6.7258e+01,
        3.0180e+01, 2.5464e+01, 2.3377e+01, 1.4975e+01, 1.2889e+01, 5.6002e+00,
        4.8238e+00, 3.6656e+00, 1.4825e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 5]) 

NULL SPACE BASIS :  tensor([[-1.9487e-01, -1.3879e-01,  1.4041e-01, -1.2299e-01,  9.1593e-02],
        [-1.2507e-02,  2.5450e-01, -4.4197e-03,  2.2445e-01, -1.6664e-01],
        [ 2.1001e-01, -1.4068e-01, -1.3623e-01, -1.2094e-01,  9.2851e-02],
        [ 3.7443e-01, -3.7173e-03, -2.5216e-01,  2.2192e-01, -1.6502e-01],
        [ 3.3881e-04,  3.5041e-03, -3.2867e-03, -4.0157e-01,  2.9655e-01],
        [-3.7670e-01,  4.2977e-04,  2.5553e-01,  2.1667e-01, -1.6569e-01],
        [-2.0445e-01,  1.4226e-01,  1.3481e-01, -1.2091e-01,  9.2422e-02],
        [ 1.4950e-02, -2.5740e-01,  8.3333e-03,  2.1986e-01, -1.6651e-01],
        [ 1.8874e-01,  1.3964e-01, -1.4301e-01, -1.1713e-01,  9.1728e-02],
        [ 8.8707e-04,  2.4087e-01, -2.4729e-01, -3.8550e-03, -1.5169e-01],
        [ 1.1435e-04, -4.4150e-01,  7.3685e-03,  8.2303e-03,  2.7432e-01],
        [-3.7873e-03,  2.4825e-01,  2.4028e-01, -4.9017e-03, -1.5227e-01],
        [-2.6121e-03,  6.4818e-03,  4.4265e-01,  7.3373e-03,  2.7228e-01],
        [ 5.8734e-04,  4.8947e-04, -2.0467e-04, -2.0019e-02, -4.8792e-01],
        [ 4.9451e-03, -7.5756e-03, -4.4246e-01,  1.3890e-02,  2.7225e-01],
        [-3.9349e-03, -2.4807e-01, -2.4036e-01, -4.6264e-03, -1.5210e-01],
        [-2.9895e-03,  4.4111e-01, -6.9205e-03,  1.3470e-02,  2.7416e-01],
        [ 6.8065e-03, -2.4034e-01,  2.4691e-01, -1.0291e-02, -1.5150e-01],
        [ 2.1946e-01, -1.2210e-01,  1.2778e-01,  1.4347e-01,  7.1666e-02],
        [ 1.3197e-02,  2.2378e-01, -3.7387e-03, -2.6353e-01, -1.2876e-01],
        [-2.3257e-01, -1.2834e-01, -1.2419e-01,  1.4304e-01,  7.1062e-02],
        [-4.1867e-01, -4.4079e-03, -2.2736e-01, -2.5875e-01, -1.2831e-01],
        [-1.2193e-03, -2.8078e-03,  3.6593e-03,  4.7690e-01,  2.2924e-01],
        [ 4.1910e-01,  7.1683e-03,  2.2364e-01, -2.6206e-01, -1.2748e-01],
        [ 2.3488e-01,  1.2744e-01,  1.2526e-01,  1.4180e-01,  7.1554e-02],
        [-1.2298e-02, -2.2166e-01, -5.0077e-04, -2.6453e-01, -1.2896e-01],
        [-2.2187e-01,  1.2125e-01, -1.2454e-01,  1.4504e-01,  7.1498e-02]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0449, -0.0342, -0.0077, -0.0672,  0.0338,  0.0301,  0.0279, -0.0037,
         -0.0239, -0.0366,  0.0386, -0.0060,  0.0384, -0.0190, -0.0174, -0.0053,
         -0.0171,  0.0242, -0.0085, -0.0061,  0.0155,  0.0315, -0.0163, -0.0137,
         -0.0254,  0.0241, -0.0011],
        [-0.0342,  0.0640, -0.0360,  0.0326, -0.0620,  0.0357, -0.0020,  0.0051,
         -0.0035,  0.0388, -0.0699,  0.0387, -0.0197,  0.0344, -0.0189, -0.0169,
          0.0312, -0.0176, -0.0063,  0.0085, -0.0040, -0.0141,  0.0305, -0.0188,
          0.0218, -0.0421,  0.0245],
        [-0.0077, -0.0360,  0.0473,  0.0319,  0.0340, -0.0696, -0.0260, -0.0017,
          0.0278, -0.0061,  0.0383, -0.0367, -0.0167, -0.0191,  0.0385,  0.0238,
         -0.0170, -0.0050,  0.0157, -0.0037, -0.0110, -0.0165, -0.0164,  0.0342,
          0.0017,  0.0218, -0.0258],
        [-0.0672,  0.0326,  0.0319,  0.1253, -0.0613, -0.0582, -0.0685,  0.0361,
          0.0291,  0.0384, -0.0195, -0.0174, -0.0697,  0.0341,  0.0320,  0.0376,
         -0.0193, -0.0161,  0.0315, -0.0144, -0.0158, -0.0607,  0.0298,  0.0283,
          0.0338, -0.0184, -0.0142],
        [ 0.0338, -0.0620,  0.0340, -0.0613,  0.1115, -0.0613,  0.0340, -0.0620,
          0.0337, -0.0187,  0.0342, -0.0193,  0.0342, -0.0611,  0.0343, -0.0194,
          0.0346, -0.0190, -0.0166,  0.0306, -0.0163,  0.0297, -0.0553,  0.0299,
         -0.0159,  0.0301, -0.0162],
        [ 0.0301,  0.0357, -0.0696, -0.0582, -0.0613,  0.1259,  0.0313,  0.0320,
         -0.0663, -0.0175, -0.0188,  0.0390,  0.0316,  0.0341, -0.0702, -0.0160,
         -0.0192,  0.0373, -0.0138, -0.0186,  0.0336,  0.0290,  0.0298, -0.0610,
         -0.0168, -0.0141,  0.0319],
        [ 0.0279, -0.0020, -0.0260, -0.0685,  0.0340,  0.0313,  0.0462, -0.0360,
         -0.0069, -0.0057, -0.0168,  0.0246,  0.0382, -0.0191, -0.0171, -0.0359,
          0.0385, -0.0067, -0.0249,  0.0217,  0.0008,  0.0330, -0.0162, -0.0155,
         -0.0105, -0.0040,  0.0156],
        [-0.0037,  0.0051, -0.0017,  0.0361, -0.0620,  0.0320, -0.0360,  0.0638,
         -0.0337, -0.0177,  0.0312, -0.0168, -0.0187,  0.0343, -0.0197,  0.0385,
         -0.0699,  0.0389,  0.0248, -0.0420,  0.0215, -0.0190,  0.0301, -0.0135,
         -0.0040,  0.0090, -0.0070],
        [-0.0239, -0.0035,  0.0278,  0.0291,  0.0337, -0.0663, -0.0069, -0.0337,
          0.0437,  0.0249, -0.0172, -0.0062, -0.0173, -0.0189,  0.0387, -0.0064,
          0.0383, -0.0359, -0.0018,  0.0239, -0.0243, -0.0128, -0.0161,  0.0300,
          0.0153, -0.0063, -0.0079],
        [-0.0366,  0.0388, -0.0061,  0.0384, -0.0187, -0.0175, -0.0057, -0.0177,
          0.0249,  0.0636, -0.0670,  0.0105, -0.0667,  0.0332,  0.0296,  0.0102,
          0.0297, -0.0429, -0.0323,  0.0337, -0.0053,  0.0337, -0.0171, -0.0147,
         -0.0051, -0.0146,  0.0216],
        [ 0.0386, -0.0699,  0.0383, -0.0195,  0.0342, -0.0188, -0.0168,  0.0312,
         -0.0172, -0.0670,  0.1209, -0.0669,  0.0336, -0.0600,  0.0335,  0.0295,
         -0.0534,  0.0296,  0.0339, -0.0610,  0.0342, -0.0166,  0.0304, -0.0173,
         -0.0154,  0.0270, -0.0151],
        [-0.0060,  0.0387, -0.0367, -0.0174, -0.0193,  0.0390,  0.0246, -0.0168,
         -0.0062,  0.0105, -0.0669,  0.0638,  0.0297,  0.0333, -0.0670, -0.0430,
          0.0295,  0.0102, -0.0054,  0.0338, -0.0324, -0.0149, -0.0166,  0.0334,
          0.0220, -0.0153, -0.0047],
        [ 0.0384, -0.0197, -0.0167, -0.0697,  0.0342,  0.0316,  0.0382, -0.0187,
         -0.0173, -0.0667,  0.0336,  0.0297,  0.1208, -0.0595, -0.0544, -0.0668,
          0.0333,  0.0297,  0.0339, -0.0166, -0.0156, -0.0610,  0.0302,  0.0274,
          0.0341, -0.0173, -0.0149],
        [-0.0190,  0.0344, -0.0191,  0.0341, -0.0611,  0.0341, -0.0191,  0.0343,
         -0.0189,  0.0332, -0.0600,  0.0333, -0.0595,  0.1066, -0.0595,  0.0332,
         -0.0598,  0.0331, -0.0169,  0.0305, -0.0169,  0.0302, -0.0543,  0.0303,
         -0.0168,  0.0305, -0.0169],
        [-0.0174, -0.0189,  0.0385,  0.0320,  0.0343, -0.0702, -0.0171, -0.0197,
          0.0387,  0.0296,  0.0335, -0.0670, -0.0544, -0.0595,  0.1208,  0.0298,
          0.0333, -0.0665, -0.0148, -0.0173,  0.0340,  0.0268,  0.0302, -0.0605,
         -0.0151, -0.0165,  0.0333],
        [-0.0053, -0.0169,  0.0238,  0.0376, -0.0194, -0.0160, -0.0359,  0.0385,
         -0.0064,  0.0102,  0.0295, -0.0430, -0.0668,  0.0332,  0.0298,  0.0637,
         -0.0669,  0.0104, -0.0057, -0.0151,  0.0229,  0.0349, -0.0167, -0.0164,
         -0.0332,  0.0340, -0.0048],
        [-0.0171,  0.0312, -0.0170, -0.0193,  0.0346, -0.0192,  0.0385, -0.0699,
          0.0383,  0.0297, -0.0534,  0.0295,  0.0333, -0.0598,  0.0333, -0.0669,
          0.1207, -0.0668, -0.0151,  0.0268, -0.0150, -0.0169,  0.0304, -0.0170,
          0.0341, -0.0611,  0.0342],
        [ 0.0242, -0.0176, -0.0050, -0.0161, -0.0190,  0.0373, -0.0067,  0.0389,
         -0.0359, -0.0429,  0.0296,  0.0102,  0.0297,  0.0331, -0.0665,  0.0104,
         -0.0668,  0.0634,  0.0224, -0.0145, -0.0061, -0.0160, -0.0170,  0.0350,
         -0.0047,  0.0337, -0.0330],
        [-0.0085, -0.0063,  0.0157,  0.0315, -0.0166, -0.0138, -0.0249,  0.0248,
         -0.0018, -0.0323,  0.0339, -0.0054,  0.0339, -0.0169, -0.0148, -0.0057,
         -0.0151,  0.0224,  0.0470, -0.0322, -0.0115, -0.0746,  0.0382,  0.0326,
          0.0346, -0.0102, -0.0239],
        [-0.0061,  0.0085, -0.0037, -0.0144,  0.0306, -0.0186,  0.0217, -0.0420,
          0.0239,  0.0337, -0.0610,  0.0338, -0.0166,  0.0305, -0.0173, -0.0151,
          0.0268, -0.0145, -0.0322,  0.0610, -0.0350,  0.0354, -0.0697,  0.0410,
         -0.0069,  0.0163, -0.0102],
        [ 0.0155, -0.0040, -0.0110, -0.0158, -0.0163,  0.0336,  0.0008,  0.0215,
         -0.0243, -0.0053,  0.0342, -0.0324, -0.0156, -0.0169,  0.0340,  0.0229,
         -0.0150, -0.0061, -0.0115, -0.0350,  0.0499,  0.0358,  0.0379, -0.0772,
         -0.0274, -0.0070,  0.0346],
        [ 0.0315, -0.0141, -0.0165, -0.0607,  0.0297,  0.0290,  0.0330, -0.0190,
         -0.0128,  0.0337, -0.0166, -0.0149, -0.0610,  0.0302,  0.0268,  0.0349,
         -0.0169, -0.0160, -0.0746,  0.0354,  0.0358,  0.1388, -0.0685, -0.0636,
         -0.0775,  0.0408,  0.0331],
        [-0.0163,  0.0305, -0.0164,  0.0298, -0.0553,  0.0298, -0.0162,  0.0301,
         -0.0161, -0.0171,  0.0304, -0.0166,  0.0302, -0.0543,  0.0302, -0.0167,
          0.0304, -0.0170,  0.0382, -0.0697,  0.0379, -0.0685,  0.1252, -0.0688,
          0.0375, -0.0694,  0.0380],
        [-0.0137, -0.0188,  0.0342,  0.0283,  0.0299, -0.0610, -0.0155, -0.0135,
          0.0300, -0.0147, -0.0173,  0.0334,  0.0274,  0.0303, -0.0605, -0.0164,
         -0.0170,  0.0350,  0.0326,  0.0410, -0.0772, -0.0636, -0.0688,  0.1389,
          0.0363,  0.0353, -0.0747],
        [-0.0254,  0.0218,  0.0017,  0.0338, -0.0159, -0.0168, -0.0105, -0.0040,
          0.0153, -0.0051, -0.0154,  0.0220,  0.0341, -0.0168, -0.0151, -0.0332,
          0.0341, -0.0047,  0.0346, -0.0069, -0.0274, -0.0775,  0.0375,  0.0363,
          0.0502, -0.0349, -0.0119],
        [ 0.0241, -0.0421,  0.0218, -0.0184,  0.0301, -0.0141, -0.0040,  0.0090,
         -0.0063, -0.0146,  0.0270, -0.0153, -0.0173,  0.0305, -0.0165,  0.0340,
         -0.0611,  0.0337, -0.0102,  0.0163, -0.0070,  0.0408, -0.0694,  0.0353,
         -0.0349,  0.0608, -0.0321],
        [-0.0011,  0.0245, -0.0258, -0.0142, -0.0162,  0.0319,  0.0156, -0.0070,
         -0.0079,  0.0216, -0.0151, -0.0047, -0.0149, -0.0169,  0.0333, -0.0048,
          0.0342, -0.0330, -0.0239, -0.0102,  0.0346,  0.0331,  0.0380, -0.0747,
         -0.0119, -0.0321,  0.0472]], device='cuda:0') 

reserving basis 75/576; cond: 10371072.0, radio:3.453010140219703e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1381e-02, -3.9979e-02, -7.1222e-03],
          [ 3.8507e-02,  2.6622e-02,  3.1508e-02],
          [ 2.2079e-02, -1.7764e-03,  8.7307e-03]],

         [[-1.4155e-02,  3.7834e-02,  1.8551e-02],
          [-2.9844e-02,  3.6937e-02, -3.2476e-03],
          [-7.0482e-03, -1.7076e-02, -2.0340e-02]],

         [[-3.0017e-03, -2.0886e-02, -3.6735e-02],
          [-2.9223e-02, -1.0190e-02,  3.3216e-02],
          [ 2.0532e-02, -3.1972e-02, -9.3314e-03]],

         ...,

         [[-2.5488e-02,  1.1637e-02,  4.6078e-06],
          [ 1.4904e-02, -6.5603e-03, -9.4551e-03],
          [-2.0593e-03,  8.6738e-03, -3.1121e-02]],

         [[-4.1542e-02,  1.3736e-02, -3.4918e-02],
          [-6.6736e-03,  1.2138e-02,  1.0808e-03],
          [ 2.0749e-02,  5.8489e-05,  4.1727e-03]],

         [[-1.6046e-02,  2.0036e-03,  1.1451e-02],
          [ 2.3194e-02, -3.9537e-02, -1.6235e-02],
          [-4.1576e-02, -1.1520e-02,  2.9666e-02]]],


        [[[-4.2243e-02, -4.7246e-02,  1.4323e-02],
          [-2.3739e-02,  2.0034e-02,  2.7855e-03],
          [ 8.1258e-03,  2.3431e-02, -4.2349e-02]],

         [[ 3.2935e-02,  8.0825e-03, -3.0280e-02],
          [ 1.1776e-02, -1.1419e-02, -1.7503e-02],
          [ 3.4513e-03, -1.1638e-02,  4.0711e-02]],

         [[ 2.6347e-02, -1.6236e-02, -4.4960e-02],
          [-1.7080e-02, -1.1214e-02,  1.0135e-02],
          [-1.2342e-02, -2.3337e-03, -6.3889e-03]],

         ...,

         [[-5.3244e-03, -3.8642e-03,  8.6938e-03],
          [ 1.5621e-02, -3.3895e-02,  1.7245e-02],
          [ 3.3154e-02,  7.4353e-03,  3.0855e-02]],

         [[-2.1767e-02,  6.7622e-03, -4.4720e-04],
          [-1.1143e-02,  2.3799e-02, -4.6498e-02],
          [-9.8228e-04, -1.4422e-02, -2.0111e-02]],

         [[-1.4791e-02, -3.2146e-02,  4.2803e-03],
          [-6.8019e-03, -3.1535e-02, -8.0831e-03],
          [-8.7284e-03,  1.5963e-02, -4.1675e-02]]],


        [[[-3.1842e-03,  1.8386e-02,  2.0738e-02],
          [ 7.9698e-03,  1.3143e-02,  4.1596e-02],
          [-1.3299e-02,  2.5438e-02,  3.2506e-02]],

         [[-3.1277e-02,  1.9945e-02,  2.8939e-02],
          [-2.2890e-03,  3.2262e-02,  6.2452e-03],
          [ 3.5341e-02,  6.8308e-03, -2.0993e-02]],

         [[ 3.8804e-02,  1.5101e-02,  3.4520e-02],
          [ 1.8304e-02,  2.5780e-02,  2.0010e-02],
          [ 1.6708e-02, -5.9318e-03,  9.5277e-03]],

         ...,

         [[ 2.1169e-02, -1.6938e-02,  4.4124e-02],
          [-3.3611e-03,  3.8420e-02, -3.4350e-03],
          [ 2.4505e-02,  3.3305e-02, -3.6215e-02]],

         [[-2.0657e-02, -3.6395e-02,  3.6736e-02],
          [ 4.5237e-02, -1.3549e-02, -1.8836e-02],
          [ 1.1950e-02,  3.1085e-02,  1.3858e-02]],

         [[-1.4898e-02, -2.3187e-02,  3.0050e-02],
          [-2.9755e-02, -4.1722e-02, -2.1324e-02],
          [ 3.5885e-03,  5.9299e-03,  6.1453e-03]]],


        ...,


        [[[-4.4349e-02, -1.8202e-02,  9.4184e-03],
          [ 2.4393e-02, -1.7239e-02, -8.6409e-03],
          [-4.2937e-03,  2.1002e-02, -3.4650e-02]],

         [[-4.1709e-02,  2.7754e-02,  5.7072e-03],
          [-1.3522e-02, -1.1942e-02,  8.8178e-03],
          [-1.9474e-02,  2.8547e-02,  1.2116e-02]],

         [[ 2.4074e-02,  3.2022e-02,  1.0675e-03],
          [ 4.7820e-02, -7.7884e-03, -3.4048e-02],
          [ 3.8573e-02, -1.4808e-02, -3.1437e-03]],

         ...,

         [[-2.5804e-02, -5.3250e-02,  2.3076e-02],
          [-2.0513e-02,  2.2703e-02, -4.7180e-02],
          [ 1.6425e-02,  4.0695e-02,  2.7460e-02]],

         [[-4.6406e-02, -5.5646e-02,  1.6262e-02],
          [-1.3027e-02, -1.4844e-02, -6.4680e-03],
          [-2.0935e-02,  9.7978e-03, -9.3250e-03]],

         [[ 3.6353e-02, -2.2503e-02,  4.1207e-02],
          [ 9.4229e-03, -5.6791e-03,  4.7749e-02],
          [ 3.1322e-02,  2.7810e-02,  4.8162e-02]]],


        [[[ 2.5169e-02,  1.4148e-02, -1.3735e-02],
          [-1.3579e-02, -3.2010e-02, -2.0581e-02],
          [ 1.6061e-02, -5.8737e-02, -4.2036e-02]],

         [[-8.3622e-03,  2.0730e-02,  9.0850e-03],
          [ 1.0107e-02,  1.1400e-03, -4.0747e-02],
          [-9.4631e-03,  3.7875e-02,  1.2390e-02]],

         [[-9.4200e-03,  8.4413e-03,  4.0511e-02],
          [ 4.9059e-04,  5.3788e-03,  3.0252e-02],
          [-3.1451e-02,  2.5833e-02,  3.2555e-02]],

         ...,

         [[ 4.0251e-02,  1.9535e-02,  3.8781e-02],
          [-3.5753e-02,  9.9746e-03,  7.8928e-04],
          [-1.3001e-03, -4.1644e-02, -3.9233e-02]],

         [[ 1.6699e-02,  2.3384e-02, -9.2587e-03],
          [ 2.6093e-02,  8.0537e-03,  1.8215e-02],
          [-4.8350e-02, -2.4986e-02, -5.2651e-03]],

         [[ 2.7827e-02,  2.5198e-02, -3.1720e-02],
          [-3.3124e-02, -2.8791e-02, -1.2105e-02],
          [-8.0705e-03, -3.9881e-02,  2.2207e-02]]],


        [[[ 2.7882e-02,  9.9174e-03, -1.6670e-02],
          [-6.1860e-03,  6.7504e-03, -1.2637e-02],
          [-1.9087e-03, -3.5112e-02, -7.6573e-03]],

         [[-4.3763e-02, -2.8460e-02, -2.9495e-02],
          [ 1.0511e-02, -5.6490e-02, -1.0164e-03],
          [ 2.6874e-02, -3.6095e-02, -7.0008e-03]],

         [[-1.7248e-02,  1.3973e-02,  2.1894e-02],
          [ 1.5438e-02, -1.3677e-02,  1.0799e-02],
          [-2.2062e-02,  3.5923e-02,  3.6056e-03]],

         ...,

         [[-2.1477e-02, -9.0338e-03,  1.0015e-02],
          [ 4.1821e-03,  3.7295e-02, -4.2812e-02],
          [-2.2991e-02,  1.5702e-02,  2.4656e-02]],

         [[-3.1377e-02, -3.8655e-02,  5.5309e-03],
          [-1.9670e-02, -1.0680e-02, -4.8775e-02],
          [-2.8299e-02, -5.1229e-02, -3.9156e-02]],

         [[-4.0615e-03, -1.0303e-02,  3.1032e-02],
          [ 3.4043e-02,  4.4346e-03,  3.5712e-02],
          [-2.8899e-02,  4.5883e-02, -3.2529e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.1624e+07, 5.3891e+05, 4.1246e+05, 3.8971e+05, 2.9148e+05, 2.1182e+05,
        1.9153e+05, 1.7836e+05, 1.5205e+05, 1.3296e+05, 9.9342e+04, 9.4047e+04,
        7.7393e+04, 5.8959e+04, 4.0244e+04, 3.0175e+04, 2.9959e+04, 2.4682e+04,
        2.1023e+04, 2.0287e+04, 1.7387e+04, 1.3487e+04, 1.2250e+04, 1.2235e+04,
        1.1599e+04, 1.0463e+04, 9.6445e+03, 9.4733e+03, 8.9317e+03, 8.6452e+03,
        7.5559e+03, 6.9236e+03, 6.4099e+03, 6.3470e+03, 5.5602e+03, 5.1439e+03,
        4.7390e+03, 4.6431e+03, 4.1957e+03, 4.0549e+03, 3.8481e+03, 3.7321e+03,
        3.5064e+03, 3.3419e+03, 3.1774e+03, 3.0545e+03, 2.8911e+03, 2.8179e+03,
        2.5615e+03, 2.4268e+03, 2.3206e+03, 2.2945e+03, 2.2235e+03, 2.0746e+03,
        2.0556e+03, 1.9748e+03, 1.9268e+03, 1.8257e+03, 1.7989e+03, 1.7180e+03,
        1.6965e+03, 1.6430e+03, 1.5732e+03, 1.5582e+03, 1.5179e+03, 1.4830e+03,
        1.4693e+03, 1.3950e+03, 1.3700e+03, 1.3480e+03, 1.2736e+03, 1.2600e+03,
        1.2409e+03, 1.2092e+03, 1.1629e+03, 1.1614e+03, 1.1019e+03, 1.0745e+03,
        1.0608e+03, 1.0425e+03, 1.0012e+03, 9.8233e+02, 9.6327e+02, 9.1251e+02,
        8.9817e+02, 8.8164e+02, 8.5978e+02, 8.2855e+02, 8.1620e+02, 7.7616e+02,
        7.6028e+02, 7.4540e+02, 7.3019e+02, 7.2550e+02, 6.9272e+02, 6.7860e+02,
        6.6451e+02, 6.5086e+02, 6.3762e+02, 6.2903e+02, 6.0607e+02, 5.9832e+02,
        5.7552e+02, 5.6474e+02, 5.5987e+02, 5.3162e+02, 5.2515e+02, 5.1205e+02,
        5.0239e+02, 4.9357e+02, 4.9238e+02, 4.6980e+02, 4.6519e+02, 4.5407e+02,
        4.4027e+02, 4.3586e+02, 4.3269e+02, 4.2540e+02, 4.1452e+02, 4.0786e+02,
        3.9831e+02, 3.9113e+02, 3.8727e+02, 3.7625e+02, 3.7298e+02, 3.7009e+02,
        3.6117e+02, 3.5928e+02, 3.5581e+02, 3.4857e+02, 3.3888e+02, 3.3801e+02,
        3.3209e+02, 3.2801e+02, 3.1534e+02, 3.1337e+02, 3.0531e+02, 3.0357e+02,
        2.9921e+02, 2.9607e+02, 2.9045e+02, 2.8839e+02, 2.8751e+02, 2.7946e+02,
        2.7368e+02, 2.6916e+02, 2.6452e+02, 2.6313e+02, 2.5977e+02, 2.5172e+02,
        2.5117e+02, 2.4520e+02, 2.4047e+02, 2.3753e+02, 2.3327e+02, 2.3180e+02,
        2.2789e+02, 2.2652e+02, 2.2176e+02, 2.1962e+02, 2.1617e+02, 2.1460e+02,
        2.1008e+02, 2.0659e+02, 2.0472e+02, 2.0210e+02, 1.9789e+02, 1.9581e+02,
        1.9302e+02, 1.9008e+02, 1.8845e+02, 1.8392e+02, 1.8314e+02, 1.8242e+02,
        1.7910e+02, 1.7642e+02, 1.7490e+02, 1.7197e+02, 1.6974e+02, 1.6812e+02,
        1.6524e+02, 1.6455e+02, 1.6278e+02, 1.6147e+02, 1.5855e+02, 1.5740e+02,
        1.5460e+02, 1.5251e+02, 1.5090e+02, 1.4913e+02, 1.4673e+02, 1.4586e+02,
        1.4473e+02, 1.4263e+02, 1.3975e+02, 1.3897e+02, 1.3791e+02, 1.3695e+02,
        1.3428e+02, 1.3307e+02, 1.3192e+02, 1.3112e+02, 1.2908e+02, 1.2735e+02,
        1.2548e+02, 1.2440e+02, 1.2358e+02, 1.2185e+02, 1.2117e+02, 1.1809e+02,
        1.1774e+02, 1.1528e+02, 1.1495e+02, 1.1345e+02, 1.1262e+02, 1.1101e+02,
        1.0956e+02, 1.0814e+02, 1.0745e+02, 1.0599e+02, 1.0554e+02, 1.0432e+02,
        1.0334e+02, 1.0249e+02, 1.0162e+02, 9.9909e+01, 9.8940e+01, 9.8288e+01,
        9.7338e+01, 9.5127e+01, 9.4154e+01, 9.2838e+01, 9.2020e+01, 9.1236e+01,
        9.0674e+01, 9.0062e+01, 8.9000e+01, 8.8039e+01, 8.7397e+01, 8.6595e+01,
        8.5449e+01, 8.4757e+01, 8.4013e+01, 8.3428e+01, 8.2327e+01, 8.1266e+01,
        8.1083e+01, 8.0445e+01, 7.9477e+01, 7.9182e+01, 7.8423e+01, 7.7271e+01,
        7.7011e+01, 7.6936e+01, 7.5827e+01, 7.4301e+01, 7.3903e+01, 7.3661e+01,
        7.2880e+01, 7.2194e+01, 7.1259e+01, 7.0682e+01, 7.0098e+01, 6.9889e+01,
        6.9070e+01, 6.8889e+01, 6.7924e+01, 6.7439e+01, 6.7050e+01, 6.5891e+01,
        6.5737e+01, 6.4832e+01, 6.4227e+01, 6.3410e+01, 6.3157e+01, 6.2792e+01,
        6.2217e+01, 6.1815e+01, 6.1201e+01, 6.0557e+01, 6.0517e+01, 6.0235e+01,
        5.9227e+01, 5.8406e+01, 5.8289e+01, 5.7632e+01, 5.7200e+01, 5.6884e+01,
        5.6681e+01, 5.5861e+01, 5.5676e+01, 5.5072e+01, 5.4535e+01, 5.3997e+01,
        5.3852e+01, 5.3261e+01, 5.2561e+01, 5.1978e+01, 5.1925e+01, 5.1794e+01,
        5.1412e+01, 5.0879e+01, 5.0196e+01, 4.9678e+01, 4.9390e+01, 4.9139e+01,
        4.8725e+01, 4.8274e+01, 4.8215e+01, 4.7769e+01, 4.7277e+01, 4.6765e+01,
        4.6495e+01, 4.6221e+01, 4.5696e+01, 4.5503e+01, 4.5354e+01, 4.4705e+01,
        4.4436e+01, 4.4157e+01, 4.3397e+01, 4.3334e+01, 4.2765e+01, 4.2470e+01,
        4.2414e+01, 4.1930e+01, 4.1711e+01, 4.1203e+01, 4.0701e+01, 4.0347e+01,
        4.0180e+01, 3.9582e+01, 3.9318e+01, 3.9184e+01, 3.8890e+01, 3.8806e+01,
        3.8478e+01, 3.8161e+01, 3.7828e+01, 3.7472e+01, 3.7416e+01, 3.6967e+01,
        3.6911e+01, 3.6568e+01, 3.6137e+01, 3.6008e+01, 3.5384e+01, 3.5314e+01,
        3.4856e+01, 3.4835e+01, 3.4736e+01, 3.4312e+01, 3.4212e+01, 3.3877e+01,
        3.3794e+01, 3.3277e+01, 3.3155e+01, 3.2699e+01, 3.2548e+01, 3.2475e+01,
        3.2165e+01, 3.1863e+01, 3.1652e+01, 3.1606e+01, 3.1284e+01, 3.1040e+01,
        3.0879e+01, 3.0703e+01, 3.0430e+01, 3.0154e+01, 2.9872e+01, 2.9648e+01,
        2.9536e+01, 2.9307e+01, 2.9081e+01, 2.9016e+01, 2.8609e+01, 2.8451e+01,
        2.8233e+01, 2.8169e+01, 2.8045e+01, 2.7781e+01, 2.7600e+01, 2.7187e+01,
        2.7139e+01, 2.7083e+01, 2.6912e+01, 2.6751e+01, 2.6587e+01, 2.6475e+01,
        2.6294e+01, 2.6162e+01, 2.6049e+01, 2.5641e+01, 2.5382e+01, 2.5156e+01,
        2.5116e+01, 2.4918e+01, 2.4793e+01, 2.4711e+01, 2.4383e+01, 2.4280e+01,
        2.4106e+01, 2.3855e+01, 2.3700e+01, 2.3607e+01, 2.3539e+01, 2.3264e+01,
        2.3072e+01, 2.3032e+01, 2.2894e+01, 2.2693e+01, 2.2597e+01, 2.2448e+01,
        2.2321e+01, 2.2213e+01, 2.2111e+01, 2.1890e+01, 2.1590e+01, 2.1556e+01,
        2.1271e+01, 2.1137e+01, 2.1065e+01, 2.0880e+01, 2.0613e+01, 2.0546e+01,
        2.0285e+01, 2.0145e+01, 1.9908e+01, 1.9857e+01, 1.9746e+01, 1.9637e+01,
        1.9615e+01, 1.9444e+01, 1.9250e+01, 1.9096e+01, 1.9021e+01, 1.8968e+01,
        1.8763e+01, 1.8658e+01, 1.8471e+01, 1.8286e+01, 1.8167e+01, 1.8060e+01,
        1.7850e+01, 1.7779e+01, 1.7627e+01, 1.7521e+01, 1.7301e+01, 1.7254e+01,
        1.7178e+01, 1.7019e+01, 1.6927e+01, 1.6859e+01, 1.6639e+01, 1.6534e+01,
        1.6306e+01, 1.6207e+01, 1.6118e+01, 1.5958e+01, 1.5920e+01, 1.5790e+01,
        1.5718e+01, 1.5575e+01, 1.5420e+01, 1.5317e+01, 1.5211e+01, 1.5063e+01,
        1.4992e+01, 1.4913e+01, 1.4675e+01, 1.4654e+01, 1.4491e+01, 1.4287e+01,
        1.4235e+01, 1.4174e+01, 1.3914e+01, 1.3872e+01, 1.3726e+01, 1.3636e+01,
        1.3612e+01, 1.3468e+01, 1.3361e+01, 1.3249e+01, 1.3092e+01, 1.2999e+01,
        1.2902e+01, 1.2773e+01, 1.2748e+01, 1.2536e+01, 1.2457e+01, 1.2264e+01,
        1.2155e+01, 1.2029e+01, 1.1998e+01, 1.1791e+01, 1.1741e+01, 1.1607e+01,
        1.1509e+01, 1.1363e+01, 1.1257e+01, 1.1160e+01, 1.1065e+01, 1.0956e+01,
        1.0808e+01, 1.0758e+01, 1.0715e+01, 1.0640e+01, 1.0483e+01, 1.0404e+01,
        1.0280e+01, 1.0191e+01, 1.0067e+01, 9.9210e+00, 9.8249e+00, 9.7214e+00,
        9.5915e+00, 9.5135e+00, 9.4195e+00, 9.3259e+00, 9.1622e+00, 9.0845e+00,
        8.7856e+00, 8.7529e+00, 8.6668e+00, 8.5629e+00, 8.4884e+00, 8.3896e+00,
        8.2251e+00, 8.1458e+00, 8.0162e+00, 7.8653e+00, 7.7529e+00, 7.6713e+00,
        7.5192e+00, 7.4837e+00, 7.4261e+00, 7.2246e+00, 7.0764e+00, 7.0160e+00,
        6.8285e+00, 6.7644e+00, 6.6620e+00, 6.5006e+00, 6.4556e+00, 6.2898e+00,
        6.2488e+00, 6.1868e+00, 6.0082e+00, 5.9009e+00, 5.7278e+00, 5.6647e+00,
        5.5894e+00, 5.3510e+00, 5.1533e+00, 5.1117e+00, 4.8957e+00, 4.6728e+00,
        4.5595e+00, 4.1312e+00, 4.0666e+00, 3.9230e+00, 3.8066e+00, 3.7342e+00,
        3.4613e+00, 3.2005e+00, 3.0391e+00, 3.0196e+00, 2.8518e+00, 2.7299e+00,
        2.5604e+00, 2.1292e+00, 1.7564e+00, 1.6185e+00, 1.5075e+00, 1.1208e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 75]) 

NULL SPACE BASIS :  tensor([[-0.0005,  0.0499, -0.0247,  ...,  0.0079,  0.0113,  0.0075],
        [-0.0419, -0.0193,  0.0395,  ..., -0.0011, -0.0012, -0.0077],
        [ 0.0252, -0.0176,  0.0109,  ..., -0.0053, -0.0049,  0.0040],
        ...,
        [-0.0367,  0.0029,  0.0042,  ...,  0.0018,  0.0026, -0.0027],
        [ 0.0632,  0.0100,  0.0480,  ..., -0.0026, -0.0028,  0.0027],
        [ 0.0036, -0.0225, -0.0355,  ..., -0.0005, -0.0023,  0.0026]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0058, -0.0052,  0.0010,  ..., -0.0002,  0.0007, -0.0003],
        [-0.0052,  0.0112, -0.0068,  ...,  0.0013, -0.0002, -0.0005],
        [ 0.0010, -0.0068,  0.0074,  ..., -0.0014,  0.0002,  0.0006],
        ...,
        [-0.0002,  0.0013, -0.0014,  ...,  0.0015, -0.0009, -0.0002],
        [ 0.0007, -0.0002,  0.0002,  ..., -0.0009,  0.0029, -0.0008],
        [-0.0003, -0.0005,  0.0006,  ..., -0.0002, -0.0008,  0.0013]],
       device='cuda:0') 

reserving basis 263/576; cond: 620920.625, radio:0.0015971217071637511
PARAMETER       :  Parameter containing:
tensor([[[[-0.0140,  0.0043,  0.0143],
          [ 0.0233,  0.0075,  0.0076],
          [-0.0531, -0.0489, -0.0112]],

         [[ 0.0030, -0.0120, -0.0063],
          [ 0.0271, -0.0296,  0.0261],
          [-0.0625, -0.0072, -0.0567]],

         [[-0.0005,  0.0194,  0.0249],
          [-0.0141, -0.0336,  0.0469],
          [-0.0164, -0.0084,  0.0514]],

         ...,

         [[-0.0400,  0.0275,  0.0212],
          [-0.0110,  0.0077, -0.0068],
          [-0.0351,  0.0127,  0.0241]],

         [[-0.0174,  0.0125,  0.0014],
          [-0.0338,  0.0200, -0.0041],
          [ 0.0175, -0.0395,  0.0337]],

         [[ 0.0218, -0.0016,  0.0244],
          [ 0.0101,  0.0105,  0.0410],
          [-0.0298,  0.0198, -0.0052]]],


        [[[ 0.0317,  0.0109, -0.0227],
          [-0.0099,  0.0014,  0.0112],
          [ 0.0032, -0.0461,  0.0086]],

         [[ 0.0339,  0.0253,  0.0228],
          [-0.0202, -0.0087, -0.0243],
          [ 0.0083, -0.0297,  0.0156]],

         [[-0.0305, -0.0143, -0.0275],
          [-0.0039,  0.0129,  0.0098],
          [ 0.0350,  0.0009, -0.0427]],

         ...,

         [[ 0.0164,  0.0333,  0.0059],
          [ 0.0365,  0.0386, -0.0078],
          [-0.0085, -0.0293,  0.0359]],

         [[-0.0057,  0.0130, -0.0178],
          [-0.0050, -0.0201,  0.0152],
          [-0.0409, -0.0233, -0.0349]],

         [[ 0.0347, -0.0159,  0.0102],
          [ 0.0095,  0.0151,  0.0022],
          [ 0.0241, -0.0126,  0.0220]]],


        [[[ 0.0310, -0.0058, -0.0449],
          [ 0.0320,  0.0367,  0.0298],
          [-0.0142, -0.0071,  0.0391]],

         [[ 0.0145,  0.0033,  0.0093],
          [ 0.0188, -0.0287, -0.0029],
          [ 0.0304,  0.0278, -0.0161]],

         [[ 0.0416,  0.0023,  0.0101],
          [ 0.0143, -0.0185,  0.0388],
          [-0.0335, -0.0382,  0.0076]],

         ...,

         [[-0.0002,  0.0308, -0.0123],
          [-0.0160, -0.0455,  0.0065],
          [-0.0106, -0.0254, -0.0252]],

         [[-0.0085,  0.0246, -0.0463],
          [ 0.0333,  0.0242, -0.0224],
          [-0.0278, -0.0248,  0.0118]],

         [[ 0.0376, -0.0034, -0.0087],
          [-0.0258, -0.0260,  0.0094],
          [ 0.0029,  0.0421,  0.0345]]],


        ...,


        [[[ 0.0374, -0.0323, -0.0334],
          [ 0.0356, -0.0354, -0.0469],
          [-0.0155, -0.0215,  0.0147]],

         [[ 0.0381,  0.0166,  0.0284],
          [-0.0414, -0.0427,  0.0030],
          [-0.0482,  0.0247, -0.0385]],

         [[ 0.0138,  0.0447,  0.0060],
          [-0.0451, -0.0430,  0.0212],
          [-0.0215,  0.0181,  0.0271]],

         ...,

         [[-0.0253, -0.0242,  0.0185],
          [ 0.0026,  0.0295,  0.0367],
          [ 0.0345, -0.0492,  0.0119]],

         [[ 0.0005, -0.0229,  0.0329],
          [-0.0383, -0.0046, -0.0035],
          [-0.0084, -0.0433,  0.0054]],

         [[-0.0030, -0.0201, -0.0079],
          [ 0.0321,  0.0484, -0.0030],
          [-0.0028,  0.0312,  0.0070]]],


        [[[-0.0003, -0.0413,  0.0144],
          [-0.0411,  0.0211,  0.0330],
          [ 0.0127,  0.0282, -0.0115]],

         [[ 0.0129, -0.0207, -0.0187],
          [ 0.0069, -0.0443,  0.0017],
          [-0.0215, -0.0202, -0.0054]],

         [[ 0.0272,  0.0248,  0.0154],
          [-0.0050,  0.0059, -0.0201],
          [ 0.0329, -0.0353, -0.0011]],

         ...,

         [[-0.0214,  0.0427,  0.0050],
          [ 0.0251,  0.0235, -0.0311],
          [-0.0041, -0.0386,  0.0146]],

         [[-0.0229, -0.0144, -0.0364],
          [-0.0041,  0.0209,  0.0030],
          [ 0.0097, -0.0309,  0.0331]],

         [[ 0.0099,  0.0431,  0.0134],
          [-0.0028,  0.0219,  0.0189],
          [-0.0125,  0.0047,  0.0222]]],


        [[[ 0.0081, -0.0094, -0.0077],
          [-0.0160,  0.0172, -0.0081],
          [-0.0403, -0.0426,  0.0427]],

         [[-0.0248,  0.0042, -0.0184],
          [-0.0317, -0.0281,  0.0010],
          [-0.0336,  0.0113,  0.0108]],

         [[ 0.0004,  0.0138, -0.0148],
          [ 0.0068, -0.0369, -0.0086],
          [-0.0241,  0.0281, -0.0469]],

         ...,

         [[ 0.0403, -0.0360, -0.0338],
          [-0.0112,  0.0115,  0.0399],
          [-0.0139,  0.0312, -0.0372]],

         [[-0.0125, -0.0314, -0.0099],
          [ 0.0155,  0.0204, -0.0296],
          [ 0.0325, -0.0316,  0.0292]],

         [[-0.0035, -0.0308, -0.0274],
          [-0.0216,  0.0336,  0.0008],
          [-0.0168,  0.0395,  0.0051]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([9.4762e+06, 3.8458e+05, 2.9403e+05, 2.7989e+05, 2.6759e+05, 1.6088e+05,
        1.4032e+05, 1.3128e+05, 1.2284e+05, 8.3580e+04, 7.9246e+04, 7.6819e+04,
        6.5556e+04, 5.0073e+04, 4.5369e+04, 3.4262e+04, 3.2345e+04, 2.7795e+04,
        2.7223e+04, 2.3160e+04, 2.0138e+04, 1.8923e+04, 1.7198e+04, 1.5761e+04,
        1.4234e+04, 1.3539e+04, 1.3108e+04, 1.2179e+04, 1.1112e+04, 1.0148e+04,
        9.6109e+03, 9.1124e+03, 8.4562e+03, 7.6664e+03, 7.4658e+03, 7.2566e+03,
        6.8786e+03, 6.1055e+03, 5.9565e+03, 5.6753e+03, 5.2043e+03, 5.0282e+03,
        4.8300e+03, 4.7912e+03, 4.4485e+03, 4.1306e+03, 3.9129e+03, 3.7632e+03,
        3.6438e+03, 3.4930e+03, 3.4156e+03, 3.3257e+03, 3.2052e+03, 3.1781e+03,
        3.0765e+03, 2.8835e+03, 2.8713e+03, 2.8009e+03, 2.6428e+03, 2.5313e+03,
        2.5076e+03, 2.4349e+03, 2.3058e+03, 2.2480e+03, 2.1829e+03, 2.1544e+03,
        2.1207e+03, 2.0787e+03, 2.0470e+03, 1.9905e+03, 1.9604e+03, 1.9407e+03,
        1.8679e+03, 1.8222e+03, 1.7907e+03, 1.7640e+03, 1.7117e+03, 1.7000e+03,
        1.6668e+03, 1.6304e+03, 1.6250e+03, 1.5742e+03, 1.5181e+03, 1.5082e+03,
        1.4661e+03, 1.4532e+03, 1.4345e+03, 1.3858e+03, 1.3534e+03, 1.3334e+03,
        1.2957e+03, 1.2617e+03, 1.2462e+03, 1.2300e+03, 1.2246e+03, 1.2012e+03,
        1.1857e+03, 1.1603e+03, 1.1282e+03, 1.1063e+03, 1.0964e+03, 1.0893e+03,
        1.0707e+03, 1.0677e+03, 1.0498e+03, 1.0436e+03, 1.0184e+03, 9.9936e+02,
        9.7629e+02, 9.6072e+02, 9.4384e+02, 9.3220e+02, 9.2141e+02, 8.9557e+02,
        8.7989e+02, 8.7071e+02, 8.5824e+02, 8.4995e+02, 8.3904e+02, 8.3477e+02,
        8.2410e+02, 8.1698e+02, 8.0330e+02, 7.9038e+02, 7.8209e+02, 7.7358e+02,
        7.6786e+02, 7.5060e+02, 7.3527e+02, 7.3142e+02, 7.2336e+02, 7.0887e+02,
        7.0507e+02, 6.9877e+02, 6.9420e+02, 6.7894e+02, 6.6374e+02, 6.5756e+02,
        6.4768e+02, 6.4153e+02, 6.3219e+02, 6.3075e+02, 6.2237e+02, 6.1334e+02,
        6.0910e+02, 6.0039e+02, 5.8946e+02, 5.7943e+02, 5.6606e+02, 5.6143e+02,
        5.5981e+02, 5.5470e+02, 5.5401e+02, 5.4140e+02, 5.3327e+02, 5.3228e+02,
        5.2708e+02, 5.2443e+02, 5.1566e+02, 5.1256e+02, 5.0787e+02, 5.0131e+02,
        4.9395e+02, 4.9189e+02, 4.8194e+02, 4.7933e+02, 4.7475e+02, 4.7315e+02,
        4.6728e+02, 4.6329e+02, 4.5855e+02, 4.5250e+02, 4.4942e+02, 4.4674e+02,
        4.3820e+02, 4.3756e+02, 4.3182e+02, 4.2674e+02, 4.2382e+02, 4.2141e+02,
        4.1748e+02, 4.1145e+02, 4.1056e+02, 4.0801e+02, 4.0365e+02, 3.9679e+02,
        3.9484e+02, 3.9047e+02, 3.8870e+02, 3.8276e+02, 3.8080e+02, 3.7606e+02,
        3.6961e+02, 3.6756e+02, 3.6480e+02, 3.5922e+02, 3.5661e+02, 3.5422e+02,
        3.5268e+02, 3.5061e+02, 3.4758e+02, 3.4344e+02, 3.4000e+02, 3.3758e+02,
        3.3506e+02, 3.3295e+02, 3.3106e+02, 3.2786e+02, 3.2514e+02, 3.2092e+02,
        3.1976e+02, 3.1721e+02, 3.1523e+02, 3.0961e+02, 3.0901e+02, 3.0520e+02,
        3.0340e+02, 3.0162e+02, 2.9795e+02, 2.9693e+02, 2.9527e+02, 2.9192e+02,
        2.8983e+02, 2.8781e+02, 2.8567e+02, 2.8342e+02, 2.8208e+02, 2.7979e+02,
        2.7627e+02, 2.7414e+02, 2.7220e+02, 2.6909e+02, 2.6749e+02, 2.6597e+02,
        2.6502e+02, 2.6291e+02, 2.5908e+02, 2.5733e+02, 2.5587e+02, 2.5351e+02,
        2.4989e+02, 2.4914e+02, 2.4598e+02, 2.4494e+02, 2.4269e+02, 2.4163e+02,
        2.3962e+02, 2.3625e+02, 2.3498e+02, 2.3362e+02, 2.3159e+02, 2.3047e+02,
        2.2800e+02, 2.2782e+02, 2.2575e+02, 2.2356e+02, 2.2303e+02, 2.2051e+02,
        2.1971e+02, 2.1852e+02, 2.1649e+02, 2.1574e+02, 2.1497e+02, 2.1358e+02,
        2.1101e+02, 2.0936e+02, 2.0874e+02, 2.0707e+02, 2.0498e+02, 2.0423e+02,
        2.0248e+02, 2.0168e+02, 1.9934e+02, 1.9802e+02, 1.9735e+02, 1.9665e+02,
        1.9610e+02, 1.9489e+02, 1.9270e+02, 1.8993e+02, 1.8900e+02, 1.8790e+02,
        1.8738e+02, 1.8576e+02, 1.8412e+02, 1.8319e+02, 1.8165e+02, 1.7997e+02,
        1.7827e+02, 1.7655e+02, 1.7632e+02, 1.7535e+02, 1.7440e+02, 1.7239e+02,
        1.7224e+02, 1.6959e+02, 1.6874e+02, 1.6743e+02, 1.6692e+02, 1.6636e+02,
        1.6523e+02, 1.6472e+02, 1.6342e+02, 1.6201e+02, 1.6082e+02, 1.6012e+02,
        1.5800e+02, 1.5734e+02, 1.5647e+02, 1.5608e+02, 1.5533e+02, 1.5406e+02,
        1.5343e+02, 1.5168e+02, 1.5114e+02, 1.5026e+02, 1.4968e+02, 1.4871e+02,
        1.4826e+02, 1.4750e+02, 1.4738e+02, 1.4496e+02, 1.4462e+02, 1.4320e+02,
        1.4210e+02, 1.4139e+02, 1.4067e+02, 1.3947e+02, 1.3907e+02, 1.3763e+02,
        1.3713e+02, 1.3631e+02, 1.3528e+02, 1.3470e+02, 1.3348e+02, 1.3341e+02,
        1.3267e+02, 1.3104e+02, 1.3062e+02, 1.2940e+02, 1.2914e+02, 1.2878e+02,
        1.2767e+02, 1.2702e+02, 1.2597e+02, 1.2506e+02, 1.2407e+02, 1.2338e+02,
        1.2280e+02, 1.2245e+02, 1.2095e+02, 1.2088e+02, 1.2015e+02, 1.1958e+02,
        1.1898e+02, 1.1801e+02, 1.1715e+02, 1.1679e+02, 1.1567e+02, 1.1523e+02,
        1.1492e+02, 1.1432e+02, 1.1283e+02, 1.1211e+02, 1.1166e+02, 1.1096e+02,
        1.1084e+02, 1.1008e+02, 1.0983e+02, 1.0837e+02, 1.0799e+02, 1.0683e+02,
        1.0616e+02, 1.0562e+02, 1.0535e+02, 1.0437e+02, 1.0368e+02, 1.0342e+02,
        1.0280e+02, 1.0204e+02, 1.0161e+02, 1.0133e+02, 1.0102e+02, 9.9938e+01,
        9.9791e+01, 9.9194e+01, 9.8859e+01, 9.7385e+01, 9.6788e+01, 9.6007e+01,
        9.5204e+01, 9.4805e+01, 9.4537e+01, 9.4298e+01, 9.3825e+01, 9.3227e+01,
        9.2486e+01, 9.2030e+01, 9.1442e+01, 9.1098e+01, 9.0265e+01, 8.9549e+01,
        8.9271e+01, 8.8702e+01, 8.8288e+01, 8.8114e+01, 8.7463e+01, 8.6220e+01,
        8.5791e+01, 8.5527e+01, 8.5019e+01, 8.4410e+01, 8.4151e+01, 8.3221e+01,
        8.2222e+01, 8.2033e+01, 8.1802e+01, 8.1537e+01, 8.0833e+01, 8.0346e+01,
        8.0114e+01, 7.9580e+01, 7.8506e+01, 7.7893e+01, 7.7594e+01, 7.7253e+01,
        7.6410e+01, 7.6318e+01, 7.5952e+01, 7.5098e+01, 7.4613e+01, 7.4344e+01,
        7.3949e+01, 7.3448e+01, 7.2896e+01, 7.2115e+01, 7.1693e+01, 7.1541e+01,
        7.0903e+01, 7.0526e+01, 7.0066e+01, 6.9650e+01, 6.9464e+01, 6.9156e+01,
        6.8535e+01, 6.7855e+01, 6.7251e+01, 6.6746e+01, 6.6296e+01, 6.6107e+01,
        6.5840e+01, 6.5610e+01, 6.5283e+01, 6.4513e+01, 6.4363e+01, 6.4022e+01,
        6.3652e+01, 6.3410e+01, 6.2621e+01, 6.2255e+01, 6.1944e+01, 6.1572e+01,
        6.1365e+01, 6.1054e+01, 6.0638e+01, 6.0248e+01, 5.9777e+01, 5.9455e+01,
        5.8922e+01, 5.8618e+01, 5.7950e+01, 5.7815e+01, 5.7679e+01, 5.7016e+01,
        5.6484e+01, 5.5993e+01, 5.5776e+01, 5.5592e+01, 5.5237e+01, 5.4922e+01,
        5.4729e+01, 5.3936e+01, 5.3805e+01, 5.3701e+01, 5.3341e+01, 5.3124e+01,
        5.2699e+01, 5.2599e+01, 5.2094e+01, 5.1378e+01, 5.0697e+01, 5.0289e+01,
        5.0231e+01, 4.9748e+01, 4.9381e+01, 4.9267e+01, 4.8963e+01, 4.8836e+01,
        4.8341e+01, 4.8013e+01, 4.7669e+01, 4.7571e+01, 4.6981e+01, 4.6655e+01,
        4.5863e+01, 4.5624e+01, 4.5097e+01, 4.5083e+01, 4.4987e+01, 4.4096e+01,
        4.3636e+01, 4.3318e+01, 4.2990e+01, 4.2596e+01, 4.2572e+01, 4.2176e+01,
        4.1900e+01, 4.1243e+01, 4.0977e+01, 4.0591e+01, 4.0325e+01, 3.9852e+01,
        3.9644e+01, 3.9378e+01, 3.9283e+01, 3.8568e+01, 3.8163e+01, 3.7732e+01,
        3.7031e+01, 3.6796e+01, 3.6591e+01, 3.6381e+01, 3.5861e+01, 3.5586e+01,
        3.5262e+01, 3.4956e+01, 3.4841e+01, 3.4362e+01, 3.3799e+01, 3.3421e+01,
        3.2993e+01, 3.2738e+01, 3.2327e+01, 3.2092e+01, 3.1945e+01, 3.1343e+01,
        3.0941e+01, 3.0536e+01, 2.9997e+01, 2.9820e+01, 2.9475e+01, 2.9125e+01,
        2.8591e+01, 2.7860e+01, 2.7701e+01, 2.7515e+01, 2.7171e+01, 2.6048e+01,
        2.5919e+01, 2.5439e+01, 2.5121e+01, 2.4656e+01, 2.4196e+01, 2.3416e+01,
        2.3175e+01, 2.3035e+01, 2.2677e+01, 2.2119e+01, 2.0912e+01, 2.0757e+01,
        2.0503e+01, 1.9668e+01, 1.8096e+01, 1.7327e+01, 1.6756e+01, 1.5262e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 263]) 

NULL SPACE BASIS :  tensor([[-8.2073e-03,  9.1803e-02, -2.0295e-02,  ...,  2.9430e-02,
          8.9105e-03,  3.7228e-02],
        [ 3.8823e-03,  4.4500e-02,  3.6874e-02,  ..., -6.9256e-02,
         -2.0548e-02, -5.2189e-02],
        [ 1.9846e-02, -2.8185e-02, -4.3450e-02,  ...,  4.5487e-02,
          6.2902e-03,  2.0520e-02],
        ...,
        [-4.0907e-02, -3.2929e-02, -1.3876e-03,  ...,  4.6307e-02,
          1.1056e-01,  2.6972e-02],
        [-9.3705e-05, -2.0826e-02,  5.3486e-02,  ..., -8.9637e-02,
         -1.9456e-01, -3.1093e-02],
        [ 4.9767e-02,  6.3080e-02, -2.6626e-02,  ...,  4.5971e-02,
          1.0009e-01,  1.0919e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.8607e-02, -1.3011e-02, -5.2439e-03,  ...,  1.0370e-03,
          3.1804e-04, -7.0039e-04],
        [-1.3011e-02,  3.9117e-02, -1.2471e-02,  ...,  4.6244e-04,
          1.1043e-04, -1.3120e-05],
        [-5.2439e-03, -1.2471e-02,  3.6801e-02,  ..., -2.1592e-04,
          1.0176e-03,  2.4111e-04],
        ...,
        [ 1.0370e-03,  4.6244e-04, -2.1592e-04,  ...,  3.7350e-02,
         -1.3860e-02, -5.2445e-03],
        [ 3.1804e-04,  1.1043e-04,  1.0176e-03,  ..., -1.3860e-02,
          4.1004e-02, -1.3206e-02],
        [-7.0039e-04, -1.3120e-05,  2.4111e-04,  ..., -5.2445e-03,
         -1.3206e-02,  3.5790e-02]], device='cuda:0') 

reserving basis 242/576; cond: 1036304.5, radio:0.0009130160324275494
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0194,  0.0401,  0.0315],
          [ 0.0034, -0.0028, -0.0092],
          [-0.0352,  0.0286,  0.0169]],

         [[-0.0432, -0.0195, -0.0217],
          [-0.0476, -0.0458, -0.0278],
          [-0.0232,  0.0053, -0.0061]],

         [[ 0.0174,  0.0244, -0.0513],
          [-0.0149, -0.0291, -0.0191],
          [ 0.0106,  0.0105,  0.0189]],

         ...,

         [[-0.0552,  0.0317,  0.0450],
          [-0.0296, -0.0071, -0.0153],
          [ 0.0238, -0.0233, -0.0337]],

         [[ 0.0110, -0.0287,  0.0348],
          [ 0.0405, -0.0258, -0.0034],
          [-0.0190, -0.0330,  0.0199]],

         [[-0.0264, -0.0206,  0.0062],
          [ 0.0166,  0.0149, -0.0247],
          [-0.0263,  0.0038,  0.0046]]],


        [[[-0.0308, -0.0203, -0.0103],
          [ 0.0370,  0.0203,  0.0294],
          [-0.0136,  0.0460,  0.0016]],

         [[-0.0385, -0.0322, -0.0314],
          [-0.0001, -0.0401, -0.0246],
          [-0.0146,  0.0199,  0.0223]],

         [[-0.0091,  0.0147, -0.0226],
          [-0.0352, -0.0311,  0.0146],
          [ 0.0311,  0.0114, -0.0176]],

         ...,

         [[ 0.0247, -0.0463,  0.0171],
          [-0.0123,  0.0124, -0.0202],
          [ 0.0268,  0.0258, -0.0071]],

         [[ 0.0119, -0.0406, -0.0074],
          [-0.0136, -0.0186, -0.0413],
          [-0.0042,  0.0052, -0.0003]],

         [[-0.0013, -0.0282, -0.0109],
          [-0.0303, -0.0282, -0.0150],
          [ 0.0345,  0.0211, -0.0181]]],


        [[[-0.0400, -0.0091, -0.0022],
          [-0.0575, -0.0444, -0.0342],
          [-0.0013,  0.0353,  0.0386]],

         [[-0.0181, -0.0097, -0.0044],
          [-0.0237, -0.0405, -0.0393],
          [-0.0168, -0.0180, -0.0098]],

         [[ 0.0084, -0.0152,  0.0054],
          [-0.0361,  0.0513, -0.0082],
          [-0.0057,  0.0270,  0.0369]],

         ...,

         [[-0.0452,  0.0229,  0.0367],
          [-0.0185, -0.0398,  0.0333],
          [-0.0115, -0.0298,  0.0131]],

         [[ 0.0271,  0.0186,  0.0400],
          [ 0.0230, -0.0304, -0.0220],
          [-0.0340, -0.0500,  0.0199]],

         [[-0.0294,  0.0508,  0.0318],
          [ 0.0208,  0.0084, -0.0114],
          [-0.0154, -0.0012, -0.0374]]],


        ...,


        [[[ 0.0350,  0.0058, -0.0065],
          [ 0.0383,  0.0449,  0.0017],
          [-0.0147,  0.0119, -0.0378]],

         [[-0.0314,  0.0019, -0.0287],
          [-0.0100,  0.0346,  0.0142],
          [-0.0045, -0.0182,  0.0210]],

         [[-0.0427, -0.0408, -0.0365],
          [-0.0281,  0.0367,  0.0422],
          [-0.0109, -0.0334, -0.0282]],

         ...,

         [[ 0.0339, -0.0060,  0.0338],
          [-0.0114,  0.0428, -0.0121],
          [-0.0394,  0.0262, -0.0239]],

         [[-0.0182, -0.0426, -0.0144],
          [-0.0118,  0.0188, -0.0350],
          [ 0.0246,  0.0224,  0.0134]],

         [[ 0.0221, -0.0154,  0.0105],
          [ 0.0326, -0.0040, -0.0265],
          [-0.0405,  0.0141, -0.0116]]],


        [[[-0.0378,  0.0241, -0.0484],
          [ 0.0039, -0.0417,  0.0199],
          [-0.0165, -0.0067,  0.0363]],

         [[ 0.0181, -0.0309,  0.0079],
          [-0.0083,  0.0273,  0.0061],
          [ 0.0244, -0.0128,  0.0166]],

         [[-0.0346, -0.0414, -0.0255],
          [-0.0238, -0.0166,  0.0184],
          [ 0.0070, -0.0333, -0.0372]],

         ...,

         [[-0.0088, -0.0159, -0.0303],
          [-0.0207,  0.0283, -0.0228],
          [-0.0404,  0.0341,  0.0311]],

         [[-0.0305, -0.0108, -0.0292],
          [ 0.0371,  0.0024, -0.0164],
          [ 0.0298,  0.0322, -0.0205]],

         [[ 0.0143, -0.0265, -0.0077],
          [-0.0404, -0.0340,  0.0388],
          [ 0.0390,  0.0227,  0.0385]]],


        [[[-0.0381,  0.0316, -0.0034],
          [-0.0333,  0.0343, -0.0082],
          [-0.0376, -0.0232,  0.0305]],

         [[-0.0034, -0.0249,  0.0080],
          [ 0.0184,  0.0029, -0.0086],
          [-0.0150, -0.0025,  0.0312]],

         [[-0.0226, -0.0380, -0.0065],
          [-0.0015, -0.0021,  0.0026],
          [ 0.0356, -0.0256,  0.0045]],

         ...,

         [[ 0.0126, -0.0166, -0.0201],
          [-0.0201,  0.0309, -0.0441],
          [-0.0002,  0.0158, -0.0301]],

         [[ 0.0355, -0.0067,  0.0318],
          [ 0.0049, -0.0208, -0.0384],
          [-0.0205,  0.0083, -0.0361]],

         [[ 0.0104,  0.0410, -0.0176],
          [-0.0320,  0.0412,  0.0366],
          [ 0.0274,  0.0152,  0.0074]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.0542e+07, 4.0843e+05, 3.6227e+05, 3.0605e+05, 2.4610e+05, 1.7226e+05,
        1.5874e+05, 1.3211e+05, 1.1646e+05, 9.4831e+04, 8.7021e+04, 7.6998e+04,
        6.5701e+04, 4.7973e+04, 2.5669e+04, 2.3459e+04, 2.1615e+04, 2.0157e+04,
        1.8040e+04, 1.6808e+04, 1.5809e+04, 1.4481e+04, 1.3611e+04, 1.3281e+04,
        1.2369e+04, 1.0753e+04, 9.8699e+03, 9.5034e+03, 8.8760e+03, 8.2764e+03,
        7.7778e+03, 7.6860e+03, 7.1808e+03, 6.7676e+03, 6.2767e+03, 6.0914e+03,
        5.8352e+03, 5.7536e+03, 5.6166e+03, 5.4524e+03, 5.2598e+03, 5.0760e+03,
        4.8964e+03, 4.8089e+03, 4.6110e+03, 4.3698e+03, 4.1104e+03, 4.0253e+03,
        3.7748e+03, 3.6055e+03, 3.5231e+03, 3.3877e+03, 3.1831e+03, 3.0233e+03,
        2.8800e+03, 2.7843e+03, 2.7570e+03, 2.6463e+03, 2.5811e+03, 2.4919e+03,
        2.4425e+03, 2.3645e+03, 2.3409e+03, 2.2405e+03, 2.1754e+03, 2.1400e+03,
        2.0853e+03, 2.0224e+03, 1.9837e+03, 1.9251e+03, 1.8812e+03, 1.8242e+03,
        1.7719e+03, 1.7544e+03, 1.7291e+03, 1.6595e+03, 1.5947e+03, 1.5441e+03,
        1.5293e+03, 1.5030e+03, 1.4596e+03, 1.4246e+03, 1.4001e+03, 1.3821e+03,
        1.3702e+03, 1.3398e+03, 1.3345e+03, 1.2982e+03, 1.2708e+03, 1.2629e+03,
        1.2194e+03, 1.1892e+03, 1.1798e+03, 1.1420e+03, 1.1342e+03, 1.0996e+03,
        1.0650e+03, 1.0549e+03, 1.0308e+03, 1.0132e+03, 1.0031e+03, 9.8427e+02,
        9.5999e+02, 9.5204e+02, 9.3995e+02, 9.2822e+02, 9.1378e+02, 8.9593e+02,
        8.7698e+02, 8.6012e+02, 8.4086e+02, 8.2547e+02, 8.1847e+02, 8.0195e+02,
        7.9123e+02, 7.7848e+02, 7.5488e+02, 7.4666e+02, 7.2968e+02, 7.2574e+02,
        7.1338e+02, 7.0436e+02, 6.9113e+02, 6.8322e+02, 6.7603e+02, 6.6689e+02,
        6.5437e+02, 6.5025e+02, 6.4480e+02, 6.3910e+02, 6.2747e+02, 6.0321e+02,
        5.8676e+02, 5.8436e+02, 5.7673e+02, 5.7371e+02, 5.6475e+02, 5.5967e+02,
        5.5662e+02, 5.4865e+02, 5.3568e+02, 5.3042e+02, 5.2665e+02, 5.1689e+02,
        5.0962e+02, 5.0665e+02, 4.9518e+02, 4.9306e+02, 4.8009e+02, 4.7913e+02,
        4.7291e+02, 4.6625e+02, 4.6181e+02, 4.5822e+02, 4.4999e+02, 4.4659e+02,
        4.4188e+02, 4.3387e+02, 4.3188e+02, 4.2353e+02, 4.2094e+02, 4.1657e+02,
        4.1497e+02, 4.0969e+02, 4.0492e+02, 4.0137e+02, 3.9844e+02, 3.9207e+02,
        3.8759e+02, 3.8566e+02, 3.7994e+02, 3.7750e+02, 3.6823e+02, 3.6611e+02,
        3.6272e+02, 3.5634e+02, 3.5435e+02, 3.5275e+02, 3.4939e+02, 3.4238e+02,
        3.4017e+02, 3.3720e+02, 3.3422e+02, 3.3247e+02, 3.2908e+02, 3.2711e+02,
        3.2192e+02, 3.1495e+02, 3.1413e+02, 3.1160e+02, 3.1107e+02, 3.0696e+02,
        3.0242e+02, 2.9808e+02, 2.9468e+02, 2.9300e+02, 2.9236e+02, 2.8456e+02,
        2.8293e+02, 2.8064e+02, 2.7990e+02, 2.7866e+02, 2.7736e+02, 2.7101e+02,
        2.7072e+02, 2.6958e+02, 2.6353e+02, 2.6216e+02, 2.6085e+02, 2.5712e+02,
        2.5491e+02, 2.5348e+02, 2.5072e+02, 2.4883e+02, 2.4703e+02, 2.4324e+02,
        2.4217e+02, 2.4074e+02, 2.3912e+02, 2.3605e+02, 2.3318e+02, 2.3255e+02,
        2.2882e+02, 2.2719e+02, 2.2681e+02, 2.2490e+02, 2.2229e+02, 2.2094e+02,
        2.2044e+02, 2.1872e+02, 2.1623e+02, 2.1449e+02, 2.1275e+02, 2.1033e+02,
        2.0698e+02, 2.0605e+02, 2.0418e+02, 2.0312e+02, 2.0192e+02, 2.0097e+02,
        1.9975e+02, 1.9679e+02, 1.9633e+02, 1.9310e+02, 1.9229e+02, 1.8924e+02,
        1.8878e+02, 1.8660e+02, 1.8583e+02, 1.8456e+02, 1.8357e+02, 1.8187e+02,
        1.7952e+02, 1.7836e+02, 1.7787e+02, 1.7572e+02, 1.7442e+02, 1.7249e+02,
        1.7172e+02, 1.7123e+02, 1.7029e+02, 1.6829e+02, 1.6793e+02, 1.6716e+02,
        1.6616e+02, 1.6500e+02, 1.6231e+02, 1.6134e+02, 1.6040e+02, 1.5769e+02,
        1.5694e+02, 1.5576e+02, 1.5453e+02, 1.5374e+02, 1.5313e+02, 1.5222e+02,
        1.5089e+02, 1.4924e+02, 1.4842e+02, 1.4764e+02, 1.4585e+02, 1.4497e+02,
        1.4328e+02, 1.4323e+02, 1.4233e+02, 1.4132e+02, 1.4018e+02, 1.3969e+02,
        1.3843e+02, 1.3783e+02, 1.3672e+02, 1.3567e+02, 1.3544e+02, 1.3414e+02,
        1.3340e+02, 1.3231e+02, 1.3159e+02, 1.3016e+02, 1.2913e+02, 1.2845e+02,
        1.2698e+02, 1.2629e+02, 1.2573e+02, 1.2496e+02, 1.2427e+02, 1.2368e+02,
        1.2316e+02, 1.2211e+02, 1.2098e+02, 1.1984e+02, 1.1914e+02, 1.1834e+02,
        1.1776e+02, 1.1766e+02, 1.1616e+02, 1.1496e+02, 1.1446e+02, 1.1407e+02,
        1.1298e+02, 1.1266e+02, 1.1209e+02, 1.1048e+02, 1.0941e+02, 1.0924e+02,
        1.0854e+02, 1.0780e+02, 1.0712e+02, 1.0618e+02, 1.0559e+02, 1.0426e+02,
        1.0415e+02, 1.0328e+02, 1.0280e+02, 1.0199e+02, 1.0143e+02, 1.0093e+02,
        9.9687e+01, 9.9205e+01, 9.8223e+01, 9.7535e+01, 9.7005e+01, 9.6300e+01,
        9.5410e+01, 9.4985e+01, 9.4382e+01, 9.3831e+01, 9.3723e+01, 9.3097e+01,
        9.2482e+01, 9.1943e+01, 9.0940e+01, 9.0259e+01, 8.9794e+01, 8.9537e+01,
        8.8994e+01, 8.8137e+01, 8.7569e+01, 8.7487e+01, 8.6738e+01, 8.5761e+01,
        8.5357e+01, 8.4877e+01, 8.4307e+01, 8.4239e+01, 8.3539e+01, 8.2834e+01,
        8.2664e+01, 8.2293e+01, 8.1563e+01, 8.1050e+01, 8.0940e+01, 7.9998e+01,
        7.9657e+01, 7.8560e+01, 7.8264e+01, 7.8024e+01, 7.7352e+01, 7.6577e+01,
        7.6343e+01, 7.5925e+01, 7.5260e+01, 7.4395e+01, 7.4000e+01, 7.3628e+01,
        7.3022e+01, 7.2595e+01, 7.2112e+01, 7.1378e+01, 7.0832e+01, 7.0558e+01,
        7.0252e+01, 6.9981e+01, 6.9785e+01, 6.9595e+01, 6.8775e+01, 6.8640e+01,
        6.8453e+01, 6.7966e+01, 6.7568e+01, 6.6768e+01, 6.6402e+01, 6.5805e+01,
        6.5766e+01, 6.5193e+01, 6.4656e+01, 6.4174e+01, 6.4038e+01, 6.3542e+01,
        6.3232e+01, 6.2819e+01, 6.2563e+01, 6.2416e+01, 6.2071e+01, 6.1743e+01,
        6.1370e+01, 6.0631e+01, 6.0415e+01, 5.9981e+01, 5.9513e+01, 5.8963e+01,
        5.8747e+01, 5.8317e+01, 5.7921e+01, 5.7511e+01, 5.7379e+01, 5.6975e+01,
        5.6313e+01, 5.6240e+01, 5.5874e+01, 5.5337e+01, 5.5201e+01, 5.4702e+01,
        5.4625e+01, 5.3790e+01, 5.3714e+01, 5.3122e+01, 5.2867e+01, 5.2614e+01,
        5.2344e+01, 5.1980e+01, 5.1769e+01, 5.1511e+01, 5.1170e+01, 5.0955e+01,
        5.0592e+01, 5.0565e+01, 4.9817e+01, 4.9801e+01, 4.9392e+01, 4.8889e+01,
        4.7879e+01, 4.7622e+01, 4.7295e+01, 4.7157e+01, 4.6863e+01, 4.6439e+01,
        4.6235e+01, 4.6124e+01, 4.5810e+01, 4.5339e+01, 4.5118e+01, 4.4945e+01,
        4.4800e+01, 4.3994e+01, 4.3836e+01, 4.3685e+01, 4.3291e+01, 4.3036e+01,
        4.2766e+01, 4.2647e+01, 4.2444e+01, 4.1969e+01, 4.1558e+01, 4.1270e+01,
        4.1150e+01, 4.0864e+01, 4.0522e+01, 4.0136e+01, 3.9965e+01, 3.9745e+01,
        3.9472e+01, 3.9169e+01, 3.8888e+01, 3.8727e+01, 3.8451e+01, 3.8160e+01,
        3.8035e+01, 3.7945e+01, 3.7736e+01, 3.7374e+01, 3.7107e+01, 3.6595e+01,
        3.6212e+01, 3.6011e+01, 3.5744e+01, 3.5488e+01, 3.5087e+01, 3.4832e+01,
        3.4553e+01, 3.4367e+01, 3.4156e+01, 3.3707e+01, 3.3476e+01, 3.3317e+01,
        3.2850e+01, 3.2616e+01, 3.2420e+01, 3.2239e+01, 3.1883e+01, 3.1699e+01,
        3.1412e+01, 3.1145e+01, 3.0678e+01, 3.0381e+01, 3.0242e+01, 2.9975e+01,
        2.9711e+01, 2.9583e+01, 2.9126e+01, 2.8798e+01, 2.8698e+01, 2.8336e+01,
        2.7957e+01, 2.7753e+01, 2.7603e+01, 2.7343e+01, 2.7082e+01, 2.7017e+01,
        2.6844e+01, 2.6445e+01, 2.6022e+01, 2.5934e+01, 2.5677e+01, 2.5493e+01,
        2.5225e+01, 2.4895e+01, 2.4852e+01, 2.4574e+01, 2.4057e+01, 2.3989e+01,
        2.3765e+01, 2.3379e+01, 2.3301e+01, 2.2625e+01, 2.2257e+01, 2.1987e+01,
        2.1907e+01, 2.1879e+01, 2.1264e+01, 2.1175e+01, 2.0831e+01, 2.0503e+01,
        2.0432e+01, 2.0152e+01, 1.9642e+01, 1.9407e+01, 1.9224e+01, 1.9059e+01,
        1.8614e+01, 1.8365e+01, 1.8241e+01, 1.7651e+01, 1.7488e+01, 1.7049e+01,
        1.6687e+01, 1.6296e+01, 1.5904e+01, 1.5589e+01, 1.5399e+01, 1.4314e+01,
        1.3815e+01, 1.3467e+01, 1.3020e+01, 1.1823e+01, 1.1373e+01, 1.0173e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 242]) 

NULL SPACE BASIS :  tensor([[ 0.0179,  0.0178,  0.0411,  ..., -0.0046,  0.0076, -0.0204],
        [ 0.0056, -0.0250,  0.0044,  ...,  0.0042, -0.0118,  0.0258],
        [ 0.0198,  0.0172, -0.0462,  ..., -0.0035,  0.0083, -0.0081],
        ...,
        [-0.0800,  0.0571,  0.0080,  ..., -0.0135, -0.0023, -0.0070],
        [ 0.0686, -0.0218, -0.0163,  ...,  0.0036,  0.0028,  0.0114],
        [ 0.0055, -0.0310,  0.0070,  ...,  0.0039, -0.0033, -0.0028]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0249, -0.0169, -0.0012,  ..., -0.0004,  0.0003, -0.0003],
        [-0.0169,  0.0341, -0.0130,  ..., -0.0017,  0.0005, -0.0012],
        [-0.0012, -0.0130,  0.0203,  ..., -0.0003, -0.0005,  0.0008],
        ...,
        [-0.0004, -0.0017, -0.0003,  ...,  0.0207, -0.0097, -0.0015],
        [ 0.0003,  0.0005, -0.0005,  ..., -0.0097,  0.0267, -0.0093],
        [-0.0003, -0.0012,  0.0008,  ..., -0.0015, -0.0093,  0.0216]],
       device='cuda:0') 

reserving basis 334/576; cond: 242966.671875, radio:0.004961597267538309
PARAMETER       :  Parameter containing:
tensor([[[[-0.0277, -0.0049,  0.0195],
          [-0.0237, -0.0265, -0.0125],
          [ 0.0114,  0.0119,  0.0076]],

         [[ 0.0005,  0.0153,  0.0058],
          [-0.0051,  0.0009,  0.0187],
          [-0.0200, -0.0171, -0.0470]],

         [[ 0.0025,  0.0428,  0.0428],
          [-0.0030, -0.0370, -0.0040],
          [-0.0359, -0.0040, -0.0232]],

         ...,

         [[-0.0197,  0.0438,  0.0030],
          [-0.0268, -0.0084,  0.0420],
          [-0.0145, -0.0374, -0.0164]],

         [[ 0.0291,  0.0155, -0.0442],
          [ 0.0282,  0.0055,  0.0470],
          [-0.0329,  0.0217, -0.0216]],

         [[ 0.0346,  0.0221,  0.0215],
          [-0.0358, -0.0093,  0.0266],
          [ 0.0303, -0.0160, -0.0271]]],


        [[[-0.0266, -0.0236, -0.0477],
          [ 0.0095, -0.0093,  0.0090],
          [-0.0527,  0.0068,  0.0017]],

         [[-0.0321,  0.0180,  0.0507],
          [ 0.0033,  0.0229, -0.0120],
          [-0.0138,  0.0299,  0.0469]],

         [[-0.0011, -0.0165,  0.0305],
          [-0.0001, -0.0310,  0.0367],
          [ 0.0341,  0.0331, -0.0026]],

         ...,

         [[-0.0068,  0.0210, -0.0226],
          [-0.0263,  0.0231,  0.0141],
          [-0.0387,  0.0267, -0.0448]],

         [[-0.0080,  0.0218, -0.0233],
          [-0.0184, -0.0293, -0.0366],
          [-0.0450, -0.0106, -0.0387]],

         [[ 0.0232, -0.0382, -0.0354],
          [-0.0207, -0.0342, -0.0163],
          [-0.0226,  0.0014,  0.0098]]],


        [[[-0.0051, -0.0060, -0.0166],
          [-0.0004, -0.0442, -0.0387],
          [-0.0397, -0.0004, -0.0127]],

         [[-0.0255, -0.0234, -0.0196],
          [ 0.0013, -0.0264, -0.0115],
          [ 0.0164, -0.0265,  0.0006]],

         [[ 0.0490,  0.0412,  0.0023],
          [ 0.0294,  0.0043, -0.0395],
          [ 0.0120,  0.0086, -0.0010]],

         ...,

         [[ 0.0140, -0.0131, -0.0027],
          [ 0.0143,  0.0164, -0.0410],
          [-0.0351,  0.0104, -0.0126]],

         [[ 0.0115,  0.0066, -0.0255],
          [-0.0276,  0.0101,  0.0249],
          [ 0.0046, -0.0228,  0.0004]],

         [[ 0.0178, -0.0123, -0.0318],
          [ 0.0112, -0.0335, -0.0336],
          [ 0.0319, -0.0149,  0.0014]]],


        ...,


        [[[ 0.0141,  0.0320,  0.0140],
          [-0.0023, -0.0152, -0.0192],
          [ 0.0510,  0.0227, -0.0547]],

         [[ 0.0044, -0.0271,  0.0031],
          [-0.0299,  0.0243, -0.0195],
          [ 0.0196, -0.0255,  0.0436]],

         [[ 0.0254, -0.0190,  0.0435],
          [ 0.0318, -0.0383, -0.0392],
          [ 0.0172, -0.0138, -0.0036]],

         ...,

         [[-0.0174, -0.0140,  0.0027],
          [ 0.0128,  0.0087,  0.0144],
          [ 0.0032, -0.0112,  0.0042]],

         [[ 0.0146,  0.0224, -0.0465],
          [-0.0297, -0.0202, -0.0036],
          [ 0.0090,  0.0090,  0.0156]],

         [[ 0.0302,  0.0066,  0.0277],
          [ 0.0159,  0.0444,  0.0018],
          [ 0.0389, -0.0083,  0.0200]]],


        [[[-0.0119,  0.0200,  0.0348],
          [ 0.0167,  0.0244,  0.0074],
          [-0.0028,  0.0290, -0.0510]],

         [[ 0.0351, -0.0142, -0.0269],
          [-0.0371,  0.0045, -0.0358],
          [-0.0388, -0.0040,  0.0405]],

         [[ 0.0193, -0.0417, -0.0245],
          [-0.0359,  0.0271, -0.0175],
          [-0.0354,  0.0143, -0.0031]],

         ...,

         [[-0.0183, -0.0285,  0.0146],
          [ 0.0117,  0.0382,  0.0087],
          [-0.0200, -0.0331, -0.0052]],

         [[-0.0200,  0.0376, -0.0253],
          [ 0.0057, -0.0158,  0.0388],
          [ 0.0462, -0.0017, -0.0095]],

         [[-0.0226,  0.0055,  0.0292],
          [-0.0335, -0.0221,  0.0101],
          [ 0.0105,  0.0115, -0.0021]]],


        [[[ 0.0382,  0.0430,  0.0010],
          [ 0.0201, -0.0111,  0.0259],
          [ 0.0060, -0.0348, -0.0509]],

         [[ 0.0461, -0.0053,  0.0340],
          [ 0.0191,  0.0344,  0.0088],
          [-0.0006, -0.0087,  0.0362]],

         [[-0.0232,  0.0002, -0.0420],
          [-0.0044, -0.0080,  0.0406],
          [ 0.0206, -0.0164,  0.0275]],

         ...,

         [[-0.0076,  0.0371, -0.0333],
          [-0.0093, -0.0387,  0.0275],
          [-0.0371,  0.0067, -0.0002]],

         [[ 0.0021, -0.0368, -0.0150],
          [-0.0368,  0.0262, -0.0206],
          [-0.0239, -0.0400,  0.0077]],

         [[ 0.0416, -0.0052,  0.0352],
          [ 0.0337, -0.0140,  0.0242],
          [ 0.0005,  0.0262,  0.0243]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.1129e+07, 3.1505e+05, 2.7209e+05, 2.3440e+05, 2.0028e+05, 1.3703e+05,
        1.3208e+05, 1.1299e+05, 1.0315e+05, 8.9729e+04, 8.6774e+04, 7.5537e+04,
        5.5141e+04, 4.3111e+04, 3.5043e+04, 3.0240e+04, 2.7895e+04, 2.6781e+04,
        2.5608e+04, 2.2157e+04, 2.0733e+04, 1.9931e+04, 1.8868e+04, 1.7288e+04,
        1.6406e+04, 1.6084e+04, 1.5548e+04, 1.4490e+04, 1.3901e+04, 1.2949e+04,
        1.1340e+04, 1.0932e+04, 1.0824e+04, 9.5750e+03, 9.2444e+03, 8.9800e+03,
        8.3896e+03, 8.0185e+03, 7.5991e+03, 7.2572e+03, 7.0774e+03, 6.6882e+03,
        6.5087e+03, 6.1064e+03, 5.8438e+03, 5.6644e+03, 5.5993e+03, 5.4651e+03,
        5.3282e+03, 5.1739e+03, 4.9508e+03, 4.8538e+03, 4.7278e+03, 4.5025e+03,
        4.3905e+03, 4.2450e+03, 4.2132e+03, 4.1894e+03, 3.9381e+03, 3.8751e+03,
        3.8160e+03, 3.6787e+03, 3.5872e+03, 3.4448e+03, 3.4037e+03, 3.2252e+03,
        3.1743e+03, 3.1620e+03, 3.0463e+03, 3.0029e+03, 2.9011e+03, 2.8592e+03,
        2.8372e+03, 2.8199e+03, 2.7752e+03, 2.6971e+03, 2.6380e+03, 2.5854e+03,
        2.5443e+03, 2.4939e+03, 2.4610e+03, 2.4186e+03, 2.3470e+03, 2.3117e+03,
        2.2857e+03, 2.2129e+03, 2.1954e+03, 2.1442e+03, 2.0948e+03, 2.0696e+03,
        2.0371e+03, 2.0168e+03, 2.0023e+03, 1.9557e+03, 1.8805e+03, 1.8668e+03,
        1.8437e+03, 1.8026e+03, 1.7911e+03, 1.7746e+03, 1.7202e+03, 1.7055e+03,
        1.6523e+03, 1.6264e+03, 1.6124e+03, 1.5964e+03, 1.5785e+03, 1.5484e+03,
        1.5406e+03, 1.5131e+03, 1.4947e+03, 1.4793e+03, 1.4589e+03, 1.4487e+03,
        1.4153e+03, 1.4015e+03, 1.3900e+03, 1.3705e+03, 1.3478e+03, 1.3412e+03,
        1.3222e+03, 1.3093e+03, 1.2890e+03, 1.2786e+03, 1.2708e+03, 1.2660e+03,
        1.2440e+03, 1.2260e+03, 1.2163e+03, 1.2059e+03, 1.1984e+03, 1.1692e+03,
        1.1570e+03, 1.1332e+03, 1.1327e+03, 1.1120e+03, 1.1103e+03, 1.0816e+03,
        1.0660e+03, 1.0546e+03, 1.0522e+03, 1.0463e+03, 1.0264e+03, 1.0156e+03,
        1.0103e+03, 1.0062e+03, 9.9437e+02, 9.8086e+02, 9.7433e+02, 9.5811e+02,
        9.4707e+02, 9.3908e+02, 9.3204e+02, 9.2199e+02, 9.1185e+02, 9.0167e+02,
        8.9205e+02, 8.8721e+02, 8.8215e+02, 8.7550e+02, 8.7390e+02, 8.6034e+02,
        8.5759e+02, 8.4713e+02, 8.3889e+02, 8.3467e+02, 8.2825e+02, 8.1525e+02,
        8.1140e+02, 8.0566e+02, 7.9245e+02, 7.8852e+02, 7.8579e+02, 7.7790e+02,
        7.6901e+02, 7.6294e+02, 7.5516e+02, 7.4864e+02, 7.4236e+02, 7.3934e+02,
        7.3242e+02, 7.2329e+02, 7.1332e+02, 7.1120e+02, 7.0550e+02, 7.0334e+02,
        7.0154e+02, 6.9234e+02, 6.8453e+02, 6.7672e+02, 6.7645e+02, 6.6103e+02,
        6.5928e+02, 6.5372e+02, 6.4960e+02, 6.4818e+02, 6.4073e+02, 6.3733e+02,
        6.2875e+02, 6.2662e+02, 6.2578e+02, 6.2174e+02, 6.1124e+02, 6.0679e+02,
        6.0171e+02, 5.9997e+02, 5.9822e+02, 5.9221e+02, 5.8923e+02, 5.8441e+02,
        5.7914e+02, 5.7301e+02, 5.6791e+02, 5.6030e+02, 5.5838e+02, 5.5212e+02,
        5.4941e+02, 5.4829e+02, 5.4221e+02, 5.3532e+02, 5.3433e+02, 5.3121e+02,
        5.2821e+02, 5.2683e+02, 5.2275e+02, 5.1944e+02, 5.1776e+02, 5.1502e+02,
        5.0856e+02, 5.0405e+02, 5.0180e+02, 4.9907e+02, 4.9628e+02, 4.9367e+02,
        4.8856e+02, 4.8653e+02, 4.8396e+02, 4.7892e+02, 4.7554e+02, 4.6995e+02,
        4.6896e+02, 4.6358e+02, 4.5726e+02, 4.5487e+02, 4.5267e+02, 4.5207e+02,
        4.5001e+02, 4.4680e+02, 4.4403e+02, 4.4306e+02, 4.4134e+02, 4.3707e+02,
        4.3444e+02, 4.2987e+02, 4.2792e+02, 4.2362e+02, 4.2290e+02, 4.1904e+02,
        4.1720e+02, 4.1014e+02, 4.0687e+02, 4.0588e+02, 4.0491e+02, 4.0188e+02,
        4.0020e+02, 3.9658e+02, 3.9391e+02, 3.9320e+02, 3.9002e+02, 3.8898e+02,
        3.8409e+02, 3.8261e+02, 3.8194e+02, 3.7952e+02, 3.7744e+02, 3.7594e+02,
        3.7038e+02, 3.6961e+02, 3.6725e+02, 3.6476e+02, 3.6291e+02, 3.6004e+02,
        3.5790e+02, 3.5730e+02, 3.5450e+02, 3.5245e+02, 3.4982e+02, 3.4823e+02,
        3.4682e+02, 3.4504e+02, 3.4345e+02, 3.4294e+02, 3.3911e+02, 3.3717e+02,
        3.3454e+02, 3.3258e+02, 3.3149e+02, 3.2947e+02, 3.2457e+02, 3.2447e+02,
        3.2255e+02, 3.2116e+02, 3.1968e+02, 3.1680e+02, 3.1484e+02, 3.1419e+02,
        3.1363e+02, 3.1111e+02, 3.0985e+02, 3.0805e+02, 3.0515e+02, 3.0378e+02,
        3.0295e+02, 3.0251e+02, 3.0179e+02, 3.0035e+02, 2.9858e+02, 2.9735e+02,
        2.9485e+02, 2.9374e+02, 2.9022e+02, 2.8941e+02, 2.8770e+02, 2.8643e+02,
        2.8549e+02, 2.8265e+02, 2.8113e+02, 2.8049e+02, 2.7858e+02, 2.7826e+02,
        2.7522e+02, 2.7422e+02, 2.7385e+02, 2.7214e+02, 2.7043e+02, 2.6871e+02,
        2.6524e+02, 2.6428e+02, 2.6375e+02, 2.6111e+02, 2.5947e+02, 2.5782e+02,
        2.5747e+02, 2.5620e+02, 2.5435e+02, 2.5290e+02, 2.5069e+02, 2.4946e+02,
        2.4795e+02, 2.4778e+02, 2.4728e+02, 2.4595e+02, 2.4372e+02, 2.4231e+02,
        2.4116e+02, 2.3988e+02, 2.3823e+02, 2.3768e+02, 2.3690e+02, 2.3460e+02,
        2.3401e+02, 2.3381e+02, 2.3135e+02, 2.3084e+02, 2.2932e+02, 2.2918e+02,
        2.2677e+02, 2.2633e+02, 2.2444e+02, 2.2233e+02, 2.2190e+02, 2.2018e+02,
        2.2001e+02, 2.1833e+02, 2.1757e+02, 2.1722e+02, 2.1654e+02, 2.1460e+02,
        2.1415e+02, 2.1342e+02, 2.1170e+02, 2.1058e+02, 2.0961e+02, 2.0888e+02,
        2.0732e+02, 2.0662e+02, 2.0437e+02, 2.0398e+02, 2.0322e+02, 2.0126e+02,
        2.0057e+02, 1.9936e+02, 1.9865e+02, 1.9644e+02, 1.9634e+02, 1.9463e+02,
        1.9415e+02, 1.9374e+02, 1.9204e+02, 1.9179e+02, 1.9063e+02, 1.8913e+02,
        1.8834e+02, 1.8687e+02, 1.8658e+02, 1.8561e+02, 1.8508e+02, 1.8447e+02,
        1.8268e+02, 1.8197e+02, 1.8143e+02, 1.8098e+02, 1.7936e+02, 1.7815e+02,
        1.7741e+02, 1.7631e+02, 1.7481e+02, 1.7345e+02, 1.7181e+02, 1.7137e+02,
        1.7104e+02, 1.6945e+02, 1.6853e+02, 1.6818e+02, 1.6716e+02, 1.6679e+02,
        1.6571e+02, 1.6523e+02, 1.6439e+02, 1.6274e+02, 1.6257e+02, 1.6146e+02,
        1.6075e+02, 1.5924e+02, 1.5842e+02, 1.5786e+02, 1.5723e+02, 1.5645e+02,
        1.5591e+02, 1.5433e+02, 1.5404e+02, 1.5337e+02, 1.5223e+02, 1.5133e+02,
        1.4984e+02, 1.4969e+02, 1.4855e+02, 1.4795e+02, 1.4739e+02, 1.4610e+02,
        1.4537e+02, 1.4475e+02, 1.4395e+02, 1.4332e+02, 1.4281e+02, 1.4203e+02,
        1.4118e+02, 1.4041e+02, 1.3982e+02, 1.3911e+02, 1.3820e+02, 1.3763e+02,
        1.3717e+02, 1.3618e+02, 1.3550e+02, 1.3519e+02, 1.3447e+02, 1.3302e+02,
        1.3253e+02, 1.3216e+02, 1.3188e+02, 1.3058e+02, 1.3025e+02, 1.2905e+02,
        1.2767e+02, 1.2755e+02, 1.2731e+02, 1.2625e+02, 1.2520e+02, 1.2443e+02,
        1.2393e+02, 1.2278e+02, 1.2236e+02, 1.2148e+02, 1.2094e+02, 1.1978e+02,
        1.1869e+02, 1.1788e+02, 1.1723e+02, 1.1694e+02, 1.1638e+02, 1.1483e+02,
        1.1472e+02, 1.1393e+02, 1.1306e+02, 1.1282e+02, 1.1244e+02, 1.1126e+02,
        1.1073e+02, 1.1010e+02, 1.0927e+02, 1.0890e+02, 1.0775e+02, 1.0737e+02,
        1.0595e+02, 1.0567e+02, 1.0491e+02, 1.0363e+02, 1.0328e+02, 1.0257e+02,
        1.0230e+02, 1.0126e+02, 1.0075e+02, 1.0023e+02, 9.9741e+01, 9.8648e+01,
        9.8290e+01, 9.7242e+01, 9.6427e+01, 9.5723e+01, 9.5110e+01, 9.4635e+01,
        9.3365e+01, 9.2937e+01, 9.1686e+01, 9.1500e+01, 9.1259e+01, 9.0403e+01,
        9.0068e+01, 8.8539e+01, 8.7867e+01, 8.7124e+01, 8.6290e+01, 8.5308e+01,
        8.4891e+01, 8.2907e+01, 8.2801e+01, 8.2007e+01, 8.1788e+01, 8.1325e+01,
        8.0693e+01, 7.9716e+01, 7.9335e+01, 7.8219e+01, 7.8070e+01, 7.7018e+01,
        7.5646e+01, 7.5012e+01, 7.4499e+01, 7.4018e+01, 7.3306e+01, 7.3010e+01,
        7.0988e+01, 7.0437e+01, 6.9982e+01, 6.9397e+01, 6.8562e+01, 6.7157e+01,
        6.5758e+01, 6.4725e+01, 6.4377e+01, 6.3381e+01, 6.3155e+01, 6.1218e+01,
        6.0512e+01, 5.9457e+01, 5.9009e+01, 5.7199e+01, 5.5628e+01, 5.4766e+01,
        5.4101e+01, 5.3229e+01, 5.1234e+01, 4.8372e+01, 4.7127e+01, 4.5805e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 334]) 

NULL SPACE BASIS :  tensor([[ 0.0002, -0.0807,  0.1079,  ..., -0.0009, -0.0010, -0.0060],
        [-0.0069, -0.0259,  0.0871,  ..., -0.0043,  0.0126,  0.0015],
        [-0.0679, -0.0377, -0.0509,  ..., -0.0054, -0.0007,  0.0023],
        ...,
        [-0.0187,  0.0221, -0.0285,  ...,  0.0179,  0.0318, -0.0059],
        [ 0.0383,  0.0358, -0.0408,  ..., -0.0134, -0.0406,  0.0165],
        [ 0.0458, -0.0297, -0.0264,  ..., -0.0007,  0.0151, -0.0141]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0275, -0.0087, -0.0010,  ...,  0.0016, -0.0002, -0.0004],
        [-0.0087,  0.0310, -0.0092,  ..., -0.0004,  0.0005, -0.0002],
        [-0.0010, -0.0092,  0.0240,  ..., -0.0006, -0.0008, -0.0006],
        ...,
        [ 0.0016, -0.0004, -0.0006,  ...,  0.0335, -0.0071, -0.0041],
        [-0.0002,  0.0005, -0.0008,  ..., -0.0071,  0.0368, -0.0077],
        [-0.0004, -0.0002, -0.0006,  ..., -0.0041, -0.0077,  0.0319]],
       device='cuda:0') 

reserving basis 306/576; cond: 458030.3125, radio:0.0024403315037488937
PARAMETER       :  Parameter containing:
tensor([[[[-2.7130e-02, -1.2491e-02, -2.4555e-02],
          [ 3.3850e-02, -1.6434e-02, -2.4046e-02],
          [ 1.6539e-02, -7.4605e-03,  4.4623e-02]],

         [[ 3.4912e-02, -7.2017e-03, -2.6438e-02],
          [ 3.2824e-02,  6.6098e-03,  3.6192e-02],
          [ 8.5062e-03, -2.0817e-02,  1.7474e-03]],

         [[-7.0568e-03, -2.2833e-02, -1.7368e-02],
          [ 2.6196e-02, -4.7039e-02,  1.9799e-02],
          [-3.2051e-02,  2.7465e-03, -1.0908e-02]],

         ...,

         [[-1.7408e-02,  1.5396e-02,  2.5133e-02],
          [ 1.4632e-02, -1.3025e-02, -1.0173e-02],
          [ 1.5420e-02, -1.4878e-02, -3.2846e-02]],

         [[ 3.3310e-03,  3.2070e-02,  4.3614e-02],
          [ 4.5799e-03,  2.3681e-02,  2.3360e-02],
          [ 5.2622e-03,  9.6858e-03,  3.3888e-02]],

         [[-4.0473e-02, -2.3619e-02, -1.2975e-02],
          [-2.1043e-02, -2.2644e-02,  2.2856e-02],
          [-3.5410e-02, -2.0009e-03, -3.7455e-02]]],


        [[[ 5.5036e-03, -1.7825e-02, -5.0138e-02],
          [-1.1581e-02, -2.3253e-02, -4.4673e-02],
          [-5.9650e-03, -1.9028e-02,  1.2666e-02]],

         [[-1.4385e-02, -5.1517e-03,  9.8666e-03],
          [ 2.0687e-02,  3.9775e-02,  2.5816e-02],
          [-1.2914e-02, -4.8428e-03,  7.3485e-03]],

         [[-2.1512e-02,  1.0094e-02, -1.6444e-02],
          [ 2.7980e-02, -3.3934e-02,  3.8002e-02],
          [-1.1869e-02,  7.9008e-03, -5.5966e-04]],

         ...,

         [[ 3.6879e-03,  4.2401e-02, -1.3681e-02],
          [-1.5182e-03, -8.2044e-04, -2.0417e-02],
          [-2.5807e-02, -1.0381e-02, -1.2019e-02]],

         [[-5.1005e-02,  3.2580e-03,  1.4760e-02],
          [ 1.4167e-02, -1.5139e-03, -1.0986e-02],
          [ 4.8271e-02, -9.4136e-03,  7.4662e-05]],

         [[-4.1885e-02,  7.5541e-03,  1.5126e-02],
          [-1.4181e-02,  2.6624e-02, -9.3311e-03],
          [ 2.3487e-02,  3.6056e-02,  3.7826e-02]]],


        [[[-3.9811e-02,  2.2712e-02,  1.3825e-02],
          [ 3.1379e-02,  1.8075e-02,  2.9717e-02],
          [ 3.3218e-02,  1.6079e-02,  2.2375e-02]],

         [[-5.2497e-03,  4.6113e-03, -8.3152e-03],
          [ 2.4280e-02, -2.1340e-02,  2.0642e-02],
          [-9.3971e-03, -3.6425e-02, -2.5781e-02]],

         [[ 4.3231e-02, -1.5324e-02, -9.4611e-03],
          [ 1.0742e-02,  3.6126e-02, -4.7524e-03],
          [-2.2497e-02,  1.2214e-02,  3.8030e-02]],

         ...,

         [[ 1.9378e-02, -2.4250e-02,  2.3657e-02],
          [ 4.3196e-02,  1.2510e-02, -2.0758e-02],
          [ 3.4193e-02,  2.5049e-02, -1.3241e-02]],

         [[ 2.0377e-02, -4.0580e-02, -1.7225e-02],
          [-2.9458e-02,  2.7090e-02,  5.6603e-03],
          [ 3.5256e-03,  2.9562e-02,  2.0668e-02]],

         [[-2.3667e-02,  8.5102e-03,  7.4149e-03],
          [-2.4758e-02,  4.1655e-03, -3.4546e-02],
          [ 3.7814e-03,  1.6913e-02,  4.8420e-03]]],


        ...,


        [[[-1.4950e-02, -2.8864e-02, -3.5433e-02],
          [ 2.3500e-04,  1.1780e-02, -4.9584e-02],
          [-9.7998e-03, -9.8213e-03, -3.8277e-02]],

         [[-2.1065e-02,  2.7195e-02,  3.9231e-02],
          [-1.5217e-02, -2.9930e-02, -3.1714e-02],
          [-1.3024e-02,  3.8099e-02,  4.2941e-02]],

         [[-3.0915e-03, -1.8457e-02,  4.0269e-02],
          [-5.5686e-03,  2.9621e-02,  3.8736e-02],
          [ 2.5996e-03,  4.2337e-02,  1.8492e-02]],

         ...,

         [[ 3.0126e-02, -4.3009e-02, -3.5711e-02],
          [ 1.6573e-02,  2.1364e-02, -1.0018e-02],
          [ 1.5228e-03, -4.7028e-03,  8.7149e-03]],

         [[ 2.2762e-02, -8.2395e-04,  9.1619e-03],
          [ 2.6304e-02,  8.5239e-03, -3.9714e-02],
          [ 4.5588e-02,  3.7922e-02, -3.8790e-02]],

         [[ 2.8896e-03,  4.2231e-02, -2.3162e-04],
          [ 3.0364e-02, -3.4825e-02, -7.1193e-03],
          [ 3.9135e-03, -3.2923e-02,  1.8706e-02]]],


        [[[ 3.6487e-02, -1.2175e-02,  4.5612e-02],
          [-4.7049e-02,  2.0647e-02, -1.4222e-02],
          [ 3.0704e-02, -1.4086e-02, -2.0430e-02]],

         [[ 2.6305e-02, -2.2225e-02, -1.3071e-02],
          [ 2.4246e-02,  8.0956e-03, -2.2444e-02],
          [ 5.2162e-02, -2.3599e-02,  4.5925e-02]],

         [[-3.1429e-02, -3.4711e-02,  2.7784e-02],
          [ 1.6250e-02, -3.6714e-02, -5.4311e-02],
          [-3.8134e-02,  4.8991e-04, -1.3036e-02]],

         ...,

         [[-1.9999e-02,  1.9594e-02,  3.1987e-02],
          [-3.5398e-02,  9.5351e-03, -9.3604e-03],
          [ 3.1690e-02, -3.4037e-02,  3.5774e-02]],

         [[-5.2949e-03, -1.8665e-02, -3.2497e-02],
          [ 2.4104e-02, -1.0325e-02,  1.3261e-02],
          [-7.9532e-04,  3.4966e-02,  1.3044e-02]],

         [[-3.0437e-02, -1.4163e-02, -4.1696e-02],
          [ 2.6535e-02, -4.2439e-02, -2.9793e-02],
          [-3.6440e-02,  4.3903e-02,  1.6713e-02]]],


        [[[-1.2372e-03, -4.8231e-03,  1.5462e-02],
          [-3.8777e-02, -6.5042e-03, -7.9201e-03],
          [ 2.5130e-02,  4.1975e-03,  3.7388e-02]],

         [[-2.2837e-02,  2.8455e-02,  1.5153e-02],
          [ 3.8458e-02,  2.6005e-02,  1.8456e-02],
          [-5.2651e-03,  2.3797e-02,  1.2210e-02]],

         [[-1.4465e-02, -2.7009e-02,  3.5467e-02],
          [-3.7246e-02,  2.0182e-02,  1.7203e-02],
          [ 3.4821e-03, -2.7333e-02, -3.3277e-02]],

         ...,

         [[ 5.1432e-02, -2.0433e-02,  2.7402e-02],
          [ 3.3455e-02, -3.8668e-02,  2.7775e-02],
          [-8.0741e-03, -4.2032e-02, -1.9543e-02]],

         [[-1.6109e-02, -2.1417e-04, -1.8555e-02],
          [-3.0639e-02, -4.8458e-02, -3.9205e-02],
          [ 2.2167e-02,  1.8833e-02, -1.4456e-02]],

         [[-8.6350e-03, -1.3230e-02,  4.7272e-02],
          [ 2.3347e-02,  1.8101e-02,  2.4400e-03],
          [ 1.1753e-02,  3.0164e-02, -6.2329e-04]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.5273e+06, 1.2962e+05, 1.1389e+05, 6.9883e+04, 5.2124e+04, 3.0153e+04,
        2.8632e+04, 1.8259e+04, 1.3974e+04, 1.2080e+04, 8.1783e+03, 7.0491e+03,
        6.4326e+03, 5.8045e+03, 5.4666e+03, 5.0505e+03, 4.9312e+03, 4.5736e+03,
        4.1989e+03, 3.9354e+03, 3.5166e+03, 3.1084e+03, 3.0192e+03, 2.9370e+03,
        2.8504e+03, 2.6300e+03, 2.4505e+03, 2.4445e+03, 2.2422e+03, 2.1735e+03,
        2.0236e+03, 1.9364e+03, 1.7919e+03, 1.6723e+03, 1.5973e+03, 1.5307e+03,
        1.4391e+03, 1.3529e+03, 1.2789e+03, 1.2690e+03, 1.1177e+03, 1.0827e+03,
        1.0720e+03, 1.0612e+03, 9.8662e+02, 9.4517e+02, 9.1968e+02, 8.9564e+02,
        8.7710e+02, 8.3885e+02, 7.9896e+02, 7.8742e+02, 7.5665e+02, 7.2180e+02,
        6.9474e+02, 6.8855e+02, 6.6346e+02, 6.4110e+02, 6.3431e+02, 6.1853e+02,
        6.0279e+02, 5.6775e+02, 5.6509e+02, 5.4395e+02, 5.4200e+02, 5.3873e+02,
        5.2934e+02, 5.1930e+02, 5.0731e+02, 4.9780e+02, 4.8713e+02, 4.7798e+02,
        4.5241e+02, 4.4634e+02, 4.2969e+02, 4.2605e+02, 4.2138e+02, 4.1743e+02,
        3.9532e+02, 3.9103e+02, 3.8102e+02, 3.7525e+02, 3.6451e+02, 3.5909e+02,
        3.4814e+02, 3.4505e+02, 3.4305e+02, 3.4063e+02, 3.2967e+02, 3.2338e+02,
        3.1407e+02, 3.1162e+02, 3.0979e+02, 3.0175e+02, 2.9665e+02, 2.9085e+02,
        2.8973e+02, 2.8408e+02, 2.8080e+02, 2.7190e+02, 2.6680e+02, 2.6418e+02,
        2.6325e+02, 2.5753e+02, 2.5362e+02, 2.4989e+02, 2.4630e+02, 2.4457e+02,
        2.4110e+02, 2.3791e+02, 2.3396e+02, 2.3202e+02, 2.2537e+02, 2.2359e+02,
        2.1920e+02, 2.1738e+02, 2.1384e+02, 2.1102e+02, 2.1053e+02, 2.0267e+02,
        2.0192e+02, 1.9904e+02, 1.9567e+02, 1.9446e+02, 1.9164e+02, 1.8774e+02,
        1.8723e+02, 1.8549e+02, 1.8313e+02, 1.8258e+02, 1.7901e+02, 1.7743e+02,
        1.7467e+02, 1.7387e+02, 1.7232e+02, 1.7054e+02, 1.6679e+02, 1.6609e+02,
        1.6368e+02, 1.6156e+02, 1.6009e+02, 1.5931e+02, 1.5649e+02, 1.5375e+02,
        1.5143e+02, 1.5124e+02, 1.5066e+02, 1.4893e+02, 1.4686e+02, 1.4575e+02,
        1.4406e+02, 1.4184e+02, 1.3942e+02, 1.3830e+02, 1.3695e+02, 1.3490e+02,
        1.3415e+02, 1.3232e+02, 1.3146e+02, 1.3005e+02, 1.2967e+02, 1.2764e+02,
        1.2669e+02, 1.2590e+02, 1.2365e+02, 1.2339e+02, 1.2189e+02, 1.2105e+02,
        1.2072e+02, 1.1958e+02, 1.1808e+02, 1.1638e+02, 1.1494e+02, 1.1369e+02,
        1.1289e+02, 1.1161e+02, 1.1090e+02, 1.1036e+02, 1.0923e+02, 1.0862e+02,
        1.0630e+02, 1.0549e+02, 1.0487e+02, 1.0406e+02, 1.0336e+02, 1.0303e+02,
        1.0234e+02, 1.0089e+02, 9.9561e+01, 9.8456e+01, 9.8226e+01, 9.7505e+01,
        9.7148e+01, 9.5901e+01, 9.5615e+01, 9.4755e+01, 9.3564e+01, 9.3025e+01,
        9.2877e+01, 9.1844e+01, 9.0917e+01, 9.0432e+01, 8.9798e+01, 8.8831e+01,
        8.8124e+01, 8.7889e+01, 8.7402e+01, 8.5916e+01, 8.5103e+01, 8.4665e+01,
        8.3763e+01, 8.3253e+01, 8.2439e+01, 8.1658e+01, 8.1161e+01, 8.0517e+01,
        7.9467e+01, 7.9069e+01, 7.8614e+01, 7.8334e+01, 7.8063e+01, 7.7145e+01,
        7.6895e+01, 7.5908e+01, 7.5722e+01, 7.4772e+01, 7.4171e+01, 7.3730e+01,
        7.3510e+01, 7.3007e+01, 7.2639e+01, 7.1148e+01, 7.0985e+01, 7.0643e+01,
        6.9538e+01, 6.9027e+01, 6.8445e+01, 6.7727e+01, 6.7595e+01, 6.6991e+01,
        6.6886e+01, 6.6462e+01, 6.6277e+01, 6.5581e+01, 6.5416e+01, 6.4241e+01,
        6.4096e+01, 6.3610e+01, 6.3186e+01, 6.2617e+01, 6.2383e+01, 6.2118e+01,
        6.1582e+01, 6.1041e+01, 6.0727e+01, 6.0324e+01, 5.9978e+01, 5.9501e+01,
        5.9021e+01, 5.8662e+01, 5.8398e+01, 5.8172e+01, 5.8028e+01, 5.7275e+01,
        5.7033e+01, 5.6751e+01, 5.6319e+01, 5.5673e+01, 5.5439e+01, 5.5232e+01,
        5.4597e+01, 5.4359e+01, 5.4085e+01, 5.3544e+01, 5.3218e+01, 5.2943e+01,
        5.2686e+01, 5.2420e+01, 5.2029e+01, 5.1861e+01, 5.1236e+01, 5.0930e+01,
        5.0649e+01, 5.0267e+01, 5.0028e+01, 4.9801e+01, 4.9388e+01, 4.8845e+01,
        4.8639e+01, 4.8452e+01, 4.7990e+01, 4.7857e+01, 4.7446e+01, 4.7331e+01,
        4.6945e+01, 4.6779e+01, 4.6506e+01, 4.6316e+01, 4.5940e+01, 4.5587e+01,
        4.5474e+01, 4.5031e+01, 4.4794e+01, 4.4391e+01, 4.4290e+01, 4.3934e+01,
        4.3794e+01, 4.3475e+01, 4.3089e+01, 4.3054e+01, 4.2662e+01, 4.2489e+01,
        4.1925e+01, 4.1596e+01, 4.1251e+01, 4.0848e+01, 4.0661e+01, 4.0521e+01,
        4.0239e+01, 4.0024e+01, 3.9837e+01, 3.9765e+01, 3.9526e+01, 3.9278e+01,
        3.8942e+01, 3.8899e+01, 3.8654e+01, 3.8452e+01, 3.8328e+01, 3.8039e+01,
        3.7848e+01, 3.7663e+01, 3.7479e+01, 3.7290e+01, 3.7010e+01, 3.6696e+01,
        3.6520e+01, 3.6409e+01, 3.5998e+01, 3.5809e+01, 3.5647e+01, 3.5540e+01,
        3.5258e+01, 3.5072e+01, 3.4904e+01, 3.4721e+01, 3.4514e+01, 3.4251e+01,
        3.4172e+01, 3.4046e+01, 3.3626e+01, 3.3423e+01, 3.3165e+01, 3.3009e+01,
        3.2850e+01, 3.2627e+01, 3.2546e+01, 3.2440e+01, 3.2335e+01, 3.2038e+01,
        3.1803e+01, 3.1562e+01, 3.1475e+01, 3.1400e+01, 3.1240e+01, 3.1147e+01,
        3.0922e+01, 3.0861e+01, 3.0655e+01, 3.0466e+01, 3.0042e+01, 2.9993e+01,
        2.9890e+01, 2.9593e+01, 2.9534e+01, 2.9290e+01, 2.9086e+01, 2.8885e+01,
        2.8718e+01, 2.8595e+01, 2.8403e+01, 2.8260e+01, 2.8193e+01, 2.8151e+01,
        2.8054e+01, 2.7981e+01, 2.7780e+01, 2.7675e+01, 2.7478e+01, 2.7365e+01,
        2.7054e+01, 2.7048e+01, 2.6872e+01, 2.6795e+01, 2.6468e+01, 2.6357e+01,
        2.6325e+01, 2.6079e+01, 2.5807e+01, 2.5747e+01, 2.5592e+01, 2.5505e+01,
        2.5390e+01, 2.5302e+01, 2.5164e+01, 2.4915e+01, 2.4839e+01, 2.4702e+01,
        2.4622e+01, 2.4511e+01, 2.4343e+01, 2.4110e+01, 2.3941e+01, 2.3684e+01,
        2.3605e+01, 2.3446e+01, 2.3416e+01, 2.3309e+01, 2.3177e+01, 2.3041e+01,
        2.2995e+01, 2.2907e+01, 2.2721e+01, 2.2554e+01, 2.2514e+01, 2.2412e+01,
        2.2327e+01, 2.2122e+01, 2.2051e+01, 2.2006e+01, 2.1836e+01, 2.1703e+01,
        2.1651e+01, 2.1482e+01, 2.1412e+01, 2.1147e+01, 2.1019e+01, 2.0925e+01,
        2.0855e+01, 2.0781e+01, 2.0551e+01, 2.0426e+01, 2.0296e+01, 2.0248e+01,
        2.0110e+01, 1.9982e+01, 1.9833e+01, 1.9816e+01, 1.9752e+01, 1.9605e+01,
        1.9531e+01, 1.9405e+01, 1.9248e+01, 1.9169e+01, 1.9138e+01, 1.8944e+01,
        1.8740e+01, 1.8696e+01, 1.8561e+01, 1.8512e+01, 1.8328e+01, 1.8295e+01,
        1.8131e+01, 1.8071e+01, 1.7985e+01, 1.7843e+01, 1.7665e+01, 1.7567e+01,
        1.7563e+01, 1.7427e+01, 1.7308e+01, 1.7228e+01, 1.7136e+01, 1.7070e+01,
        1.6823e+01, 1.6801e+01, 1.6701e+01, 1.6667e+01, 1.6420e+01, 1.6285e+01,
        1.6271e+01, 1.6177e+01, 1.6158e+01, 1.6052e+01, 1.5930e+01, 1.5873e+01,
        1.5710e+01, 1.5572e+01, 1.5449e+01, 1.5381e+01, 1.5261e+01, 1.5123e+01,
        1.4965e+01, 1.4927e+01, 1.4829e+01, 1.4803e+01, 1.4655e+01, 1.4637e+01,
        1.4495e+01, 1.4360e+01, 1.4301e+01, 1.4202e+01, 1.4033e+01, 1.3818e+01,
        1.3811e+01, 1.3788e+01, 1.3687e+01, 1.3577e+01, 1.3500e+01, 1.3387e+01,
        1.3259e+01, 1.3156e+01, 1.3105e+01, 1.2945e+01, 1.2901e+01, 1.2765e+01,
        1.2692e+01, 1.2583e+01, 1.2493e+01, 1.2440e+01, 1.2243e+01, 1.2206e+01,
        1.2056e+01, 1.2014e+01, 1.1861e+01, 1.1858e+01, 1.1684e+01, 1.1624e+01,
        1.1447e+01, 1.1407e+01, 1.1364e+01, 1.1290e+01, 1.1229e+01, 1.1100e+01,
        1.1008e+01, 1.0864e+01, 1.0816e+01, 1.0756e+01, 1.0611e+01, 1.0499e+01,
        1.0379e+01, 1.0318e+01, 1.0241e+01, 1.0113e+01, 9.9774e+00, 9.8502e+00,
        9.8312e+00, 9.7386e+00, 9.6448e+00, 9.5276e+00, 9.4952e+00, 9.3789e+00,
        9.2602e+00, 9.1855e+00, 8.9849e+00, 8.7902e+00, 8.7001e+00, 8.5560e+00,
        8.4045e+00, 8.3212e+00, 8.2324e+00, 8.2181e+00, 8.1393e+00, 8.0499e+00,
        7.6921e+00, 7.5985e+00, 7.3673e+00, 7.2730e+00, 7.1514e+00, 7.0071e+00,
        6.8563e+00, 6.5330e+00, 6.4628e+00, 6.2572e+00, 5.8703e+00, 5.5177e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 306]) 

NULL SPACE BASIS :  tensor([[-0.0341, -0.0330, -0.0073,  ...,  0.0024, -0.0180,  0.0148],
        [ 0.0201,  0.0384, -0.0143,  ...,  0.0072,  0.0279, -0.0096],
        [ 0.0036,  0.0036,  0.0024,  ..., -0.0136, -0.0129,  0.0034],
        ...,
        [ 0.0380,  0.0323,  0.0029,  ..., -0.0131, -0.0014, -0.0089],
        [ 0.0410,  0.0204,  0.0799,  ...,  0.0103, -0.0100,  0.0193],
        [-0.0195,  0.0400,  0.0359,  ...,  0.0013,  0.0100, -0.0060]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.6921e-02, -1.6293e-02, -1.8266e-03,  ..., -3.3420e-04,
         -7.5985e-04, -2.0271e-04],
        [-1.6293e-02,  3.3592e-02, -1.4737e-02,  ..., -9.7631e-04,
         -3.9776e-05, -6.0460e-04],
        [-1.8266e-03, -1.4737e-02,  2.2081e-02,  ..., -1.7789e-04,
          4.1437e-04,  4.6854e-04],
        ...,
        [-3.3420e-04, -9.7631e-04, -1.7789e-04,  ...,  1.7844e-02,
         -8.1563e-03, -3.1714e-04],
        [-7.5985e-04, -3.9776e-05,  4.1437e-04,  ..., -8.1563e-03,
          2.2843e-02, -6.8693e-03],
        [-2.0271e-04, -6.0460e-04,  4.6854e-04,  ..., -3.1714e-04,
         -6.8693e-03,  1.6501e-02]], device='cuda:0') 

reserving basis 846/1152; cond: 256086.59375, radio:0.010168297216296196
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0171,  0.0308, -0.0182],
          [ 0.0002, -0.0163,  0.0092],
          [ 0.0196,  0.0196, -0.0029]],

         [[-0.0182, -0.0232, -0.0255],
          [-0.0214, -0.0221,  0.0074],
          [-0.0146,  0.0097, -0.0280]],

         [[ 0.0010, -0.0145,  0.0040],
          [ 0.0083,  0.0150, -0.0154],
          [-0.0142, -0.0249,  0.0272]],

         ...,

         [[-0.0127,  0.0016,  0.0052],
          [-0.0310, -0.0218, -0.0019],
          [ 0.0055,  0.0147,  0.0284]],

         [[ 0.0092, -0.0338, -0.0184],
          [ 0.0232,  0.0158,  0.0008],
          [ 0.0150, -0.0148,  0.0346]],

         [[ 0.0187,  0.0057,  0.0153],
          [-0.0231, -0.0341, -0.0260],
          [-0.0290,  0.0187, -0.0148]]],


        [[[ 0.0251,  0.0005,  0.0071],
          [-0.0153, -0.0136,  0.0212],
          [-0.0279, -0.0303, -0.0161]],

         [[ 0.0079, -0.0245, -0.0229],
          [ 0.0100, -0.0128,  0.0023],
          [-0.0253,  0.0070, -0.0119]],

         [[ 0.0003, -0.0169, -0.0180],
          [-0.0051,  0.0139, -0.0287],
          [ 0.0257,  0.0184,  0.0029]],

         ...,

         [[ 0.0042,  0.0136, -0.0040],
          [-0.0218,  0.0258,  0.0271],
          [ 0.0147,  0.0208,  0.0215]],

         [[ 0.0251,  0.0014,  0.0066],
          [-0.0085, -0.0221, -0.0279],
          [ 0.0408, -0.0195, -0.0300]],

         [[-0.0159, -0.0189, -0.0330],
          [-0.0240, -0.0136, -0.0082],
          [ 0.0068, -0.0115, -0.0018]]],


        [[[-0.0170,  0.0187, -0.0279],
          [-0.0045,  0.0177, -0.0024],
          [ 0.0006,  0.0280,  0.0167]],

         [[ 0.0053, -0.0121,  0.0232],
          [-0.0037, -0.0088,  0.0054],
          [-0.0215,  0.0052,  0.0091]],

         [[-0.0185, -0.0159, -0.0136],
          [-0.0255, -0.0270, -0.0316],
          [-0.0130, -0.0180,  0.0169]],

         ...,

         [[ 0.0268,  0.0082, -0.0179],
          [-0.0308,  0.0139,  0.0035],
          [ 0.0198, -0.0167, -0.0318]],

         [[-0.0406,  0.0026, -0.0110],
          [-0.0165,  0.0180,  0.0086],
          [-0.0138,  0.0087, -0.0144]],

         [[-0.0134, -0.0191,  0.0109],
          [-0.0037, -0.0101,  0.0049],
          [ 0.0109, -0.0178,  0.0302]]],


        ...,


        [[[ 0.0271, -0.0001,  0.0297],
          [ 0.0247,  0.0174, -0.0309],
          [ 0.0050,  0.0008,  0.0275]],

         [[ 0.0101, -0.0067,  0.0132],
          [ 0.0051, -0.0252,  0.0051],
          [-0.0162, -0.0072,  0.0163]],

         [[ 0.0161,  0.0120, -0.0110],
          [-0.0139,  0.0276,  0.0052],
          [ 0.0062, -0.0068, -0.0147]],

         ...,

         [[-0.0287,  0.0146, -0.0139],
          [ 0.0071, -0.0288, -0.0150],
          [-0.0188,  0.0026,  0.0093]],

         [[ 0.0189,  0.0169,  0.0283],
          [-0.0220,  0.0004,  0.0165],
          [-0.0055,  0.0110,  0.0053]],

         [[-0.0148, -0.0033,  0.0237],
          [ 0.0151,  0.0087,  0.0314],
          [ 0.0051,  0.0090,  0.0194]]],


        [[[-0.0048, -0.0122,  0.0049],
          [-0.0295, -0.0091,  0.0202],
          [ 0.0099, -0.0278, -0.0221]],

         [[-0.0122, -0.0159,  0.0237],
          [-0.0180, -0.0348, -0.0049],
          [-0.0238, -0.0006, -0.0227]],

         [[-0.0158, -0.0022, -0.0204],
          [-0.0117,  0.0148,  0.0227],
          [ 0.0093, -0.0191,  0.0204]],

         ...,

         [[-0.0149,  0.0109,  0.0045],
          [-0.0293, -0.0214, -0.0145],
          [-0.0232, -0.0499, -0.0177]],

         [[-0.0038, -0.0075, -0.0141],
          [-0.0308,  0.0303,  0.0289],
          [-0.0367,  0.0219, -0.0090]],

         [[ 0.0215, -0.0220, -0.0089],
          [ 0.0308, -0.0155,  0.0321],
          [-0.0045,  0.0185,  0.0008]]],


        [[[ 0.0041, -0.0270,  0.0294],
          [-0.0240,  0.0050, -0.0359],
          [-0.0016, -0.0283, -0.0180]],

         [[ 0.0141,  0.0142, -0.0163],
          [ 0.0030,  0.0131,  0.0090],
          [-0.0172, -0.0278,  0.0230]],

         [[-0.0079, -0.0076,  0.0081],
          [-0.0178,  0.0021,  0.0264],
          [-0.0165,  0.0241, -0.0044]],

         ...,

         [[ 0.0266,  0.0118, -0.0066],
          [-0.0177,  0.0318,  0.0153],
          [ 0.0281, -0.0095, -0.0004]],

         [[-0.0162,  0.0091, -0.0046],
          [-0.0229,  0.0321,  0.0286],
          [ 0.0165,  0.0133,  0.0084]],

         [[ 0.0108,  0.0388, -0.0176],
          [-0.0148,  0.0123,  0.0323],
          [ 0.0055, -0.0247, -0.0171]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([4.6047e+06, 1.7661e+05, 1.7172e+05,  ..., 2.0020e+01, 1.9771e+01,
        1.7981e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 846]) 

NULL SPACE BASIS :  tensor([[-0.0050,  0.0018, -0.0174,  ..., -0.0140,  0.0256, -0.0454],
        [-0.0375, -0.0281,  0.0028,  ...,  0.0159, -0.0899,  0.1186],
        [-0.0179, -0.0462,  0.0118,  ...,  0.0119,  0.0737, -0.0903],
        ...,
        [ 0.0106, -0.0759,  0.0071,  ...,  0.0236,  0.0208, -0.0130],
        [ 0.0100,  0.0487,  0.0300,  ..., -0.0265, -0.0192,  0.0149],
        [-0.0290,  0.0209,  0.0201,  ...,  0.0173,  0.0128, -0.0072]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.0092e-02, -8.9314e-04, -4.9451e-04,  ..., -2.8563e-04,
          2.0112e-04, -1.4890e-06],
        [-8.9314e-04,  3.0807e-02, -4.7020e-04,  ...,  8.2995e-05,
         -1.6214e-04,  3.0018e-04],
        [-4.9451e-04, -4.7020e-04,  3.0418e-02,  ..., -1.9343e-04,
         -6.0235e-05,  3.6437e-06],
        ...,
        [-2.8563e-04,  8.2995e-05, -1.9343e-04,  ...,  2.3613e-02,
         -2.5243e-03, -5.9880e-04],
        [ 2.0112e-04, -1.6214e-04, -6.0235e-05,  ..., -2.5243e-03,
          2.4966e-02, -2.5566e-03],
        [-1.4890e-06,  3.0018e-04,  3.6437e-06,  ..., -5.9880e-04,
         -2.5566e-03,  2.5358e-02]], device='cuda:0') 

reserving basis 45/64; cond: 7972.693359375, radio:0.016628965735435486
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0113]],

         [[ 0.0904]],

         [[ 0.0547]],

         ...,

         [[-0.0798]],

         [[-0.0913]],

         [[-0.0540]]],


        [[[ 0.0241]],

         [[-0.0265]],

         [[-0.0948]],

         ...,

         [[-0.0391]],

         [[ 0.0127]],

         [[ 0.0213]]],


        [[[ 0.0216]],

         [[-0.0634]],

         [[ 0.0070]],

         ...,

         [[-0.1445]],

         [[ 0.0936]],

         [[ 0.0188]]],


        ...,


        [[[ 0.0281]],

         [[-0.0799]],

         [[-0.1054]],

         ...,

         [[-0.0169]],

         [[ 0.0585]],

         [[-0.0268]]],


        [[[ 0.0098]],

         [[-0.0785]],

         [[ 0.1162]],

         ...,

         [[-0.0074]],

         [[ 0.0405]],

         [[ 0.0747]]],


        [[[ 0.0090]],

         [[ 0.0029]],

         [[ 0.0838]],

         ...,

         [[-0.0108]],

         [[ 0.0753]],

         [[ 0.0943]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.1970e+05, 1.7382e+04, 1.5525e+04, 6.4347e+03, 4.6383e+03, 3.6049e+03,
        2.5439e+03, 1.8960e+03, 1.3106e+03, 1.1643e+03, 9.4627e+02, 8.8833e+02,
        8.1017e+02, 7.6294e+02, 7.2435e+02, 6.2267e+02, 5.4631e+02, 5.3434e+02,
        5.0753e+02, 3.8772e+02, 3.4814e+02, 3.4304e+02, 3.1662e+02, 2.8475e+02,
        2.7220e+02, 2.6896e+02, 2.4140e+02, 2.3013e+02, 2.1289e+02, 1.9743e+02,
        1.8683e+02, 1.7758e+02, 1.7308e+02, 1.6648e+02, 1.5528e+02, 1.4343e+02,
        1.3980e+02, 1.3477e+02, 1.3123e+02, 1.2798e+02, 1.1878e+02, 1.1252e+02,
        1.0650e+02, 1.0187e+02, 9.9597e+01, 9.5392e+01, 9.2035e+01, 8.7295e+01,
        8.1839e+01, 7.9093e+01, 7.5358e+01, 7.2756e+01, 7.0370e+01, 6.7957e+01,
        6.5483e+01, 6.0542e+01, 5.8905e+01, 5.8281e+01, 5.4514e+01, 5.2179e+01,
        5.0126e+01, 4.7280e+01, 4.6507e+01, 4.0099e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 45]) 

NULL SPACE BASIS :  tensor([[-0.2973,  0.1390, -0.2622,  ...,  0.0208,  0.1542, -0.1157],
        [-0.0977, -0.1470, -0.1556,  ...,  0.0957, -0.0736, -0.0154],
        [-0.0553, -0.1179, -0.0674,  ..., -0.0360, -0.0693, -0.0693],
        ...,
        [-0.0483,  0.0821,  0.0965,  ..., -0.0150,  0.0123,  0.0189],
        [ 0.0606,  0.1218, -0.0206,  ...,  0.0651, -0.0392, -0.0614],
        [ 0.1933,  0.0600, -0.1821,  ..., -0.0497,  0.0792,  0.0323]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.1150, -0.0022,  0.0057,  ...,  0.0002, -0.0085, -0.0109],
        [-0.0022,  0.1173,  0.0001,  ...,  0.0135, -0.0100, -0.0073],
        [ 0.0057,  0.0001,  0.1207,  ..., -0.0011,  0.0123,  0.0004],
        ...,
        [ 0.0002,  0.0135, -0.0011,  ...,  0.0341, -0.0322, -0.0024],
        [-0.0085, -0.0100,  0.0123,  ..., -0.0322,  0.1050, -0.0004],
        [-0.0109, -0.0073,  0.0004,  ..., -0.0024, -0.0004,  0.0901]],
       device='cuda:0') 

reserving basis 833/1152; cond: 266698.9375, radio:0.01008894294500351
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1634e-02, -2.9655e-03,  1.0244e-02],
          [-3.5563e-03,  1.6293e-02,  1.0988e-02],
          [-1.0751e-02, -1.2358e-02, -1.3527e-02]],

         [[ 3.5840e-03, -1.8844e-02, -2.2634e-02],
          [ 2.4504e-02, -3.1803e-03, -9.6121e-05],
          [ 6.1193e-03,  1.7995e-02, -1.6892e-02]],

         [[ 5.6492e-03, -1.0752e-03,  6.4356e-03],
          [ 1.6982e-02,  1.7780e-02,  1.6852e-02],
          [-2.3943e-02, -1.3627e-02,  1.2805e-02]],

         ...,

         [[ 1.6790e-02,  6.8368e-03,  4.2532e-04],
          [-1.1756e-02,  2.7370e-02,  6.9062e-03],
          [ 1.4930e-02, -1.3926e-03, -6.8704e-03]],

         [[ 9.4935e-03,  1.1501e-02,  6.0197e-03],
          [ 2.5637e-03,  2.2346e-02,  2.0277e-02],
          [ 2.4167e-02,  2.8319e-02, -5.7741e-03]],

         [[ 1.0987e-03,  2.6089e-02,  4.6535e-02],
          [-1.9701e-02,  1.4169e-02,  2.1026e-02],
          [ 5.0505e-03, -1.9950e-02,  1.7778e-02]]],


        [[[-1.0763e-02,  7.7516e-03, -2.0490e-02],
          [ 1.4537e-02, -6.2610e-03,  6.3417e-03],
          [ 2.8049e-03, -2.3545e-02,  2.0031e-02]],

         [[-1.4358e-03,  1.9541e-02,  2.6693e-02],
          [ 2.1126e-02,  1.6740e-02,  7.7382e-04],
          [-9.6667e-03,  1.6959e-02,  1.9084e-02]],

         [[ 1.9753e-02,  1.8936e-02,  1.0241e-02],
          [ 2.7222e-02, -4.4402e-03,  2.2393e-02],
          [-1.7076e-02,  6.3776e-04,  9.0400e-03]],

         ...,

         [[-4.3436e-03,  1.5009e-02, -1.8896e-02],
          [-2.7131e-02, -2.2704e-02, -3.1151e-02],
          [-2.0945e-02, -2.1705e-02, -2.3651e-02]],

         [[-2.2927e-02,  2.3530e-03,  3.2340e-02],
          [-1.5250e-02, -1.7637e-02, -1.8030e-03],
          [ 1.4964e-02, -1.0044e-02, -1.7278e-02]],

         [[ 2.2484e-02, -2.3617e-02,  1.3081e-02],
          [-7.6882e-03, -1.3606e-02, -2.8792e-02],
          [ 2.6727e-03,  1.6183e-02, -3.2282e-02]]],


        [[[ 1.8944e-02,  2.0881e-02,  1.4383e-02],
          [-6.0121e-03, -2.9147e-02,  2.7867e-02],
          [-1.2374e-02, -2.3765e-04, -2.5132e-02]],

         [[-1.0471e-02,  5.5264e-03, -1.6520e-02],
          [ 3.2265e-02,  6.4146e-03, -4.9092e-03],
          [-7.5063e-03, -1.8529e-02,  1.8969e-02]],

         [[-3.0099e-02, -1.5908e-02, -2.2899e-02],
          [-1.5035e-02, -1.5038e-02,  2.4972e-03],
          [ 2.5324e-02, -1.3080e-02, -7.2533e-03]],

         ...,

         [[-3.5116e-02, -6.5942e-03, -2.1660e-02],
          [-6.9915e-03, -3.4844e-02, -2.7082e-02],
          [-3.5675e-03, -2.7393e-02, -6.3885e-04]],

         [[ 2.4957e-02, -1.3313e-02,  5.3979e-03],
          [ 1.9986e-02, -1.6819e-02, -2.0441e-02],
          [-2.9477e-03, -3.0066e-02,  1.0085e-02]],

         [[ 1.1316e-02,  1.4184e-02, -3.9143e-05],
          [ 6.0653e-03,  1.0786e-02, -1.0592e-02],
          [ 4.4890e-03,  2.3534e-02, -1.1664e-02]]],


        ...,


        [[[-1.4057e-02, -2.3562e-02, -3.3327e-02],
          [-9.9086e-03, -1.5468e-02,  1.6128e-03],
          [ 1.5755e-02,  7.3262e-03,  2.4013e-02]],

         [[-3.1168e-02, -7.2579e-03, -2.4890e-02],
          [-1.2780e-02,  1.4927e-02,  1.0891e-02],
          [-1.2044e-02,  3.1660e-02,  2.8206e-02]],

         [[ 3.0285e-02,  5.6234e-03,  2.6197e-04],
          [-1.6273e-02,  1.9809e-03, -2.7785e-02],
          [ 3.1212e-02,  2.3311e-02,  8.7531e-03]],

         ...,

         [[-2.1512e-02, -2.6805e-02,  7.6082e-03],
          [ 1.7145e-02, -1.3833e-02,  1.6735e-02],
          [-1.1297e-02,  3.4076e-02, -6.5169e-03]],

         [[ 1.1450e-02,  3.5036e-02,  1.1341e-02],
          [-1.1416e-02, -1.9700e-02,  2.5346e-02],
          [ 1.6023e-02, -2.8418e-03, -4.4787e-03]],

         [[ 3.7905e-02,  7.7237e-03,  1.6922e-02],
          [ 2.0965e-02, -1.5090e-02,  1.5704e-02],
          [-1.1180e-02, -3.4291e-02, -3.8439e-02]]],


        [[[-2.7202e-02, -1.7349e-03,  1.3760e-02],
          [ 2.1660e-02,  1.4052e-02, -2.0694e-02],
          [ 2.8914e-02,  2.4697e-02, -1.8174e-03]],

         [[ 7.7366e-03, -3.8139e-03,  1.8335e-03],
          [ 1.9491e-02,  3.8475e-02,  1.8258e-02],
          [ 2.1580e-02,  3.6946e-02, -1.2072e-02]],

         [[-2.0359e-02, -1.6294e-02, -2.7716e-02],
          [-1.7545e-02, -9.8827e-04, -9.5352e-03],
          [-2.7343e-03,  5.3441e-03, -2.8096e-02]],

         ...,

         [[-2.6449e-02,  1.6559e-02, -2.5273e-02],
          [ 1.6075e-02, -1.2047e-02, -6.8874e-03],
          [-1.0501e-02,  2.7200e-02,  1.2078e-02]],

         [[-2.8112e-02,  2.7099e-03,  1.4563e-02],
          [-2.8818e-03, -5.3133e-03,  3.3799e-03],
          [ 2.2072e-02,  2.2621e-02,  1.8622e-03]],

         [[-2.8114e-02, -7.2096e-03,  9.0076e-03],
          [-2.6960e-02,  1.1807e-02,  2.8011e-02],
          [-2.8333e-02,  8.5311e-04,  1.5469e-02]]],


        [[[-1.6154e-02,  1.4851e-02, -2.4424e-03],
          [-2.5273e-03,  1.1898e-02,  5.4440e-03],
          [-4.0523e-03, -1.7737e-03,  1.9828e-02]],

         [[-1.3091e-03, -2.2283e-02, -1.9178e-02],
          [-2.4702e-02, -1.3050e-02, -3.8523e-02],
          [ 3.7941e-02,  1.6189e-02,  3.5884e-03]],

         [[ 2.9285e-02,  1.3348e-03, -8.8280e-03],
          [-1.6563e-02, -1.2454e-02, -1.4650e-02],
          [ 2.6683e-02, -7.1272e-03, -2.0957e-02]],

         ...,

         [[ 9.7908e-03, -4.3729e-03,  9.9225e-03],
          [-2.0277e-02, -9.8260e-03,  4.9604e-03],
          [ 1.4153e-02,  9.9832e-04, -1.3658e-02]],

         [[-1.6513e-02, -9.8625e-03,  3.3057e-03],
          [ 1.6517e-02,  1.0368e-02, -3.1872e-02],
          [-1.6677e-02, -6.3061e-03, -8.1740e-03]],

         [[-2.4931e-03, -2.9748e-02, -1.6860e-02],
          [ 3.2968e-03,  2.4156e-02, -2.7576e-02],
          [ 2.6869e-02,  1.2912e-02,  2.0548e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.6093e+06, 1.7697e+05, 1.7005e+05,  ..., 2.0364e+01, 1.9627e+01,
        1.7283e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 833]) 

NULL SPACE BASIS :  tensor([[ 0.0351, -0.0078,  0.0084,  ...,  0.0036, -0.0082,  0.0251],
        [-0.0167,  0.0305, -0.0019,  ...,  0.0009,  0.0159, -0.0159],
        [-0.0133, -0.0146,  0.0060,  ..., -0.0080, -0.0072, -0.0005],
        ...,
        [-0.0061, -0.0420, -0.0394,  ..., -0.0050,  0.0232,  0.0026],
        [ 0.0416,  0.0279, -0.0817,  ..., -0.0010, -0.0178, -0.0010],
        [-0.0282,  0.0210, -0.0159,  ...,  0.0041, -0.0011,  0.0008]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.5404e-02, -3.9219e-03, -1.2341e-03,  ..., -3.3535e-04,
         -5.0017e-04,  1.4476e-04],
        [-3.9219e-03,  2.6609e-02, -3.8722e-03,  ..., -2.2021e-04,
         -1.1227e-05,  1.9946e-05],
        [-1.2341e-03, -3.8722e-03,  2.5415e-02,  ..., -2.7100e-04,
         -3.9421e-04,  2.8219e-05],
        ...,
        [-3.3535e-04, -2.2021e-04, -2.7100e-04,  ...,  2.1279e-02,
         -5.5025e-03, -1.9289e-03],
        [-5.0017e-04, -1.1227e-05, -3.9421e-04,  ..., -5.5025e-03,
          2.2183e-02, -5.2909e-03],
        [ 1.4476e-04,  1.9946e-05,  2.8219e-05,  ..., -1.9289e-03,
         -5.2909e-03,  2.1648e-02]], device='cuda:0') 

reserving basis 724/1152; cond: 337963.46875, radio:0.008476560935378075
PARAMETER       :  Parameter containing:
tensor([[[[ 2.0606e-03, -1.3184e-02, -7.3377e-03],
          [ 1.8885e-02,  1.7539e-02,  3.7683e-02],
          [-1.1943e-02, -6.5746e-03,  1.5647e-02]],

         [[-1.6334e-02,  1.8867e-02, -1.0328e-02],
          [-1.1717e-02, -2.3463e-02, -2.5177e-02],
          [-8.7562e-04, -1.5809e-02, -2.1643e-03]],

         [[-2.1422e-02, -1.4009e-02,  3.0216e-02],
          [ 3.7646e-02,  2.5594e-02, -6.9833e-04],
          [ 4.8378e-03,  7.2926e-03, -7.7482e-03]],

         ...,

         [[ 3.8296e-02,  1.2897e-02, -6.2098e-03],
          [ 3.9446e-02,  1.4586e-02,  1.5861e-02],
          [-1.5600e-02,  1.5657e-03, -1.5607e-02]],

         [[ 4.2354e-02,  5.0746e-03,  2.8399e-02],
          [ 3.0289e-02,  1.3096e-02,  1.9396e-02],
          [-2.2874e-02, -3.3344e-03,  1.9092e-02]],

         [[ 1.1389e-02, -1.3908e-02, -1.4194e-02],
          [ 2.0919e-02,  1.8708e-02,  3.8125e-02],
          [-1.5418e-02,  1.4919e-02,  6.2971e-04]]],


        [[[ 1.9212e-02,  3.6416e-02, -2.6513e-02],
          [ 2.4348e-02, -3.6970e-03, -2.7214e-02],
          [ 9.3487e-03, -2.4810e-02, -1.8282e-02]],

         [[-3.1612e-02, -3.4602e-02, -4.2398e-02],
          [-1.9530e-02, -2.3187e-02, -6.8675e-03],
          [ 1.8184e-02, -1.4328e-02,  6.2407e-03]],

         [[ 1.1050e-02,  1.7997e-02,  2.8310e-02],
          [-3.9802e-03,  1.3922e-02, -5.3400e-03],
          [ 1.3152e-02, -8.8119e-03, -2.8664e-02]],

         ...,

         [[-1.0335e-02,  3.0978e-02, -1.6903e-02],
          [ 3.1080e-02, -8.1267e-03,  1.2546e-02],
          [ 8.8578e-03,  1.8545e-02, -2.0324e-02]],

         [[-2.6269e-02,  1.8774e-02, -1.8355e-02],
          [-1.6897e-02, -3.3397e-03, -2.0688e-02],
          [ 2.8243e-02, -1.0422e-02,  2.0060e-02]],

         [[ 1.2829e-02, -2.6244e-02,  3.3341e-02],
          [-2.0335e-02,  2.2271e-02,  6.7190e-05],
          [ 1.4996e-02, -9.5973e-04, -8.2225e-03]]],


        [[[-1.3174e-03, -1.6652e-03,  9.3733e-03],
          [-2.0045e-02,  6.7236e-03, -1.7241e-02],
          [-1.6586e-02,  1.9407e-03, -5.7086e-03]],

         [[ 1.3645e-02, -1.7435e-02, -1.5209e-02],
          [ 2.6364e-02,  4.8867e-04,  3.1139e-02],
          [ 3.3809e-02,  2.0129e-02, -1.5280e-02]],

         [[-1.4272e-02,  1.0766e-02, -2.4632e-02],
          [ 6.5838e-03,  1.7633e-02,  1.8184e-03],
          [-3.4070e-04, -6.3927e-03,  3.0910e-02]],

         ...,

         [[ 1.8885e-02,  2.3504e-02,  1.5953e-02],
          [-1.6328e-02, -2.5444e-02,  3.9601e-03],
          [ 2.8701e-03,  1.7358e-02, -9.8544e-03]],

         [[ 1.5618e-02,  1.6851e-02,  8.8255e-03],
          [ 8.8948e-03, -9.1033e-03, -1.9925e-02],
          [ 4.3743e-03,  1.0681e-02,  3.2153e-02]],

         [[-1.2441e-02, -1.4793e-02, -4.7974e-03],
          [-2.0442e-02,  2.2559e-03,  3.7927e-04],
          [-3.4534e-03,  5.8990e-03,  4.0643e-02]]],


        ...,


        [[[-1.0269e-02, -2.4380e-02,  1.1682e-02],
          [ 1.9432e-02, -7.1527e-03,  1.7078e-02],
          [ 2.7083e-03, -1.5399e-02,  2.4968e-02]],

         [[-2.2345e-02, -2.3926e-02, -9.4128e-03],
          [-1.8871e-02, -1.7476e-02,  2.0811e-03],
          [-3.2715e-02,  2.3762e-02, -5.3096e-03]],

         [[-6.6145e-03,  3.4436e-03, -6.4824e-03],
          [-7.4286e-03,  1.8913e-02,  4.0479e-03],
          [ 3.0613e-02,  9.7252e-03,  1.9876e-02]],

         ...,

         [[ 1.6091e-04, -1.1944e-02,  3.9330e-02],
          [-2.0521e-03,  2.0644e-02,  2.1254e-02],
          [ 1.4656e-02,  9.5131e-03,  8.8756e-03]],

         [[-2.0860e-02,  2.1362e-03, -5.6506e-03],
          [-1.7125e-02,  1.2411e-02,  4.8383e-03],
          [ 1.1568e-02, -2.1645e-03,  1.9413e-02]],

         [[-4.0370e-03, -5.1937e-03, -9.1809e-03],
          [ 5.4904e-03,  2.3184e-02, -1.8061e-02],
          [-9.7459e-04, -1.2903e-02, -6.0019e-03]]],


        [[[-8.2330e-03,  3.8576e-02,  1.0546e-02],
          [-2.5388e-03, -4.4990e-04, -8.6181e-04],
          [-2.3642e-02,  2.3367e-02,  2.2347e-02]],

         [[ 1.6761e-02, -8.7141e-03,  3.3221e-04],
          [-2.9342e-03,  3.2070e-02,  1.1556e-02],
          [ 5.8208e-03, -8.1897e-03, -2.4965e-02]],

         [[-9.2204e-03, -1.4633e-02,  6.4184e-03],
          [-1.3507e-02, -3.2173e-02, -3.6662e-03],
          [ 1.5765e-02, -1.4801e-02,  2.7741e-02]],

         ...,

         [[-2.3340e-03,  1.1805e-02, -2.0077e-02],
          [ 1.6120e-02,  7.1781e-04,  1.6539e-02],
          [ 3.0465e-02,  1.2255e-02,  2.3852e-02]],

         [[ 9.7682e-03, -2.8484e-02,  2.7758e-03],
          [-1.6490e-02,  8.4345e-04, -1.9593e-02],
          [ 1.7895e-02,  1.9446e-02,  2.5055e-02]],

         [[-4.5715e-03, -3.0777e-02, -1.9021e-02],
          [ 1.2330e-02,  1.0352e-02,  2.7393e-03],
          [ 1.5304e-02,  4.6364e-03, -2.7951e-02]]],


        [[[ 2.1960e-02, -2.6502e-02,  4.8102e-04],
          [ 1.1499e-02,  7.8054e-03, -2.8010e-02],
          [ 2.0775e-02,  2.5289e-02,  1.1371e-02]],

         [[-4.1121e-02, -1.7067e-02, -3.0347e-02],
          [ 1.7860e-04,  1.2789e-02, -1.4657e-03],
          [ 4.0508e-03, -2.6975e-02,  1.0231e-02]],

         [[-9.6588e-03,  1.2036e-03, -8.2147e-03],
          [-2.5726e-02, -1.6554e-02, -2.2118e-02],
          [ 1.8148e-03, -1.1809e-02,  1.5455e-02]],

         ...,

         [[-1.4927e-02, -2.8249e-02, -2.8546e-02],
          [ 1.0695e-02,  1.0985e-03,  1.0230e-03],
          [ 1.8668e-02, -1.5710e-02, -1.5164e-02]],

         [[-3.1437e-03, -1.7605e-02, -6.4940e-03],
          [ 2.7592e-03,  4.2271e-03,  1.9126e-02],
          [ 9.2855e-04,  1.7023e-03,  2.6368e-03]],

         [[ 2.0903e-02,  1.2649e-02,  7.3634e-04],
          [ 2.1143e-02, -7.3572e-03,  2.0619e-02],
          [-1.5161e-02, -1.5336e-02,  1.5015e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.6169e+06, 1.7090e+05, 1.4465e+05,  ..., 1.8599e+01, 1.7964e+01,
        1.3661e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 724]) 

NULL SPACE BASIS :  tensor([[ 0.0107,  0.0537,  0.0410,  ...,  0.0092, -0.0073,  0.0059],
        [-0.0012,  0.0262,  0.0041,  ..., -0.0217,  0.0024,  0.0043],
        [ 0.0283, -0.0089, -0.0314,  ...,  0.0237, -0.0041, -0.0064],
        ...,
        [-0.0197,  0.0059, -0.0221,  ...,  0.0009, -0.0029, -0.0134],
        [-0.0624, -0.0257, -0.0276,  ..., -0.0023,  0.0024,  0.0107],
        [-0.0325, -0.0106, -0.0155,  ..., -0.0052,  0.0008, -0.0034]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.0545e-02, -2.4084e-03, -1.1130e-03,  ...,  5.4819e-05,
          4.6442e-04,  4.0333e-04],
        [-2.4084e-03,  2.0980e-02, -2.7468e-03,  ..., -1.3125e-04,
          4.3897e-04,  1.7963e-04],
        [-1.1130e-03, -2.7468e-03,  1.9486e-02,  ..., -1.2341e-04,
         -3.2924e-04,  9.9469e-05],
        ...,
        [ 5.4819e-05, -1.3125e-04, -1.2341e-04,  ...,  2.3573e-02,
         -4.5985e-03, -1.2197e-03],
        [ 4.6442e-04,  4.3897e-04, -3.2924e-04,  ..., -4.5985e-03,
          2.4107e-02, -4.8135e-03],
        [ 4.0333e-04,  1.7963e-04,  9.9469e-05,  ..., -1.2197e-03,
         -4.8135e-03,  2.3034e-02]], device='cuda:0') 

reserving basis 807/1152; cond: 314778.375, radio:0.008417689241468906
PARAMETER       :  Parameter containing:
tensor([[[[-2.1035e-02, -1.5040e-02,  4.1645e-02],
          [ 2.1794e-02,  8.4500e-03, -1.3415e-03],
          [ 1.7029e-02,  2.1737e-02, -2.7125e-02]],

         [[ 6.6452e-03,  2.6205e-04,  3.2869e-02],
          [-2.7759e-03,  1.6299e-02,  2.9713e-02],
          [ 2.6236e-02, -2.5370e-02, -6.8641e-03]],

         [[ 1.7203e-02,  6.3353e-03,  8.7811e-03],
          [-6.0243e-03, -8.4699e-03,  2.6242e-02],
          [ 1.0903e-02,  2.0146e-02,  2.5052e-02]],

         ...,

         [[ 7.2247e-03,  1.6689e-02,  2.5945e-02],
          [ 3.3791e-02, -2.0466e-02,  2.3452e-02],
          [ 1.0331e-03, -1.3783e-02, -1.4263e-02]],

         [[-8.0755e-03, -7.2755e-03, -1.9420e-02],
          [-2.7449e-03,  3.2877e-03,  5.0623e-03],
          [ 1.5917e-03, -2.0572e-04, -6.9739e-03]],

         [[-3.8716e-02,  8.3076e-03, -1.7543e-02],
          [ 9.4169e-03,  2.1148e-03,  1.7316e-02],
          [ 2.6960e-02, -4.5943e-03,  2.3810e-02]]],


        [[[ 4.8933e-03, -3.1361e-03,  1.5432e-02],
          [ 2.1281e-02,  1.3212e-02, -1.7369e-03],
          [-3.2532e-03, -1.0074e-02, -3.6338e-03]],

         [[ 2.9554e-03,  1.2224e-02,  1.6890e-02],
          [-2.5874e-02, -2.7731e-03, -2.4780e-02],
          [-1.3725e-02,  1.8344e-02, -2.5811e-03]],

         [[-1.8993e-02, -6.4747e-03,  2.7415e-02],
          [ 3.4085e-03, -1.1319e-02, -1.1397e-02],
          [-3.8681e-04,  1.1824e-02, -1.3509e-02]],

         ...,

         [[-1.9983e-02, -1.6097e-02, -2.1550e-02],
          [-2.5651e-02,  7.4942e-03, -2.0863e-02],
          [ 3.2745e-02,  2.5637e-03, -3.4527e-02]],

         [[ 1.6853e-02,  2.6009e-02,  3.4445e-02],
          [-1.4309e-02,  1.9544e-02, -9.4674e-03],
          [ 2.2442e-02,  1.8611e-02,  4.0209e-02]],

         [[ 1.2779e-03,  3.8089e-03,  1.1181e-03],
          [-5.1163e-03, -9.7183e-03,  2.0328e-02],
          [ 7.5802e-03,  4.7586e-02,  4.2597e-02]]],


        [[[-2.1310e-02, -2.0794e-02,  4.3545e-03],
          [-2.2884e-02,  4.4247e-03,  1.5762e-03],
          [ 5.8355e-03,  2.9439e-03,  4.7602e-03]],

         [[ 9.1704e-06,  8.7199e-03,  1.0293e-02],
          [ 5.8828e-04,  2.2379e-02,  6.8859e-03],
          [ 2.2166e-03,  3.3759e-02, -1.9202e-02]],

         [[-4.4783e-03, -1.7102e-02, -2.1866e-02],
          [-2.3081e-02, -2.9756e-02, -5.7360e-03],
          [-8.4281e-03, -2.9182e-02, -1.1410e-02]],

         ...,

         [[ 1.3551e-02,  1.1154e-02,  4.2701e-02],
          [-2.1846e-02,  8.2729e-03, -1.6402e-02],
          [-1.8549e-02,  2.8898e-02, -2.2504e-02]],

         [[-3.6879e-02, -1.2050e-02, -2.8201e-03],
          [ 6.2540e-03, -1.1147e-02,  8.4578e-03],
          [-9.0307e-03,  1.7491e-02, -2.6668e-02]],

         [[ 1.6680e-03, -9.7498e-03, -1.5820e-02],
          [-2.9987e-02,  4.2551e-03,  2.4515e-02],
          [-2.0124e-02,  1.1269e-02,  1.9322e-02]]],


        ...,


        [[[-2.0545e-03,  1.0280e-02, -2.0723e-02],
          [-1.9747e-02, -1.4339e-02, -2.3769e-02],
          [-3.8453e-03, -1.5936e-02, -2.3459e-02]],

         [[ 3.1166e-02,  3.2131e-03,  7.1582e-03],
          [-8.4318e-03,  5.9535e-03, -2.3071e-02],
          [-3.5547e-02, -3.9296e-02, -1.7053e-02]],

         [[ 9.8251e-03,  1.2189e-02, -2.1313e-03],
          [-4.9083e-03, -1.5930e-02,  3.9156e-03],
          [-4.7857e-03,  3.6679e-02,  2.2663e-02]],

         ...,

         [[ 1.9173e-02, -7.0871e-03,  8.2041e-03],
          [-1.2895e-02, -2.2979e-02,  2.0178e-02],
          [ 1.2465e-02, -2.5620e-02, -1.8298e-02]],

         [[-8.1762e-03, -1.1862e-02,  2.9651e-02],
          [-3.5618e-02,  2.5708e-02,  1.5804e-02],
          [-1.8803e-02,  4.7407e-03,  1.6113e-02]],

         [[-2.0545e-02, -2.0093e-02,  4.0736e-03],
          [-2.3066e-03, -1.9237e-02, -2.3816e-02],
          [-5.5374e-03,  1.3575e-02, -1.9113e-02]]],


        [[[-3.0873e-02,  8.9886e-03,  1.0141e-02],
          [-8.7194e-03, -1.4838e-02, -2.5014e-02],
          [-2.5902e-02, -1.0292e-02, -7.6366e-04]],

         [[ 1.0801e-02,  1.7327e-02,  1.2674e-02],
          [-1.7402e-02, -2.1054e-02, -2.2762e-02],
          [-7.8424e-03, -8.3442e-03, -1.6938e-02]],

         [[-2.7835e-02, -2.0275e-02, -1.8154e-02],
          [ 1.9461e-02,  2.2095e-02,  3.1124e-02],
          [ 1.7280e-02, -1.7321e-02, -1.6763e-02]],

         ...,

         [[-3.6791e-02,  1.5458e-02,  1.0413e-02],
          [-3.2573e-02,  1.3381e-03, -5.2644e-03],
          [-2.6394e-03,  2.0343e-03,  1.1813e-02]],

         [[ 2.8930e-03, -1.2767e-02, -1.5858e-04],
          [ 4.5159e-03, -2.9640e-02, -6.7060e-04],
          [ 6.0618e-03, -5.0245e-03,  9.6531e-03]],

         [[-2.9061e-03,  7.0270e-03,  4.0596e-03],
          [-1.1614e-02,  7.9083e-04, -2.2103e-02],
          [ 5.3270e-03,  9.4357e-03, -1.7668e-02]]],


        [[[ 6.8643e-03, -7.7214e-03,  1.8365e-02],
          [ 1.2508e-02, -2.0351e-02,  9.9306e-03],
          [ 7.3403e-03,  2.0534e-02,  4.4236e-03]],

         [[ 3.1100e-02,  5.8461e-03,  9.3602e-03],
          [-1.0153e-02,  3.1291e-02,  1.7846e-02],
          [ 9.0647e-03,  1.3773e-03,  1.4094e-02]],

         [[ 6.4765e-03, -1.2808e-02,  4.6619e-03],
          [-6.9992e-03, -3.6308e-02, -1.8171e-02],
          [-3.6596e-02, -1.2717e-02,  4.6285e-03]],

         ...,

         [[ 8.7084e-03, -3.2096e-02,  1.2818e-02],
          [-3.6803e-02,  1.7255e-02,  8.3913e-04],
          [-1.6010e-02,  5.8271e-03, -1.2991e-02]],

         [[ 1.5966e-02, -2.0812e-03,  1.0263e-03],
          [ 2.5848e-03,  2.8059e-02,  2.7793e-02],
          [ 8.2605e-03,  1.5055e-02,  2.4229e-02]],

         [[ 2.0873e-02,  3.1448e-02,  2.2112e-02],
          [-6.1295e-03,  7.3910e-03, -6.3449e-03],
          [-2.1870e-02,  7.8324e-03, -1.5891e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.1959e+06, 7.1878e+04, 6.5743e+04,  ..., 4.1497e+00, 3.9875e+00,
        3.7991e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 807]) 

NULL SPACE BASIS :  tensor([[ 0.0455, -0.0504, -0.0515,  ..., -0.0095, -0.0249, -0.0215],
        [ 0.0088, -0.0210, -0.0917,  ...,  0.0183,  0.0149,  0.0097],
        [ 0.0870,  0.0112, -0.0067,  ..., -0.0147,  0.0059, -0.0126],
        ...,
        [ 0.0184, -0.0020, -0.0382,  ..., -0.0014, -0.0043,  0.0035],
        [ 0.0494, -0.0030,  0.0261,  ...,  0.0112,  0.0097, -0.0003],
        [-0.0014,  0.0495,  0.0056,  ...,  0.0057, -0.0124,  0.0080]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.5779e-02, -3.4029e-03, -9.7494e-04,  ..., -1.6535e-04,
         -1.6916e-04, -8.0770e-05],
        [-3.4029e-03,  2.6193e-02, -3.7210e-03,  ...,  3.9139e-04,
          1.6712e-04, -2.2804e-04],
        [-9.7494e-04, -3.7210e-03,  2.5631e-02,  ..., -2.7903e-04,
         -3.1404e-04, -3.0118e-04],
        ...,
        [-1.6535e-04,  3.9139e-04, -2.7903e-04,  ...,  2.0699e-02,
         -6.0788e-03, -2.0231e-03],
        [-1.6916e-04,  1.6712e-04, -3.1404e-04,  ..., -6.0788e-03,
          2.1111e-02, -6.0682e-03],
        [-8.0770e-05, -2.2804e-04, -3.0118e-04,  ..., -2.0231e-03,
         -6.0682e-03,  1.9430e-02]], device='cuda:0') 

reserving basis 1584/2304; cond: 591580.1875, radio:0.008403952233493328
PARAMETER       :  Parameter containing:
tensor([[[[ 4.2659e-03, -4.3833e-03,  6.9439e-04],
          [-2.3330e-02, -6.4865e-03,  1.4263e-02],
          [-2.4060e-02, -5.8405e-03, -1.7326e-02]],

         [[ 6.5901e-03,  2.0934e-02,  2.3429e-03],
          [-6.6182e-03, -1.9113e-02,  2.0139e-02],
          [-1.2130e-02, -2.1333e-03,  1.2883e-02]],

         [[-1.8452e-02, -1.7510e-02, -1.2190e-02],
          [-2.5209e-03, -1.5981e-02, -2.1300e-02],
          [ 5.7595e-03,  7.5126e-03, -1.1994e-02]],

         ...,

         [[ 1.4399e-02,  1.1549e-02,  1.4837e-02],
          [-1.7653e-02, -1.5194e-02,  8.8147e-03],
          [ 1.8581e-03, -1.2955e-03,  4.9329e-04]],

         [[ 2.1274e-02,  9.6621e-04,  1.4754e-02],
          [-1.3698e-02,  7.9595e-04,  2.3767e-02],
          [ 2.1106e-02, -1.5356e-03,  8.5730e-03]],

         [[ 7.9253e-03,  8.6660e-03, -2.0991e-02],
          [ 7.1434e-03, -3.0066e-03,  3.2451e-03],
          [ 2.5175e-02,  6.6239e-03,  1.2360e-02]]],


        [[[ 9.4698e-03, -4.9771e-03, -1.6219e-03],
          [ 5.8399e-03,  7.7891e-03, -9.3890e-03],
          [ 2.3342e-02, -1.0364e-02, -1.4613e-02]],

         [[-1.0222e-02,  1.1575e-02, -2.0551e-03],
          [-1.8160e-02,  2.2353e-02,  1.1480e-02],
          [ 1.7280e-03, -5.1692e-03,  1.3940e-04]],

         [[-1.5503e-02, -6.9274e-04,  1.1735e-02],
          [ 2.5236e-02,  5.6493e-03,  8.4825e-03],
          [-6.7374e-03,  2.3296e-02, -1.2227e-02]],

         ...,

         [[-1.2450e-02,  2.2875e-02,  2.5917e-03],
          [ 2.1021e-02, -1.3882e-02,  1.5841e-02],
          [ 3.0854e-03, -1.9778e-02, -1.2249e-02]],

         [[-1.4977e-02, -2.6242e-02, -5.7153e-03],
          [-9.0750e-03,  3.5077e-03, -5.8608e-03],
          [-1.4989e-02,  1.7203e-02,  1.8424e-02]],

         [[ 7.2979e-03, -5.2409e-03, -2.0455e-02],
          [-9.4094e-03, -1.4688e-02,  5.9706e-03],
          [-1.9486e-02,  9.6866e-03, -6.8284e-03]]],


        [[[ 1.6776e-02, -2.2032e-02, -1.1736e-02],
          [ 1.6459e-02,  1.8655e-02, -2.7655e-03],
          [ 1.7679e-02,  2.1943e-02, -2.4715e-02]],

         [[-7.3178e-04,  1.2170e-02,  1.6912e-03],
          [-1.0459e-02, -1.3896e-02, -1.9961e-02],
          [-7.9563e-03,  2.2654e-02,  1.3970e-03]],

         [[-3.5087e-03, -1.0176e-02,  1.4347e-03],
          [-5.1416e-03, -4.8338e-03,  1.1004e-02],
          [-3.6793e-04, -1.4619e-02,  9.2656e-03]],

         ...,

         [[-1.6876e-02,  2.2016e-02, -7.7366e-03],
          [ 1.7940e-02, -1.5302e-02, -3.4699e-03],
          [-1.6170e-02, -1.3104e-02, -2.1469e-02]],

         [[-3.9704e-03,  1.8879e-02, -1.2167e-02],
          [-1.7608e-02, -2.1336e-02, -2.0655e-02],
          [ 1.8571e-02, -1.8737e-02, -1.0374e-02]],

         [[ 7.8876e-03,  2.4835e-03, -1.9939e-02],
          [ 2.6312e-03, -4.8102e-03,  1.0517e-02],
          [-9.4926e-03, -7.2408e-05, -9.2581e-03]]],


        ...,


        [[[ 8.2461e-03,  1.7885e-02,  7.5036e-03],
          [ 2.0599e-02, -1.5562e-02, -1.6195e-02],
          [-3.9620e-03, -1.3763e-02, -1.2174e-02]],

         [[-1.5442e-02, -9.7175e-03,  3.0620e-02],
          [ 1.5538e-04,  9.6537e-03, -2.4834e-03],
          [ 2.4409e-02, -1.9687e-02,  1.0493e-02]],

         [[-1.6972e-02, -1.0007e-03, -1.5257e-02],
          [ 4.6367e-03,  2.4767e-02, -2.6125e-02],
          [ 9.1238e-03,  3.0727e-02,  1.2828e-02]],

         ...,

         [[-6.6960e-03,  1.1417e-02,  1.3789e-03],
          [-1.2635e-02, -1.0797e-02,  6.9601e-03],
          [ 1.8156e-03,  1.7926e-03, -2.9652e-02]],

         [[-1.1717e-02,  2.1603e-03, -1.1367e-02],
          [-1.5761e-02, -1.0323e-02, -1.7588e-03],
          [-8.2121e-03,  5.2806e-03,  2.0690e-02]],

         [[-2.4239e-02, -4.0069e-02, -2.6687e-02],
          [ 1.1576e-02, -2.7082e-02, -1.8555e-02],
          [ 1.2751e-02, -1.0818e-02,  1.0525e-02]]],


        [[[-9.0548e-03,  1.6154e-02,  1.2605e-03],
          [-1.2268e-02, -1.9489e-02,  3.9547e-03],
          [ 1.0892e-02, -3.7826e-03, -1.4170e-02]],

         [[-4.0594e-03,  5.7152e-03, -3.9245e-03],
          [ 4.5662e-03, -1.5072e-02,  1.4866e-02],
          [ 8.0314e-03, -1.3748e-02, -1.5957e-02]],

         [[-2.0981e-02, -2.7609e-02, -2.0036e-02],
          [ 2.5920e-02,  1.7991e-02,  2.5004e-04],
          [-3.9697e-03,  8.5605e-03, -4.7661e-03]],

         ...,

         [[ 7.6890e-03, -1.4186e-02, -3.1531e-02],
          [ 1.4673e-02,  1.4043e-02, -2.4959e-02],
          [-1.6059e-02, -2.3818e-02,  1.1473e-02]],

         [[ 4.0087e-03,  4.3125e-04,  1.0451e-02],
          [-1.3898e-02, -8.3031e-03, -1.0610e-02],
          [-8.1906e-03,  8.1475e-03,  4.3164e-03]],

         [[-2.2432e-02,  1.7938e-02,  1.8248e-02],
          [-2.5679e-02, -9.1150e-03,  2.6657e-02],
          [-1.4585e-02,  3.7276e-02,  4.1051e-02]]],


        [[[-1.9688e-03, -3.0884e-02,  8.5817e-03],
          [-1.5138e-02, -8.0950e-03,  2.7105e-02],
          [-2.0008e-02, -9.0178e-03,  9.3159e-03]],

         [[-1.9793e-02, -1.6294e-02,  1.4058e-02],
          [-1.1196e-02,  6.6574e-03, -6.2357e-03],
          [ 5.4877e-03,  1.2233e-02,  1.7513e-03]],

         [[ 9.9926e-03, -7.2130e-03, -5.7406e-03],
          [-1.1486e-02,  2.2589e-02, -3.5385e-03],
          [-2.1227e-02,  1.9970e-03, -4.9305e-03]],

         ...,

         [[-2.1248e-03, -2.8310e-02, -1.3497e-03],
          [-2.2139e-02, -2.5849e-02,  8.2562e-03],
          [ 1.3118e-02, -1.3542e-02,  1.8985e-02]],

         [[ 9.3652e-03,  2.5002e-03,  1.7357e-02],
          [ 6.4935e-03, -1.2692e-02, -1.2398e-03],
          [-2.3019e-02,  1.3980e-02, -2.6957e-02]],

         [[-2.4366e-02,  8.8955e-03, -3.3834e-02],
          [ 1.0431e-02, -1.4016e-02, -5.4865e-03],
          [ 1.3062e-02, -2.5005e-02, -1.9139e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.8318e+06, 1.1156e+05, 1.0532e+05,  ..., 3.1693e+00, 3.1314e+00,
        3.0965e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1584]) 

NULL SPACE BASIS :  tensor([[ 3.4855e-02, -6.3643e-03, -2.1201e-02,  ...,  9.0385e-04,
          1.6498e-02,  2.3284e-02],
        [ 9.4454e-04, -4.9435e-02, -3.7650e-02,  ...,  6.8162e-03,
          6.2233e-03, -6.8920e-03],
        [ 2.7043e-02, -1.1429e-02, -2.9838e-02,  ...,  3.5780e-03,
         -2.8269e-02, -1.3240e-02],
        ...,
        [ 1.6942e-02,  8.4145e-03, -3.6211e-02,  ..., -2.0985e-02,
          4.3860e-03, -6.2522e-02],
        [-7.1202e-03,  4.0478e-02, -3.7291e-03,  ..., -6.8788e-03,
          3.7555e-02, -1.3430e-03],
        [ 6.7982e-03,  1.6356e-03,  1.2356e-02,  ...,  5.6677e-03,
          8.6326e-06, -8.0814e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.7785e-02, -4.3693e-05, -1.6068e-04,  ..., -1.8118e-04,
         -1.5572e-04,  1.6723e-04],
        [-4.3693e-05,  1.7611e-02, -3.0655e-04,  ...,  4.9004e-05,
         -1.6730e-04,  9.9108e-05],
        [-1.6068e-04, -3.0655e-04,  1.7946e-02,  ..., -1.3202e-05,
          1.9365e-05, -1.3453e-04],
        ...,
        [-1.8118e-04,  4.9004e-05, -1.3202e-05,  ...,  1.8866e-02,
         -1.1878e-04, -1.6865e-04],
        [-1.5572e-04, -1.6730e-04,  1.9365e-05,  ..., -1.1878e-04,
          1.8518e-02, -1.7288e-04],
        [ 1.6723e-04,  9.9108e-05, -1.3453e-04,  ..., -1.6865e-04,
         -1.7288e-04,  1.8851e-02]], device='cuda:0') 

reserving basis 102/128; cond: 11596.013671875, radio:0.022496381774544716
PARAMETER       :  Parameter containing:
tensor([[[[ 2.5544e-02]],

         [[-3.5761e-02]],

         [[-3.3536e-02]],

         ...,

         [[-3.1177e-02]],

         [[-7.1047e-02]],

         [[ 7.6003e-02]]],


        [[[-4.7819e-02]],

         [[ 6.1754e-02]],

         [[ 7.3145e-02]],

         ...,

         [[-4.6940e-02]],

         [[-3.8296e-03]],

         [[-1.2165e-02]]],


        [[[ 5.6881e-02]],

         [[-2.6762e-02]],

         [[-3.6751e-02]],

         ...,

         [[ 5.7397e-02]],

         [[ 2.1832e-02]],

         [[-2.0855e-02]]],


        ...,


        [[[-6.2341e-02]],

         [[ 2.5955e-02]],

         [[ 8.1958e-02]],

         ...,

         [[-8.1788e-02]],

         [[-6.3598e-02]],

         [[ 2.6791e-02]]],


        [[[-2.6111e-02]],

         [[-7.2173e-02]],

         [[-2.2007e-05]],

         ...,

         [[ 2.8648e-02]],

         [[ 5.0678e-02]],

         [[ 2.4052e-02]]],


        [[[ 4.1159e-02]],

         [[-3.7441e-02]],

         [[-6.7452e-02]],

         ...,

         [[-5.3216e-02]],

         [[ 6.4365e-02]],

         [[-3.4924e-02]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.6748e+05, 9.2604e+03, 7.7027e+03, 2.9380e+03, 1.8771e+03, 1.5186e+03,
        1.2724e+03, 8.9804e+02, 8.7146e+02, 6.5875e+02, 5.8511e+02, 4.7158e+02,
        4.4855e+02, 3.8021e+02, 3.7202e+02, 3.1805e+02, 2.8281e+02, 2.6399e+02,
        2.4673e+02, 2.2415e+02, 2.1688e+02, 2.1395e+02, 1.7637e+02, 1.7412e+02,
        1.7050e+02, 1.5208e+02, 1.3982e+02, 1.3219e+02, 1.2819e+02, 1.2221e+02,
        1.1400e+02, 1.1003e+02, 1.0802e+02, 1.0422e+02, 9.7086e+01, 8.8276e+01,
        8.6411e+01, 8.5220e+01, 8.3095e+01, 8.1213e+01, 7.9243e+01, 7.4596e+01,
        7.2305e+01, 7.1176e+01, 7.0003e+01, 6.4896e+01, 6.3183e+01, 6.0917e+01,
        6.0747e+01, 6.0417e+01, 5.8745e+01, 5.7277e+01, 5.6702e+01, 5.4818e+01,
        5.2438e+01, 5.1586e+01, 5.0192e+01, 4.9321e+01, 4.8507e+01, 4.8039e+01,
        4.6040e+01, 4.5308e+01, 4.5120e+01, 4.3535e+01, 4.3372e+01, 4.2167e+01,
        4.1877e+01, 4.0471e+01, 3.9987e+01, 3.9247e+01, 3.9134e+01, 3.8046e+01,
        3.7660e+01, 3.6645e+01, 3.6118e+01, 3.5685e+01, 3.4534e+01, 3.4255e+01,
        3.3903e+01, 3.3166e+01, 3.2151e+01, 3.1710e+01, 3.1145e+01, 3.0522e+01,
        2.9571e+01, 2.9507e+01, 2.8732e+01, 2.8599e+01, 2.8304e+01, 2.8071e+01,
        2.7679e+01, 2.7320e+01, 2.7126e+01, 2.6829e+01, 2.6277e+01, 2.6211e+01,
        2.5643e+01, 2.4917e+01, 2.4426e+01, 2.4169e+01, 2.3940e+01, 2.3641e+01,
        2.3453e+01, 2.3349e+01, 2.3080e+01, 2.2837e+01, 2.2179e+01, 2.1773e+01,
        2.1568e+01, 2.1469e+01, 2.1105e+01, 2.0826e+01, 2.0465e+01, 2.0052e+01,
        1.9734e+01, 1.9400e+01, 1.9117e+01, 1.8627e+01, 1.8292e+01, 1.8016e+01,
        1.7985e+01, 1.7477e+01, 1.6866e+01, 1.6677e+01, 1.6270e+01, 1.5847e+01,
        1.5144e+01, 1.4443e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 102]) 

NULL SPACE BASIS :  tensor([[ 0.0844,  0.0186, -0.1691,  ...,  0.0617,  0.0760,  0.0272],
        [ 0.0203, -0.1264,  0.0280,  ..., -0.1213, -0.0947, -0.1040],
        [ 0.1385,  0.0061,  0.0356,  ..., -0.1690, -0.1392, -0.1127],
        ...,
        [ 0.0754,  0.0829, -0.0384,  ...,  0.0591,  0.0284,  0.0068],
        [ 0.1231, -0.0380,  0.0267,  ...,  0.0547,  0.0435,  0.0078],
        [-0.0556,  0.0589,  0.0811,  ..., -0.0277,  0.0094,  0.0643]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.8096e-02,  8.3503e-04, -4.6609e-05,  ..., -3.2977e-03,
          3.7565e-03, -1.0847e-02],
        [ 8.3503e-04,  6.9329e-02,  1.0542e-03,  ..., -2.1583e-03,
         -3.1048e-03,  7.6326e-03],
        [-4.6609e-05,  1.0542e-03,  8.4615e-02,  ...,  2.0160e-03,
          4.8971e-04, -2.2401e-03],
        ...,
        [-3.2977e-03, -2.1583e-03,  2.0160e-03,  ...,  7.7509e-02,
          1.5258e-03, -6.7307e-03],
        [ 3.7565e-03, -3.1048e-03,  4.8971e-04,  ...,  1.5258e-03,
          7.5920e-02,  1.0127e-04],
        [-1.0847e-02,  7.6326e-03, -2.2401e-03,  ..., -6.7307e-03,
          1.0127e-04,  6.5661e-02]], device='cuda:0') 

reserving basis 1544/2304; cond: 585402.0, radio:0.00793976429849863
PARAMETER       :  Parameter containing:
tensor([[[[ 5.8590e-03,  1.9831e-03, -1.0292e-02],
          [ 4.8314e-03, -2.0207e-02,  6.1413e-04],
          [-5.6991e-03,  2.1214e-02, -6.2169e-03]],

         [[-2.1805e-03,  1.2176e-02,  1.5998e-03],
          [ 2.6574e-02, -5.7951e-03, -2.1105e-02],
          [-3.7949e-03,  5.6117e-03,  9.6109e-03]],

         [[ 1.6100e-02,  2.7479e-02, -1.2589e-02],
          [ 1.4458e-02,  1.0716e-02,  1.2367e-02],
          [-4.3369e-03,  9.4254e-03,  4.8165e-03]],

         ...,

         [[-1.2000e-03,  2.0436e-02,  3.3708e-03],
          [-2.8535e-02, -6.0254e-03, -3.0923e-02],
          [-1.3427e-05, -1.5684e-02, -2.0211e-03]],

         [[-2.2836e-02,  1.4753e-02,  1.0222e-02],
          [ 4.7219e-03, -2.5501e-02, -1.8547e-02],
          [-1.9377e-02, -1.5195e-02, -1.5452e-02]],

         [[ 1.1131e-03, -1.5485e-03, -2.1754e-02],
          [ 7.7192e-03,  1.1633e-02,  5.0779e-03],
          [ 2.3395e-02,  2.0324e-02, -1.3713e-02]]],


        [[[-5.9917e-03,  2.2978e-02,  3.1392e-02],
          [ 1.9124e-03, -1.9026e-02,  1.6199e-02],
          [ 6.5185e-03,  1.7741e-02, -2.5135e-02]],

         [[-1.3638e-02, -2.3853e-03, -4.0095e-02],
          [ 5.8662e-04, -5.5801e-03,  8.0413e-03],
          [ 2.0853e-03,  1.8853e-02,  2.5717e-02]],

         [[-7.0046e-03, -2.2909e-02,  1.5563e-02],
          [-1.0208e-02,  6.8734e-04, -1.3807e-02],
          [ 7.4690e-03,  7.7815e-03,  1.9389e-02]],

         ...,

         [[-1.1864e-02, -2.5984e-02, -1.1239e-02],
          [-8.5617e-03,  6.8450e-03,  1.7486e-02],
          [ 4.1383e-03,  1.4343e-02, -7.8047e-03]],

         [[ 1.2601e-03, -1.7700e-02, -5.7356e-04],
          [ 1.0335e-02,  1.7015e-02,  2.2773e-02],
          [-1.6116e-02, -1.9285e-02,  2.0211e-02]],

         [[ 6.7170e-03, -1.1877e-02, -2.9139e-02],
          [-7.8734e-03,  5.2332e-03,  3.5913e-03],
          [-1.3374e-02, -2.5373e-02, -1.2934e-02]]],


        [[[-1.3887e-02,  5.5214e-04,  3.7804e-03],
          [ 1.2184e-02, -2.0341e-02, -1.7138e-02],
          [-1.0519e-02,  2.1238e-02,  3.9312e-03]],

         [[-1.6099e-02,  7.5163e-03,  3.2518e-03],
          [-1.1410e-02, -4.4259e-03,  1.8766e-02],
          [-1.8316e-02,  1.6069e-02,  1.8723e-03]],

         [[ 2.1465e-02, -9.3158e-03,  1.9464e-02],
          [-1.5456e-02, -3.6116e-03,  2.6121e-02],
          [-1.5846e-02, -1.1231e-02,  2.2361e-02]],

         ...,

         [[ 1.7080e-02, -6.0532e-03, -1.7170e-02],
          [ 1.0337e-02, -2.9819e-02, -4.0049e-03],
          [ 7.5941e-03, -2.9724e-02, -3.3648e-02]],

         [[-8.1014e-03,  7.5866e-04, -1.4541e-02],
          [ 1.3856e-02,  5.7528e-03,  4.8640e-03],
          [ 2.4649e-02, -3.1884e-02, -8.6583e-03]],

         [[-1.8306e-03, -1.1484e-02, -1.0609e-02],
          [-1.0747e-02, -1.4678e-02,  3.5873e-03],
          [ 1.7442e-02,  3.0483e-02,  1.7834e-02]]],


        ...,


        [[[-1.9898e-02,  1.1622e-02, -2.7255e-02],
          [ 3.9624e-03, -1.1341e-02, -2.9876e-02],
          [ 1.0309e-02,  3.7078e-04, -5.7195e-03]],

         [[-9.9618e-03, -2.5056e-02, -3.2154e-03],
          [ 1.4434e-02,  1.6192e-02,  7.3862e-03],
          [-1.6557e-02, -2.4503e-02, -1.1721e-02]],

         [[ 3.5677e-03, -1.8039e-02,  1.1816e-02],
          [-1.9583e-02,  3.7875e-05,  2.0542e-02],
          [ 1.7061e-02,  5.0122e-03, -1.6707e-02]],

         ...,

         [[ 1.5908e-02, -3.6692e-02,  1.2789e-02],
          [-1.3153e-02, -2.3218e-03,  5.6174e-03],
          [-1.4398e-02, -2.3962e-02,  2.8990e-03]],

         [[ 1.4708e-02, -2.3547e-03,  2.2490e-03],
          [-1.5079e-02,  1.0954e-02,  6.4938e-04],
          [-2.1083e-02,  1.5600e-03,  2.9604e-03]],

         [[-1.9543e-02, -1.1064e-02, -2.4484e-02],
          [ 9.4957e-03,  1.3368e-02, -5.1275e-03],
          [-2.1951e-02,  2.5426e-02,  3.1100e-03]]],


        [[[ 7.6691e-03, -5.4705e-03,  3.5751e-02],
          [ 6.3066e-03,  1.8893e-02,  1.1007e-02],
          [ 2.6125e-02, -5.2490e-03,  3.4533e-02]],

         [[ 4.8595e-03,  2.0749e-02, -1.7387e-02],
          [-1.3256e-02,  3.6510e-03,  2.1672e-02],
          [ 3.1657e-03,  9.4530e-03,  2.6768e-02]],

         [[-1.2744e-02,  2.5748e-03,  1.4064e-02],
          [-1.4631e-02,  7.6897e-03, -2.3544e-02],
          [ 3.6283e-03,  1.9364e-02,  9.8997e-03]],

         ...,

         [[ 1.0227e-02,  1.2969e-02, -1.4029e-02],
          [ 1.3982e-02,  1.8466e-02, -1.7659e-02],
          [-2.4303e-04,  1.7999e-02,  3.0624e-02]],

         [[ 4.6135e-03, -9.7207e-03,  1.8838e-02],
          [ 9.9712e-03,  1.5928e-02,  2.0502e-02],
          [-1.3781e-02,  7.5733e-03,  1.4354e-02]],

         [[-1.2893e-02, -9.4709e-03,  4.4335e-03],
          [ 7.3904e-03, -4.1847e-03,  2.0262e-03],
          [-1.5826e-02, -2.7855e-03,  1.8420e-02]]],


        [[[ 2.8663e-02,  3.6950e-03, -6.9851e-03],
          [ 8.9775e-03, -4.7367e-03,  8.0532e-03],
          [-2.4013e-03, -1.2312e-02, -7.8691e-03]],

         [[-1.0548e-02, -7.3299e-03,  1.3252e-02],
          [ 1.7970e-02, -2.2667e-03,  2.3561e-02],
          [-1.7317e-03,  2.6079e-02,  1.0662e-02]],

         [[-1.2285e-02, -2.1502e-02, -3.5302e-03],
          [-1.3452e-02, -2.7223e-02, -1.3854e-02],
          [ 1.5197e-03,  5.5983e-04,  4.3377e-03]],

         ...,

         [[ 1.0315e-02,  8.2930e-03,  1.7998e-02],
          [ 1.5787e-02,  1.4320e-02, -1.1917e-02],
          [-2.9455e-04, -1.3124e-02, -1.5213e-02]],

         [[ 1.4080e-02, -1.0806e-02, -1.3678e-02],
          [-2.5691e-04, -2.3554e-02, -1.4888e-02],
          [ 5.7730e-04, -2.1617e-02, -2.0512e-02]],

         [[ 6.1917e-03, -2.6279e-03, -1.1987e-02],
          [-2.2037e-05, -8.2303e-03, -1.6065e-02],
          [ 2.2585e-02,  3.8557e-03,  1.4939e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.5253e+06, 1.0238e+05, 9.7775e+04,  ..., 2.6988e+00, 2.6241e+00,
        2.6056e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1544]) 

NULL SPACE BASIS :  tensor([[-0.0045, -0.0218,  0.0005,  ...,  0.0005, -0.0098,  0.0070],
        [-0.0179, -0.0010,  0.0063,  ..., -0.0027,  0.0111,  0.0022],
        [ 0.0248, -0.0230, -0.0140,  ...,  0.0148, -0.0129, -0.0026],
        ...,
        [-0.0143, -0.0020,  0.0345,  ...,  0.0104,  0.0028,  0.0201],
        [-0.0049, -0.0324, -0.0023,  ...,  0.0031, -0.0032, -0.0251],
        [-0.0071,  0.0089, -0.0220,  ...,  0.0042, -0.0007,  0.0150]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.3131e-02, -3.8053e-03, -2.0146e-04,  ...,  2.1526e-04,
         -4.6276e-05,  9.5871e-05],
        [-3.8053e-03,  1.3608e-02, -3.5585e-03,  ..., -1.2571e-04,
          4.7956e-04,  9.7161e-06],
        [-2.0146e-04, -3.5585e-03,  1.2797e-02,  ..., -8.6122e-05,
          1.5327e-04,  3.9672e-04],
        ...,
        [ 2.1526e-04, -1.2571e-04, -8.6122e-05,  ...,  1.7977e-02,
         -6.8328e-04, -3.8995e-04],
        [-4.6276e-05,  4.7956e-04,  1.5327e-04,  ..., -6.8328e-04,
          1.7511e-02, -3.9985e-04],
        [ 9.5871e-05,  9.7161e-06,  3.9672e-04,  ..., -3.8995e-04,
         -3.9985e-04,  1.7417e-02]], device='cuda:0') 

reserving basis 1490/2304; cond: 643690.9375, radio:0.0069661857560276985
PARAMETER       :  Parameter containing:
tensor([[[[-1.3845e-02, -5.8326e-03, -1.5060e-02],
          [-5.0284e-03, -2.9721e-02,  6.8182e-03],
          [-1.4583e-02, -1.2233e-02, -4.6143e-03]],

         [[ 2.2941e-02,  2.7015e-02,  2.9042e-02],
          [-1.1240e-02,  1.1958e-02, -1.9854e-02],
          [-7.9433e-03,  2.4833e-02,  2.6084e-02]],

         [[-5.0745e-03, -1.2237e-02, -2.8236e-02],
          [-1.5119e-02, -1.8000e-02, -3.8219e-03],
          [ 1.3337e-02, -2.2545e-02, -1.6202e-02]],

         ...,

         [[-1.6607e-02,  2.2496e-02, -1.9067e-02],
          [ 6.5832e-03,  3.2093e-03, -2.2675e-02],
          [ 9.6713e-04,  2.5718e-03,  9.3733e-03]],

         [[ 2.0899e-02,  2.0854e-03, -2.0011e-02],
          [-1.1585e-02,  2.2286e-03, -8.4538e-03],
          [-2.6771e-02,  1.5559e-02,  1.0259e-02]],

         [[ 4.2563e-03,  2.5821e-02,  7.7885e-03],
          [-1.0473e-02,  1.4379e-02, -1.7570e-02],
          [ 9.3588e-03, -4.7517e-03,  1.6030e-02]]],


        [[[ 1.4728e-03,  2.0839e-02,  3.3229e-02],
          [-2.4256e-02,  1.8793e-02,  8.2197e-03],
          [ 3.9231e-03, -5.1820e-03,  7.6382e-03]],

         [[ 2.1677e-02,  1.7039e-02, -1.9529e-02],
          [ 1.2127e-02,  2.7957e-03, -4.4688e-03],
          [-2.9896e-02, -1.1719e-02, -1.8173e-02]],

         [[ 2.5179e-03, -2.9628e-03,  9.3357e-03],
          [ 4.4151e-03, -2.5941e-02, -1.6705e-02],
          [ 1.5063e-02, -8.3838e-03,  1.6715e-02]],

         ...,

         [[-1.8492e-02, -3.6413e-03, -1.4364e-02],
          [ 8.3180e-03,  1.2986e-02,  2.3588e-05],
          [-1.6443e-03,  1.4266e-02, -9.4530e-03]],

         [[ 2.9308e-03, -7.2271e-03,  2.1051e-02],
          [-9.3858e-04, -4.1209e-03,  2.0875e-02],
          [ 6.1041e-03,  1.0154e-02,  1.4841e-02]],

         [[-2.3255e-02, -1.6034e-02, -2.2389e-02],
          [-1.8627e-02, -1.5560e-02, -1.7491e-02],
          [-1.8952e-02, -1.6603e-02,  2.7368e-03]]],


        [[[-2.7782e-02, -1.9687e-02, -1.6014e-02],
          [ 1.7799e-03,  1.7500e-02,  7.1222e-03],
          [ 1.0690e-02, -8.2936e-03, -1.6677e-02]],

         [[-1.4637e-02,  1.6995e-02,  4.8425e-03],
          [-8.2569e-03,  3.2726e-03,  2.7603e-03],
          [-3.3161e-03, -1.4396e-02,  1.7859e-02]],

         [[-8.4888e-03,  7.6149e-03,  5.4414e-04],
          [-1.3757e-03, -1.6633e-02, -2.4598e-02],
          [ 9.0282e-03, -5.3497e-03,  2.9433e-03]],

         ...,

         [[-5.5139e-03,  2.7204e-02, -8.8937e-03],
          [-1.8542e-02,  1.1628e-03, -1.6065e-02],
          [-2.0636e-02, -2.3389e-02,  8.0632e-03]],

         [[-2.8321e-04,  2.1979e-03, -6.0462e-04],
          [-2.1713e-03,  1.3274e-02,  9.4124e-03],
          [-1.8392e-02, -1.7123e-02,  1.4171e-03]],

         [[-2.0259e-02, -1.0445e-02, -4.3916e-04],
          [ 1.0690e-02, -8.0184e-03,  1.9259e-02],
          [-6.8336e-03, -1.4463e-03, -1.2950e-02]]],


        ...,


        [[[-1.3987e-02, -1.6840e-02,  1.1953e-03],
          [-1.1862e-03,  1.0721e-02,  1.1191e-02],
          [-1.2320e-02, -1.2962e-02,  3.2453e-03]],

         [[ 7.5781e-04,  2.1659e-02, -2.5528e-03],
          [ 1.4781e-02, -2.0764e-03,  2.8934e-02],
          [-2.6481e-03, -1.3489e-02, -1.0412e-02]],

         [[-2.0417e-03, -1.9948e-02,  6.6341e-03],
          [-6.1006e-04,  1.9561e-02, -4.7292e-03],
          [-3.6440e-02, -6.2239e-03,  2.2695e-02]],

         ...,

         [[ 2.4500e-02,  4.3223e-03,  1.3830e-02],
          [ 9.2497e-03,  1.7867e-02,  1.5985e-02],
          [-1.4615e-03, -1.0853e-02,  6.4925e-03]],

         [[ 3.2598e-03, -1.4495e-02, -2.2725e-02],
          [-4.2754e-03,  1.8293e-02, -5.9377e-03],
          [ 1.1213e-02, -1.3137e-02,  1.7098e-02]],

         [[ 1.2438e-02,  1.3906e-02, -6.4946e-03],
          [-1.1764e-02, -1.9581e-03,  1.7775e-02],
          [ 1.2825e-02, -1.1405e-03,  7.8673e-03]]],


        [[[ 8.7474e-03, -1.8417e-02,  7.0732e-03],
          [ 2.6599e-02,  3.9050e-03, -1.0342e-02],
          [-2.7619e-03,  1.8905e-03, -1.7629e-02]],

         [[ 2.4271e-03, -9.5903e-03,  1.4732e-02],
          [ 1.7683e-02,  1.9067e-02,  3.5181e-02],
          [ 2.9782e-02,  1.9952e-02,  3.3454e-02]],

         [[-1.3748e-02,  1.5802e-02, -7.5400e-03],
          [ 2.7182e-02,  3.0015e-02,  1.1449e-02],
          [-1.7598e-02,  2.6345e-04,  2.3614e-02]],

         ...,

         [[-1.2884e-02,  7.4548e-03,  2.1274e-02],
          [ 1.7318e-02,  9.5433e-03,  1.1684e-02],
          [-1.3778e-02,  9.4752e-03, -1.5056e-02]],

         [[-2.1394e-02, -2.0687e-02,  8.2575e-03],
          [-1.0934e-02,  9.1158e-03, -7.9994e-04],
          [ 1.7360e-02, -1.4482e-02, -8.5522e-03]],

         [[-1.9100e-02,  1.4935e-03,  2.2179e-02],
          [ 1.3836e-02,  2.0239e-02, -1.1714e-02],
          [ 1.3345e-02,  1.1566e-02, -6.7500e-03]]],


        [[[ 1.4944e-02, -1.2031e-02, -2.7640e-03],
          [-6.3133e-03, -1.0870e-02, -2.6236e-03],
          [-2.1502e-02, -1.9309e-02, -9.4098e-04]],

         [[ 4.7240e-03, -6.7353e-03, -1.3307e-02],
          [-6.3023e-03, -2.7268e-03, -1.8183e-02],
          [ 3.3420e-04, -2.1821e-02, -1.0313e-02]],

         [[ 3.1313e-02,  1.7976e-03,  2.3785e-03],
          [ 2.4993e-02,  4.6094e-03,  9.9758e-03],
          [-2.9843e-03, -5.9168e-03, -1.4020e-02]],

         ...,

         [[ 1.4387e-02,  1.2731e-02,  2.4223e-03],
          [ 1.9764e-02, -7.8499e-03,  6.1172e-03],
          [-6.1247e-03, -3.3533e-03,  6.5549e-04]],

         [[-6.4990e-03,  3.7697e-03, -1.9900e-02],
          [ 2.0163e-02,  1.8560e-02,  1.2471e-02],
          [ 4.4901e-05,  3.8004e-02,  3.2787e-02]],

         [[ 2.0453e-02, -7.2925e-04, -1.1296e-02],
          [ 7.6153e-03,  7.6937e-03, -1.2441e-02],
          [-3.0396e-02, -3.2891e-03, -1.7038e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.5308e+06, 1.0873e+05, 1.0052e+05,  ..., 2.5116e+00, 2.4526e+00,
        2.3782e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1490]) 

NULL SPACE BASIS :  tensor([[ 0.0265, -0.0243,  0.0319,  ..., -0.0152, -0.0159,  0.0128],
        [ 0.0243, -0.0239,  0.0269,  ..., -0.0064,  0.0086, -0.0166],
        [-0.0224, -0.0205,  0.0032,  ..., -0.0067, -0.0064,  0.0192],
        ...,
        [-0.0036, -0.0275,  0.0085,  ...,  0.0101,  0.0036, -0.0326],
        [ 0.0106, -0.0057,  0.0108,  ...,  0.0258,  0.0074,  0.0360],
        [-0.0045, -0.0162, -0.0019,  ..., -0.0398,  0.0066, -0.0221]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.5816e-02, -2.5606e-03, -2.2743e-04,  ...,  3.7815e-04,
          1.7314e-04,  2.7179e-04],
        [-2.5606e-03,  1.5892e-02, -2.9600e-03,  ..., -1.4411e-04,
          4.8198e-04,  2.3342e-04],
        [-2.2743e-04, -2.9600e-03,  1.7062e-02,  ..., -1.2781e-05,
         -5.2017e-05,  2.8785e-04],
        ...,
        [ 3.7815e-04, -1.4411e-04, -1.2781e-05,  ...,  2.0273e-02,
         -8.3973e-04, -1.2807e-04],
        [ 1.7314e-04,  4.8198e-04, -5.2017e-05,  ..., -8.3973e-04,
          2.0285e-02, -7.3246e-04],
        [ 2.7179e-04,  2.3342e-04,  2.8785e-04,  ..., -1.2807e-04,
         -7.3246e-04,  2.0324e-02]], device='cuda:0') 

reserving basis 144/2304; cond: 73772952.0, radio:6.577309704880463e-06
PARAMETER       :  Parameter containing:
tensor([[[[-5.8381e-03, -9.0222e-03, -6.8012e-03],
          [-7.8822e-03,  1.6134e-02,  1.3042e-02],
          [-2.3823e-02, -4.2755e-03, -2.9235e-03]],

         [[ 3.0807e-05,  2.0676e-03, -1.2361e-02],
          [-8.0996e-03,  1.2375e-02, -4.6189e-03],
          [ 1.6566e-02, -2.4838e-02,  4.9757e-03]],

         [[ 1.5137e-02, -5.4737e-03,  8.4102e-03],
          [ 1.2616e-02, -1.0148e-02,  2.1340e-03],
          [-1.7879e-02, -1.1480e-02, -7.1885e-03]],

         ...,

         [[ 1.4754e-02, -1.5132e-02,  1.5445e-03],
          [ 9.8035e-03,  1.0560e-02,  1.4427e-02],
          [-1.4284e-03, -2.5470e-02,  1.2357e-02]],

         [[ 1.9946e-02,  1.1980e-02, -2.3372e-03],
          [-1.0376e-02,  2.5322e-03, -1.0637e-02],
          [-2.4244e-03,  1.8057e-02, -1.6818e-02]],

         [[-5.7933e-03, -4.4273e-03,  7.9753e-03],
          [-3.1223e-02,  2.8970e-03, -8.8324e-05],
          [ 5.3928e-03, -1.0342e-02,  2.0269e-02]]],


        [[[-1.1540e-02,  8.2088e-03,  7.5165e-03],
          [ 1.0598e-02, -7.2583e-03,  1.5003e-02],
          [ 1.6006e-02, -5.8581e-03, -8.9754e-04]],

         [[ 1.7195e-02, -2.0294e-03, -1.9460e-02],
          [ 1.8375e-03, -4.6660e-03,  8.5936e-03],
          [ 1.3262e-02, -2.1341e-02, -1.5869e-02]],

         [[ 1.5582e-02, -2.6624e-02,  2.3243e-03],
          [-2.2414e-02,  7.2891e-04,  8.4997e-03],
          [-1.0669e-02, -1.8237e-02,  4.2803e-03]],

         ...,

         [[-2.0774e-02, -1.7127e-02, -7.9961e-03],
          [ 7.5502e-04, -1.2220e-02, -2.5838e-02],
          [ 4.0822e-03, -4.3293e-03, -3.0395e-05]],

         [[-8.8941e-03,  6.3136e-03, -6.2622e-03],
          [ 9.1632e-03,  2.4227e-02, -9.7524e-03],
          [-1.5452e-02,  1.1754e-02, -9.4366e-03]],

         [[ 1.1008e-02,  2.1150e-02,  4.0943e-03],
          [ 1.2737e-03,  5.7916e-03, -8.4913e-03],
          [-8.4176e-03,  2.5853e-02, -2.8201e-03]]],


        [[[ 3.3911e-03, -2.4151e-02, -4.0833e-03],
          [-3.3251e-03,  1.0456e-02, -4.7645e-03],
          [ 7.2096e-03,  1.0438e-02,  3.3346e-03]],

         [[ 2.1417e-02,  2.0066e-02,  2.2104e-02],
          [ 2.0884e-02,  9.2978e-03,  5.7164e-03],
          [ 8.8046e-03, -3.5065e-03,  9.9525e-03]],

         [[-1.7655e-02, -1.3699e-02,  3.0347e-03],
          [ 5.8297e-03, -1.9543e-02, -5.6578e-03],
          [-1.2840e-02, -8.3522e-03, -3.0598e-02]],

         ...,

         [[ 1.0916e-03, -3.9499e-03, -3.1805e-02],
          [-5.2081e-03,  3.2320e-03, -8.2906e-03],
          [ 1.8662e-02,  1.5402e-03, -2.9308e-03]],

         [[ 1.2858e-02,  3.9957e-03, -4.3603e-03],
          [-5.9751e-03, -1.4404e-03,  6.0232e-03],
          [-2.2605e-02, -2.2694e-02, -1.4665e-02]],

         [[ 3.0113e-04,  4.8873e-03, -6.7803e-03],
          [ 8.5646e-04,  1.6418e-02, -2.4683e-04],
          [ 1.0013e-02,  1.0661e-02, -9.9674e-03]]],


        ...,


        [[[-8.8589e-04,  1.1588e-03, -1.4486e-02],
          [-1.4499e-02,  2.7970e-03,  2.9163e-05],
          [ 7.3081e-03, -1.0839e-02, -7.8261e-03]],

         [[ 1.5797e-02, -4.3462e-03, -1.1844e-02],
          [ 2.1505e-02,  1.4293e-02, -1.4628e-02],
          [-6.8427e-03,  2.8154e-03, -1.4075e-02]],

         [[-2.0620e-02, -2.4158e-03,  8.0435e-03],
          [-5.9533e-03, -1.4771e-02,  7.0953e-03],
          [ 4.5581e-03,  3.3524e-03, -2.5439e-02]],

         ...,

         [[ 8.6755e-03, -1.2572e-02,  7.5471e-03],
          [ 1.7750e-03,  5.2800e-03, -5.6368e-03],
          [-5.1643e-03, -7.2803e-03, -2.9868e-02]],

         [[-1.5495e-02,  7.1471e-03, -3.4071e-03],
          [ 7.6381e-03, -2.3092e-02, -2.1043e-02],
          [-7.6690e-03, -1.3695e-02, -1.1646e-02]],

         [[ 1.0256e-02,  1.8571e-02,  6.5582e-03],
          [-1.0244e-02,  1.2506e-02, -1.0180e-02],
          [-2.1362e-03,  1.0163e-02, -6.1700e-03]]],


        [[[-1.9543e-02, -3.4918e-03, -1.1424e-03],
          [-1.0308e-02,  2.4148e-03,  7.6172e-03],
          [ 2.1123e-03,  1.1586e-02,  7.6923e-03]],

         [[-1.1946e-02,  2.1441e-02,  1.1367e-02],
          [-3.2809e-03,  2.8837e-03,  1.1842e-02],
          [ 8.4068e-03, -9.8446e-03,  1.2077e-02]],

         [[-7.9283e-03,  5.0832e-03,  1.8502e-02],
          [ 1.3778e-02, -6.1454e-03,  1.4214e-02],
          [-5.0561e-03, -1.0394e-02, -1.2496e-02]],

         ...,

         [[ 1.2012e-02, -1.8180e-02, -1.3374e-03],
          [-2.9758e-03, -1.5039e-02,  1.0688e-02],
          [-1.5722e-02, -1.2359e-02,  3.8655e-03]],

         [[-2.7298e-03,  6.8825e-03, -2.6423e-02],
          [-1.5887e-02, -5.9772e-03, -3.3839e-02],
          [-1.4816e-02, -2.0167e-02, -5.5119e-03]],

         [[ 1.2006e-02,  1.7306e-02,  2.7432e-02],
          [ 1.5158e-02,  2.6627e-03,  2.9084e-03],
          [ 8.1128e-03, -1.4859e-03,  1.2419e-02]]],


        [[[ 2.0216e-03,  8.0463e-03,  2.2741e-02],
          [-4.0163e-03, -2.8261e-03, -1.6860e-02],
          [ 6.2065e-03,  5.1855e-03, -7.8850e-03]],

         [[ 2.2007e-04, -4.5189e-03, -1.0545e-02],
          [-1.0438e-02, -1.1172e-02, -1.7095e-02],
          [ 6.6727e-03, -2.6576e-02,  3.9433e-03]],

         [[ 1.2867e-03, -1.8363e-02,  8.4997e-03],
          [-1.3385e-02,  1.0206e-02,  4.7243e-03],
          [-2.1369e-02, -2.3673e-04, -8.9933e-03]],

         ...,

         [[-1.4905e-02, -1.5674e-02,  2.0347e-02],
          [-1.1521e-02, -1.1991e-02, -2.2922e-02],
          [ 8.8392e-03, -1.3647e-02,  1.8552e-02]],

         [[ 2.1619e-02,  9.0217e-03, -8.9641e-03],
          [ 1.3991e-02,  1.1078e-02,  1.6696e-02],
          [-9.3905e-03, -2.1583e-02, -1.3318e-03]],

         [[-9.8635e-03, -9.4080e-03,  1.5625e-03],
          [-1.7805e-02,  2.7936e-02,  1.5911e-02],
          [-2.0638e-02, -4.2822e-03, -1.7943e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.9018e+05, 4.5318e+04, 4.3078e+04,  ..., 5.5855e-03, 5.3320e-03,
        5.2890e-03], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 144]) 

NULL SPACE BASIS :  tensor([[-0.0428, -0.0254, -0.0057,  ..., -0.0578, -0.0022, -0.0274],
        [ 0.0231, -0.0081,  0.0024,  ...,  0.0136,  0.0056,  0.0087],
        [ 0.0078,  0.0108, -0.0103,  ...,  0.0096,  0.0204,  0.0135],
        ...,
        [ 0.0365, -0.0261, -0.0283,  ...,  0.0221,  0.0208, -0.0359],
        [ 0.0037,  0.0061, -0.0165,  ..., -0.0073, -0.0017, -0.0008],
        [ 0.0055, -0.0076,  0.0181,  ...,  0.0330,  0.0137,  0.0042]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 5.8619e-03, -1.3227e-03, -5.6470e-04,  ..., -8.1340e-04,
          6.7391e-04, -2.9646e-04],
        [-1.3227e-03,  4.0428e-03, -1.4046e-03,  ...,  1.2831e-04,
         -4.9775e-04, -2.1281e-05],
        [-5.6470e-04, -1.4046e-03,  3.1914e-03,  ...,  2.9938e-04,
         -2.7230e-04,  2.9436e-04],
        ...,
        [-8.1340e-04,  1.2831e-04,  2.9938e-04,  ...,  5.6042e-03,
         -6.5076e-04,  7.6543e-04],
        [ 6.7391e-04, -4.9775e-04, -2.7230e-04,  ..., -6.5076e-04,
          3.3423e-03, -6.6067e-04],
        [-2.9646e-04, -2.1281e-05,  2.9436e-04,  ...,  7.6543e-04,
         -6.6067e-04,  2.7439e-03]], device='cuda:0') 

reserving basis 12/4608; cond: 743510507520.0, radio:5.617816628555872e-11
PARAMETER       :  Parameter containing:
tensor([[[[ 5.8335e-03,  4.5170e-03,  1.3530e-02],
          [-7.7091e-03, -8.3337e-03,  1.2801e-02],
          [-4.6648e-03, -5.4460e-03,  9.5384e-03]],

         [[ 4.1568e-03, -6.1259e-03,  1.9229e-03],
          [ 5.6487e-03,  9.5428e-03,  2.3548e-03],
          [ 1.7644e-02, -6.9436e-03,  8.4094e-03]],

         [[-2.7458e-03, -1.4004e-02, -1.4852e-04],
          [-5.9761e-03, -6.2376e-03, -2.2880e-02],
          [-1.1689e-02,  1.0307e-02, -2.0958e-02]],

         ...,

         [[-4.4839e-03,  1.7011e-03,  4.5515e-03],
          [-2.1191e-03, -2.4963e-03, -1.8560e-02],
          [-1.0242e-02, -1.2056e-03, -2.5571e-02]],

         [[-4.3653e-03,  9.7264e-03,  1.2994e-02],
          [ 2.2070e-03,  1.6211e-02,  3.4723e-03],
          [ 1.9403e-04,  5.0582e-03,  4.8151e-03]],

         [[-1.4392e-02, -3.2970e-02, -2.7960e-02],
          [-6.7503e-03, -7.2759e-04, -1.9885e-02],
          [ 7.2797e-03,  5.7980e-04, -4.6150e-04]]],


        [[[ 8.2204e-03, -1.6193e-02,  8.5583e-03],
          [-3.9101e-03,  5.5768e-03,  7.1737e-03],
          [ 5.0067e-03, -1.3072e-02, -1.1918e-02]],

         [[-1.5148e-02,  8.1681e-03, -9.6401e-03],
          [-4.0711e-03, -2.2508e-03, -3.3560e-03],
          [ 7.0291e-03,  5.0940e-03, -1.0562e-02]],

         [[-7.5007e-04,  1.7995e-04,  1.1258e-02],
          [ 6.9497e-04,  1.2678e-02,  1.5518e-03],
          [ 1.0892e-02,  2.2892e-02,  1.8284e-02]],

         ...,

         [[-2.4069e-03,  3.0934e-03, -9.1296e-03],
          [ 2.2925e-04, -6.6606e-03, -2.2789e-03],
          [-1.3665e-03,  6.9415e-03,  1.4014e-02]],

         [[ 5.4562e-03, -4.3691e-03, -1.3109e-02],
          [-4.4030e-03, -3.8265e-03,  3.1718e-03],
          [-4.9143e-03,  3.1071e-04, -1.2027e-02]],

         [[-1.4486e-04,  3.2183e-04, -4.7366e-03],
          [-1.5557e-02, -2.0065e-03, -2.0635e-04],
          [ 1.0804e-02, -6.6609e-03, -4.4897e-03]]],


        [[[ 3.5335e-03,  1.0248e-02,  3.4244e-03],
          [ 8.0285e-03,  1.5216e-02,  1.9538e-02],
          [-7.0659e-03, -1.9089e-03,  4.6115e-03]],

         [[ 1.0444e-03,  1.3190e-02,  8.4748e-03],
          [-1.2357e-02, -7.1186e-03,  3.7007e-03],
          [-9.9232e-03, -7.5874e-03, -5.6641e-03]],

         [[-4.1966e-03, -7.0747e-03,  6.6284e-03],
          [-6.5011e-03, -1.1337e-02,  9.1817e-03],
          [-1.2116e-03, -4.9661e-03, -9.7023e-04]],

         ...,

         [[ 8.6576e-03,  2.9583e-03, -9.1735e-03],
          [ 6.9799e-03, -1.5119e-03, -1.1652e-02],
          [ 9.3632e-03,  4.0482e-03,  1.5931e-03]],

         [[ 9.5519e-03,  5.5875e-03,  2.0755e-02],
          [-6.6548e-03, -4.9056e-03,  1.9178e-02],
          [-4.2733e-03,  3.4274e-03, -3.0584e-03]],

         [[-4.8455e-03,  4.7119e-03, -1.2403e-02],
          [-1.1990e-02,  2.8950e-03, -6.9253e-03],
          [-7.5854e-03,  1.0393e-02,  8.3969e-03]]],


        ...,


        [[[ 6.2492e-03, -3.5783e-03, -1.6081e-02],
          [-1.0030e-02,  5.2119e-03, -3.7463e-03],
          [ 1.9723e-03,  1.0956e-02, -1.3889e-02]],

         [[-6.4477e-03,  2.8548e-04,  4.6333e-03],
          [-7.5104e-03,  1.5332e-02, -4.1647e-04],
          [-6.3611e-03,  8.9555e-03, -8.4865e-03]],

         [[ 1.0698e-03, -9.8594e-03,  1.1016e-03],
          [-2.9756e-03, -2.5863e-02, -1.1194e-02],
          [ 1.2472e-02, -1.9859e-02, -1.7497e-02]],

         ...,

         [[-1.8532e-02,  6.2166e-03, -6.9514e-03],
          [-7.7720e-03, -5.4597e-03, -1.2445e-02],
          [-7.1056e-03,  1.0563e-02,  6.1055e-03]],

         [[-1.5467e-03, -3.7635e-03, -3.6147e-03],
          [-6.0640e-03,  5.6258e-03,  1.2841e-02],
          [ 9.4361e-03, -5.5075e-03, -9.9959e-03]],

         [[ 1.3755e-02,  1.3919e-02,  5.1197e-03],
          [-8.4810e-04,  4.7384e-03,  2.3490e-03],
          [-4.7861e-04,  3.0905e-04, -1.5604e-03]]],


        [[[-1.3789e-02, -4.4007e-03,  3.0757e-03],
          [-1.2833e-02,  8.5359e-03, -1.8524e-02],
          [-5.8314e-03, -2.1918e-03,  3.0361e-03]],

         [[ 2.2050e-03, -1.3624e-02, -2.2672e-02],
          [ 9.7661e-03,  4.2160e-03, -1.6311e-02],
          [ 9.4220e-03,  4.8127e-03,  5.2640e-03]],

         [[-6.7535e-03,  9.7613e-04,  1.0809e-02],
          [ 1.8640e-03, -1.5907e-03, -5.9759e-03],
          [-6.8505e-03, -5.1647e-03, -7.1362e-03]],

         ...,

         [[ 2.4174e-03,  1.0952e-03, -7.5832e-03],
          [-2.4210e-02, -1.7913e-02, -1.6835e-02],
          [-4.7442e-04, -2.9312e-03,  1.3886e-02]],

         [[ 4.2101e-03, -3.6928e-03,  3.2789e-03],
          [-3.8412e-03, -7.0608e-05, -9.1971e-04],
          [ 1.0182e-02,  9.6368e-03,  5.2847e-03]],

         [[ 7.6851e-03, -1.3723e-02, -1.3581e-02],
          [ 1.6828e-03, -3.0905e-03,  1.4040e-02],
          [-1.3520e-02,  3.5591e-03, -4.7927e-03]]],


        [[[ 9.2210e-03, -6.2122e-03, -3.6738e-03],
          [-1.4669e-02, -5.6742e-04,  4.9768e-03],
          [ 1.9549e-03, -9.2530e-03, -1.1824e-02]],

         [[-5.2343e-03,  1.3736e-02,  2.7838e-03],
          [ 5.4056e-03, -1.0003e-02,  3.0394e-04],
          [ 4.5191e-03,  7.0683e-03, -6.0910e-03]],

         [[-1.2424e-02, -3.2686e-03,  1.6510e-02],
          [-2.3878e-03,  4.3776e-03,  1.8182e-02],
          [-8.3953e-03, -1.4304e-03,  2.8216e-03]],

         ...,

         [[ 1.3590e-02,  5.7620e-03,  8.3254e-05],
          [ 1.0645e-02, -1.3706e-02,  6.0813e-03],
          [ 2.4053e-03, -1.4943e-02,  1.0346e-02]],

         [[ 1.9032e-02,  2.4254e-02,  3.0929e-03],
          [ 1.7147e-02,  1.9142e-03, -4.9715e-03],
          [-9.2044e-03,  1.3026e-02, -1.1910e-02]],

         [[-4.6852e-03, -1.3280e-02,  7.9286e-03],
          [-1.1140e-02, -5.8273e-03, -5.0814e-03],
          [ 1.1036e-02, -3.0266e-03,  2.1302e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.2354e+05, 9.0543e+04, 8.4172e+04,  ..., 2.5536e-06, 1.0838e-06,
        7.0414e-07], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 12]) 

NULL SPACE BASIS :  tensor([[-0.0098,  0.0055, -0.0054,  ...,  0.0045, -0.0003,  0.0008],
        [ 0.0076, -0.0073,  0.0086,  ...,  0.0082, -0.0028,  0.0196],
        [ 0.0179, -0.0183,  0.0145,  ..., -0.0245, -0.0102,  0.0134],
        ...,
        [-0.0002, -0.0339, -0.0052,  ..., -0.0121,  0.0058,  0.0107],
        [ 0.0119, -0.0087, -0.0090,  ...,  0.0145, -0.0115,  0.0107],
        [-0.0015,  0.0162, -0.0177,  ..., -0.0056, -0.0032, -0.0031]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 5.1148e-04,  4.6988e-05,  3.7334e-04,  ...,  2.1039e-05,
          1.1051e-04,  9.8845e-05],
        [ 4.6988e-05,  7.6919e-04,  3.4741e-04,  ...,  2.9975e-04,
          3.6900e-05, -2.6934e-04],
        [ 3.7334e-04,  3.4741e-04,  1.3072e-03,  ...,  4.9367e-04,
          1.5458e-04, -2.5117e-04],
        ...,
        [ 2.1039e-05,  2.9975e-04,  4.9367e-04,  ...,  7.8807e-04,
          1.4705e-04, -1.0425e-04],
        [ 1.1051e-04,  3.6900e-05,  1.5458e-04,  ...,  1.4705e-04,
          4.3066e-04, -9.1451e-05],
        [ 9.8845e-05, -2.6934e-04, -2.5117e-04,  ..., -1.0425e-04,
         -9.1451e-05,  6.6923e-04]], device='cuda:0') 

reserving basis 199/256; cond: 46543.7734375, radio:0.01286792941391468
PARAMETER       :  Parameter containing:
tensor([[[[-0.0196]],

         [[-0.0362]],

         [[-0.0415]],

         ...,

         [[ 0.0077]],

         [[-0.0438]],

         [[-0.0512]]],


        [[[ 0.0656]],

         [[-0.0589]],

         [[ 0.0158]],

         ...,

         [[ 0.0032]],

         [[ 0.0063]],

         [[ 0.0189]]],


        [[[ 0.0424]],

         [[ 0.0274]],

         [[-0.0027]],

         ...,

         [[ 0.0180]],

         [[ 0.0368]],

         [[-0.0326]]],


        ...,


        [[[-0.0267]],

         [[ 0.0040]],

         [[ 0.0488]],

         ...,

         [[ 0.0197]],

         [[-0.0180]],

         [[-0.0093]]],


        [[[ 0.0186]],

         [[ 0.0234]],

         [[-0.0241]],

         ...,

         [[ 0.0409]],

         [[ 0.0118]],

         [[ 0.0440]]],


        [[[-0.0392]],

         [[ 0.0257]],

         [[ 0.0418]],

         ...,

         [[ 0.0498]],

         [[-0.0631]],

         [[ 0.0400]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([7.9118e+04, 4.8018e+03, 3.9876e+03, 1.5056e+03, 1.1372e+03, 7.6233e+02,
        4.4305e+02, 3.4531e+02, 2.3820e+02, 2.2083e+02, 1.7079e+02, 1.5914e+02,
        1.5186e+02, 1.3886e+02, 1.2554e+02, 1.2346e+02, 1.1008e+02, 1.0169e+02,
        9.5568e+01, 8.7630e+01, 8.3987e+01, 7.7505e+01, 7.0901e+01, 6.7524e+01,
        6.1615e+01, 5.9501e+01, 5.3491e+01, 5.1306e+01, 4.8050e+01, 4.5169e+01,
        4.2810e+01, 4.1948e+01, 4.0476e+01, 3.8296e+01, 3.5528e+01, 3.4847e+01,
        3.4532e+01, 3.2497e+01, 3.1463e+01, 3.0295e+01, 2.7738e+01, 2.6945e+01,
        2.5673e+01, 2.5626e+01, 2.4795e+01, 2.3814e+01, 2.3009e+01, 2.2918e+01,
        2.1643e+01, 2.0248e+01, 1.9733e+01, 1.9356e+01, 1.9261e+01, 1.8935e+01,
        1.7845e+01, 1.7727e+01, 1.7167e+01, 1.6733e+01, 1.6527e+01, 1.6386e+01,
        1.6146e+01, 1.5732e+01, 1.5174e+01, 1.4902e+01, 1.4725e+01, 1.4416e+01,
        1.4040e+01, 1.3738e+01, 1.3622e+01, 1.3490e+01, 1.3247e+01, 1.3124e+01,
        1.2942e+01, 1.2762e+01, 1.2594e+01, 1.2372e+01, 1.2152e+01, 1.1924e+01,
        1.1795e+01, 1.1782e+01, 1.1532e+01, 1.1266e+01, 1.1149e+01, 1.0859e+01,
        1.0792e+01, 1.0711e+01, 1.0654e+01, 1.0480e+01, 1.0403e+01, 1.0210e+01,
        1.0083e+01, 9.8383e+00, 9.7294e+00, 9.5461e+00, 9.4871e+00, 9.3956e+00,
        9.3015e+00, 9.1498e+00, 9.1411e+00, 9.0188e+00, 8.9457e+00, 8.8409e+00,
        8.6076e+00, 8.4636e+00, 8.3704e+00, 8.3549e+00, 8.3121e+00, 8.2697e+00,
        8.0608e+00, 7.9470e+00, 7.9199e+00, 7.8819e+00, 7.7009e+00, 7.6834e+00,
        7.5611e+00, 7.4892e+00, 7.3785e+00, 7.3516e+00, 7.2644e+00, 7.2383e+00,
        7.1000e+00, 7.0689e+00, 7.0403e+00, 6.9449e+00, 6.9223e+00, 6.8818e+00,
        6.7913e+00, 6.6537e+00, 6.6296e+00, 6.5700e+00, 6.5090e+00, 6.4300e+00,
        6.3786e+00, 6.3182e+00, 6.2537e+00, 6.2113e+00, 6.1729e+00, 6.1006e+00,
        6.0433e+00, 6.0048e+00, 5.9177e+00, 5.8686e+00, 5.7976e+00, 5.7579e+00,
        5.7020e+00, 5.6348e+00, 5.6077e+00, 5.5077e+00, 5.4669e+00, 5.4053e+00,
        5.3999e+00, 5.3694e+00, 5.3290e+00, 5.2881e+00, 5.2292e+00, 5.1989e+00,
        5.1109e+00, 5.0772e+00, 5.0183e+00, 4.9854e+00, 4.9596e+00, 4.8799e+00,
        4.8715e+00, 4.8023e+00, 4.7715e+00, 4.7098e+00, 4.6763e+00, 4.6310e+00,
        4.6051e+00, 4.5561e+00, 4.5322e+00, 4.4771e+00, 4.4240e+00, 4.3592e+00,
        4.3367e+00, 4.3120e+00, 4.3054e+00, 4.2621e+00, 4.2228e+00, 4.2065e+00,
        4.1469e+00, 4.0955e+00, 4.0797e+00, 4.0311e+00, 4.0102e+00, 3.9720e+00,
        3.9406e+00, 3.9281e+00, 3.8941e+00, 3.8667e+00, 3.8393e+00, 3.8165e+00,
        3.7735e+00, 3.7552e+00, 3.7243e+00, 3.6839e+00, 3.6736e+00, 3.6233e+00,
        3.6137e+00, 3.5790e+00, 3.5091e+00, 3.4716e+00, 3.4407e+00, 3.4186e+00,
        3.3602e+00, 3.3513e+00, 3.3227e+00, 3.2584e+00, 3.2359e+00, 3.2195e+00,
        3.1757e+00, 3.1676e+00, 3.1455e+00, 3.1278e+00, 3.0845e+00, 3.0683e+00,
        3.0340e+00, 3.0055e+00, 2.9274e+00, 2.9213e+00, 2.9026e+00, 2.8681e+00,
        2.8585e+00, 2.8175e+00, 2.7877e+00, 2.7790e+00, 2.7124e+00, 2.7061e+00,
        2.7038e+00, 2.6582e+00, 2.6385e+00, 2.6295e+00, 2.5732e+00, 2.5390e+00,
        2.5117e+00, 2.4880e+00, 2.4545e+00, 2.4353e+00, 2.4246e+00, 2.4008e+00,
        2.3376e+00, 2.3215e+00, 2.2920e+00, 2.2497e+00, 2.2043e+00, 2.2021e+00,
        2.1593e+00, 2.1332e+00, 2.0834e+00, 2.0290e+00, 1.9912e+00, 1.9421e+00,
        1.9077e+00, 1.8532e+00, 1.8231e+00, 1.6999e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 199]) 

NULL SPACE BASIS :  tensor([[ 0.0431,  0.0331,  0.0823,  ...,  0.0360,  0.0002, -0.0145],
        [-0.0115, -0.0968,  0.0671,  ..., -0.0360, -0.0182,  0.0081],
        [ 0.0588,  0.0506,  0.0252,  ...,  0.0636, -0.0143, -0.0817],
        ...,
        [-0.0371,  0.0415,  0.0498,  ..., -0.0125,  0.0509,  0.0278],
        [-0.0125,  0.0675,  0.1187,  ..., -0.0533, -0.0192, -0.0166],
        [ 0.0700, -0.0365,  0.0399,  ...,  0.0720,  0.0502, -0.0244]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 4.4454e-02,  2.3475e-03,  2.6233e-04,  ...,  2.0421e-03,
         -1.3924e-03,  2.3751e-03],
        [ 2.3475e-03,  5.3954e-02, -2.9818e-03,  ..., -5.2408e-03,
          3.0163e-03,  2.1218e-03],
        [ 2.6233e-04, -2.9818e-03,  5.9951e-02,  ..., -2.8246e-03,
          2.7936e-03, -1.2568e-03],
        ...,
        [ 2.0421e-03, -5.2408e-03, -2.8246e-03,  ...,  5.4958e-02,
         -1.6089e-03,  2.1039e-03],
        [-1.3924e-03,  3.0163e-03,  2.7936e-03,  ..., -1.6089e-03,
          5.2639e-02,  9.3986e-05],
        [ 2.3751e-03,  2.1218e-03, -1.2568e-03,  ...,  2.1039e-03,
          9.3986e-05,  5.6885e-02]], device='cuda:0') 

reserving basis 5/4608; cond: 1958158794752.0, radio:9.302540508737689e-12
PARAMETER       :  Parameter containing:
tensor([[[[-3.8390e-03,  5.1867e-03, -1.1765e-02],
          [-7.8186e-03,  5.4739e-03, -1.6083e-02],
          [ 9.7249e-03, -2.9964e-03,  6.9407e-03]],

         [[-7.9828e-03, -3.0963e-03,  3.7767e-03],
          [ 8.3771e-03,  4.3958e-03, -3.7259e-03],
          [ 4.4896e-03, -5.2015e-03,  5.5886e-03]],

         [[ 7.0681e-04, -1.4252e-02, -1.4771e-02],
          [-3.8504e-03, -1.7244e-03, -1.2362e-02],
          [ 1.4161e-03,  1.4070e-02, -4.7302e-03]],

         ...,

         [[-3.7704e-03,  9.7351e-03, -1.0059e-03],
          [ 1.0247e-02,  4.3983e-04,  5.7026e-03],
          [ 4.8078e-03,  1.1048e-02, -4.7625e-03]],

         [[-1.2595e-02,  1.1129e-03, -1.1141e-02],
          [ 2.0596e-03, -7.3155e-03, -2.8379e-03],
          [-3.8374e-05, -1.0380e-02,  9.6012e-03]],

         [[-8.2330e-04,  1.0536e-02,  8.0315e-03],
          [ 7.2151e-03,  9.8257e-03,  1.3061e-02],
          [-1.2658e-02,  3.6331e-03, -9.7549e-03]]],


        [[[ 6.0458e-03, -3.9381e-03,  7.4745e-03],
          [-1.3576e-02, -8.9102e-03,  5.4177e-03],
          [-2.3174e-02,  1.0251e-03,  6.7925e-03]],

         [[-7.2708e-03,  1.4947e-03,  2.6450e-03],
          [-6.4589e-03,  1.6555e-02, -2.0048e-03],
          [ 1.1905e-02,  8.1807e-03,  1.6210e-02]],

         [[ 5.3650e-03, -6.9078e-03,  5.8692e-03],
          [ 9.3702e-04, -1.0917e-02,  5.2254e-03],
          [ 1.1881e-02,  2.5926e-04,  7.3801e-03]],

         ...,

         [[ 9.3355e-03,  3.5206e-03,  8.1115e-03],
          [ 1.2026e-02, -3.3796e-03, -4.1403e-03],
          [-6.2698e-03, -1.1562e-03,  2.6991e-03]],

         [[ 2.0745e-03, -2.0498e-03, -1.0122e-02],
          [ 8.4769e-04, -5.3569e-03, -2.4207e-03],
          [-3.4618e-03,  1.2693e-02,  3.0501e-03]],

         [[ 1.2274e-02, -7.4065e-03, -9.6838e-03],
          [ 1.2812e-02, -5.4539e-03,  1.5585e-03],
          [-1.8805e-03,  5.9564e-03, -1.0474e-02]]],


        [[[-5.7601e-03, -1.8020e-02, -8.4984e-03],
          [-2.6632e-03,  7.5170e-03, -1.5057e-02],
          [ 1.4997e-02,  1.7776e-02,  5.6876e-03]],

         [[ 7.1849e-03, -1.0425e-02,  1.0357e-03],
          [-5.4778e-03, -2.9852e-03, -2.1223e-02],
          [-4.0029e-03, -4.3784e-03, -2.1223e-02]],

         [[-1.6427e-03, -2.1480e-03,  5.4510e-03],
          [ 1.3205e-03,  2.2862e-03, -3.0993e-04],
          [-5.6435e-03,  1.0459e-02, -2.0691e-03]],

         ...,

         [[ 7.3414e-03,  1.3511e-02,  7.9677e-03],
          [-5.2887e-03,  9.6036e-03, -8.0806e-03],
          [ 9.9051e-04, -1.2668e-02, -1.9384e-03]],

         [[ 5.4514e-03, -1.1467e-02, -5.9488e-03],
          [ 4.3222e-03, -6.9083e-03, -1.9569e-02],
          [-3.5456e-03,  2.0316e-03, -1.3299e-02]],

         [[-1.1905e-02, -1.8976e-02, -4.3293e-03],
          [ 7.7705e-03,  6.1743e-03, -1.1324e-02],
          [-5.1900e-03, -1.1319e-02,  6.4413e-03]]],


        ...,


        [[[-1.1471e-02,  8.7987e-03, -8.3485e-03],
          [ 9.4133e-03,  7.5511e-03,  1.0135e-02],
          [-1.2287e-02, -1.7519e-02,  1.1656e-03]],

         [[-1.6008e-02, -1.6646e-03, -2.5706e-03],
          [ 4.4602e-03,  3.2558e-03, -1.7730e-03],
          [-9.6145e-03,  5.1489e-03, -2.9646e-03]],

         [[-1.4887e-02,  1.4198e-03,  1.5638e-03],
          [-1.3766e-02, -5.5305e-03, -1.6226e-02],
          [-7.7052e-03, -1.4136e-02, -2.7679e-03]],

         ...,

         [[ 9.9386e-03, -1.5215e-02,  5.6808e-03],
          [-3.3982e-03,  3.5831e-03, -1.0943e-02],
          [-8.7642e-03, -1.4164e-03,  5.2748e-03]],

         [[-2.0107e-03, -2.4701e-03, -1.6054e-03],
          [-1.2614e-02, -8.3998e-03, -1.8938e-02],
          [-1.5289e-02, -4.9041e-03, -3.4063e-03]],

         [[-3.8925e-03, -4.2445e-03, -1.0240e-02],
          [ 8.5778e-03,  1.0004e-02, -1.6275e-03],
          [ 1.0718e-02,  5.8779e-03,  9.7766e-03]]],


        [[[-1.4543e-02, -2.8358e-03,  1.0076e-02],
          [ 6.6130e-03, -1.4453e-03,  2.2945e-02],
          [-6.2953e-03, -7.6621e-03,  2.0941e-02]],

         [[ 1.0690e-02, -7.7135e-03, -1.3786e-03],
          [-4.7580e-03, -5.2694e-03,  8.2543e-03],
          [-1.9583e-02, -7.2929e-03, -1.8918e-02]],

         [[-1.5856e-02, -1.4715e-02, -1.3822e-02],
          [ 1.4262e-02,  1.4264e-03, -1.5178e-02],
          [ 7.5684e-03, -1.2526e-02, -3.2542e-03]],

         ...,

         [[ 1.2749e-02,  7.7697e-03, -2.2411e-03],
          [ 1.3109e-02,  1.0004e-02,  1.5713e-02],
          [-5.8951e-03,  9.7716e-03, -1.5222e-02]],

         [[-8.2194e-03,  2.0454e-03, -4.3020e-03],
          [ 5.4377e-03, -3.8555e-03,  1.4642e-03],
          [ 1.6790e-02,  5.6745e-03,  6.4498e-03]],

         [[ 3.2153e-03,  1.2185e-02,  1.0859e-02],
          [-3.9964e-04,  4.3305e-03,  3.5393e-03],
          [-1.7712e-03,  1.6783e-02,  2.5512e-03]]],


        [[[ 1.9326e-02,  4.7278e-03, -1.9304e-03],
          [ 2.8013e-03, -5.4950e-03, -7.2036e-03],
          [-1.0512e-02,  2.5667e-03,  3.3064e-03]],

         [[ 1.5829e-02,  5.2253e-04, -3.1091e-03],
          [-1.1715e-03,  1.7169e-02, -2.5834e-03],
          [ 6.8936e-03,  2.7114e-03,  1.4362e-02]],

         [[ 1.8251e-02,  2.6115e-03,  6.2801e-03],
          [-5.5144e-03, -9.6293e-03, -9.2816e-03],
          [-1.0670e-02, -4.4622e-04, -9.1634e-03]],

         ...,

         [[ 1.0672e-02, -1.4457e-02,  1.8684e-03],
          [ 3.6515e-03,  9.4238e-03,  4.5894e-03],
          [ 1.9679e-02,  2.3536e-02,  2.6547e-02]],

         [[-1.5994e-03,  1.1540e-02, -8.0562e-03],
          [-1.3618e-02,  8.7957e-04, -1.7695e-02],
          [ 7.1552e-03,  8.4578e-04, -1.3674e-03]],

         [[ 2.8593e-04,  3.9477e-03, -1.3785e-02],
          [-1.0554e-02,  6.2936e-03, -1.1073e-02],
          [-5.0035e-03, -1.1756e-02,  1.0843e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.7346e+05, 1.2111e+05, 1.1735e+05,  ..., 1.9892e-06, 1.9845e-06,
        2.9286e-07], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 5]) 

NULL SPACE BASIS :  tensor([[ 2.1904e-02,  1.2705e-02,  3.8305e-04, -8.8861e-03,  4.7220e-03],
        [ 1.5350e-03, -1.1216e-02,  1.2719e-02,  5.8782e-03, -9.4949e-05],
        [-4.0020e-02,  2.2175e-03,  4.3050e-03, -1.9323e-02, -1.1488e-02],
        ...,
        [-9.3041e-03,  2.7606e-04,  5.9105e-03,  1.9619e-03, -4.5608e-03],
        [ 1.5581e-02,  1.1770e-02,  1.2747e-02,  7.9138e-03,  2.4218e-02],
        [ 2.3497e-03, -6.6125e-03,  1.4071e-02, -1.7329e-03,  1.3181e-02]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.3132e-04, -6.9908e-05, -3.2540e-04,  ..., -1.0574e-04,
          2.4082e-04,  2.2527e-05],
        [-6.9908e-05,  1.4478e-04, -6.4267e-05,  ...,  3.1127e-05,
          4.3837e-05,  1.0945e-04],
        [-3.2540e-04, -6.4267e-05,  9.5052e-04,  ...,  1.8422e-04,
         -4.3445e-04, -7.4094e-05],
        ...,
        [-1.0574e-04,  3.1127e-05,  1.8422e-04,  ...,  6.5243e-05,
         -7.1971e-05, -1.8004e-06],
        [ 2.4082e-04,  4.3837e-05, -4.3445e-04,  ..., -7.1971e-05,
          5.3226e-04,  1.9795e-04],
        [ 2.2527e-05,  1.0945e-04, -7.4094e-05,  ..., -1.8004e-06,
          1.9795e-04,  1.8918e-04]], device='cuda:0') 

reserving basis 12/4608; cond: 1013062107136.0, radio:3.119801639250852e-11
PARAMETER       :  Parameter containing:
tensor([[[[ 9.1033e-03,  2.4611e-03, -5.9525e-03],
          [ 9.2735e-04, -8.8888e-03, -2.7184e-03],
          [ 3.4397e-03, -7.7989e-03,  4.7923e-03]],

         [[-6.0387e-03, -1.1412e-02,  4.5947e-04],
          [-4.9855e-03, -4.6154e-04, -2.3889e-03],
          [ 2.7263e-03,  9.0508e-03, -2.7848e-03]],

         [[-1.0462e-02,  6.0816e-03,  1.1388e-02],
          [ 1.5855e-02,  1.2463e-02,  3.1133e-03],
          [-2.0202e-03,  1.7779e-02,  5.5666e-03]],

         ...,

         [[ 1.1521e-02,  4.5624e-03, -3.2209e-04],
          [ 2.8710e-03, -9.2333e-03,  1.6409e-04],
          [ 6.0403e-03, -5.9058e-03,  7.4913e-03]],

         [[ 6.7033e-03, -6.6825e-04,  2.5677e-03],
          [ 3.1482e-03,  1.4087e-03,  7.7102e-03],
          [ 8.2304e-03,  8.5482e-03, -4.9006e-04]],

         [[-8.1432e-03,  1.2686e-02, -1.6027e-04],
          [-1.0313e-02, -1.2898e-02,  1.8356e-03],
          [-1.2703e-02,  5.2728e-03, -1.3563e-02]]],


        [[[-2.2070e-04,  7.3276e-03, -3.2195e-03],
          [ 2.3294e-03, -4.0414e-03, -8.3589e-03],
          [-4.7995e-03, -1.5025e-02,  7.5597e-03]],

         [[ 3.4764e-03,  5.8002e-03,  1.8840e-03],
          [ 9.6638e-03,  1.3784e-02, -1.2451e-02],
          [ 1.7506e-02,  7.6110e-03, -9.5281e-03]],

         [[ 6.7545e-03,  5.8702e-03,  2.3138e-03],
          [ 3.7262e-03, -4.8213e-03,  6.4529e-03],
          [-2.4099e-03, -7.5404e-03,  8.6153e-06]],

         ...,

         [[-1.9491e-03,  9.4241e-03,  4.3553e-03],
          [-8.9247e-03, -8.3040e-03,  6.1112e-03],
          [-6.7584e-03,  5.9626e-03, -2.5456e-03]],

         [[ 4.7576e-03, -3.0383e-03,  8.8042e-03],
          [-3.1689e-03, -1.4707e-02, -4.4770e-03],
          [ 3.7599e-03, -1.0880e-03, -1.4107e-02]],

         [[ 1.4761e-02,  1.9480e-02, -2.2184e-03],
          [ 4.7701e-04, -2.3144e-04,  5.7970e-03],
          [ 4.0085e-03, -2.0338e-03, -6.7006e-04]]],


        [[[-4.1028e-03, -4.8685e-03, -7.4068e-03],
          [-2.5618e-03, -1.4526e-02, -6.4400e-03],
          [-9.0106e-03, -6.9325e-04, -2.9082e-04]],

         [[-4.0732e-03,  5.4229e-03, -1.0982e-02],
          [ 9.8807e-04, -1.0020e-02, -1.2763e-02],
          [-4.6612e-03, -2.9893e-03, -9.2967e-03]],

         [[-1.1784e-03, -1.3210e-02,  5.1164e-03],
          [-4.8914e-03, -7.1723e-03, -1.3308e-02],
          [-5.2261e-03,  1.3641e-02, -6.3427e-03]],

         ...,

         [[ 3.2157e-03, -1.8762e-02, -8.6894e-04],
          [ 1.5831e-02,  7.9770e-04, -6.2525e-03],
          [-4.8848e-03,  7.8515e-03, -2.6208e-03]],

         [[ 4.0406e-03,  1.7864e-02,  4.6400e-03],
          [-2.5325e-03, -5.6222e-03,  9.2171e-03],
          [ 5.7265e-04, -3.7032e-03, -2.9589e-04]],

         [[-1.2533e-02, -1.5764e-02, -1.6476e-02],
          [-1.2487e-02, -4.1784e-04,  6.8650e-04],
          [-1.2074e-02, -1.1702e-02, -1.7069e-02]]],


        ...,


        [[[-1.4080e-03,  4.3736e-05,  9.3656e-03],
          [ 7.9169e-04,  8.6112e-03, -8.9599e-03],
          [ 8.7612e-03,  3.7546e-04,  4.5568e-03]],

         [[-8.0835e-03, -4.6939e-03,  8.8876e-03],
          [ 7.9785e-03,  7.2849e-03, -7.4496e-03],
          [ 1.1957e-02, -3.9941e-03, -2.8241e-03]],

         [[-1.4362e-02, -1.5962e-03, -1.1681e-02],
          [-1.2073e-02, -1.1641e-02, -3.6094e-03],
          [ 1.1466e-02,  1.6363e-02, -7.0460e-03]],

         ...,

         [[-4.2192e-03,  6.8943e-03,  5.4990e-03],
          [ 9.7571e-03,  1.7393e-03,  5.0271e-03],
          [-4.8830e-03, -3.7843e-03, -2.4389e-03]],

         [[-8.3592e-03,  1.1290e-02, -9.1213e-03],
          [ 9.7726e-03, -1.2506e-02, -8.1369e-03],
          [-1.9858e-04,  1.3074e-03, -1.6984e-02]],

         [[ 1.6992e-02,  2.0278e-02,  2.4228e-02],
          [ 1.1839e-02, -1.7045e-03,  3.6455e-03],
          [ 1.1033e-02,  1.6103e-02,  6.7955e-03]]],


        [[[-4.2688e-04, -3.3223e-03, -1.9839e-03],
          [-4.8983e-03, -1.2094e-02, -9.1981e-03],
          [-1.0451e-02,  1.1549e-03, -1.8659e-03]],

         [[-4.5227e-03,  8.8970e-03,  2.6095e-03],
          [-7.1000e-04,  7.1361e-03,  1.3240e-02],
          [-6.8074e-03, -1.0831e-02,  2.7165e-04]],

         [[ 4.5570e-03, -8.2808e-05,  4.8462e-03],
          [-1.2290e-02, -8.3310e-03,  9.7602e-03],
          [ 3.5311e-03,  9.3523e-03, -7.5511e-03]],

         ...,

         [[ 4.3344e-03,  5.5419e-03, -3.9822e-03],
          [ 9.1594e-03,  4.5238e-03,  2.2322e-03],
          [ 1.7733e-03,  2.0943e-03, -1.3425e-02]],

         [[-8.0372e-03, -3.4098e-03, -6.0470e-03],
          [ 1.9799e-03,  1.8479e-03, -1.4939e-02],
          [-1.2622e-03, -8.3866e-03,  2.6801e-03]],

         [[-1.3433e-02,  2.3797e-05,  2.1067e-04],
          [ 2.4956e-03, -4.1017e-03,  4.1603e-03],
          [-1.1639e-02, -1.1363e-02,  1.2274e-03]]],


        [[[-8.1111e-03, -1.0033e-02, -9.2313e-03],
          [ 2.6447e-03, -6.6568e-03,  1.4767e-03],
          [ 5.4760e-03,  3.3505e-03, -4.9858e-03]],

         [[-6.4019e-03,  8.3530e-03,  8.7519e-04],
          [ 1.6655e-03,  8.4716e-03,  2.0450e-03],
          [ 1.7901e-03,  1.1244e-02,  1.8426e-03]],

         [[-1.9545e-03,  5.9225e-03,  6.3478e-03],
          [-7.0265e-03,  1.0232e-02, -1.3169e-02],
          [ 3.1405e-04,  1.9689e-03,  2.2170e-03]],

         ...,

         [[ 3.7224e-03,  2.3267e-04,  1.3946e-02],
          [ 1.0450e-02,  2.9805e-03,  1.4187e-02],
          [ 6.8791e-03,  1.1685e-02,  4.0300e-03]],

         [[-7.8777e-03, -1.9164e-02, -1.7127e-03],
          [-1.1770e-02, -1.1509e-02,  3.4693e-03],
          [ 5.3049e-03, -8.7236e-05,  5.1523e-03]],

         [[-7.3045e-03,  4.5902e-03, -1.4911e-02],
          [-5.2437e-03,  1.0189e-02, -1.2142e-02],
          [ 6.9618e-03, -3.5621e-03, -5.4806e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.2004e+05, 7.1232e+04, 6.2352e+04,  ..., 1.0556e-06, 9.2472e-07,
        3.1591e-07], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 12]) 

NULL SPACE BASIS :  tensor([[-0.0193,  0.0014,  0.0047,  ...,  0.0017,  0.0035, -0.0012],
        [ 0.0017, -0.0238,  0.0279,  ...,  0.0251, -0.0280, -0.0131],
        [-0.0224,  0.0084,  0.0227,  ...,  0.0231,  0.0144,  0.0523],
        ...,
        [ 0.0079,  0.0046, -0.0092,  ..., -0.0126,  0.0074,  0.0054],
        [-0.0015,  0.0091, -0.0126,  ...,  0.0129,  0.0018, -0.0035],
        [ 0.0037,  0.0045,  0.0038,  ..., -0.0088, -0.0032,  0.0003]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 8.3092e-04, -8.8707e-05,  5.8641e-04,  ..., -2.5470e-04,
          1.2281e-04,  1.2492e-04],
        [-8.8707e-05,  1.3425e-03, -2.9838e-04,  ..., -2.6005e-04,
          5.1554e-05, -1.9112e-04],
        [ 5.8641e-04, -2.9838e-04,  2.7268e-03,  ..., -3.2069e-04,
          2.1899e-04,  3.4869e-04],
        ...,
        [-2.5470e-04, -2.6005e-04, -3.2069e-04,  ...,  2.0229e-04,
         -4.9910e-05, -3.2373e-05],
        [ 1.2281e-04,  5.1554e-05,  2.1899e-04,  ..., -4.9910e-05,
          2.8097e-04,  1.5829e-05],
        [ 1.2492e-04, -1.9112e-04,  3.4869e-04,  ..., -3.2373e-05,
          1.5829e-05,  1.7760e-04]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 88.200, Total time 0.61
 * Val loss 0.608, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 99.980, Total time 3.39
 * Val loss 0.002, Total time 0.00
**************************************************
====================== 2 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0086 (0.0086)	2.412 (2.412)	15.62 (15.62)
[10/157]	0.1207 (0.1017)	0.0801 (0.0634)	2.135 (2.219)	31.25 (18.75)
[20/157]	0.1210 (0.1105)	0.0804 (0.0715)	2.095 (2.110)	9.38 (25.30)
[30/157]	0.1213 (0.1136)	0.0805 (0.0743)	1.880 (2.015)	37.50 (30.24)
[40/157]	0.1206 (0.1152)	0.0797 (0.0757)	1.636 (1.934)	43.75 (33.84)
[50/157]	0.1207 (0.1162)	0.0799 (0.0765)	1.437 (1.868)	59.38 (36.40)
[60/157]	0.1210 (0.1168)	0.0799 (0.0771)	1.531 (1.819)	53.12 (37.55)
[70/157]	0.1213 (0.1173)	0.0798 (0.0775)	1.296 (1.777)	53.12 (39.04)
[80/157]	0.1067 (0.1174)	0.0666 (0.0775)	1.485 (1.743)	46.88 (40.24)
[90/157]	0.1065 (0.1159)	0.0652 (0.0761)	1.356 (1.705)	46.88 (41.17)
[100/157]	0.1216 (0.1154)	0.0806 (0.0755)	1.507 (1.685)	37.50 (41.27)
[110/157]	0.1207 (0.1158)	0.0801 (0.0759)	1.197 (1.672)	65.62 (41.67)
[120/157]	0.1211 (0.1162)	0.0802 (0.0763)	1.244 (1.654)	68.75 (42.23)
[130/157]	0.1210 (0.1165)	0.0800 (0.0766)	1.464 (1.630)	40.62 (42.80)
[140/157]	0.1206 (0.1168)	0.0802 (0.0769)	1.324 (1.620)	50.00 (43.31)
[150/157]	0.1214 (0.1171)	0.0803 (0.0771)	1.662 (1.610)	34.38 (43.65)
[156/157]	0.1023 (0.1170)	0.0738 (0.0772)	1.810 (1.602)	37.50 (43.92)
 * Train Acc 43.920
 * Val Acc 52.400, Total time 0.68
 * Val loss 1.310, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0091 (0.0091)	1.568 (1.568)	40.62 (40.62)
[10/157]	0.1078 (0.0999)	0.0679 (0.0612)	1.289 (1.405)	34.38 (47.16)
[20/157]	0.1076 (0.1027)	0.0674 (0.0638)	1.560 (1.424)	50.00 (47.47)
[30/157]	0.1071 (0.1038)	0.0669 (0.0647)	1.215 (1.414)	59.38 (48.19)
[40/157]	0.1067 (0.1044)	0.0662 (0.0652)	1.400 (1.423)	56.25 (48.09)
[50/157]	0.1063 (0.1048)	0.0664 (0.0656)	1.250 (1.397)	50.00 (49.20)
[60/157]	0.1034 (0.1049)	0.0640 (0.0656)	1.164 (1.398)	56.25 (49.23)
[70/157]	0.1122 (0.1050)	0.0717 (0.0657)	1.819 (1.402)	43.75 (49.25)
[80/157]	0.1119 (0.1060)	0.0711 (0.0667)	1.424 (1.388)	50.00 (50.08)
[90/157]	0.1129 (0.1069)	0.0721 (0.0675)	1.054 (1.394)	65.62 (49.97)
[100/157]	0.1116 (0.1075)	0.0717 (0.0680)	1.251 (1.394)	50.00 (50.03)
[110/157]	0.1153 (0.1080)	0.0734 (0.0685)	1.206 (1.384)	46.88 (50.25)
[120/157]	0.1134 (0.1085)	0.0729 (0.0690)	1.433 (1.379)	40.62 (50.62)
[130/157]	0.1135 (0.1089)	0.0733 (0.0694)	1.331 (1.381)	56.25 (50.52)
[140/157]	0.1155 (0.1092)	0.0744 (0.0697)	1.139 (1.372)	56.25 (50.91)
[150/157]	0.1140 (0.1095)	0.0731 (0.0700)	1.234 (1.368)	59.38 (51.08)
[156/157]	0.0970 (0.1096)	0.0695 (0.0701)	1.380 (1.363)	62.50 (51.22)
 * Train Acc 51.220
 * Val Acc 55.400, Total time 0.65
 * Val loss 1.227, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0466 (0.0466)	0.0092 (0.0092)	1.386 (1.386)	53.12 (53.12)
[10/157]	0.1145 (0.1065)	0.0744 (0.0672)	1.423 (1.278)	40.62 (52.84)
[20/157]	0.1141 (0.1099)	0.0734 (0.0703)	1.585 (1.339)	40.62 (51.64)
[30/157]	0.1114 (0.1111)	0.0710 (0.0715)	1.204 (1.357)	50.00 (50.50)
[40/157]	0.1132 (0.1119)	0.0727 (0.0722)	1.179 (1.348)	62.50 (50.84)
[50/157]	0.1127 (0.1123)	0.0726 (0.0726)	1.504 (1.349)	46.88 (50.98)
[60/157]	0.1130 (0.1126)	0.0726 (0.0729)	1.107 (1.334)	59.38 (51.74)
[70/157]	0.1169 (0.1127)	0.0759 (0.0729)	1.131 (1.325)	59.38 (51.94)
[80/157]	0.1134 (0.1129)	0.0722 (0.0731)	1.296 (1.327)	46.88 (51.77)
[90/157]	0.1141 (0.1130)	0.0733 (0.0732)	1.313 (1.329)	53.12 (51.96)
[100/157]	0.1136 (0.1130)	0.0728 (0.0732)	1.250 (1.312)	53.12 (52.63)
[110/157]	0.1132 (0.1131)	0.0726 (0.0733)	1.083 (1.313)	62.50 (52.73)
[120/157]	0.1150 (0.1131)	0.0742 (0.0733)	1.125 (1.306)	68.75 (53.15)
[130/157]	0.1147 (0.1132)	0.0743 (0.0734)	1.199 (1.307)	53.12 (53.15)
[140/157]	0.1149 (0.1132)	0.0734 (0.0734)	1.520 (1.305)	37.50 (52.93)
[150/157]	0.1158 (0.1133)	0.0750 (0.0734)	1.032 (1.304)	68.75 (52.98)
[156/157]	0.0955 (0.1132)	0.0680 (0.0735)	1.153 (1.297)	62.50 (53.36)
 * Train Acc 53.360
 * Val Acc 59.700, Total time 0.66
 * Val loss 1.144, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0461 (0.0461)	0.0093 (0.0093)	0.819 (0.819)	65.62 (65.62)
[10/157]	0.1140 (0.1067)	0.0738 (0.0673)	1.779 (1.229)	40.62 (58.81)
[20/157]	0.1162 (0.1100)	0.0754 (0.0705)	1.274 (1.203)	50.00 (58.93)
[30/157]	0.1144 (0.1111)	0.0737 (0.0715)	1.157 (1.189)	56.25 (58.37)
[40/157]	0.1146 (0.1119)	0.0738 (0.0722)	1.425 (1.234)	56.25 (56.63)
[50/157]	0.1145 (0.1123)	0.0734 (0.0726)	1.292 (1.230)	50.00 (56.19)
[60/157]	0.1149 (0.1126)	0.0742 (0.0729)	1.411 (1.247)	50.00 (55.17)
[70/157]	0.1137 (0.1127)	0.0729 (0.0730)	1.425 (1.253)	43.75 (55.06)
[80/157]	0.1145 (0.1129)	0.0736 (0.0731)	0.770 (1.248)	71.88 (55.48)
[90/157]	0.1114 (0.1130)	0.0711 (0.0731)	1.073 (1.242)	62.50 (55.70)
[100/157]	0.1152 (0.1131)	0.0709 (0.0732)	1.168 (1.228)	53.12 (56.50)
[110/157]	0.1149 (0.1131)	0.0736 (0.0731)	1.127 (1.226)	62.50 (56.70)
[120/157]	0.1132 (0.1131)	0.0723 (0.0732)	0.974 (1.222)	65.62 (56.90)
[130/157]	0.1128 (0.1132)	0.0721 (0.0733)	1.545 (1.220)	40.62 (56.70)
[140/157]	0.1136 (0.1133)	0.0718 (0.0733)	1.190 (1.223)	62.50 (56.87)
[150/157]	0.1139 (0.1133)	0.0730 (0.0733)	1.275 (1.225)	59.38 (56.71)
[156/157]	0.0963 (0.1132)	0.0685 (0.0733)	1.078 (1.225)	62.50 (56.72)
 * Train Acc 56.720
 * Val Acc 59.500, Total time 0.65
 * Val loss 1.100, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0460 (0.0460)	0.0092 (0.0092)	0.823 (0.823)	75.00 (75.00)
[10/157]	0.1145 (0.1068)	0.0735 (0.0670)	1.308 (1.135)	53.12 (59.09)
[20/157]	0.1143 (0.1101)	0.0725 (0.0701)	1.100 (1.085)	56.25 (60.86)
[30/157]	0.1136 (0.1113)	0.0724 (0.0713)	1.597 (1.140)	43.75 (58.77)
[40/157]	0.1158 (0.1119)	0.0740 (0.0718)	1.469 (1.176)	56.25 (58.84)
[50/157]	0.1140 (0.1122)	0.0732 (0.0721)	0.944 (1.194)	62.50 (57.84)
[60/157]	0.1118 (0.1124)	0.0703 (0.0722)	1.319 (1.202)	53.12 (57.07)
[70/157]	0.1141 (0.1126)	0.0728 (0.0723)	0.911 (1.217)	78.12 (57.00)
[80/157]	0.1144 (0.1128)	0.0720 (0.0724)	0.933 (1.216)	68.75 (57.18)
[90/157]	0.1137 (0.1130)	0.0721 (0.0725)	1.254 (1.210)	53.12 (57.42)
[100/157]	0.1113 (0.1130)	0.0707 (0.0726)	1.114 (1.204)	53.12 (57.61)
[110/157]	0.1155 (0.1130)	0.0745 (0.0727)	1.102 (1.205)	56.25 (57.52)
[120/157]	0.1161 (0.1131)	0.0754 (0.0728)	1.268 (1.206)	53.12 (57.49)
[130/157]	0.1141 (0.1131)	0.0736 (0.0728)	1.044 (1.207)	65.62 (57.61)
[140/157]	0.1156 (0.1132)	0.0746 (0.0729)	0.957 (1.209)	75.00 (57.58)
[150/157]	0.1143 (0.1132)	0.0729 (0.0730)	1.057 (1.206)	65.62 (57.53)
[156/157]	0.1009 (0.1132)	0.0701 (0.0730)	1.324 (1.204)	37.50 (57.58)
 * Train Acc 57.580
 * Val Acc 59.400, Total time 0.65
 * Val loss 1.070, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0464 (0.0464)	0.0093 (0.0093)	0.798 (0.798)	75.00 (75.00)
[10/157]	0.1140 (0.1072)	0.0721 (0.0669)	1.042 (1.085)	65.62 (63.35)
[20/157]	0.1178 (0.1105)	0.0733 (0.0700)	0.989 (1.086)	71.88 (61.46)
[30/157]	0.1143 (0.1116)	0.0715 (0.0711)	1.148 (1.112)	56.25 (60.48)
[40/157]	0.1145 (0.1123)	0.0735 (0.0717)	1.271 (1.138)	53.12 (58.77)
[50/157]	0.1208 (0.1134)	0.0797 (0.0728)	1.339 (1.164)	56.25 (58.09)
[60/157]	0.1216 (0.1146)	0.0798 (0.0740)	1.288 (1.164)	56.25 (58.56)
[70/157]	0.1214 (0.1154)	0.0803 (0.0748)	1.426 (1.181)	46.88 (58.19)
[80/157]	0.1213 (0.1161)	0.0805 (0.0754)	1.299 (1.182)	56.25 (58.06)
[90/157]	0.1215 (0.1166)	0.0804 (0.0759)	1.462 (1.191)	56.25 (58.24)
[100/157]	0.1209 (0.1170)	0.0799 (0.0763)	1.136 (1.188)	53.12 (58.26)
[110/157]	0.1209 (0.1173)	0.0801 (0.0767)	1.133 (1.184)	75.00 (58.36)
[120/157]	0.1240 (0.1176)	0.0804 (0.0769)	0.977 (1.172)	71.88 (58.86)
[130/157]	0.1214 (0.1178)	0.0801 (0.0771)	1.456 (1.170)	53.12 (59.18)
[140/157]	0.1219 (0.1180)	0.0805 (0.0773)	0.924 (1.166)	71.88 (59.29)
[150/157]	0.1219 (0.1181)	0.0803 (0.0775)	1.237 (1.170)	62.50 (59.11)
[156/157]	0.1028 (0.1181)	0.0725 (0.0775)	0.978 (1.171)	75.00 (59.12)
 * Train Acc 59.120
 * Val Acc 60.800, Total time 0.73
 * Val loss 1.028, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0097 (0.0097)	1.099 (1.099)	68.75 (68.75)
[10/157]	0.1116 (0.1032)	0.0705 (0.0639)	1.314 (1.085)	56.25 (60.23)
[20/157]	0.1096 (0.1064)	0.0682 (0.0667)	1.165 (1.131)	65.62 (59.38)
[30/157]	0.1123 (0.1076)	0.0711 (0.0677)	0.957 (1.139)	68.75 (59.07)
[40/157]	0.1148 (0.1083)	0.0709 (0.0682)	1.255 (1.152)	50.00 (58.84)
[50/157]	0.1098 (0.1084)	0.0695 (0.0685)	1.056 (1.152)	75.00 (59.13)
[60/157]	0.1116 (0.1087)	0.0700 (0.0688)	0.920 (1.141)	65.62 (59.63)
[70/157]	0.1207 (0.1086)	0.0800 (0.0687)	1.325 (1.140)	56.25 (59.86)
[80/157]	0.1207 (0.1101)	0.0801 (0.0701)	1.168 (1.140)	59.38 (60.19)
[90/157]	0.1207 (0.1112)	0.0797 (0.0712)	1.038 (1.134)	56.25 (60.06)
[100/157]	0.1221 (0.1122)	0.0808 (0.0720)	1.190 (1.142)	53.12 (59.81)
[110/157]	0.1217 (0.1129)	0.0798 (0.0727)	0.964 (1.141)	71.88 (60.11)
[120/157]	0.1195 (0.1136)	0.0781 (0.0733)	1.226 (1.143)	56.25 (60.33)
[130/157]	0.1217 (0.1141)	0.0804 (0.0738)	0.993 (1.150)	71.88 (60.26)
[140/157]	0.1213 (0.1145)	0.0804 (0.0742)	0.912 (1.147)	71.88 (60.37)
[150/157]	0.1205 (0.1149)	0.0800 (0.0746)	1.118 (1.151)	59.38 (60.20)
[156/157]	0.1020 (0.1150)	0.0734 (0.0747)	1.668 (1.152)	62.50 (60.02)
 * Train Acc 60.020
 * Val Acc 64.200, Total time 0.75
 * Val loss 1.035, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0090 (0.0090)	0.815 (0.815)	81.25 (81.25)
[10/157]	0.1100 (0.1012)	0.0694 (0.0622)	1.014 (1.070)	62.50 (61.93)
[20/157]	0.1105 (0.1049)	0.0697 (0.0657)	1.151 (1.067)	71.88 (63.69)
[30/157]	0.1097 (0.1062)	0.0690 (0.0668)	0.741 (1.056)	75.00 (64.62)
[40/157]	0.1222 (0.1091)	0.0801 (0.0695)	1.572 (1.086)	37.50 (62.80)
[50/157]	0.1213 (0.1113)	0.0799 (0.0716)	0.960 (1.101)	68.75 (62.44)
[60/157]	0.1194 (0.1128)	0.0782 (0.0729)	1.217 (1.104)	56.25 (62.14)
[70/157]	0.1220 (0.1139)	0.0803 (0.0739)	1.266 (1.098)	62.50 (62.68)
[80/157]	0.1208 (0.1147)	0.0801 (0.0746)	1.221 (1.111)	53.12 (62.00)
[90/157]	0.1223 (0.1153)	0.0806 (0.0753)	0.959 (1.125)	75.00 (61.40)
[100/157]	0.1209 (0.1158)	0.0800 (0.0757)	1.143 (1.123)	56.25 (61.36)
[110/157]	0.1205 (0.1162)	0.0793 (0.0761)	1.082 (1.127)	53.12 (61.04)
[120/157]	0.1223 (0.1166)	0.0805 (0.0764)	0.865 (1.129)	75.00 (60.92)
[130/157]	0.1223 (0.1169)	0.0804 (0.0767)	1.071 (1.123)	62.50 (61.43)
[140/157]	0.1215 (0.1172)	0.0802 (0.0769)	0.736 (1.120)	81.25 (61.59)
[150/157]	0.1224 (0.1174)	0.0806 (0.0771)	1.371 (1.127)	53.12 (61.38)
[156/157]	0.1020 (0.1174)	0.0731 (0.0772)	2.337 (1.134)	62.50 (61.30)
 * Train Acc 61.300
 * Val Acc 63.900, Total time 0.76
 * Val loss 0.989, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0096 (0.0096)	0.855 (0.855)	81.25 (81.25)
[10/157]	0.1068 (0.0987)	0.0663 (0.0599)	0.787 (1.074)	78.12 (65.06)
[20/157]	0.1116 (0.1040)	0.0703 (0.0648)	1.124 (1.125)	56.25 (61.31)
[30/157]	0.1109 (0.1061)	0.0697 (0.0667)	0.880 (1.113)	59.38 (61.39)
[40/157]	0.1051 (0.1067)	0.0637 (0.0672)	1.179 (1.113)	62.50 (62.04)
[50/157]	0.1184 (0.1070)	0.0774 (0.0674)	1.010 (1.103)	68.75 (62.38)
[60/157]	0.1208 (0.1092)	0.0803 (0.0695)	1.062 (1.094)	56.25 (62.24)
[70/157]	0.1207 (0.1108)	0.0796 (0.0710)	1.054 (1.103)	59.38 (61.44)
[80/157]	0.1219 (0.1120)	0.0805 (0.0721)	1.156 (1.111)	56.25 (61.27)
[90/157]	0.1219 (0.1129)	0.0802 (0.0730)	1.240 (1.107)	53.12 (61.44)
[100/157]	0.1207 (0.1136)	0.0802 (0.0737)	0.994 (1.102)	68.75 (61.79)
[110/157]	0.1211 (0.1142)	0.0793 (0.0742)	1.180 (1.111)	62.50 (61.91)
[120/157]	0.1208 (0.1148)	0.0800 (0.0747)	0.952 (1.116)	68.75 (61.85)
[130/157]	0.1188 (0.1152)	0.0782 (0.0751)	1.047 (1.119)	65.62 (61.90)
[140/157]	0.1211 (0.1156)	0.0797 (0.0754)	1.190 (1.114)	65.62 (62.23)
[150/157]	0.1206 (0.1159)	0.0799 (0.0757)	0.911 (1.122)	62.50 (62.11)
[156/157]	0.1045 (0.1159)	0.0742 (0.0758)	1.828 (1.121)	50.00 (62.22)
 * Train Acc 62.220
 * Val Acc 64.500, Total time 0.75
 * Val loss 0.994, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0493 (0.0493)	0.0094 (0.0094)	0.811 (0.811)	78.12 (78.12)
[10/157]	0.1190 (0.1101)	0.0769 (0.0700)	1.088 (1.181)	62.50 (60.51)
[20/157]	0.1191 (0.1137)	0.0776 (0.0734)	0.940 (1.160)	68.75 (60.42)
[30/157]	0.1184 (0.1151)	0.0772 (0.0748)	1.513 (1.170)	40.62 (59.98)
[40/157]	0.1178 (0.1159)	0.0758 (0.0753)	0.995 (1.144)	71.88 (60.98)
[50/157]	0.1184 (0.1162)	0.0770 (0.0757)	1.147 (1.147)	53.12 (60.60)
[60/157]	0.1193 (0.1165)	0.0780 (0.0760)	0.973 (1.131)	62.50 (61.17)
[70/157]	0.1193 (0.1167)	0.0780 (0.0762)	0.968 (1.110)	75.00 (62.59)
[80/157]	0.1192 (0.1169)	0.0778 (0.0764)	1.103 (1.096)	59.38 (63.27)
[90/157]	0.1189 (0.1170)	0.0772 (0.0766)	1.228 (1.093)	62.50 (63.67)
[100/157]	0.1184 (0.1171)	0.0770 (0.0766)	0.808 (1.103)	68.75 (63.40)
[110/157]	0.1181 (0.1172)	0.0769 (0.0767)	1.216 (1.096)	56.25 (63.65)
[120/157]	0.1180 (0.1172)	0.0768 (0.0767)	0.979 (1.099)	78.12 (63.48)
[130/157]	0.1192 (0.1173)	0.0776 (0.0768)	0.929 (1.093)	59.38 (63.31)
[140/157]	0.1166 (0.1174)	0.0754 (0.0769)	0.988 (1.092)	62.50 (63.34)
[150/157]	0.1188 (0.1174)	0.0779 (0.0769)	1.316 (1.094)	59.38 (63.37)
[156/157]	0.1008 (0.1173)	0.0724 (0.0769)	0.829 (1.094)	62.50 (63.36)
 * Train Acc 63.360
 * Val Acc 65.000, Total time 0.68
 * Val loss 0.968, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0083 (0.0083)	1.333 (1.333)	56.25 (56.25)
[10/157]	0.1185 (0.1104)	0.0775 (0.0707)	1.124 (1.181)	65.62 (58.24)
[20/157]	0.1193 (0.1141)	0.0770 (0.0737)	0.772 (1.134)	71.88 (59.82)
[30/157]	0.1165 (0.1152)	0.0756 (0.0749)	1.228 (1.111)	53.12 (61.39)
[40/157]	0.1180 (0.1158)	0.0770 (0.0757)	1.194 (1.138)	56.25 (60.06)
[50/157]	0.1190 (0.1163)	0.0775 (0.0760)	0.949 (1.125)	68.75 (60.60)
[60/157]	0.1192 (0.1165)	0.0779 (0.0763)	1.119 (1.100)	62.50 (61.63)
[70/157]	0.1186 (0.1167)	0.0772 (0.0764)	1.181 (1.100)	56.25 (61.88)
[80/157]	0.1167 (0.1168)	0.0754 (0.0766)	1.044 (1.106)	62.50 (61.96)
[90/157]	0.1177 (0.1169)	0.0765 (0.0767)	1.419 (1.102)	46.88 (62.23)
[100/157]	0.1182 (0.1169)	0.0772 (0.0768)	1.198 (1.094)	71.88 (62.84)
[110/157]	0.1188 (0.1170)	0.0776 (0.0768)	0.889 (1.086)	71.88 (63.32)
[120/157]	0.1187 (0.1171)	0.0771 (0.0769)	1.229 (1.087)	59.38 (63.48)
[130/157]	0.1198 (0.1172)	0.0776 (0.0770)	1.272 (1.090)	56.25 (63.43)
[140/157]	0.1191 (0.1172)	0.0783 (0.0770)	1.164 (1.090)	59.38 (63.41)
[150/157]	0.1169 (0.1173)	0.0761 (0.0771)	1.415 (1.090)	62.50 (63.29)
[156/157]	0.1013 (0.1172)	0.0729 (0.0771)	1.304 (1.088)	62.50 (63.44)
 * Train Acc 63.440
 * Val Acc 66.300, Total time 0.67
 * Val loss 0.939, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0467 (0.0467)	0.0092 (0.0092)	1.136 (1.136)	68.75 (68.75)
[10/157]	0.1191 (0.1108)	0.0781 (0.0713)	0.865 (1.075)	71.88 (65.34)
[20/157]	0.1184 (0.1143)	0.0779 (0.0744)	1.174 (1.073)	68.75 (64.73)
[30/157]	0.1183 (0.1155)	0.0770 (0.0756)	0.889 (1.085)	71.88 (63.91)
[40/157]	0.1179 (0.1161)	0.0771 (0.0761)	0.780 (1.060)	71.88 (64.41)
[50/157]	0.1186 (0.1164)	0.0776 (0.0765)	1.170 (1.060)	56.25 (64.22)
[60/157]	0.1189 (0.1167)	0.0777 (0.0767)	1.145 (1.074)	50.00 (63.63)
[70/157]	0.1174 (0.1170)	0.0758 (0.0768)	0.885 (1.069)	75.00 (64.04)
[80/157]	0.1188 (0.1171)	0.0762 (0.0769)	1.116 (1.068)	68.75 (63.93)
[90/157]	0.1191 (0.1172)	0.0778 (0.0768)	1.291 (1.067)	65.62 (63.98)
[100/157]	0.1179 (0.1172)	0.0767 (0.0769)	0.959 (1.070)	65.62 (63.64)
[110/157]	0.1171 (0.1173)	0.0764 (0.0769)	0.877 (1.074)	68.75 (63.54)
[120/157]	0.1184 (0.1174)	0.0764 (0.0770)	0.984 (1.071)	75.00 (63.71)
[130/157]	0.1186 (0.1174)	0.0779 (0.0770)	0.929 (1.072)	65.62 (63.84)
[140/157]	0.1179 (0.1174)	0.0771 (0.0771)	1.017 (1.075)	75.00 (63.67)
[150/157]	0.1201 (0.1175)	0.0787 (0.0772)	1.173 (1.085)	62.50 (63.56)
[156/157]	0.1026 (0.1174)	0.0708 (0.0771)	0.874 (1.089)	62.50 (63.36)
 * Train Acc 63.360
 * Val Acc 66.600, Total time 0.69
 * Val loss 0.946, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0470 (0.0470)	0.0097 (0.0097)	0.826 (0.826)	71.88 (71.88)
[10/157]	0.1188 (0.1107)	0.0774 (0.0706)	1.551 (1.021)	40.62 (64.20)
[20/157]	0.1185 (0.1141)	0.0774 (0.0740)	1.233 (1.039)	56.25 (64.14)
[30/157]	0.1170 (0.1154)	0.0766 (0.0752)	0.915 (1.048)	56.25 (64.11)
[40/157]	0.1186 (0.1160)	0.0778 (0.0759)	1.205 (1.042)	68.75 (64.79)
[50/157]	0.1199 (0.1164)	0.0785 (0.0763)	1.187 (1.058)	62.50 (63.85)
[60/157]	0.1184 (0.1167)	0.0773 (0.0766)	1.231 (1.078)	56.25 (63.27)
[70/157]	0.1184 (0.1169)	0.0776 (0.0768)	1.567 (1.074)	46.88 (63.69)
[80/157]	0.1182 (0.1170)	0.0774 (0.0769)	0.974 (1.062)	71.88 (64.27)
[90/157]	0.1186 (0.1171)	0.0774 (0.0770)	1.034 (1.064)	65.62 (64.18)
[100/157]	0.1186 (0.1172)	0.0776 (0.0771)	1.231 (1.064)	59.38 (64.02)
[110/157]	0.1178 (0.1173)	0.0766 (0.0771)	0.773 (1.068)	78.12 (64.27)
[120/157]	0.1177 (0.1173)	0.0771 (0.0772)	0.880 (1.063)	62.50 (64.44)
[130/157]	0.1189 (0.1174)	0.0775 (0.0772)	1.034 (1.054)	56.25 (64.77)
[140/157]	0.1182 (0.1174)	0.0775 (0.0773)	0.948 (1.049)	65.62 (65.05)
[150/157]	0.1186 (0.1174)	0.0776 (0.0773)	1.242 (1.053)	62.50 (64.88)
[156/157]	0.1009 (0.1173)	0.0723 (0.0773)	0.556 (1.055)	87.50 (64.86)
 * Train Acc 64.860
 * Val Acc 66.500, Total time 0.67
 * Val loss 0.925, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0471 (0.0471)	0.0092 (0.0092)	1.047 (1.047)	65.62 (65.62)
[10/157]	0.1189 (0.1105)	0.0783 (0.0713)	0.852 (1.056)	75.00 (67.33)
[20/157]	0.1191 (0.1142)	0.0787 (0.0746)	1.308 (1.095)	50.00 (63.84)
[30/157]	0.1183 (0.1153)	0.0764 (0.0754)	0.917 (1.084)	81.25 (64.21)
[40/157]	0.1166 (0.1160)	0.0758 (0.0760)	1.319 (1.093)	59.38 (63.57)
[50/157]	0.1177 (0.1163)	0.0768 (0.0762)	0.781 (1.059)	68.75 (65.07)
[60/157]	0.1184 (0.1165)	0.0771 (0.0765)	1.009 (1.050)	62.50 (65.11)
[70/157]	0.1187 (0.1167)	0.0776 (0.0766)	0.736 (1.047)	71.88 (65.49)
[80/157]	0.1179 (0.1168)	0.0764 (0.0767)	0.947 (1.053)	65.62 (65.28)
[90/157]	0.1182 (0.1170)	0.0765 (0.0768)	1.314 (1.045)	50.00 (65.28)
[100/157]	0.1193 (0.1170)	0.0769 (0.0768)	0.799 (1.047)	75.00 (65.16)
[110/157]	0.1186 (0.1171)	0.0772 (0.0769)	1.117 (1.053)	50.00 (64.78)
[120/157]	0.1195 (0.1172)	0.0775 (0.0769)	1.080 (1.045)	56.25 (64.95)
[130/157]	0.1180 (0.1172)	0.0772 (0.0770)	1.064 (1.046)	62.50 (64.89)
[140/157]	0.1222 (0.1174)	0.0809 (0.0771)	0.735 (1.044)	81.25 (65.03)
[150/157]	0.1155 (0.1173)	0.0742 (0.0770)	1.149 (1.042)	59.38 (65.07)
[156/157]	0.0948 (0.1170)	0.0679 (0.0768)	0.821 (1.042)	87.50 (65.08)
 * Train Acc 65.080
 * Val Acc 66.200, Total time 0.64
 * Val loss 0.934, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0465 (0.0465)	0.0092 (0.0092)	1.224 (1.224)	50.00 (50.00)
[10/157]	0.1211 (0.1091)	0.0801 (0.0696)	0.940 (1.051)	75.00 (62.50)
[20/157]	0.1212 (0.1146)	0.0800 (0.0745)	0.897 (0.994)	68.75 (65.77)
[30/157]	0.1214 (0.1166)	0.0785 (0.0761)	1.197 (1.028)	65.62 (64.82)
[40/157]	0.1213 (0.1175)	0.0803 (0.0769)	1.141 (1.007)	65.62 (66.08)
[50/157]	0.1221 (0.1181)	0.0805 (0.0775)	1.135 (1.004)	56.25 (66.36)
[60/157]	0.1213 (0.1185)	0.0801 (0.0779)	1.124 (1.015)	62.50 (65.98)
[70/157]	0.1212 (0.1188)	0.0800 (0.0782)	1.120 (1.019)	56.25 (65.71)
[80/157]	0.1230 (0.1190)	0.0802 (0.0785)	1.079 (1.032)	75.00 (65.51)
[90/157]	0.1213 (0.1192)	0.0798 (0.0786)	1.474 (1.026)	53.12 (65.87)
[100/157]	0.1210 (0.1193)	0.0801 (0.0787)	1.109 (1.031)	59.38 (65.93)
[110/157]	0.1220 (0.1194)	0.0804 (0.0788)	0.988 (1.038)	62.50 (65.62)
[120/157]	0.1219 (0.1196)	0.0797 (0.0789)	0.983 (1.035)	71.88 (65.57)
[130/157]	0.1209 (0.1197)	0.0790 (0.0790)	1.043 (1.038)	59.38 (65.58)
[140/157]	0.1219 (0.1197)	0.0806 (0.0790)	1.373 (1.034)	56.25 (65.67)
[150/157]	0.1217 (0.1198)	0.0803 (0.0791)	1.322 (1.039)	50.00 (65.36)
[156/157]	0.1025 (0.1197)	0.0729 (0.0791)	0.816 (1.040)	50.00 (65.20)
 * Train Acc 65.200
 * Val Acc 67.800, Total time 0.78
 * Val loss 0.909, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0508 (0.0508)	0.0095 (0.0095)	1.293 (1.293)	65.62 (65.62)
[10/157]	0.1218 (0.1142)	0.0800 (0.0737)	1.088 (1.020)	68.75 (66.48)
[20/157]	0.1217 (0.1172)	0.0801 (0.0767)	0.979 (0.997)	62.50 (67.11)
[30/157]	0.1225 (0.1183)	0.0802 (0.0777)	0.881 (1.000)	68.75 (67.04)
[40/157]	0.1217 (0.1189)	0.0804 (0.0783)	0.801 (0.992)	71.88 (66.69)
[50/157]	0.1207 (0.1192)	0.0798 (0.0786)	0.929 (0.998)	71.88 (66.48)
[60/157]	0.1215 (0.1194)	0.0802 (0.0788)	0.966 (1.005)	59.38 (66.39)
[70/157]	0.1222 (0.1196)	0.0803 (0.0790)	1.046 (1.006)	62.50 (65.98)
[80/157]	0.1212 (0.1198)	0.0797 (0.0792)	1.452 (1.005)	56.25 (65.97)
[90/157]	0.1213 (0.1199)	0.0801 (0.0793)	0.923 (1.010)	68.75 (65.90)
[100/157]	0.1214 (0.1200)	0.0800 (0.0793)	1.278 (1.017)	59.38 (65.69)
[110/157]	0.1220 (0.1201)	0.0799 (0.0794)	0.986 (1.015)	71.88 (65.99)
[120/157]	0.1215 (0.1201)	0.0803 (0.0794)	1.023 (1.019)	65.62 (65.91)
[130/157]	0.1219 (0.1202)	0.0806 (0.0794)	0.744 (1.017)	75.00 (66.05)
[140/157]	0.1216 (0.1202)	0.0799 (0.0794)	1.051 (1.017)	59.38 (65.91)
[150/157]	0.1213 (0.1203)	0.0799 (0.0795)	1.107 (1.024)	62.50 (65.60)
[156/157]	0.1032 (0.1202)	0.0736 (0.0795)	0.839 (1.025)	75.00 (65.62)
 * Train Acc 65.620
 * Val Acc 67.700, Total time 0.72
 * Val loss 0.903, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0466 (0.0466)	0.0093 (0.0093)	1.079 (1.079)	71.88 (71.88)
[10/157]	0.1085 (0.0979)	0.0671 (0.0583)	0.671 (1.011)	84.38 (66.76)
[20/157]	0.1213 (0.1087)	0.0803 (0.0684)	1.048 (1.026)	56.25 (65.92)
[30/157]	0.1211 (0.1124)	0.0796 (0.0719)	0.995 (1.014)	71.88 (66.43)
[40/157]	0.1199 (0.1142)	0.0792 (0.0738)	1.068 (1.016)	62.50 (66.31)
[50/157]	0.1204 (0.1154)	0.0790 (0.0749)	1.103 (1.028)	59.38 (65.81)
[60/157]	0.1220 (0.1163)	0.0804 (0.0758)	0.836 (1.032)	65.62 (65.37)
[70/157]	0.1242 (0.1169)	0.0815 (0.0764)	1.105 (1.031)	65.62 (65.40)
[80/157]	0.1207 (0.1173)	0.0794 (0.0768)	1.424 (1.032)	50.00 (65.47)
[90/157]	0.1216 (0.1177)	0.0796 (0.0771)	0.954 (1.024)	71.88 (66.04)
[100/157]	0.1226 (0.1180)	0.0805 (0.0774)	1.216 (1.028)	65.62 (66.00)
[110/157]	0.1212 (0.1182)	0.0795 (0.0776)	0.618 (1.023)	78.12 (66.02)
[120/157]	0.1222 (0.1184)	0.0813 (0.0778)	1.081 (1.032)	71.88 (65.73)
[130/157]	0.1220 (0.1185)	0.0810 (0.0780)	0.975 (1.038)	75.00 (65.60)
[140/157]	0.1182 (0.1187)	0.0788 (0.0781)	0.884 (1.036)	75.00 (65.74)
[150/157]	0.1220 (0.1188)	0.0800 (0.0782)	0.921 (1.033)	68.75 (65.79)
[156/157]	0.1028 (0.1188)	0.0730 (0.0783)	1.263 (1.029)	50.00 (65.88)
 * Train Acc 65.880
 * Val Acc 67.300, Total time 0.76
 * Val loss 0.924, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0497 (0.0497)	0.0102 (0.0102)	1.186 (1.186)	56.25 (56.25)
[10/157]	0.1221 (0.1135)	0.0805 (0.0735)	1.359 (1.037)	53.12 (63.64)
[20/157]	0.1218 (0.1169)	0.0798 (0.0765)	0.962 (0.999)	59.38 (66.67)
[30/157]	0.1208 (0.1179)	0.0801 (0.0775)	1.390 (1.026)	46.88 (65.52)
[40/157]	0.1218 (0.1186)	0.0805 (0.0781)	0.809 (1.032)	68.75 (65.17)
[50/157]	0.1209 (0.1191)	0.0797 (0.0784)	0.869 (1.019)	75.00 (65.62)
[60/157]	0.1226 (0.1193)	0.0815 (0.0786)	1.134 (1.010)	62.50 (66.03)
[70/157]	0.1226 (0.1195)	0.0801 (0.0788)	1.260 (1.023)	68.75 (66.20)
[80/157]	0.1214 (0.1197)	0.0802 (0.0789)	0.922 (1.015)	62.50 (66.59)
[90/157]	0.1212 (0.1198)	0.0798 (0.0790)	1.199 (1.020)	56.25 (65.83)
[100/157]	0.1211 (0.1199)	0.0796 (0.0791)	0.686 (1.016)	81.25 (65.87)
[110/157]	0.1221 (0.1200)	0.0800 (0.0791)	0.864 (1.012)	65.62 (66.13)
[120/157]	0.1211 (0.1200)	0.0792 (0.0792)	1.076 (1.007)	59.38 (66.58)
[130/157]	0.1205 (0.1200)	0.0796 (0.0793)	0.937 (1.013)	65.62 (66.39)
[140/157]	0.1219 (0.1201)	0.0804 (0.0793)	0.791 (1.013)	68.75 (66.18)
[150/157]	0.1221 (0.1201)	0.0802 (0.0794)	1.238 (1.016)	50.00 (65.91)
[156/157]	0.1013 (0.1200)	0.0730 (0.0794)	1.485 (1.018)	37.50 (65.80)
 * Train Acc 65.800
 * Val Acc 68.300, Total time 0.77
 * Val loss 0.883, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0486 (0.0486)	0.0097 (0.0097)	1.060 (1.060)	59.38 (59.38)
[10/157]	0.1230 (0.1143)	0.0794 (0.0735)	0.756 (1.034)	68.75 (65.62)
[20/157]	0.1213 (0.1171)	0.0801 (0.0762)	0.909 (1.008)	78.12 (66.82)
[30/157]	0.1216 (0.1182)	0.0803 (0.0774)	0.697 (0.985)	78.12 (67.94)
[40/157]	0.1212 (0.1188)	0.0796 (0.0779)	1.223 (0.993)	56.25 (67.53)
[50/157]	0.1232 (0.1192)	0.0807 (0.0783)	0.672 (0.991)	78.12 (67.16)
[60/157]	0.1209 (0.1193)	0.0788 (0.0784)	0.719 (0.989)	75.00 (66.96)
[70/157]	0.1222 (0.1196)	0.0802 (0.0785)	0.815 (1.001)	65.62 (66.77)
[80/157]	0.1210 (0.1197)	0.0791 (0.0786)	1.113 (1.002)	65.62 (66.67)
[90/157]	0.1212 (0.1198)	0.0796 (0.0787)	1.022 (1.004)	68.75 (66.72)
[100/157]	0.1216 (0.1198)	0.0796 (0.0788)	0.996 (1.000)	65.62 (66.80)
[110/157]	0.1222 (0.1199)	0.0803 (0.0789)	1.111 (0.989)	59.38 (67.26)
[120/157]	0.1210 (0.1199)	0.0793 (0.0790)	0.790 (0.986)	75.00 (67.25)
[130/157]	0.1205 (0.1199)	0.0790 (0.0790)	0.994 (0.986)	71.88 (67.51)
[140/157]	0.1226 (0.1200)	0.0798 (0.0790)	1.368 (0.994)	62.50 (67.40)
[150/157]	0.1203 (0.1199)	0.0793 (0.0791)	0.855 (0.995)	68.75 (67.22)
[156/157]	0.1024 (0.1198)	0.0732 (0.0790)	0.485 (0.996)	87.50 (67.18)
 * Train Acc 67.180
 * Val Acc 67.700, Total time 0.71
 * Val loss 0.908, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0464 (0.0464)	0.0106 (0.0106)	0.914 (0.914)	65.62 (65.62)
[10/157]	0.1212 (0.0972)	0.0802 (0.0590)	1.334 (1.006)	62.50 (65.91)
[20/157]	0.1216 (0.1062)	0.0799 (0.0670)	0.717 (0.968)	87.50 (67.86)
[30/157]	0.1214 (0.1108)	0.0803 (0.0712)	1.092 (0.991)	59.38 (68.15)
[40/157]	0.1211 (0.1132)	0.0804 (0.0735)	1.003 (0.999)	62.50 (67.99)
[50/157]	0.1217 (0.1146)	0.0803 (0.0747)	0.984 (1.006)	62.50 (66.85)
[60/157]	0.1226 (0.1156)	0.0798 (0.0756)	1.057 (1.009)	59.38 (66.19)
[70/157]	0.1211 (0.1164)	0.0798 (0.0761)	1.402 (1.011)	59.38 (66.24)
[80/157]	0.1229 (0.1169)	0.0805 (0.0766)	0.957 (0.995)	75.00 (66.98)
[90/157]	0.1214 (0.1173)	0.0802 (0.0770)	1.195 (0.999)	62.50 (66.66)
[100/157]	0.1209 (0.1176)	0.0801 (0.0773)	1.063 (1.004)	65.62 (66.37)
[110/157]	0.1209 (0.1178)	0.0798 (0.0775)	0.909 (0.994)	68.75 (66.84)
[120/157]	0.1207 (0.1181)	0.0796 (0.0778)	1.016 (0.999)	56.25 (66.53)
[130/157]	0.1216 (0.1183)	0.0802 (0.0779)	0.935 (0.998)	68.75 (66.56)
[140/157]	0.1219 (0.1184)	0.0803 (0.0781)	0.890 (0.999)	65.62 (66.45)
[150/157]	0.1215 (0.1186)	0.0804 (0.0782)	0.598 (0.997)	81.25 (66.49)
[156/157]	0.1030 (0.1186)	0.0729 (0.0782)	1.408 (1.000)	37.50 (66.36)
 * Train Acc 66.360
 * Val Acc 68.800, Total time 0.78
 * Val loss 0.875, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0514 (0.0514)	0.0090 (0.0090)	1.015 (1.015)	62.50 (62.50)
[10/157]	0.1215 (0.1137)	0.0806 (0.0739)	1.093 (1.026)	59.38 (64.49)
[20/157]	0.1216 (0.1170)	0.0793 (0.0767)	1.123 (1.031)	59.38 (64.14)
[30/157]	0.1214 (0.1182)	0.0798 (0.0778)	0.766 (1.018)	71.88 (65.32)
[40/157]	0.1224 (0.1188)	0.0805 (0.0784)	0.887 (1.036)	65.62 (64.94)
[50/157]	0.1224 (0.1192)	0.0801 (0.0787)	0.725 (1.016)	81.25 (65.99)
[60/157]	0.1216 (0.1194)	0.0798 (0.0789)	1.010 (1.018)	65.62 (65.98)
[70/157]	0.1205 (0.1195)	0.0798 (0.0790)	0.743 (1.016)	71.88 (65.36)
[80/157]	0.1224 (0.1196)	0.0798 (0.0791)	1.195 (1.021)	59.38 (65.43)
[90/157]	0.1214 (0.1197)	0.0799 (0.0792)	0.790 (1.003)	84.38 (66.24)
[100/157]	0.1173 (0.1194)	0.0761 (0.0790)	0.937 (0.997)	62.50 (66.65)
[110/157]	0.1175 (0.1190)	0.0762 (0.0785)	0.907 (1.000)	71.88 (66.58)
[120/157]	0.1215 (0.1188)	0.0797 (0.0784)	1.059 (1.000)	59.38 (66.48)
[130/157]	0.1117 (0.1186)	0.0707 (0.0782)	1.090 (1.001)	71.88 (66.63)
[140/157]	0.1210 (0.1185)	0.0799 (0.0780)	0.753 (0.996)	71.88 (66.87)
[150/157]	0.1193 (0.1185)	0.0787 (0.0781)	0.943 (0.996)	71.88 (67.07)
[156/157]	0.1013 (0.1184)	0.0728 (0.0781)	1.265 (0.994)	62.50 (67.12)
 * Train Acc 67.120
 * Val Acc 68.900, Total time 0.69
 * Val loss 0.882, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0476 (0.0476)	0.0093 (0.0093)	1.225 (1.225)	59.38 (59.38)
[10/157]	0.1205 (0.1118)	0.0791 (0.0721)	1.180 (1.014)	65.62 (67.05)
[20/157]	0.1223 (0.1159)	0.0800 (0.0756)	1.302 (0.972)	50.00 (67.86)
[30/157]	0.1216 (0.1174)	0.0800 (0.0769)	1.525 (0.990)	43.75 (66.73)
[40/157]	0.1214 (0.1182)	0.0801 (0.0776)	1.108 (0.990)	56.25 (66.31)
[50/157]	0.1215 (0.1187)	0.0799 (0.0780)	0.937 (0.975)	65.62 (67.71)
[60/157]	0.1211 (0.1191)	0.0798 (0.0784)	0.689 (0.973)	71.88 (67.67)
[70/157]	0.1203 (0.1193)	0.0793 (0.0786)	0.956 (0.974)	75.00 (68.00)
[80/157]	0.1216 (0.1194)	0.0802 (0.0788)	0.811 (0.973)	68.75 (67.90)
[90/157]	0.1230 (0.1196)	0.0805 (0.0789)	1.036 (0.981)	68.75 (67.62)
[100/157]	0.1209 (0.1197)	0.0796 (0.0790)	1.035 (0.989)	65.62 (67.26)
[110/157]	0.1216 (0.1197)	0.0801 (0.0791)	1.021 (0.990)	56.25 (67.29)
[120/157]	0.1218 (0.1198)	0.0807 (0.0791)	1.020 (0.991)	71.88 (67.36)
[130/157]	0.1207 (0.1199)	0.0797 (0.0792)	0.814 (0.990)	65.62 (67.44)
[140/157]	0.1217 (0.1199)	0.0804 (0.0793)	1.005 (0.995)	75.00 (67.18)
[150/157]	0.1213 (0.1199)	0.0805 (0.0793)	0.908 (0.995)	68.75 (67.28)
[156/157]	0.1028 (0.1198)	0.0736 (0.0793)	1.724 (0.992)	50.00 (67.36)
 * Train Acc 67.360
 * Val Acc 67.500, Total time 0.79
 * Val loss 0.882, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0508 (0.0508)	0.0093 (0.0093)	0.907 (0.907)	75.00 (75.00)
[10/157]	0.1210 (0.1140)	0.0798 (0.0741)	0.712 (0.943)	81.25 (68.75)
[20/157]	0.1204 (0.1169)	0.0797 (0.0769)	0.912 (0.951)	68.75 (68.30)
[30/157]	0.1208 (0.1181)	0.0798 (0.0779)	0.864 (0.940)	68.75 (68.75)
[40/157]	0.1215 (0.1186)	0.0806 (0.0784)	1.060 (0.946)	59.38 (68.37)
[50/157]	0.1213 (0.1189)	0.0798 (0.0786)	0.734 (0.959)	71.88 (68.20)
[60/157]	0.1210 (0.1190)	0.0798 (0.0788)	0.915 (0.961)	71.88 (68.65)
[70/157]	0.1210 (0.1192)	0.0797 (0.0789)	1.290 (0.975)	53.12 (67.91)
[80/157]	0.1198 (0.1193)	0.0792 (0.0790)	1.138 (0.985)	71.88 (68.17)
[90/157]	0.1200 (0.1193)	0.0798 (0.0791)	1.256 (0.978)	59.38 (68.58)
[100/157]	0.1209 (0.1194)	0.0794 (0.0792)	1.282 (0.984)	53.12 (68.32)
[110/157]	0.1211 (0.1195)	0.0795 (0.0792)	0.986 (0.984)	62.50 (68.33)
[120/157]	0.1215 (0.1195)	0.0795 (0.0792)	1.032 (0.973)	62.50 (68.70)
[130/157]	0.1211 (0.1196)	0.0792 (0.0792)	1.133 (0.978)	62.50 (68.58)
[140/157]	0.1208 (0.1196)	0.0789 (0.0792)	1.106 (0.981)	56.25 (68.09)
[150/157]	0.1208 (0.1196)	0.0797 (0.0792)	1.068 (0.976)	59.38 (68.21)
[156/157]	0.0997 (0.1195)	0.0719 (0.0791)	0.923 (0.977)	62.50 (68.16)
 * Train Acc 68.160
 * Val Acc 69.600, Total time 0.74
 * Val loss 0.873, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0492 (0.0492)	0.0094 (0.0094)	1.033 (1.033)	59.38 (59.38)
[10/157]	0.1205 (0.1128)	0.0788 (0.0729)	1.021 (1.097)	65.62 (63.07)
[20/157]	0.1213 (0.1161)	0.0805 (0.0762)	0.970 (0.993)	65.62 (67.86)
[30/157]	0.1216 (0.1174)	0.0803 (0.0773)	0.971 (1.004)	68.75 (67.34)
[40/157]	0.1211 (0.1180)	0.0801 (0.0780)	0.951 (0.990)	71.88 (67.84)
[50/157]	0.1221 (0.1184)	0.0803 (0.0783)	1.300 (0.981)	62.50 (68.57)
[60/157]	0.1221 (0.1187)	0.0804 (0.0786)	0.869 (0.987)	78.12 (68.24)
[70/157]	0.1209 (0.1189)	0.0798 (0.0787)	0.684 (0.957)	84.38 (69.32)
[80/157]	0.1205 (0.1190)	0.0799 (0.0788)	1.100 (0.971)	59.38 (69.21)
[90/157]	0.1214 (0.1191)	0.0797 (0.0789)	0.869 (0.957)	65.62 (69.78)
[100/157]	0.1206 (0.1192)	0.0795 (0.0790)	0.892 (0.961)	68.75 (69.65)
[110/157]	0.1214 (0.1193)	0.0798 (0.0791)	0.888 (0.963)	62.50 (69.59)
[120/157]	0.1230 (0.1193)	0.0808 (0.0791)	0.937 (0.970)	71.88 (68.98)
[130/157]	0.1212 (0.1194)	0.0799 (0.0791)	0.744 (0.967)	78.12 (69.13)
[140/157]	0.1215 (0.1194)	0.0794 (0.0792)	1.419 (0.979)	62.50 (68.88)
[150/157]	0.1206 (0.1195)	0.0792 (0.0792)	0.932 (0.983)	65.62 (68.79)
[156/157]	0.1029 (0.1193)	0.0735 (0.0792)	0.756 (0.979)	62.50 (68.92)
 * Train Acc 68.920
 * Val Acc 69.900, Total time 0.74
 * Val loss 0.866, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0482 (0.0482)	0.0096 (0.0096)	1.176 (1.176)	68.75 (68.75)
[10/157]	0.1205 (0.1128)	0.0802 (0.0730)	1.056 (0.980)	65.62 (69.32)
[20/157]	0.1214 (0.1165)	0.0805 (0.0761)	0.854 (0.974)	65.62 (68.75)
[30/157]	0.1208 (0.1177)	0.0791 (0.0770)	0.672 (0.971)	78.12 (69.76)
[40/157]	0.1202 (0.1182)	0.0790 (0.0775)	1.034 (0.962)	65.62 (69.89)
[50/157]	0.1204 (0.1186)	0.0791 (0.0778)	1.301 (0.961)	68.75 (69.79)
[60/157]	0.1207 (0.1188)	0.0795 (0.0781)	0.964 (0.973)	56.25 (69.11)
[70/157]	0.1203 (0.1190)	0.0790 (0.0782)	0.947 (0.970)	71.88 (69.01)
[80/157]	0.1207 (0.1191)	0.0790 (0.0783)	1.110 (0.976)	71.88 (68.71)
[90/157]	0.1200 (0.1192)	0.0789 (0.0784)	0.999 (0.980)	56.25 (68.41)
[100/157]	0.1213 (0.1192)	0.0800 (0.0785)	0.791 (0.972)	75.00 (68.44)
[110/157]	0.1220 (0.1193)	0.0807 (0.0786)	1.001 (0.969)	71.88 (68.72)
[120/157]	0.1220 (0.1194)	0.0804 (0.0787)	0.841 (0.971)	81.25 (68.57)
[130/157]	0.1210 (0.1194)	0.0795 (0.0788)	0.768 (0.972)	81.25 (68.37)
[140/157]	0.1206 (0.1194)	0.0792 (0.0789)	1.134 (0.970)	65.62 (68.33)
[150/157]	0.1196 (0.1195)	0.0792 (0.0789)	0.896 (0.971)	71.88 (68.38)
[156/157]	0.1041 (0.1194)	0.0740 (0.0789)	1.774 (0.967)	62.50 (68.54)
 * Train Acc 68.540
 * Val Acc 70.200, Total time 0.74
 * Val loss 0.867, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0488 (0.0488)	0.0095 (0.0095)	1.011 (1.011)	71.88 (71.88)
[10/157]	0.1203 (0.1130)	0.0795 (0.0729)	0.808 (0.885)	71.88 (73.30)
[20/157]	0.1235 (0.1168)	0.0779 (0.0759)	0.691 (0.910)	78.12 (71.13)
[30/157]	0.1187 (0.1176)	0.0777 (0.0767)	0.872 (0.888)	68.75 (71.88)
[40/157]	0.1208 (0.1182)	0.0776 (0.0774)	1.227 (0.916)	62.50 (70.43)
[50/157]	0.1198 (0.1185)	0.0796 (0.0778)	1.028 (0.937)	65.62 (69.91)
[60/157]	0.1220 (0.1188)	0.0798 (0.0782)	1.059 (0.948)	62.50 (69.57)
[70/157]	0.1202 (0.1189)	0.0790 (0.0784)	0.802 (0.949)	75.00 (69.63)
[80/157]	0.1207 (0.1190)	0.0800 (0.0785)	0.905 (0.959)	75.00 (68.94)
[90/157]	0.1214 (0.1191)	0.0798 (0.0786)	0.773 (0.955)	81.25 (68.92)
[100/157]	0.1205 (0.1192)	0.0792 (0.0787)	0.879 (0.957)	71.88 (68.81)
[110/157]	0.1219 (0.1193)	0.0802 (0.0788)	1.101 (0.958)	68.75 (68.81)
[120/157]	0.1225 (0.1194)	0.0799 (0.0788)	1.049 (0.959)	71.88 (68.67)
[130/157]	0.1198 (0.1194)	0.0784 (0.0788)	1.089 (0.962)	62.50 (68.44)
[140/157]	0.1212 (0.1195)	0.0794 (0.0789)	1.017 (0.959)	56.25 (68.48)
[150/157]	0.1207 (0.1195)	0.0789 (0.0789)	1.169 (0.955)	59.38 (68.77)
[156/157]	0.1018 (0.1194)	0.0730 (0.0789)	1.037 (0.954)	62.50 (68.90)
 * Train Acc 68.900
 * Val Acc 70.000, Total time 0.74
 * Val loss 0.854, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0490 (0.0490)	0.0093 (0.0093)	0.803 (0.803)	75.00 (75.00)
[10/157]	0.1211 (0.1130)	0.0802 (0.0732)	0.827 (1.011)	71.88 (65.91)
[20/157]	0.1220 (0.1165)	0.0796 (0.0762)	1.041 (0.988)	62.50 (67.71)
[30/157]	0.1221 (0.1176)	0.0809 (0.0772)	0.873 (0.971)	75.00 (69.76)
[40/157]	0.1212 (0.1182)	0.0792 (0.0776)	0.739 (0.964)	81.25 (69.44)
[50/157]	0.1210 (0.1186)	0.0799 (0.0779)	0.685 (0.953)	84.38 (70.47)
[60/157]	0.1207 (0.1190)	0.0797 (0.0782)	1.082 (0.953)	59.38 (70.03)
[70/157]	0.1214 (0.1192)	0.0792 (0.0783)	1.130 (0.945)	71.88 (70.42)
[80/157]	0.1214 (0.1193)	0.0799 (0.0785)	1.177 (0.944)	59.38 (70.29)
[90/157]	0.1210 (0.1194)	0.0794 (0.0786)	1.110 (0.946)	71.88 (70.16)
[100/157]	0.1202 (0.1194)	0.0794 (0.0787)	0.765 (0.949)	75.00 (69.89)
[110/157]	0.1216 (0.1195)	0.0796 (0.0788)	0.969 (0.943)	62.50 (69.99)
[120/157]	0.1193 (0.1195)	0.0793 (0.0788)	0.876 (0.945)	68.75 (69.63)
[130/157]	0.1215 (0.1196)	0.0796 (0.0789)	0.920 (0.941)	62.50 (69.63)
[140/157]	0.1208 (0.1196)	0.0792 (0.0789)	1.052 (0.945)	62.50 (69.48)
[150/157]	0.1199 (0.1196)	0.0784 (0.0789)	0.648 (0.945)	75.00 (69.52)
[156/157]	0.1022 (0.1195)	0.0719 (0.0789)	0.933 (0.945)	62.50 (69.48)
 * Train Acc 69.480
 * Val Acc 70.000, Total time 0.73
 * Val loss 0.839, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0092 (0.0092)	0.958 (0.958)	65.62 (65.62)
[10/157]	0.1040 (0.0985)	0.0639 (0.0593)	1.155 (0.952)	62.50 (72.16)
[20/157]	0.1199 (0.1078)	0.0793 (0.0680)	1.105 (0.981)	50.00 (67.71)
[30/157]	0.1226 (0.1120)	0.0810 (0.0719)	0.871 (0.992)	71.88 (67.94)
[40/157]	0.1208 (0.1141)	0.0798 (0.0738)	1.067 (0.992)	65.62 (67.91)
[50/157]	0.1217 (0.1153)	0.0802 (0.0750)	0.732 (0.979)	71.88 (68.75)
[60/157]	0.1208 (0.1161)	0.0798 (0.0758)	0.607 (0.964)	87.50 (68.95)
[70/157]	0.1222 (0.1168)	0.0804 (0.0764)	0.968 (0.954)	75.00 (68.84)
[80/157]	0.1211 (0.1172)	0.0795 (0.0768)	0.815 (0.950)	75.00 (69.33)
[90/157]	0.1212 (0.1176)	0.0804 (0.0771)	1.086 (0.944)	68.75 (69.61)
[100/157]	0.1215 (0.1178)	0.0804 (0.0774)	1.235 (0.953)	65.62 (69.34)
[110/157]	0.1209 (0.1181)	0.0799 (0.0777)	0.834 (0.949)	84.38 (69.65)
[120/157]	0.1209 (0.1182)	0.0798 (0.0779)	1.085 (0.943)	56.25 (69.83)
[130/157]	0.1214 (0.1184)	0.0799 (0.0780)	1.044 (0.947)	62.50 (69.54)
[140/157]	0.1214 (0.1185)	0.0799 (0.0782)	0.817 (0.946)	68.75 (69.66)
[150/157]	0.1201 (0.1187)	0.0796 (0.0783)	0.986 (0.947)	65.62 (69.52)
[156/157]	0.1016 (0.1186)	0.0735 (0.0783)	1.183 (0.946)	75.00 (69.62)
 * Train Acc 69.620
 * Val Acc 70.300, Total time 0.78
 * Val loss 0.834, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0466 (0.0466)	0.0092 (0.0092)	0.741 (0.741)	71.88 (71.88)
[10/157]	0.1165 (0.1093)	0.0756 (0.0692)	1.097 (1.008)	62.50 (67.90)
[20/157]	0.1170 (0.1127)	0.0759 (0.0725)	0.830 (0.981)	65.62 (68.45)
[30/157]	0.1155 (0.1138)	0.0745 (0.0735)	0.954 (0.962)	68.75 (69.56)
[40/157]	0.1167 (0.1145)	0.0745 (0.0742)	1.081 (0.960)	68.75 (69.51)
[50/157]	0.1148 (0.1148)	0.0735 (0.0746)	1.027 (0.957)	62.50 (69.73)
[60/157]	0.1166 (0.1151)	0.0758 (0.0749)	1.010 (0.964)	71.88 (69.16)
[70/157]	0.1170 (0.1152)	0.0766 (0.0751)	1.548 (0.981)	56.25 (68.31)
[80/157]	0.1165 (0.1154)	0.0761 (0.0753)	0.763 (0.970)	78.12 (68.63)
[90/157]	0.1176 (0.1155)	0.0763 (0.0754)	1.241 (0.978)	50.00 (68.17)
[100/157]	0.1153 (0.1156)	0.0742 (0.0754)	1.219 (0.980)	68.75 (68.25)
[110/157]	0.1185 (0.1157)	0.0774 (0.0755)	0.679 (0.967)	75.00 (68.50)
[120/157]	0.1170 (0.1158)	0.0758 (0.0756)	1.110 (0.958)	65.62 (68.78)
[130/157]	0.1169 (0.1158)	0.0754 (0.0756)	0.870 (0.962)	71.88 (68.70)
[140/157]	0.1168 (0.1159)	0.0757 (0.0756)	1.258 (0.961)	68.75 (68.82)
[150/157]	0.1162 (0.1159)	0.0750 (0.0756)	0.860 (0.961)	75.00 (68.71)
[156/157]	0.0994 (0.1158)	0.0716 (0.0756)	0.777 (0.955)	87.50 (68.92)
 * Train Acc 68.920
 * Val Acc 69.800, Total time 0.66
 * Val loss 0.852, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0093 (0.0093)	0.928 (0.928)	65.62 (65.62)
[10/157]	0.1167 (0.1094)	0.0753 (0.0700)	0.925 (0.944)	68.75 (69.89)
[20/157]	0.1169 (0.1128)	0.0757 (0.0728)	1.186 (0.961)	65.62 (68.60)
[30/157]	0.1177 (0.1139)	0.0768 (0.0737)	0.717 (0.914)	81.25 (70.67)
[40/157]	0.1171 (0.1145)	0.0758 (0.0742)	0.950 (0.911)	62.50 (70.43)
[50/157]	0.1134 (0.1148)	0.0710 (0.0744)	1.359 (0.913)	53.12 (70.40)
[60/157]	0.1171 (0.1150)	0.0761 (0.0747)	0.764 (0.910)	75.00 (70.59)
[70/157]	0.1158 (0.1151)	0.0751 (0.0748)	0.809 (0.917)	78.12 (70.82)
[80/157]	0.1170 (0.1152)	0.0762 (0.0750)	0.944 (0.916)	75.00 (71.10)
[90/157]	0.1174 (0.1153)	0.0764 (0.0751)	1.086 (0.936)	59.38 (70.60)
[100/157]	0.1153 (0.1154)	0.0745 (0.0752)	0.908 (0.943)	78.12 (70.39)
[110/157]	0.1154 (0.1154)	0.0743 (0.0752)	0.858 (0.939)	65.62 (70.41)
[120/157]	0.1164 (0.1155)	0.0756 (0.0753)	0.807 (0.937)	71.88 (70.33)
[130/157]	0.1185 (0.1156)	0.0763 (0.0754)	0.847 (0.944)	75.00 (70.09)
[140/157]	0.1172 (0.1156)	0.0756 (0.0754)	1.038 (0.943)	65.62 (70.21)
[150/157]	0.1167 (0.1157)	0.0754 (0.0754)	0.733 (0.935)	84.38 (70.34)
[156/157]	0.0999 (0.1156)	0.0706 (0.0754)	1.422 (0.939)	62.50 (70.24)
 * Train Acc 70.240
 * Val Acc 70.700, Total time 0.68
 * Val loss 0.829, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0467 (0.0467)	0.0093 (0.0093)	0.881 (0.881)	71.88 (71.88)
[10/157]	0.1158 (0.1088)	0.0755 (0.0692)	0.917 (0.931)	75.00 (70.45)
[20/157]	0.1166 (0.1122)	0.0749 (0.0725)	0.891 (0.925)	68.75 (69.49)
[30/157]	0.1166 (0.1135)	0.0748 (0.0736)	0.968 (0.927)	71.88 (70.16)
[40/157]	0.1155 (0.1142)	0.0744 (0.0742)	0.913 (0.959)	75.00 (69.21)
[50/157]	0.1178 (0.1145)	0.0769 (0.0745)	0.801 (0.958)	78.12 (69.42)
[60/157]	0.1187 (0.1147)	0.0774 (0.0747)	0.789 (0.949)	81.25 (69.93)
[70/157]	0.1181 (0.1150)	0.0755 (0.0749)	0.816 (0.940)	65.62 (70.11)
[80/157]	0.1179 (0.1151)	0.0771 (0.0750)	0.878 (0.932)	68.75 (70.41)
[90/157]	0.1166 (0.1152)	0.0750 (0.0751)	0.780 (0.926)	84.38 (70.60)
[100/157]	0.1158 (0.1153)	0.0750 (0.0752)	0.868 (0.929)	75.00 (70.30)
[110/157]	0.1174 (0.1154)	0.0766 (0.0753)	1.382 (0.935)	53.12 (70.05)
[120/157]	0.1201 (0.1155)	0.0781 (0.0754)	0.967 (0.934)	59.38 (70.20)
[130/157]	0.1169 (0.1155)	0.0763 (0.0755)	1.019 (0.932)	53.12 (70.16)
[140/157]	0.1174 (0.1156)	0.0752 (0.0755)	0.711 (0.923)	78.12 (70.32)
[150/157]	0.1174 (0.1157)	0.0762 (0.0755)	0.600 (0.922)	81.25 (70.24)
[156/157]	0.0993 (0.1156)	0.0706 (0.0755)	0.627 (0.925)	87.50 (70.36)
 * Train Acc 70.360
 * Val Acc 71.100, Total time 0.66
 * Val loss 0.832, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0460 (0.0460)	0.0090 (0.0090)	1.081 (1.081)	62.50 (62.50)
[10/157]	0.1179 (0.1089)	0.0772 (0.0695)	0.632 (0.890)	87.50 (73.86)
[20/157]	0.1172 (0.1126)	0.0760 (0.0729)	0.792 (0.913)	78.12 (71.58)
[30/157]	0.1179 (0.1140)	0.0756 (0.0740)	0.734 (0.910)	81.25 (71.37)
[40/157]	0.1176 (0.1145)	0.0758 (0.0745)	1.025 (0.938)	65.62 (69.66)
[50/157]	0.1163 (0.1148)	0.0758 (0.0748)	0.835 (0.918)	78.12 (70.83)
[60/157]	0.1157 (0.1151)	0.0742 (0.0750)	0.950 (0.912)	68.75 (70.85)
[70/157]	0.1196 (0.1154)	0.0767 (0.0752)	0.705 (0.919)	65.62 (70.73)
[80/157]	0.1180 (0.1155)	0.0751 (0.0752)	0.977 (0.920)	59.38 (70.64)
[90/157]	0.1181 (0.1156)	0.0758 (0.0753)	0.778 (0.916)	75.00 (70.91)
[100/157]	0.1153 (0.1156)	0.0747 (0.0753)	0.759 (0.917)	75.00 (70.92)
[110/157]	0.1178 (0.1157)	0.0767 (0.0754)	0.928 (0.916)	65.62 (70.78)
[120/157]	0.1169 (0.1158)	0.0758 (0.0755)	0.974 (0.915)	65.62 (70.84)
[130/157]	0.1203 (0.1158)	0.0768 (0.0755)	0.934 (0.916)	71.88 (70.80)
[140/157]	0.1164 (0.1158)	0.0750 (0.0755)	0.877 (0.915)	68.75 (70.79)
[150/157]	0.1171 (0.1159)	0.0752 (0.0755)	1.097 (0.917)	65.62 (70.86)
[156/157]	0.0982 (0.1158)	0.0699 (0.0755)	1.265 (0.919)	50.00 (70.76)
 * Train Acc 70.760
 * Val Acc 70.400, Total time 0.66
 * Val loss 0.840, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0464 (0.0464)	0.0091 (0.0091)	0.764 (0.764)	87.50 (87.50)
[10/157]	0.1159 (0.1089)	0.0754 (0.0693)	0.807 (0.842)	68.75 (73.86)
[20/157]	0.1153 (0.1122)	0.0747 (0.0724)	0.784 (0.899)	71.88 (70.83)
[30/157]	0.1182 (0.1134)	0.0774 (0.0736)	0.722 (0.884)	90.62 (72.28)
[40/157]	0.1162 (0.1141)	0.0753 (0.0742)	0.880 (0.908)	62.50 (71.65)
[50/157]	0.1174 (0.1145)	0.0760 (0.0746)	0.817 (0.924)	71.88 (70.71)
[60/157]	0.1177 (0.1149)	0.0749 (0.0748)	0.925 (0.923)	75.00 (70.85)
[70/157]	0.1187 (0.1151)	0.0764 (0.0750)	1.113 (0.924)	53.12 (70.82)
[80/157]	0.1205 (0.1154)	0.0764 (0.0751)	0.882 (0.923)	68.75 (71.06)
[90/157]	0.1193 (0.1155)	0.0766 (0.0753)	0.780 (0.925)	75.00 (70.98)
[100/157]	0.1149 (0.1155)	0.0746 (0.0753)	0.889 (0.934)	68.75 (70.51)
[110/157]	0.1184 (0.1156)	0.0762 (0.0754)	0.614 (0.930)	81.25 (70.64)
[120/157]	0.1177 (0.1157)	0.0763 (0.0754)	1.120 (0.925)	59.38 (70.74)
[130/157]	0.1181 (0.1157)	0.0761 (0.0754)	0.924 (0.923)	62.50 (70.54)
[140/157]	0.1151 (0.1157)	0.0736 (0.0754)	1.148 (0.923)	59.38 (70.48)
[150/157]	0.1176 (0.1158)	0.0761 (0.0755)	1.230 (0.926)	62.50 (70.38)
[156/157]	0.0980 (0.1157)	0.0688 (0.0754)	2.148 (0.926)	50.00 (70.28)
 * Train Acc 70.280
 * Val Acc 71.800, Total time 0.67
 * Val loss 0.821, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0468 (0.0468)	0.0093 (0.0093)	0.832 (0.832)	78.12 (78.12)
[10/157]	0.1164 (0.1090)	0.0751 (0.0694)	0.861 (0.921)	75.00 (71.88)
[20/157]	0.1168 (0.1126)	0.0757 (0.0727)	0.746 (0.937)	78.12 (70.83)
[30/157]	0.1207 (0.1139)	0.0774 (0.0740)	0.671 (0.942)	81.25 (70.26)
[40/157]	0.1182 (0.1145)	0.0762 (0.0744)	0.626 (0.912)	84.38 (71.04)
[50/157]	0.1162 (0.1147)	0.0750 (0.0746)	1.005 (0.912)	75.00 (70.96)
[60/157]	0.1171 (0.1150)	0.0750 (0.0749)	0.811 (0.906)	71.88 (71.11)
[70/157]	0.1180 (0.1151)	0.0762 (0.0750)	0.779 (0.896)	78.12 (71.43)
[80/157]	0.1167 (0.1152)	0.0755 (0.0751)	1.200 (0.901)	71.88 (71.06)
[90/157]	0.1182 (0.1154)	0.0765 (0.0752)	1.043 (0.908)	68.75 (70.81)
[100/157]	0.1180 (0.1155)	0.0757 (0.0753)	0.884 (0.904)	75.00 (71.26)
[110/157]	0.1164 (0.1155)	0.0751 (0.0753)	0.875 (0.901)	65.62 (70.97)
[120/157]	0.1199 (0.1156)	0.0776 (0.0753)	0.811 (0.901)	71.88 (70.97)
[130/157]	0.1181 (0.1156)	0.0775 (0.0753)	1.083 (0.908)	65.62 (70.87)
[140/157]	0.1195 (0.1156)	0.0777 (0.0754)	1.170 (0.912)	65.62 (70.61)
[150/157]	0.1163 (0.1157)	0.0752 (0.0754)	1.055 (0.919)	59.38 (70.36)
[156/157]	0.0982 (0.1155)	0.0707 (0.0754)	0.561 (0.922)	87.50 (70.20)
 * Train Acc 70.200
 * Val Acc 71.000, Total time 0.66
 * Val loss 0.826, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0090 (0.0090)	0.920 (0.920)	68.75 (68.75)
[10/157]	0.1164 (0.1087)	0.0755 (0.0694)	1.036 (0.881)	53.12 (67.05)
[20/157]	0.1176 (0.1122)	0.0762 (0.0725)	1.000 (0.899)	65.62 (69.35)
[30/157]	0.1154 (0.1133)	0.0749 (0.0734)	1.112 (0.919)	65.62 (69.35)
[40/157]	0.1182 (0.1139)	0.0772 (0.0740)	0.939 (0.944)	68.75 (68.45)
[50/157]	0.1186 (0.1143)	0.0772 (0.0744)	0.570 (0.926)	87.50 (69.18)
[60/157]	0.1159 (0.1146)	0.0752 (0.0746)	0.885 (0.931)	68.75 (69.06)
[70/157]	0.1173 (0.1148)	0.0764 (0.0747)	0.592 (0.918)	84.38 (69.50)
[80/157]	0.1154 (0.1149)	0.0746 (0.0748)	1.033 (0.919)	75.00 (69.87)
[90/157]	0.1129 (0.1150)	0.0721 (0.0748)	0.798 (0.920)	75.00 (70.16)
[100/157]	0.1178 (0.1150)	0.0762 (0.0749)	0.883 (0.922)	71.88 (70.02)
[110/157]	0.1163 (0.1151)	0.0754 (0.0750)	0.721 (0.924)	75.00 (69.90)
[120/157]	0.1173 (0.1151)	0.0761 (0.0750)	0.822 (0.919)	81.25 (70.07)
[130/157]	0.1185 (0.1152)	0.0770 (0.0751)	0.888 (0.924)	62.50 (69.80)
[140/157]	0.1159 (0.1152)	0.0754 (0.0751)	1.077 (0.926)	65.62 (69.90)
[150/157]	0.1165 (0.1153)	0.0757 (0.0752)	1.073 (0.926)	62.50 (69.99)
[156/157]	0.0978 (0.1151)	0.0702 (0.0752)	0.717 (0.925)	75.00 (70.08)
 * Train Acc 70.080
 * Val Acc 71.300, Total time 0.66
 * Val loss 0.826, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0091 (0.0091)	1.104 (1.104)	56.25 (56.25)
[10/157]	0.1160 (0.1080)	0.0752 (0.0688)	1.040 (0.910)	68.75 (69.89)
[20/157]	0.1153 (0.1116)	0.0743 (0.0722)	0.834 (0.930)	78.12 (70.09)
[30/157]	0.1163 (0.1131)	0.0748 (0.0733)	0.826 (0.921)	68.75 (69.86)
[40/157]	0.1152 (0.1136)	0.0742 (0.0738)	0.984 (0.897)	78.12 (70.35)
[50/157]	0.1156 (0.1139)	0.0747 (0.0741)	1.200 (0.891)	65.62 (70.65)
[60/157]	0.1164 (0.1142)	0.0750 (0.0743)	0.742 (0.876)	75.00 (71.52)
[70/157]	0.1158 (0.1144)	0.0745 (0.0745)	0.647 (0.883)	78.12 (71.57)
[80/157]	0.1172 (0.1146)	0.0755 (0.0747)	1.214 (0.896)	65.62 (71.49)
[90/157]	0.1155 (0.1148)	0.0749 (0.0748)	1.102 (0.896)	65.62 (71.63)
[100/157]	0.1170 (0.1149)	0.0745 (0.0749)	0.781 (0.898)	81.25 (71.50)
[110/157]	0.1165 (0.1150)	0.0751 (0.0749)	0.797 (0.894)	68.75 (71.59)
[120/157]	0.1180 (0.1151)	0.0758 (0.0750)	1.088 (0.902)	53.12 (71.33)
[130/157]	0.1171 (0.1152)	0.0755 (0.0751)	0.737 (0.907)	78.12 (71.18)
[140/157]	0.1159 (0.1152)	0.0751 (0.0751)	1.058 (0.911)	68.75 (71.01)
[150/157]	0.1130 (0.1153)	0.0721 (0.0752)	1.121 (0.914)	56.25 (70.76)
[156/157]	0.0984 (0.1152)	0.0704 (0.0752)	1.112 (0.916)	62.50 (70.62)
 * Train Acc 70.620
 * Val Acc 70.900, Total time 0.66
 * Val loss 0.819, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0471 (0.0471)	0.0095 (0.0095)	0.843 (0.843)	62.50 (62.50)
[10/157]	0.1155 (0.1083)	0.0751 (0.0689)	0.957 (0.905)	62.50 (68.47)
[20/157]	0.1167 (0.1118)	0.0750 (0.0722)	0.848 (0.899)	75.00 (70.54)
[30/157]	0.1138 (0.1130)	0.0726 (0.0732)	0.861 (0.922)	78.12 (70.06)
[40/157]	0.1165 (0.1137)	0.0741 (0.0736)	1.240 (0.921)	65.62 (70.27)
[50/157]	0.1178 (0.1141)	0.0768 (0.0739)	1.128 (0.907)	62.50 (71.08)
[60/157]	0.1166 (0.1144)	0.0756 (0.0742)	0.959 (0.905)	71.88 (71.47)
[70/157]	0.1155 (0.1145)	0.0750 (0.0744)	1.031 (0.903)	68.75 (71.61)
[80/157]	0.1148 (0.1146)	0.0745 (0.0745)	0.873 (0.907)	71.88 (71.18)
[90/157]	0.1184 (0.1147)	0.0768 (0.0746)	0.867 (0.900)	65.62 (71.29)
[100/157]	0.1156 (0.1147)	0.0753 (0.0747)	0.993 (0.893)	71.88 (71.38)
[110/157]	0.1154 (0.1148)	0.0750 (0.0748)	0.920 (0.891)	75.00 (71.20)
[120/157]	0.1141 (0.1148)	0.0737 (0.0748)	0.847 (0.891)	68.75 (71.15)
[130/157]	0.1131 (0.1148)	0.0733 (0.0749)	0.751 (0.898)	75.00 (70.90)
[140/157]	0.1178 (0.1149)	0.0772 (0.0750)	0.880 (0.907)	71.88 (70.77)
[150/157]	0.1168 (0.1149)	0.0760 (0.0750)	1.351 (0.909)	56.25 (70.70)
[156/157]	0.0974 (0.1148)	0.0705 (0.0750)	1.118 (0.912)	50.00 (70.58)
 * Train Acc 70.580
 * Val Acc 71.200, Total time 0.60
 * Val loss 0.830, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0088 (0.0088)	0.873 (0.873)	71.88 (71.88)
[10/157]	0.0963 (0.0924)	0.0582 (0.0551)	0.754 (0.839)	78.12 (72.73)
[20/157]	0.1215 (0.1019)	0.0804 (0.0638)	0.858 (0.851)	68.75 (73.21)
[30/157]	0.1202 (0.1077)	0.0799 (0.0690)	0.884 (0.885)	68.75 (71.88)
[40/157]	0.1207 (0.1106)	0.0797 (0.0717)	0.896 (0.897)	71.88 (71.57)
[50/157]	0.1210 (0.1126)	0.0801 (0.0734)	0.565 (0.883)	87.50 (72.06)
[60/157]	0.1204 (0.1138)	0.0803 (0.0745)	1.063 (0.880)	62.50 (71.52)
[70/157]	0.1210 (0.1146)	0.0803 (0.0753)	0.650 (0.872)	81.25 (71.92)
[80/157]	0.1055 (0.1146)	0.0650 (0.0753)	0.786 (0.872)	81.25 (71.95)
[90/157]	0.1065 (0.1136)	0.0657 (0.0743)	0.847 (0.889)	81.25 (71.57)
[100/157]	0.1055 (0.1129)	0.0647 (0.0735)	1.261 (0.890)	62.50 (71.57)
[110/157]	0.1045 (0.1122)	0.0642 (0.0728)	0.845 (0.886)	81.25 (71.76)
[120/157]	0.1035 (0.1117)	0.0636 (0.0723)	0.815 (0.895)	71.88 (71.41)
[130/157]	0.1055 (0.1112)	0.0655 (0.0718)	1.126 (0.898)	50.00 (71.06)
[140/157]	0.1059 (0.1109)	0.0662 (0.0714)	1.024 (0.906)	68.75 (70.57)
[150/157]	0.1057 (0.1105)	0.0656 (0.0711)	0.782 (0.912)	78.12 (70.41)
[156/157]	0.0891 (0.1102)	0.0626 (0.0709)	0.920 (0.911)	62.50 (70.32)
 * Train Acc 70.320
 * Val Acc 71.300, Total time 0.62
 * Val loss 0.821, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0089 (0.0089)	0.708 (0.708)	71.88 (71.88)
[10/157]	0.1054 (0.0974)	0.0657 (0.0591)	0.863 (0.884)	75.00 (71.59)
[20/157]	0.1041 (0.1014)	0.0646 (0.0628)	0.892 (0.895)	71.88 (70.68)
[30/157]	0.1057 (0.1029)	0.0661 (0.0641)	0.719 (0.874)	71.88 (71.47)
[40/157]	0.1211 (0.1071)	0.0803 (0.0680)	0.778 (0.873)	78.12 (72.64)
[50/157]	0.1210 (0.1096)	0.0803 (0.0703)	0.955 (0.873)	78.12 (72.92)
[60/157]	0.1213 (0.1114)	0.0803 (0.0719)	0.871 (0.888)	71.88 (72.44)
[70/157]	0.1205 (0.1126)	0.0801 (0.0730)	0.785 (0.892)	68.75 (71.96)
[80/157]	0.1212 (0.1135)	0.0800 (0.0739)	0.945 (0.891)	75.00 (71.49)
[90/157]	0.1213 (0.1142)	0.0803 (0.0745)	1.243 (0.888)	62.50 (71.74)
[100/157]	0.1207 (0.1148)	0.0802 (0.0751)	1.147 (0.901)	59.38 (71.10)
[110/157]	0.1177 (0.1151)	0.0771 (0.0753)	0.661 (0.904)	81.25 (71.03)
[120/157]	0.1177 (0.1152)	0.0771 (0.0755)	0.667 (0.906)	84.38 (71.05)
[130/157]	0.1181 (0.1154)	0.0771 (0.0756)	0.765 (0.907)	71.88 (71.18)
[140/157]	0.1009 (0.1153)	0.0608 (0.0756)	1.271 (0.907)	62.50 (71.19)
[150/157]	0.0970 (0.1143)	0.0578 (0.0745)	0.664 (0.910)	81.25 (71.01)
[156/157]	0.0778 (0.1134)	0.0523 (0.0739)	1.032 (0.905)	62.50 (71.28)
 * Train Acc 71.280
 * Val Acc 71.300, Total time 0.61
 * Val loss 0.814, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0454 (0.0454)	0.0090 (0.0090)	0.934 (0.934)	75.00 (75.00)
[10/157]	0.1113 (0.1045)	0.0708 (0.0652)	0.577 (0.848)	90.62 (75.28)
[20/157]	0.1121 (0.1080)	0.0705 (0.0684)	0.821 (0.865)	71.88 (72.62)
[30/157]	0.1110 (0.1090)	0.0709 (0.0693)	0.710 (0.875)	78.12 (72.38)
[40/157]	0.1118 (0.1096)	0.0713 (0.0699)	0.561 (0.881)	81.25 (72.03)
[50/157]	0.1123 (0.1099)	0.0711 (0.0701)	0.844 (0.867)	68.75 (71.69)
[60/157]	0.1127 (0.1102)	0.0707 (0.0704)	0.915 (0.872)	59.38 (71.31)
[70/157]	0.1105 (0.1103)	0.0694 (0.0705)	1.162 (0.887)	71.88 (71.21)
[80/157]	0.1117 (0.1104)	0.0696 (0.0706)	0.579 (0.875)	87.50 (71.49)
[90/157]	0.1123 (0.1105)	0.0716 (0.0706)	1.241 (0.875)	56.25 (71.46)
[100/157]	0.1102 (0.1106)	0.0701 (0.0707)	0.881 (0.881)	71.88 (71.47)
[110/157]	0.1109 (0.1106)	0.0701 (0.0707)	0.961 (0.882)	75.00 (71.73)
[120/157]	0.1117 (0.1107)	0.0703 (0.0707)	0.878 (0.883)	75.00 (71.75)
[130/157]	0.1123 (0.1107)	0.0715 (0.0707)	0.718 (0.878)	78.12 (71.92)
[140/157]	0.1104 (0.1107)	0.0707 (0.0708)	0.787 (0.880)	75.00 (71.99)
[150/157]	0.1115 (0.1107)	0.0707 (0.0708)	0.878 (0.876)	75.00 (72.16)
[156/157]	0.0933 (0.1106)	0.0664 (0.0708)	0.811 (0.880)	62.50 (71.82)
 * Train Acc 71.820
 * Val Acc 71.200, Total time 0.58
 * Val loss 0.823, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0089 (0.0089)	0.932 (0.932)	65.62 (65.62)
[10/157]	0.1121 (0.0991)	0.0721 (0.0607)	0.671 (0.934)	84.38 (69.89)
[20/157]	0.1135 (0.1058)	0.0725 (0.0667)	1.226 (0.954)	62.50 (69.35)
[30/157]	0.1148 (0.1083)	0.0739 (0.0690)	0.650 (0.914)	78.12 (69.56)
[40/157]	0.0979 (0.1085)	0.0583 (0.0691)	0.892 (0.899)	68.75 (69.97)
[50/157]	0.0967 (0.1065)	0.0577 (0.0673)	0.887 (0.891)	62.50 (70.47)
[60/157]	0.1220 (0.1061)	0.0798 (0.0671)	0.775 (0.907)	68.75 (69.88)
[70/157]	0.1210 (0.1081)	0.0801 (0.0689)	0.818 (0.911)	75.00 (69.85)
[80/157]	0.1177 (0.1092)	0.0767 (0.0699)	0.920 (0.916)	62.50 (69.56)
[90/157]	0.1011 (0.1082)	0.0615 (0.0690)	0.730 (0.920)	81.25 (69.44)
[100/157]	0.0945 (0.1073)	0.0567 (0.0680)	1.284 (0.924)	59.38 (69.18)
[110/157]	0.1214 (0.1082)	0.0800 (0.0689)	0.707 (0.926)	78.12 (69.28)
[120/157]	0.1089 (0.1084)	0.0686 (0.0690)	0.730 (0.923)	75.00 (69.55)
[130/157]	0.1094 (0.1084)	0.0688 (0.0690)	0.735 (0.915)	78.12 (69.99)
[140/157]	0.1092 (0.1083)	0.0689 (0.0690)	0.831 (0.919)	68.75 (69.88)
[150/157]	0.1093 (0.1084)	0.0685 (0.0690)	1.109 (0.914)	68.75 (70.26)
[156/157]	0.0905 (0.1082)	0.0641 (0.0690)	1.149 (0.915)	50.00 (70.32)
 * Train Acc 70.320
 * Val Acc 72.400, Total time 0.63
 * Val loss 0.808, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0091 (0.0091)	0.547 (0.547)	90.62 (90.62)
[10/157]	0.1088 (0.1017)	0.0685 (0.0630)	1.344 (0.852)	56.25 (72.73)
[20/157]	0.1079 (0.1048)	0.0676 (0.0659)	0.982 (0.884)	68.75 (71.28)
[30/157]	0.1089 (0.1059)	0.0682 (0.0669)	0.773 (0.920)	71.88 (70.26)
[40/157]	0.1085 (0.1064)	0.0676 (0.0673)	0.767 (0.896)	78.12 (71.27)
[50/157]	0.1095 (0.1068)	0.0680 (0.0675)	0.784 (0.883)	78.12 (71.75)
[60/157]	0.1093 (0.1070)	0.0697 (0.0678)	1.370 (0.896)	65.62 (71.82)
[70/157]	0.1062 (0.1072)	0.0663 (0.0679)	0.901 (0.901)	65.62 (71.65)
[80/157]	0.1100 (0.1073)	0.0699 (0.0680)	1.195 (0.910)	65.62 (71.37)
[90/157]	0.1097 (0.1075)	0.0689 (0.0682)	0.860 (0.903)	65.62 (71.63)
[100/157]	0.1088 (0.1075)	0.0647 (0.0681)	0.803 (0.910)	71.88 (70.98)
[110/157]	0.1098 (0.1076)	0.0681 (0.0682)	0.994 (0.912)	71.88 (71.09)
[120/157]	0.1104 (0.1077)	0.0688 (0.0682)	0.695 (0.917)	75.00 (70.89)
[130/157]	0.1108 (0.1077)	0.0698 (0.0683)	1.200 (0.919)	56.25 (70.83)
[140/157]	0.1052 (0.1077)	0.0652 (0.0683)	0.783 (0.915)	71.88 (70.92)
[150/157]	0.1079 (0.1078)	0.0677 (0.0683)	0.717 (0.910)	68.75 (70.86)
[156/157]	0.0899 (0.1076)	0.0636 (0.0683)	1.110 (0.913)	75.00 (70.68)
 * Train Acc 70.680
 * Val Acc 71.600, Total time 0.58
 * Val loss 0.816, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0086 (0.0086)	0.697 (0.697)	75.00 (75.00)
[10/157]	0.1005 (0.0939)	0.0600 (0.0553)	0.995 (0.857)	68.75 (73.58)
[20/157]	0.1158 (0.0965)	0.0754 (0.0582)	0.996 (0.874)	71.88 (73.21)
[30/157]	0.1207 (0.1044)	0.0800 (0.0652)	0.881 (0.878)	68.75 (72.88)
[40/157]	0.1057 (0.1064)	0.0660 (0.0672)	0.861 (0.880)	71.88 (72.87)
[50/157]	0.1085 (0.1065)	0.0674 (0.0671)	1.055 (0.894)	62.50 (71.94)
[60/157]	0.1075 (0.1065)	0.0674 (0.0671)	0.984 (0.901)	65.62 (71.62)
[70/157]	0.1078 (0.1065)	0.0672 (0.0671)	1.296 (0.906)	59.38 (71.52)
[80/157]	0.1070 (0.1064)	0.0674 (0.0670)	0.772 (0.905)	78.12 (71.41)
[90/157]	0.1069 (0.1064)	0.0669 (0.0671)	0.907 (0.897)	71.88 (71.57)
[100/157]	0.1078 (0.1064)	0.0671 (0.0671)	1.070 (0.896)	59.38 (71.41)
[110/157]	0.1081 (0.1064)	0.0681 (0.0671)	0.933 (0.899)	75.00 (71.26)
[120/157]	0.1067 (0.1064)	0.0671 (0.0670)	1.085 (0.901)	62.50 (71.23)
[130/157]	0.1067 (0.1064)	0.0674 (0.0671)	0.663 (0.904)	84.38 (71.25)
[140/157]	0.1072 (0.1063)	0.0676 (0.0671)	0.800 (0.898)	75.00 (71.43)
[150/157]	0.1077 (0.1063)	0.0688 (0.0671)	0.672 (0.897)	84.38 (71.54)
[156/157]	0.0875 (0.1062)	0.0609 (0.0671)	1.017 (0.898)	50.00 (71.48)
 * Train Acc 71.480
 * Val Acc 72.100, Total time 0.62
 * Val loss 0.804, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0093 (0.0093)	0.756 (0.756)	75.00 (75.00)
[10/157]	0.1059 (0.1000)	0.0655 (0.0613)	0.881 (0.833)	65.62 (70.45)
[20/157]	0.1053 (0.1029)	0.0650 (0.0640)	0.857 (0.837)	71.88 (71.88)
[30/157]	0.1065 (0.1040)	0.0663 (0.0651)	1.562 (0.877)	53.12 (71.17)
[40/157]	0.1070 (0.1046)	0.0672 (0.0657)	1.091 (0.874)	59.38 (71.27)
[50/157]	0.1069 (0.1048)	0.0667 (0.0659)	0.880 (0.870)	78.12 (71.51)
[60/157]	0.1071 (0.1050)	0.0676 (0.0661)	0.725 (0.860)	78.12 (72.03)
[70/157]	0.1072 (0.1052)	0.0677 (0.0662)	0.989 (0.871)	59.38 (71.88)
[80/157]	0.1091 (0.1053)	0.0689 (0.0664)	0.759 (0.865)	75.00 (71.88)
[90/157]	0.1070 (0.1054)	0.0677 (0.0664)	0.739 (0.876)	75.00 (71.29)
[100/157]	0.1040 (0.1054)	0.0644 (0.0665)	0.956 (0.882)	62.50 (71.04)
[110/157]	0.1060 (0.1055)	0.0656 (0.0665)	1.046 (0.880)	68.75 (71.31)
[120/157]	0.1058 (0.1056)	0.0654 (0.0666)	0.939 (0.886)	75.00 (71.18)
[130/157]	0.1070 (0.1057)	0.0666 (0.0666)	0.865 (0.888)	78.12 (71.23)
[140/157]	0.1074 (0.1057)	0.0675 (0.0667)	1.243 (0.891)	59.38 (71.19)
[150/157]	0.1070 (0.1057)	0.0678 (0.0667)	0.804 (0.894)	78.12 (71.11)
[156/157]	0.0898 (0.1056)	0.0634 (0.0667)	0.979 (0.895)	75.00 (71.08)
 * Train Acc 71.080
 * Val Acc 71.700, Total time 0.58
 * Val loss 0.818, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0090 (0.0090)	1.147 (1.147)	59.38 (59.38)
[10/157]	0.1137 (0.1059)	0.0737 (0.0673)	0.648 (0.862)	81.25 (72.44)
[20/157]	0.0993 (0.1030)	0.0605 (0.0648)	1.056 (0.884)	62.50 (72.47)
[30/157]	0.1065 (0.1029)	0.0664 (0.0645)	1.271 (0.893)	56.25 (71.88)
[40/157]	0.1082 (0.1040)	0.0686 (0.0655)	0.956 (0.888)	68.75 (71.57)
[50/157]	0.1077 (0.1047)	0.0673 (0.0659)	1.050 (0.892)	65.62 (70.77)
[60/157]	0.1072 (0.1052)	0.0669 (0.0662)	0.704 (0.899)	78.12 (71.31)
[70/157]	0.1060 (0.1054)	0.0660 (0.0665)	0.789 (0.903)	71.88 (70.86)
[80/157]	0.1021 (0.1056)	0.0640 (0.0666)	0.749 (0.896)	81.25 (71.10)
[90/157]	0.0947 (0.1045)	0.0573 (0.0657)	0.968 (0.898)	62.50 (70.98)
[100/157]	0.1190 (0.1057)	0.0783 (0.0668)	0.668 (0.902)	84.38 (71.10)
[110/157]	0.0999 (0.1054)	0.0605 (0.0664)	1.121 (0.905)	68.75 (70.95)
[120/157]	0.0996 (0.1049)	0.0604 (0.0661)	0.710 (0.905)	78.12 (70.82)
[130/157]	0.1119 (0.1049)	0.0712 (0.0660)	1.128 (0.910)	56.25 (70.61)
[140/157]	0.1109 (0.1054)	0.0707 (0.0664)	0.912 (0.906)	68.75 (70.86)
[150/157]	0.0964 (0.1053)	0.0588 (0.0664)	0.815 (0.903)	75.00 (71.09)
[156/157]	0.0797 (0.1049)	0.0544 (0.0661)	0.552 (0.901)	75.00 (71.16)
 * Train Acc 71.160
 * Val Acc 71.900, Total time 0.58
 * Val loss 0.807, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0454 (0.0454)	0.0095 (0.0095)	0.825 (0.825)	71.88 (71.88)
[10/157]	0.1007 (0.0956)	0.0611 (0.0570)	0.749 (0.911)	78.12 (70.17)
[20/157]	0.1021 (0.0984)	0.0611 (0.0596)	0.615 (0.867)	78.12 (71.88)
[30/157]	0.1014 (0.0992)	0.0619 (0.0603)	1.175 (0.860)	65.62 (71.88)
[40/157]	0.1026 (0.0998)	0.0625 (0.0610)	0.900 (0.853)	65.62 (72.18)
[50/157]	0.1039 (0.1003)	0.0630 (0.0612)	0.900 (0.873)	62.50 (71.32)
[60/157]	0.1009 (0.1006)	0.0608 (0.0615)	0.982 (0.867)	71.88 (71.67)
[70/157]	0.1181 (0.1020)	0.0757 (0.0628)	1.071 (0.870)	71.88 (71.96)
[80/157]	0.0999 (0.1027)	0.0611 (0.0635)	0.992 (0.873)	65.62 (71.76)
[90/157]	0.1010 (0.1023)	0.0598 (0.0632)	0.779 (0.872)	75.00 (71.88)
[100/157]	0.0996 (0.1020)	0.0602 (0.0630)	0.988 (0.874)	56.25 (71.57)
[110/157]	0.1000 (0.1018)	0.0606 (0.0627)	1.108 (0.875)	68.75 (71.51)
[120/157]	0.1001 (0.1017)	0.0607 (0.0626)	0.942 (0.878)	71.88 (71.31)
[130/157]	0.0973 (0.1014)	0.0588 (0.0624)	0.940 (0.889)	62.50 (70.71)
[140/157]	0.1182 (0.1027)	0.0778 (0.0636)	1.013 (0.887)	59.38 (70.90)
[150/157]	0.1075 (0.1034)	0.0673 (0.0644)	0.768 (0.887)	84.38 (71.03)
[156/157]	0.0884 (0.1034)	0.0609 (0.0644)	2.443 (0.893)	50.00 (70.94)
 * Train Acc 70.940
 * Val Acc 72.700, Total time 0.59
 * Val loss 0.800, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0462 (0.0462)	0.0095 (0.0095)	1.064 (1.064)	65.62 (65.62)
[10/157]	0.0976 (0.1032)	0.0592 (0.0636)	1.079 (0.881)	68.75 (72.73)
[20/157]	0.1025 (0.1011)	0.0620 (0.0621)	1.039 (0.907)	56.25 (69.79)
[30/157]	0.1017 (0.1019)	0.0608 (0.0628)	0.839 (0.911)	75.00 (71.47)
[40/157]	0.1044 (0.1022)	0.0647 (0.0632)	0.891 (0.901)	68.75 (71.49)
[50/157]	0.1027 (0.1025)	0.0637 (0.0635)	0.967 (0.897)	65.62 (71.81)
[60/157]	0.1025 (0.1026)	0.0633 (0.0636)	0.848 (0.901)	71.88 (71.62)
[70/157]	0.1031 (0.1028)	0.0633 (0.0637)	0.737 (0.898)	71.88 (71.52)
[80/157]	0.1052 (0.1029)	0.0642 (0.0637)	0.846 (0.892)	71.88 (71.80)
[90/157]	0.1019 (0.1029)	0.0624 (0.0637)	0.759 (0.890)	68.75 (71.81)
[100/157]	0.1035 (0.1029)	0.0648 (0.0638)	0.819 (0.895)	78.12 (71.94)
[110/157]	0.1057 (0.1030)	0.0662 (0.0640)	0.979 (0.898)	65.62 (71.68)
[120/157]	0.1051 (0.1031)	0.0662 (0.0641)	0.599 (0.893)	78.12 (71.80)
[130/157]	0.1041 (0.1032)	0.0650 (0.0641)	1.060 (0.891)	62.50 (71.68)
[140/157]	0.1049 (0.1032)	0.0650 (0.0642)	0.715 (0.893)	78.12 (71.63)
[150/157]	0.1026 (0.1032)	0.0643 (0.0642)	0.919 (0.891)	71.88 (71.69)
[156/157]	0.0868 (0.1031)	0.0601 (0.0642)	1.257 (0.891)	50.00 (71.64)
 * Train Acc 71.640
 * Val Acc 71.500, Total time 0.60
 * Val loss 0.807, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0086 (0.0086)	0.954 (0.954)	62.50 (62.50)
[10/157]	0.1037 (0.0972)	0.0646 (0.0589)	0.700 (0.862)	84.38 (72.73)
[20/157]	0.1053 (0.1001)	0.0648 (0.0617)	0.898 (0.877)	68.75 (72.02)
[30/157]	0.1045 (0.1012)	0.0652 (0.0627)	0.990 (0.882)	75.00 (71.77)
[40/157]	0.1039 (0.1017)	0.0641 (0.0632)	0.797 (0.886)	68.75 (71.04)
[50/157]	0.1042 (0.1019)	0.0641 (0.0634)	0.408 (0.860)	96.88 (72.06)
[60/157]	0.1023 (0.1021)	0.0633 (0.0636)	0.987 (0.867)	62.50 (71.93)
[70/157]	0.1032 (0.1023)	0.0632 (0.0637)	0.978 (0.878)	75.00 (71.83)
[80/157]	0.1042 (0.1024)	0.0641 (0.0637)	0.765 (0.862)	75.00 (72.61)
[90/157]	0.1017 (0.1025)	0.0624 (0.0638)	1.076 (0.875)	71.88 (72.05)
[100/157]	0.1060 (0.1026)	0.0661 (0.0639)	0.988 (0.886)	62.50 (71.75)
[110/157]	0.1048 (0.1027)	0.0637 (0.0639)	0.828 (0.896)	81.25 (71.40)
[120/157]	0.1036 (0.1027)	0.0635 (0.0639)	0.708 (0.886)	81.25 (71.93)
[130/157]	0.1032 (0.1028)	0.0637 (0.0640)	1.030 (0.890)	75.00 (71.80)
[140/157]	0.1040 (0.1028)	0.0644 (0.0639)	0.919 (0.890)	71.88 (71.81)
[150/157]	0.1028 (0.1028)	0.0627 (0.0640)	1.063 (0.885)	62.50 (71.96)
[156/157]	0.0860 (0.1027)	0.0586 (0.0639)	1.224 (0.886)	62.50 (71.92)
 * Train Acc 71.920
 * Val Acc 71.600, Total time 0.61
 * Val loss 0.826, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0090 (0.0090)	0.950 (0.950)	68.75 (68.75)
[10/157]	0.1053 (0.0973)	0.0653 (0.0587)	1.138 (0.979)	68.75 (65.91)
[20/157]	0.1045 (0.1003)	0.0642 (0.0614)	0.935 (0.957)	68.75 (67.41)
[30/157]	0.1036 (0.1012)	0.0642 (0.0623)	1.265 (0.922)	62.50 (69.15)
[40/157]	0.1021 (0.1017)	0.0625 (0.0627)	0.890 (0.927)	71.88 (69.05)
[50/157]	0.1039 (0.1020)	0.0634 (0.0629)	0.596 (0.915)	84.38 (70.10)
[60/157]	0.1014 (0.1021)	0.0617 (0.0631)	1.130 (0.910)	59.38 (70.29)
[70/157]	0.1055 (0.1023)	0.0642 (0.0632)	0.820 (0.903)	81.25 (70.73)
[80/157]	0.1027 (0.1023)	0.0635 (0.0633)	1.130 (0.908)	65.62 (70.45)
[90/157]	0.1047 (0.1025)	0.0654 (0.0634)	1.097 (0.901)	56.25 (70.81)
[100/157]	0.1025 (0.1025)	0.0630 (0.0634)	0.817 (0.901)	65.62 (70.48)
[110/157]	0.1005 (0.1026)	0.0610 (0.0635)	0.748 (0.900)	84.38 (70.64)
[120/157]	0.1051 (0.1026)	0.0648 (0.0636)	0.917 (0.895)	75.00 (70.87)
[130/157]	0.1046 (0.1027)	0.0652 (0.0637)	0.997 (0.894)	68.75 (70.85)
[140/157]	0.1043 (0.1027)	0.0652 (0.0637)	0.682 (0.898)	84.38 (70.94)
[150/157]	0.1046 (0.1028)	0.0650 (0.0638)	0.662 (0.896)	78.12 (71.09)
[156/157]	0.0876 (0.1027)	0.0609 (0.0638)	0.901 (0.896)	62.50 (71.08)
 * Train Acc 71.080
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.802, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0089 (0.0089)	1.184 (1.184)	56.25 (56.25)
[10/157]	0.1023 (0.0968)	0.0634 (0.0589)	1.106 (0.860)	62.50 (73.58)
[20/157]	0.1038 (0.0998)	0.0639 (0.0615)	0.645 (0.843)	78.12 (73.51)
[30/157]	0.1044 (0.1009)	0.0648 (0.0624)	1.484 (0.876)	53.12 (72.48)
[40/157]	0.1051 (0.1014)	0.0651 (0.0629)	0.851 (0.884)	75.00 (72.03)
[50/157]	0.1031 (0.1017)	0.0635 (0.0633)	0.706 (0.884)	75.00 (71.81)
[60/157]	0.1029 (0.1020)	0.0637 (0.0636)	1.010 (0.879)	62.50 (71.72)
[70/157]	0.1052 (0.1022)	0.0654 (0.0637)	0.749 (0.888)	75.00 (71.65)
[80/157]	0.1038 (0.1024)	0.0635 (0.0638)	0.868 (0.887)	75.00 (71.95)
[90/157]	0.1025 (0.1024)	0.0632 (0.0638)	0.844 (0.887)	71.88 (72.08)
[100/157]	0.1053 (0.1025)	0.0651 (0.0639)	0.674 (0.878)	78.12 (72.43)
[110/157]	0.1018 (0.1025)	0.0629 (0.0639)	0.952 (0.876)	81.25 (72.52)
[120/157]	0.1041 (0.1026)	0.0654 (0.0640)	1.089 (0.882)	65.62 (72.18)
[130/157]	0.1072 (0.1027)	0.0659 (0.0640)	0.873 (0.884)	81.25 (72.07)
[140/157]	0.1017 (0.1027)	0.0629 (0.0640)	0.845 (0.885)	71.88 (72.25)
[150/157]	0.1035 (0.1027)	0.0639 (0.0640)	0.886 (0.891)	71.88 (72.00)
[156/157]	0.0886 (0.1026)	0.0606 (0.0640)	1.460 (0.890)	50.00 (71.98)
 * Train Acc 71.980
 * Val Acc 72.100, Total time 0.60
 * Val loss 0.809, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0084 (0.0084)	0.893 (0.893)	75.00 (75.00)
[10/157]	0.1024 (0.0967)	0.0632 (0.0589)	1.024 (0.931)	65.62 (71.31)
[20/157]	0.1072 (0.0998)	0.0662 (0.0615)	0.657 (0.929)	87.50 (71.13)
[30/157]	0.1008 (0.1008)	0.0615 (0.0624)	0.778 (0.903)	78.12 (72.18)
[40/157]	0.1048 (0.1015)	0.0624 (0.0627)	0.819 (0.896)	81.25 (72.18)
[50/157]	0.1004 (0.1017)	0.0614 (0.0630)	0.876 (0.889)	68.75 (72.00)
[60/157]	0.1041 (0.1020)	0.0650 (0.0633)	0.994 (0.892)	68.75 (71.88)
[70/157]	0.1041 (0.1022)	0.0631 (0.0634)	1.210 (0.908)	65.62 (70.95)
[80/157]	0.1044 (0.1023)	0.0651 (0.0635)	0.663 (0.907)	84.38 (70.91)
[90/157]	0.1058 (0.1023)	0.0655 (0.0636)	0.642 (0.905)	78.12 (70.64)
[100/157]	0.1038 (0.1024)	0.0642 (0.0636)	0.849 (0.893)	65.62 (71.04)
[110/157]	0.0955 (0.1019)	0.0574 (0.0632)	1.097 (0.892)	68.75 (71.06)
[120/157]	0.1014 (0.1014)	0.0618 (0.0628)	1.067 (0.890)	71.88 (71.13)
[130/157]	0.1000 (0.1013)	0.0604 (0.0626)	0.753 (0.896)	78.12 (71.16)
[140/157]	0.0982 (0.1011)	0.0587 (0.0625)	0.810 (0.898)	75.00 (71.14)
[150/157]	0.0991 (0.1009)	0.0604 (0.0623)	0.439 (0.894)	87.50 (71.40)
[156/157]	0.0807 (0.1007)	0.0547 (0.0622)	1.616 (0.890)	37.50 (71.60)
 * Train Acc 71.600
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.810, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0092 (0.0092)	1.014 (1.014)	71.88 (71.88)
[10/157]	0.0988 (0.0931)	0.0603 (0.0549)	0.947 (0.909)	62.50 (69.03)
[20/157]	0.1005 (0.0962)	0.0616 (0.0579)	0.598 (0.875)	81.25 (71.73)
[30/157]	0.0997 (0.0972)	0.0600 (0.0588)	0.828 (0.862)	78.12 (72.88)
[40/157]	0.0976 (0.0976)	0.0600 (0.0592)	0.719 (0.864)	81.25 (72.41)
[50/157]	0.0987 (0.0978)	0.0597 (0.0594)	0.842 (0.853)	71.88 (72.86)
[60/157]	0.0971 (0.0981)	0.0589 (0.0596)	0.807 (0.845)	71.88 (72.69)
[70/157]	0.0996 (0.0982)	0.0599 (0.0598)	0.751 (0.848)	75.00 (72.40)
[80/157]	0.1004 (0.0984)	0.0604 (0.0599)	0.722 (0.858)	78.12 (71.88)
[90/157]	0.0997 (0.0984)	0.0607 (0.0599)	1.000 (0.870)	62.50 (71.46)
[100/157]	0.0978 (0.0985)	0.0591 (0.0600)	0.914 (0.879)	62.50 (71.41)
[110/157]	0.0990 (0.0985)	0.0606 (0.0601)	0.806 (0.894)	71.88 (71.06)
[120/157]	0.0982 (0.0985)	0.0602 (0.0602)	0.810 (0.887)	78.12 (71.44)
[130/157]	0.0991 (0.0985)	0.0603 (0.0602)	0.940 (0.892)	68.75 (71.64)
[140/157]	0.0937 (0.0984)	0.0568 (0.0602)	0.700 (0.895)	78.12 (71.56)
[150/157]	0.0971 (0.0982)	0.0594 (0.0600)	0.860 (0.894)	62.50 (71.27)
[156/157]	0.0862 (0.0981)	0.0595 (0.0600)	0.876 (0.895)	62.50 (71.32)
 * Train Acc 71.320
 * Val Acc 72.300, Total time 0.61
 * Val loss 0.795, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0090 (0.0090)	0.684 (0.684)	71.88 (71.88)
[10/157]	0.1038 (0.0970)	0.0646 (0.0586)	0.653 (0.879)	71.88 (72.16)
[20/157]	0.1017 (0.1000)	0.0625 (0.0615)	0.680 (0.837)	78.12 (74.11)
[30/157]	0.1024 (0.1009)	0.0634 (0.0624)	0.497 (0.832)	84.38 (73.89)
[40/157]	0.1017 (0.1015)	0.0628 (0.0629)	0.763 (0.839)	71.88 (73.70)
[50/157]	0.1047 (0.1018)	0.0649 (0.0633)	0.839 (0.872)	75.00 (72.43)
[60/157]	0.1041 (0.1020)	0.0650 (0.0635)	0.707 (0.864)	84.38 (72.75)
[70/157]	0.1062 (0.1022)	0.0663 (0.0636)	0.929 (0.869)	71.88 (72.23)
[80/157]	0.1042 (0.1023)	0.0642 (0.0638)	0.882 (0.884)	68.75 (71.49)
[90/157]	0.1041 (0.1024)	0.0644 (0.0638)	1.256 (0.887)	68.75 (71.74)
[100/157]	0.1043 (0.1025)	0.0634 (0.0638)	1.057 (0.883)	68.75 (71.72)
[110/157]	0.1012 (0.1025)	0.0621 (0.0638)	1.114 (0.886)	68.75 (72.07)
[120/157]	0.1026 (0.1026)	0.0622 (0.0638)	0.776 (0.877)	75.00 (72.68)
[130/157]	0.1037 (0.1026)	0.0641 (0.0639)	0.859 (0.878)	78.12 (72.69)
[140/157]	0.1038 (0.1027)	0.0649 (0.0640)	0.911 (0.881)	81.25 (72.63)
[150/157]	0.1061 (0.1027)	0.0666 (0.0640)	0.988 (0.885)	75.00 (72.33)
[156/157]	0.0851 (0.1026)	0.0580 (0.0640)	1.699 (0.885)	37.50 (72.38)
 * Train Acc 72.380
 * Val Acc 73.200, Total time 0.61
 * Val loss 0.801, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0090 (0.0090)	1.039 (1.039)	62.50 (62.50)
[10/157]	0.1059 (0.0966)	0.0663 (0.0587)	0.942 (0.908)	75.00 (72.16)
[20/157]	0.1031 (0.0997)	0.0636 (0.0615)	0.711 (0.880)	78.12 (72.02)
[30/157]	0.1052 (0.1006)	0.0654 (0.0623)	0.767 (0.883)	78.12 (71.88)
[40/157]	0.1033 (0.1012)	0.0643 (0.0629)	0.832 (0.860)	65.62 (72.41)
[50/157]	0.0962 (0.1005)	0.0585 (0.0624)	0.903 (0.848)	59.38 (72.86)
[60/157]	0.1048 (0.1023)	0.0650 (0.0640)	1.358 (0.861)	59.38 (72.28)
[70/157]	0.1050 (0.1027)	0.0655 (0.0643)	0.940 (0.865)	78.12 (72.45)
[80/157]	0.1061 (0.1030)	0.0667 (0.0645)	1.004 (0.879)	62.50 (71.60)
[90/157]	0.0951 (0.1023)	0.0566 (0.0639)	0.889 (0.879)	62.50 (71.63)
[100/157]	0.1092 (0.1018)	0.0687 (0.0634)	0.940 (0.882)	65.62 (71.57)
[110/157]	0.1087 (0.1023)	0.0694 (0.0639)	1.018 (0.882)	65.62 (71.45)
[120/157]	0.0946 (0.1026)	0.0573 (0.0641)	0.525 (0.873)	90.62 (71.95)
[130/157]	0.0979 (0.1020)	0.0597 (0.0636)	1.023 (0.881)	75.00 (71.88)
[140/157]	0.0978 (0.1016)	0.0588 (0.0633)	1.053 (0.880)	65.62 (71.90)
[150/157]	0.0981 (0.1012)	0.0591 (0.0630)	0.925 (0.886)	68.75 (71.71)
[156/157]	0.1021 (0.1011)	0.0752 (0.0630)	1.153 (0.890)	62.50 (71.48)
 * Train Acc 71.480
 * Val Acc 72.400, Total time 0.64
 * Val loss 0.794, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0097 (0.0097)	0.864 (0.864)	71.88 (71.88)
[10/157]	0.0992 (0.0925)	0.0595 (0.0547)	0.727 (0.902)	71.88 (72.16)
[20/157]	0.0994 (0.0951)	0.0600 (0.0571)	1.182 (0.906)	56.25 (71.58)
[30/157]	0.0995 (0.0961)	0.0603 (0.0580)	0.661 (0.883)	78.12 (71.47)
[40/157]	0.0983 (0.0965)	0.0592 (0.0584)	0.835 (0.856)	78.12 (72.79)
[50/157]	0.0974 (0.0967)	0.0588 (0.0586)	0.890 (0.840)	68.75 (73.04)
[60/157]	0.0993 (0.0969)	0.0599 (0.0588)	1.134 (0.866)	68.75 (72.03)
[70/157]	0.0978 (0.0971)	0.0592 (0.0590)	0.869 (0.869)	71.88 (71.83)
[80/157]	0.1085 (0.0986)	0.0684 (0.0605)	0.921 (0.870)	71.88 (71.84)
[90/157]	0.1052 (0.0993)	0.0657 (0.0611)	0.698 (0.866)	75.00 (72.12)
[100/157]	0.1048 (0.0998)	0.0653 (0.0616)	0.877 (0.867)	78.12 (72.25)
[110/157]	0.0950 (0.1000)	0.0578 (0.0618)	1.147 (0.878)	59.38 (72.02)
[120/157]	0.1077 (0.1002)	0.0671 (0.0620)	0.645 (0.882)	84.38 (71.88)
[130/157]	0.0966 (0.1003)	0.0584 (0.0621)	1.031 (0.882)	56.25 (71.78)
[140/157]	0.0968 (0.1000)	0.0584 (0.0619)	0.759 (0.884)	75.00 (71.79)
[150/157]	0.1218 (0.1000)	0.0804 (0.0619)	0.977 (0.888)	65.62 (71.65)
[156/157]	0.1009 (0.1006)	0.0723 (0.0625)	0.722 (0.888)	87.50 (71.60)
 * Train Acc 71.600
 * Val Acc 72.400, Total time 0.63
 * Val loss 0.787, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0090 (0.0090)	1.041 (1.041)	65.62 (65.62)
[10/157]	0.1018 (0.0957)	0.0618 (0.0570)	0.971 (0.921)	68.75 (71.59)
[20/157]	0.1012 (0.0980)	0.0625 (0.0594)	1.098 (0.937)	62.50 (70.24)
[30/157]	0.1006 (0.0991)	0.0619 (0.0605)	0.719 (0.911)	78.12 (71.07)
[40/157]	0.1027 (0.0996)	0.0629 (0.0610)	0.926 (0.913)	68.75 (71.19)
[50/157]	0.0992 (0.0998)	0.0603 (0.0612)	0.679 (0.896)	81.25 (71.81)
[60/157]	0.1023 (0.1000)	0.0625 (0.0613)	0.822 (0.884)	81.25 (72.44)
[70/157]	0.0971 (0.0994)	0.0578 (0.0609)	0.994 (0.878)	78.12 (72.49)
[80/157]	0.0956 (0.0989)	0.0582 (0.0606)	1.176 (0.879)	65.62 (72.42)
[90/157]	0.0967 (0.0985)	0.0585 (0.0604)	0.654 (0.879)	75.00 (72.36)
[100/157]	0.1016 (0.0987)	0.0625 (0.0606)	0.771 (0.879)	71.88 (72.37)
[110/157]	0.1013 (0.0989)	0.0626 (0.0608)	0.940 (0.875)	68.75 (72.41)
[120/157]	0.1015 (0.0991)	0.0628 (0.0609)	0.812 (0.873)	68.75 (72.42)
[130/157]	0.1010 (0.0992)	0.0618 (0.0611)	1.040 (0.875)	53.12 (72.07)
[140/157]	0.1011 (0.0994)	0.0631 (0.0612)	0.696 (0.875)	81.25 (72.19)
[150/157]	0.1000 (0.0995)	0.0611 (0.0613)	0.749 (0.874)	84.38 (72.14)
[156/157]	0.0836 (0.0994)	0.0566 (0.0613)	0.746 (0.875)	87.50 (72.04)
 * Train Acc 72.040
 * Val Acc 72.300, Total time 0.60
 * Val loss 0.793, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0090 (0.0090)	0.785 (0.785)	65.62 (65.62)
[10/157]	0.1029 (0.0952)	0.0628 (0.0574)	1.010 (0.864)	68.75 (73.86)
[20/157]	0.1003 (0.0977)	0.0614 (0.0597)	1.409 (0.833)	53.12 (74.26)
[30/157]	0.1023 (0.0989)	0.0625 (0.0607)	0.955 (0.854)	71.88 (73.69)
[40/157]	0.0995 (0.0992)	0.0609 (0.0611)	0.942 (0.853)	78.12 (73.86)
[50/157]	0.1005 (0.0995)	0.0618 (0.0614)	0.771 (0.868)	81.25 (73.16)
[60/157]	0.1004 (0.0998)	0.0616 (0.0616)	0.980 (0.859)	75.00 (73.31)
[70/157]	0.1028 (0.1000)	0.0642 (0.0618)	0.902 (0.858)	68.75 (73.37)
[80/157]	0.1000 (0.1001)	0.0606 (0.0619)	0.927 (0.859)	71.88 (73.30)
[90/157]	0.1002 (0.1003)	0.0610 (0.0620)	0.908 (0.862)	68.75 (73.25)
[100/157]	0.1010 (0.1003)	0.0621 (0.0621)	0.898 (0.868)	68.75 (73.02)
[110/157]	0.1000 (0.1004)	0.0608 (0.0622)	0.652 (0.868)	78.12 (72.97)
[120/157]	0.1019 (0.1005)	0.0630 (0.0622)	0.809 (0.868)	75.00 (72.93)
[130/157]	0.0999 (0.1005)	0.0611 (0.0622)	1.329 (0.871)	68.75 (72.95)
[140/157]	0.1024 (0.1005)	0.0624 (0.0623)	0.803 (0.879)	75.00 (72.76)
[150/157]	0.1007 (0.1006)	0.0619 (0.0622)	1.103 (0.881)	59.38 (72.64)
[156/157]	0.0854 (0.1005)	0.0577 (0.0622)	0.797 (0.884)	75.00 (72.42)
 * Train Acc 72.420
 * Val Acc 72.200, Total time 0.59
 * Val loss 0.802, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0466 (0.0466)	0.0088 (0.0088)	0.689 (0.689)	78.12 (78.12)
[10/157]	0.1093 (0.1018)	0.0686 (0.0627)	0.789 (0.800)	75.00 (74.72)
[20/157]	0.0945 (0.1023)	0.0573 (0.0638)	0.940 (0.844)	65.62 (73.36)
[30/157]	0.0971 (0.1001)	0.0597 (0.0622)	1.173 (0.845)	59.38 (72.68)
[40/157]	0.0953 (0.0989)	0.0576 (0.0613)	0.671 (0.862)	78.12 (71.57)
[50/157]	0.0948 (0.0982)	0.0574 (0.0608)	0.993 (0.866)	78.12 (72.00)
[60/157]	0.1118 (0.0981)	0.0715 (0.0606)	0.500 (0.870)	90.62 (72.03)
[70/157]	0.1113 (0.0998)	0.0708 (0.0621)	0.923 (0.863)	59.38 (72.32)
[80/157]	0.0974 (0.1000)	0.0593 (0.0623)	0.878 (0.865)	65.62 (72.30)
[90/157]	0.0960 (0.0996)	0.0586 (0.0620)	0.944 (0.864)	68.75 (72.18)
[100/157]	0.1156 (0.0994)	0.0754 (0.0618)	0.997 (0.868)	71.88 (71.97)
[110/157]	0.0994 (0.1002)	0.0605 (0.0625)	0.861 (0.862)	68.75 (72.16)
[120/157]	0.1000 (0.1002)	0.0612 (0.0625)	0.687 (0.859)	71.88 (72.26)
[130/157]	0.1001 (0.1001)	0.0608 (0.0624)	0.914 (0.858)	68.75 (72.28)
[140/157]	0.1007 (0.1001)	0.0616 (0.0623)	0.859 (0.860)	65.62 (72.23)
[150/157]	0.1003 (0.1001)	0.0617 (0.0623)	0.870 (0.863)	78.12 (72.12)
[156/157]	0.0832 (0.1000)	0.0567 (0.0622)	1.505 (0.863)	62.50 (72.10)
 * Train Acc 72.100
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.800, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0086 (0.0086)	0.898 (0.898)	78.12 (78.12)
[10/157]	0.1018 (0.0943)	0.0630 (0.0571)	0.535 (0.814)	84.38 (73.01)
[20/157]	0.1008 (0.0973)	0.0626 (0.0598)	1.006 (0.822)	68.75 (73.66)
[30/157]	0.1013 (0.0985)	0.0625 (0.0606)	1.075 (0.827)	68.75 (73.69)
[40/157]	0.1012 (0.0989)	0.0620 (0.0609)	0.706 (0.823)	75.00 (74.01)
[50/157]	0.1017 (0.0993)	0.0625 (0.0613)	0.631 (0.825)	71.88 (74.02)
[60/157]	0.1021 (0.0995)	0.0616 (0.0614)	1.188 (0.851)	59.38 (73.00)
[70/157]	0.1012 (0.0996)	0.0617 (0.0614)	0.745 (0.847)	71.88 (73.02)
[80/157]	0.1009 (0.0997)	0.0624 (0.0615)	1.083 (0.858)	65.62 (72.34)
[90/157]	0.1009 (0.0997)	0.0625 (0.0616)	0.736 (0.853)	78.12 (72.66)
[100/157]	0.1017 (0.0999)	0.0626 (0.0617)	0.780 (0.869)	65.62 (71.97)
[110/157]	0.1013 (0.0999)	0.0622 (0.0617)	0.820 (0.872)	68.75 (72.10)
[120/157]	0.1002 (0.1001)	0.0606 (0.0618)	0.979 (0.875)	56.25 (71.93)
[130/157]	0.1001 (0.1001)	0.0612 (0.0618)	1.202 (0.875)	65.62 (72.02)
[140/157]	0.1019 (0.1002)	0.0620 (0.0619)	0.686 (0.875)	84.38 (71.92)
[150/157]	0.1017 (0.1002)	0.0623 (0.0618)	1.359 (0.876)	56.25 (71.85)
[156/157]	0.0847 (0.1001)	0.0569 (0.0618)	1.216 (0.877)	62.50 (71.78)
 * Train Acc 71.780
 * Val Acc 72.800, Total time 0.60
 * Val loss 0.791, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0089 (0.0089)	0.857 (0.857)	71.88 (71.88)
[10/157]	0.1003 (0.0945)	0.0610 (0.0562)	0.896 (0.857)	65.62 (72.44)
[20/157]	0.1008 (0.0972)	0.0616 (0.0586)	0.714 (0.902)	78.12 (71.58)
[30/157]	0.1001 (0.0982)	0.0610 (0.0596)	0.928 (0.899)	68.75 (71.47)
[40/157]	0.1025 (0.0988)	0.0627 (0.0602)	0.858 (0.876)	71.88 (72.79)
[50/157]	0.1005 (0.0992)	0.0604 (0.0604)	0.741 (0.883)	84.38 (72.67)
[60/157]	0.1020 (0.0994)	0.0626 (0.0606)	0.881 (0.879)	75.00 (72.75)
[70/157]	0.0988 (0.0995)	0.0606 (0.0607)	0.959 (0.883)	59.38 (72.58)
[80/157]	0.1005 (0.0996)	0.0613 (0.0608)	0.819 (0.892)	78.12 (72.11)
[90/157]	0.1010 (0.0998)	0.0618 (0.0610)	1.245 (0.898)	65.62 (71.70)
[100/157]	0.1033 (0.0998)	0.0636 (0.0611)	0.813 (0.892)	71.88 (71.66)
[110/157]	0.1007 (0.0998)	0.0615 (0.0612)	0.673 (0.897)	84.38 (71.40)
[120/157]	0.1005 (0.0998)	0.0620 (0.0612)	0.691 (0.886)	78.12 (71.59)
[130/157]	0.1011 (0.0998)	0.0618 (0.0613)	0.946 (0.887)	62.50 (71.35)
[140/157]	0.1003 (0.0999)	0.0617 (0.0614)	0.902 (0.893)	68.75 (71.14)
[150/157]	0.0997 (0.1000)	0.0607 (0.0614)	0.955 (0.889)	75.00 (71.50)
[156/157]	0.0856 (0.0999)	0.0579 (0.0614)	1.176 (0.891)	62.50 (71.40)
 * Train Acc 71.400
 * Val Acc 72.800, Total time 0.60
 * Val loss 0.785, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0085 (0.0085)	0.985 (0.985)	71.88 (71.88)
[10/157]	0.1003 (0.0953)	0.0614 (0.0572)	0.769 (0.790)	75.00 (76.70)
[20/157]	0.1002 (0.0976)	0.0614 (0.0595)	1.146 (0.821)	71.88 (73.81)
[30/157]	0.0994 (0.0984)	0.0607 (0.0602)	0.619 (0.826)	84.38 (74.40)
[40/157]	0.1001 (0.0992)	0.0620 (0.0608)	1.106 (0.840)	68.75 (73.40)
[50/157]	0.0981 (0.0993)	0.0601 (0.0610)	1.180 (0.850)	56.25 (73.35)
[60/157]	0.0993 (0.0995)	0.0590 (0.0611)	0.925 (0.850)	68.75 (72.59)
[70/157]	0.1034 (0.0996)	0.0609 (0.0611)	0.832 (0.853)	75.00 (72.71)
[80/157]	0.0992 (0.0996)	0.0610 (0.0612)	0.946 (0.861)	81.25 (72.72)
[90/157]	0.1005 (0.0997)	0.0613 (0.0613)	0.789 (0.867)	75.00 (72.70)
[100/157]	0.1016 (0.0998)	0.0618 (0.0614)	1.175 (0.856)	78.12 (73.33)
[110/157]	0.1001 (0.0999)	0.0608 (0.0614)	0.730 (0.852)	81.25 (73.48)
[120/157]	0.0999 (0.0999)	0.0614 (0.0614)	0.885 (0.853)	65.62 (73.30)
[130/157]	0.1018 (0.0999)	0.0627 (0.0614)	0.881 (0.851)	68.75 (73.21)
[140/157]	0.0992 (0.0999)	0.0602 (0.0614)	0.579 (0.849)	84.38 (73.27)
[150/157]	0.0994 (0.0999)	0.0608 (0.0614)	0.824 (0.848)	78.12 (73.30)
[156/157]	0.0844 (0.0998)	0.0571 (0.0614)	1.468 (0.853)	50.00 (73.24)
 * Train Acc 73.240
 * Val Acc 72.800, Total time 0.60
 * Val loss 0.781, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0092 (0.0092)	0.674 (0.674)	87.50 (87.50)
[10/157]	0.1008 (0.0942)	0.0618 (0.0568)	0.897 (0.857)	75.00 (72.73)
[20/157]	0.1013 (0.0971)	0.0627 (0.0595)	1.057 (0.821)	65.62 (75.00)
[30/157]	0.1004 (0.0982)	0.0619 (0.0605)	0.979 (0.861)	65.62 (72.68)
[40/157]	0.0993 (0.0989)	0.0600 (0.0608)	0.856 (0.897)	75.00 (71.95)
[50/157]	0.0995 (0.0991)	0.0606 (0.0608)	0.814 (0.902)	68.75 (71.57)
[60/157]	0.0997 (0.0993)	0.0599 (0.0609)	1.016 (0.901)	62.50 (70.95)
[70/157]	0.1005 (0.0994)	0.0617 (0.0610)	1.029 (0.896)	71.88 (70.91)
[80/157]	0.1002 (0.0994)	0.0611 (0.0611)	0.829 (0.887)	75.00 (70.79)
[90/157]	0.0954 (0.0991)	0.0576 (0.0608)	0.777 (0.882)	81.25 (71.53)
[100/157]	0.0960 (0.0987)	0.0572 (0.0606)	0.800 (0.872)	75.00 (71.91)
[110/157]	0.0959 (0.0985)	0.0583 (0.0604)	0.887 (0.871)	78.12 (72.27)
[120/157]	0.0959 (0.0982)	0.0583 (0.0602)	0.937 (0.868)	68.75 (72.39)
[130/157]	0.0969 (0.0980)	0.0593 (0.0601)	1.068 (0.871)	71.88 (72.35)
[140/157]	0.1014 (0.0979)	0.0630 (0.0601)	0.963 (0.865)	75.00 (72.61)
[150/157]	0.1012 (0.0981)	0.0613 (0.0602)	0.991 (0.871)	71.88 (72.50)
[156/157]	0.0828 (0.0980)	0.0562 (0.0602)	0.861 (0.870)	62.50 (72.46)
 * Train Acc 72.460
 * Val Acc 73.300, Total time 0.60
 * Val loss 0.780, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0089 (0.0089)	1.102 (1.102)	65.62 (65.62)
[10/157]	0.1004 (0.0947)	0.0607 (0.0567)	0.948 (0.826)	62.50 (74.15)
[20/157]	0.1011 (0.0972)	0.0621 (0.0589)	0.953 (0.866)	71.88 (72.17)
[30/157]	0.1002 (0.0981)	0.0609 (0.0598)	0.755 (0.856)	78.12 (73.49)
[40/157]	0.1007 (0.0986)	0.0615 (0.0602)	0.925 (0.844)	65.62 (74.09)
[50/157]	0.1011 (0.0989)	0.0624 (0.0605)	0.815 (0.861)	68.75 (73.16)
[60/157]	0.0999 (0.0991)	0.0610 (0.0608)	0.929 (0.873)	78.12 (72.95)
[70/157]	0.0997 (0.0992)	0.0607 (0.0609)	0.953 (0.872)	71.88 (73.06)
[80/157]	0.1002 (0.0993)	0.0616 (0.0610)	0.868 (0.861)	71.88 (73.50)
[90/157]	0.1009 (0.0994)	0.0606 (0.0610)	0.940 (0.858)	65.62 (73.45)
[100/157]	0.1010 (0.0995)	0.0625 (0.0611)	0.880 (0.861)	65.62 (73.24)
[110/157]	0.1006 (0.0996)	0.0614 (0.0612)	0.925 (0.862)	75.00 (73.25)
[120/157]	0.1018 (0.0997)	0.0627 (0.0613)	0.721 (0.863)	81.25 (73.22)
[130/157]	0.0999 (0.0997)	0.0620 (0.0613)	0.953 (0.863)	71.88 (73.09)
[140/157]	0.0994 (0.0997)	0.0603 (0.0613)	0.604 (0.856)	84.38 (73.34)
[150/157]	0.0994 (0.0997)	0.0605 (0.0613)	0.734 (0.861)	71.88 (73.22)
[156/157]	0.0842 (0.0996)	0.0570 (0.0613)	1.524 (0.862)	37.50 (73.18)
 * Train Acc 73.180
 * Val Acc 72.300, Total time 0.60
 * Val loss 0.797, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0082 (0.0082)	0.860 (0.860)	68.75 (68.75)
[10/157]	0.0981 (0.0942)	0.0605 (0.0563)	1.142 (0.881)	59.38 (72.16)
[20/157]	0.1009 (0.0968)	0.0613 (0.0588)	0.952 (0.877)	81.25 (72.17)
[30/157]	0.0987 (0.0977)	0.0601 (0.0597)	1.149 (0.885)	56.25 (71.67)
[40/157]	0.1011 (0.0983)	0.0619 (0.0602)	0.857 (0.881)	68.75 (72.10)
[50/157]	0.1013 (0.0986)	0.0617 (0.0605)	0.732 (0.876)	81.25 (72.12)
[60/157]	0.1026 (0.0989)	0.0638 (0.0606)	0.919 (0.877)	78.12 (71.93)
[70/157]	0.1002 (0.0990)	0.0616 (0.0608)	0.669 (0.868)	81.25 (72.01)
[80/157]	0.1009 (0.0991)	0.0611 (0.0609)	0.847 (0.872)	71.88 (72.07)
[90/157]	0.1008 (0.0993)	0.0610 (0.0610)	0.772 (0.863)	78.12 (72.36)
[100/157]	0.1018 (0.0994)	0.0621 (0.0611)	0.829 (0.865)	75.00 (72.49)
[110/157]	0.1016 (0.0994)	0.0626 (0.0611)	0.730 (0.860)	84.38 (72.52)
[120/157]	0.0950 (0.0991)	0.0572 (0.0609)	0.974 (0.863)	75.00 (72.52)
[130/157]	0.0974 (0.0988)	0.0590 (0.0607)	0.835 (0.862)	75.00 (72.57)
[140/157]	0.0947 (0.0986)	0.0573 (0.0605)	0.878 (0.861)	65.62 (72.58)
[150/157]	0.0961 (0.0983)	0.0584 (0.0604)	0.665 (0.861)	84.38 (72.66)
[156/157]	0.0786 (0.0981)	0.0538 (0.0603)	1.206 (0.864)	50.00 (72.58)
 * Train Acc 72.580
 * Val Acc 72.700, Total time 0.58
 * Val loss 0.784, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0422 (0.0422)	0.0090 (0.0090)	0.533 (0.533)	90.62 (90.62)
[10/157]	0.0943 (0.0912)	0.0567 (0.0543)	1.166 (0.897)	53.12 (71.31)
[20/157]	0.1099 (0.0985)	0.0660 (0.0608)	0.862 (0.882)	68.75 (71.43)
[30/157]	0.1126 (0.0979)	0.0721 (0.0604)	0.829 (0.863)	81.25 (72.68)
[40/157]	0.0949 (0.0994)	0.0577 (0.0618)	0.872 (0.871)	71.88 (72.26)
[50/157]	0.0963 (0.1006)	0.0583 (0.0630)	0.885 (0.871)	68.75 (71.94)
[60/157]	0.0959 (0.0997)	0.0591 (0.0622)	0.947 (0.863)	68.75 (72.69)
[70/157]	0.0986 (0.0991)	0.0582 (0.0617)	0.718 (0.854)	87.50 (72.80)
[80/157]	0.0960 (0.0987)	0.0574 (0.0612)	0.796 (0.863)	78.12 (72.57)
[90/157]	0.1053 (0.0990)	0.0651 (0.0614)	0.769 (0.859)	81.25 (72.77)
[100/157]	0.0945 (0.0992)	0.0568 (0.0615)	0.582 (0.862)	84.38 (72.56)
[110/157]	0.0949 (0.0989)	0.0574 (0.0612)	0.866 (0.858)	71.88 (72.47)
[120/157]	0.0969 (0.0986)	0.0579 (0.0610)	1.060 (0.852)	68.75 (72.70)
[130/157]	0.0940 (0.0983)	0.0568 (0.0607)	0.624 (0.855)	84.38 (72.57)
[140/157]	0.1090 (0.0989)	0.0689 (0.0612)	1.095 (0.861)	62.50 (72.38)
[150/157]	0.0976 (0.0987)	0.0593 (0.0611)	1.082 (0.867)	68.75 (72.12)
[156/157]	0.0788 (0.0985)	0.0544 (0.0610)	1.231 (0.866)	50.00 (72.12)
 * Train Acc 72.120
 * Val Acc 72.400, Total time 0.58
 * Val loss 0.779, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0419 (0.0419)	0.0086 (0.0086)	0.952 (0.952)	62.50 (62.50)
[10/157]	0.1025 (0.1051)	0.0636 (0.0667)	0.552 (0.852)	87.50 (75.00)
[20/157]	0.1071 (0.1052)	0.0669 (0.0665)	1.151 (0.878)	62.50 (73.07)
[30/157]	0.0956 (0.1043)	0.0577 (0.0657)	0.750 (0.861)	71.88 (73.89)
[40/157]	0.0951 (0.1021)	0.0571 (0.0638)	0.852 (0.846)	68.75 (73.86)
[50/157]	0.0967 (0.1007)	0.0581 (0.0627)	0.792 (0.851)	81.25 (73.47)
[60/157]	0.0975 (0.0998)	0.0585 (0.0620)	0.658 (0.837)	71.88 (73.67)
[70/157]	0.0946 (0.0992)	0.0572 (0.0614)	0.751 (0.832)	84.38 (73.81)
[80/157]	0.0959 (0.0989)	0.0568 (0.0610)	1.171 (0.835)	65.62 (73.69)
[90/157]	0.0987 (0.0986)	0.0592 (0.0608)	0.753 (0.847)	75.00 (73.21)
[100/157]	0.1016 (0.0987)	0.0622 (0.0608)	0.787 (0.846)	65.62 (73.14)
[110/157]	0.1004 (0.0990)	0.0616 (0.0611)	0.731 (0.844)	75.00 (73.00)
[120/157]	0.1031 (0.0993)	0.0639 (0.0612)	0.773 (0.838)	71.88 (73.22)
[130/157]	0.1007 (0.0995)	0.0619 (0.0614)	1.112 (0.844)	71.88 (73.07)
[140/157]	0.1022 (0.0996)	0.0627 (0.0615)	0.920 (0.852)	75.00 (72.89)
[150/157]	0.0939 (0.0995)	0.0568 (0.0614)	0.697 (0.851)	75.00 (72.76)
[156/157]	0.0779 (0.0992)	0.0536 (0.0613)	1.052 (0.853)	62.50 (72.62)
 * Train Acc 72.620
 * Val Acc 72.600, Total time 0.57
 * Val loss 0.787, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0089 (0.0089)	0.992 (0.992)	71.88 (71.88)
[10/157]	0.0929 (0.0916)	0.0563 (0.0542)	1.014 (0.913)	62.50 (69.32)
[20/157]	0.0959 (0.0933)	0.0582 (0.0563)	0.747 (0.914)	78.12 (70.39)
[30/157]	0.0971 (0.0942)	0.0597 (0.0571)	0.922 (0.881)	68.75 (71.67)
[40/157]	0.0953 (0.0948)	0.0569 (0.0574)	0.592 (0.875)	87.50 (72.10)
[50/157]	0.0953 (0.0951)	0.0565 (0.0576)	0.779 (0.869)	78.12 (71.75)
[60/157]	0.0978 (0.0954)	0.0595 (0.0579)	0.645 (0.861)	78.12 (72.34)
[70/157]	0.0962 (0.0955)	0.0587 (0.0580)	0.995 (0.851)	65.62 (72.58)
[80/157]	0.1047 (0.0961)	0.0649 (0.0585)	1.041 (0.864)	68.75 (72.15)
[90/157]	0.1050 (0.0968)	0.0648 (0.0591)	0.970 (0.871)	68.75 (72.08)
[100/157]	0.1038 (0.0973)	0.0644 (0.0595)	0.670 (0.868)	71.88 (72.25)
[110/157]	0.1007 (0.0978)	0.0619 (0.0599)	1.151 (0.861)	75.00 (72.92)
[120/157]	0.1030 (0.0982)	0.0637 (0.0603)	0.923 (0.860)	71.88 (72.80)
[130/157]	0.1037 (0.0985)	0.0641 (0.0606)	0.690 (0.860)	78.12 (72.78)
[140/157]	0.1017 (0.0988)	0.0633 (0.0608)	1.042 (0.859)	62.50 (72.72)
[150/157]	0.1003 (0.0990)	0.0618 (0.0610)	1.048 (0.861)	59.38 (72.70)
[156/157]	0.0862 (0.0990)	0.0587 (0.0610)	1.324 (0.862)	50.00 (72.60)
 * Train Acc 72.600
 * Val Acc 72.500, Total time 0.58
 * Val loss 0.779, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0088 (0.0088)	0.915 (0.915)	68.75 (68.75)
[10/157]	0.0963 (0.0923)	0.0574 (0.0553)	0.626 (0.867)	81.25 (73.58)
[20/157]	0.0932 (0.0936)	0.0564 (0.0568)	0.749 (0.843)	75.00 (73.96)
[30/157]	0.0961 (0.0941)	0.0582 (0.0573)	0.767 (0.855)	71.88 (72.88)
[40/157]	0.0946 (0.0943)	0.0571 (0.0575)	0.743 (0.846)	84.38 (73.40)
[50/157]	0.0967 (0.0946)	0.0593 (0.0578)	0.628 (0.856)	84.38 (73.35)
[60/157]	0.0979 (0.0950)	0.0593 (0.0581)	0.412 (0.857)	90.62 (73.26)
[70/157]	0.1203 (0.0955)	0.0799 (0.0585)	0.649 (0.843)	78.12 (73.59)
[80/157]	0.0975 (0.0967)	0.0586 (0.0595)	1.176 (0.840)	62.50 (73.50)
[90/157]	0.0968 (0.0966)	0.0588 (0.0593)	0.849 (0.845)	75.00 (73.08)
[100/157]	0.0981 (0.0966)	0.0591 (0.0593)	0.856 (0.843)	68.75 (72.99)
[110/157]	0.0941 (0.0973)	0.0564 (0.0599)	0.810 (0.842)	71.88 (72.94)
[120/157]	0.0956 (0.0971)	0.0586 (0.0598)	0.561 (0.844)	81.25 (72.88)
[130/157]	0.0983 (0.0970)	0.0586 (0.0597)	0.769 (0.844)	81.25 (72.88)
[140/157]	0.0974 (0.0971)	0.0594 (0.0598)	0.770 (0.842)	81.25 (72.92)
[150/157]	0.0963 (0.0971)	0.0587 (0.0598)	0.992 (0.851)	68.75 (72.48)
[156/157]	0.0814 (0.0970)	0.0561 (0.0598)	0.421 (0.852)	87.50 (72.44)
 * Train Acc 72.440
 * Val Acc 73.500, Total time 0.59
 * Val loss 0.775, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0091 (0.0091)	0.915 (0.915)	65.62 (65.62)
[10/157]	0.0997 (0.0928)	0.0612 (0.0552)	0.997 (0.910)	62.50 (70.74)
[20/157]	0.0980 (0.0953)	0.0602 (0.0578)	0.776 (0.887)	81.25 (71.13)
[30/157]	0.0971 (0.0963)	0.0591 (0.0586)	0.748 (0.903)	78.12 (70.26)
[40/157]	0.0978 (0.0966)	0.0607 (0.0589)	0.671 (0.897)	81.25 (70.96)
[50/157]	0.0981 (0.0969)	0.0598 (0.0591)	1.062 (0.897)	59.38 (71.26)
[60/157]	0.0975 (0.0970)	0.0594 (0.0593)	0.902 (0.904)	71.88 (70.95)
[70/157]	0.0984 (0.0971)	0.0596 (0.0594)	0.829 (0.892)	75.00 (71.57)
[80/157]	0.1020 (0.0972)	0.0574 (0.0594)	0.697 (0.884)	75.00 (71.80)
[90/157]	0.1008 (0.0973)	0.0601 (0.0594)	0.926 (0.875)	68.75 (72.01)
[100/157]	0.1007 (0.0974)	0.0611 (0.0594)	0.809 (0.871)	71.88 (71.94)
[110/157]	0.0980 (0.0974)	0.0591 (0.0594)	0.839 (0.869)	68.75 (71.93)
[120/157]	0.0998 (0.0975)	0.0601 (0.0595)	1.038 (0.867)	65.62 (72.06)
[130/157]	0.0989 (0.0975)	0.0606 (0.0595)	0.894 (0.862)	65.62 (72.19)
[140/157]	0.0970 (0.0975)	0.0591 (0.0595)	0.925 (0.870)	71.88 (72.03)
[150/157]	0.0977 (0.0975)	0.0586 (0.0595)	0.737 (0.868)	75.00 (72.08)
[156/157]	0.0804 (0.0974)	0.0550 (0.0595)	1.141 (0.864)	75.00 (72.18)
 * Train Acc 72.180
 * Val Acc 72.400, Total time 0.59
 * Val loss 0.781, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0080 (0.0080)	0.888 (0.888)	75.00 (75.00)
[10/157]	0.0977 (0.0927)	0.0580 (0.0549)	0.708 (0.881)	75.00 (71.59)
[20/157]	0.1017 (0.0954)	0.0604 (0.0575)	0.799 (0.847)	84.38 (72.47)
[30/157]	0.0976 (0.0960)	0.0589 (0.0580)	0.985 (0.867)	68.75 (72.08)
[40/157]	0.0995 (0.0966)	0.0603 (0.0585)	0.972 (0.857)	65.62 (72.03)
[50/157]	0.0987 (0.0969)	0.0598 (0.0589)	1.053 (0.859)	65.62 (72.18)
[60/157]	0.0983 (0.0971)	0.0593 (0.0591)	0.522 (0.838)	81.25 (72.85)
[70/157]	0.0967 (0.0971)	0.0587 (0.0591)	0.694 (0.840)	78.12 (72.67)
[80/157]	0.0984 (0.0971)	0.0598 (0.0592)	0.661 (0.848)	84.38 (72.57)
[90/157]	0.0987 (0.0972)	0.0600 (0.0593)	0.778 (0.840)	81.25 (72.97)
[100/157]	0.0998 (0.0973)	0.0612 (0.0594)	0.952 (0.847)	78.12 (73.02)
[110/157]	0.0992 (0.0974)	0.0596 (0.0595)	0.999 (0.854)	62.50 (72.72)
[120/157]	0.0961 (0.0974)	0.0586 (0.0595)	1.255 (0.860)	65.62 (72.55)
[130/157]	0.0985 (0.0974)	0.0601 (0.0596)	0.857 (0.859)	68.75 (72.57)
[140/157]	0.0980 (0.0974)	0.0599 (0.0596)	0.818 (0.867)	78.12 (72.43)
[150/157]	0.1001 (0.0975)	0.0594 (0.0596)	0.801 (0.862)	71.88 (72.62)
[156/157]	0.0787 (0.0974)	0.0541 (0.0596)	0.631 (0.861)	87.50 (72.66)
 * Train Acc 72.660
 * Val Acc 72.200, Total time 0.57
 * Val loss 0.779, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0081 (0.0081)	0.624 (0.624)	84.38 (84.38)
[10/157]	0.0989 (0.0926)	0.0604 (0.0547)	0.928 (0.869)	71.88 (73.30)
[20/157]	0.0978 (0.0951)	0.0594 (0.0571)	0.949 (0.899)	65.62 (72.32)
[30/157]	0.0978 (0.0960)	0.0597 (0.0581)	0.800 (0.894)	71.88 (71.88)
[40/157]	0.0989 (0.0965)	0.0596 (0.0587)	0.915 (0.896)	68.75 (71.80)
[50/157]	0.0989 (0.0968)	0.0593 (0.0588)	1.196 (0.895)	59.38 (71.94)
[60/157]	0.0995 (0.0970)	0.0603 (0.0590)	0.960 (0.890)	71.88 (71.88)
[70/157]	0.1009 (0.0972)	0.0614 (0.0591)	0.792 (0.875)	71.88 (72.36)
[80/157]	0.0985 (0.0972)	0.0592 (0.0592)	0.896 (0.870)	71.88 (72.49)
[90/157]	0.0993 (0.0974)	0.0599 (0.0593)	0.725 (0.868)	75.00 (72.32)
[100/157]	0.0995 (0.0974)	0.0607 (0.0593)	0.804 (0.870)	81.25 (72.31)
[110/157]	0.0965 (0.0975)	0.0590 (0.0594)	1.060 (0.862)	75.00 (72.66)
[120/157]	0.0986 (0.0975)	0.0601 (0.0595)	0.874 (0.870)	71.88 (72.42)
[130/157]	0.0989 (0.0976)	0.0601 (0.0595)	0.971 (0.865)	65.62 (72.71)
[140/157]	0.0968 (0.0976)	0.0587 (0.0596)	0.758 (0.861)	75.00 (72.81)
[150/157]	0.0998 (0.0977)	0.0611 (0.0596)	0.871 (0.858)	71.88 (72.87)
[156/157]	0.0833 (0.0976)	0.0556 (0.0597)	1.547 (0.859)	37.50 (72.86)
 * Train Acc 72.860
 * Val Acc 72.800, Total time 0.59
 * Val loss 0.775, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0092 (0.0092)	0.742 (0.742)	75.00 (75.00)
[10/157]	0.0991 (0.0925)	0.0603 (0.0548)	0.918 (0.845)	65.62 (73.58)
[20/157]	0.0985 (0.0951)	0.0588 (0.0570)	0.742 (0.828)	78.12 (74.11)
[30/157]	0.0983 (0.0961)	0.0598 (0.0579)	0.623 (0.832)	84.38 (73.69)
[40/157]	0.0988 (0.0966)	0.0599 (0.0583)	0.729 (0.812)	75.00 (73.86)
[50/157]	0.0993 (0.0969)	0.0603 (0.0587)	0.716 (0.810)	78.12 (74.08)
[60/157]	0.0960 (0.0970)	0.0565 (0.0588)	0.581 (0.809)	84.38 (74.13)
[70/157]	0.0992 (0.0971)	0.0604 (0.0590)	0.695 (0.814)	81.25 (74.25)
[80/157]	0.0992 (0.0973)	0.0603 (0.0591)	0.755 (0.818)	81.25 (74.19)
[90/157]	0.0992 (0.0973)	0.0606 (0.0592)	0.833 (0.832)	68.75 (73.39)
[100/157]	0.0992 (0.0974)	0.0601 (0.0593)	1.208 (0.842)	53.12 (72.96)
[110/157]	0.0990 (0.0974)	0.0605 (0.0594)	0.892 (0.850)	71.88 (72.64)
[120/157]	0.0982 (0.0974)	0.0597 (0.0594)	0.736 (0.847)	87.50 (72.83)
[130/157]	0.0978 (0.0974)	0.0594 (0.0594)	0.670 (0.849)	71.88 (72.66)
[140/157]	0.0977 (0.0975)	0.0583 (0.0595)	1.018 (0.852)	78.12 (72.54)
[150/157]	0.0979 (0.0976)	0.0587 (0.0595)	0.801 (0.852)	75.00 (72.72)
[156/157]	0.0806 (0.0975)	0.0549 (0.0594)	1.296 (0.853)	62.50 (72.74)
 * Train Acc 72.740
 * Val Acc 73.100, Total time 0.60
 * Val loss 0.781, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0094 (0.0094)	0.616 (0.616)	81.25 (81.25)
[10/157]	0.0988 (0.0927)	0.0590 (0.0548)	0.797 (0.884)	78.12 (72.73)
[20/157]	0.0989 (0.0952)	0.0597 (0.0572)	1.076 (0.878)	53.12 (71.58)
[30/157]	0.0997 (0.0962)	0.0603 (0.0580)	0.817 (0.842)	75.00 (72.98)
[40/157]	0.0985 (0.0967)	0.0595 (0.0585)	0.672 (0.846)	81.25 (72.79)
[50/157]	0.0975 (0.0970)	0.0589 (0.0587)	0.840 (0.843)	78.12 (73.16)
[60/157]	0.0972 (0.0972)	0.0578 (0.0589)	1.253 (0.864)	65.62 (72.28)
[70/157]	0.0977 (0.0973)	0.0589 (0.0590)	0.661 (0.871)	81.25 (72.23)
[80/157]	0.0974 (0.0973)	0.0592 (0.0591)	0.786 (0.867)	71.88 (72.45)
[90/157]	0.0982 (0.0974)	0.0600 (0.0591)	0.545 (0.867)	87.50 (72.36)
[100/157]	0.0966 (0.0974)	0.0584 (0.0592)	0.949 (0.867)	71.88 (72.34)
[110/157]	0.0990 (0.0974)	0.0601 (0.0593)	1.299 (0.873)	59.38 (72.13)
[120/157]	0.0989 (0.0975)	0.0600 (0.0594)	0.909 (0.867)	68.75 (72.39)
[130/157]	0.0992 (0.0975)	0.0606 (0.0594)	0.975 (0.859)	56.25 (72.47)
[140/157]	0.0982 (0.0975)	0.0599 (0.0595)	0.871 (0.861)	71.88 (72.34)
[150/157]	0.0960 (0.0975)	0.0590 (0.0595)	0.780 (0.858)	78.12 (72.56)
[156/157]	0.0787 (0.0974)	0.0539 (0.0595)	1.486 (0.856)	50.00 (72.70)
 * Train Acc 72.700
 * Val Acc 73.300, Total time 0.59
 * Val loss 0.772, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0083 (0.0083)	0.978 (0.978)	62.50 (62.50)
[10/157]	0.0973 (0.0923)	0.0592 (0.0546)	1.328 (0.899)	46.88 (68.18)
[20/157]	0.0987 (0.0950)	0.0607 (0.0571)	0.677 (0.889)	81.25 (70.83)
[30/157]	0.0978 (0.0959)	0.0599 (0.0579)	1.016 (0.886)	59.38 (71.47)
[40/157]	0.0966 (0.0964)	0.0589 (0.0584)	0.867 (0.863)	71.88 (72.64)
[50/157]	0.0984 (0.0966)	0.0601 (0.0587)	0.839 (0.829)	75.00 (74.14)
[60/157]	0.0971 (0.0968)	0.0589 (0.0589)	0.880 (0.842)	75.00 (73.92)
[70/157]	0.0971 (0.0970)	0.0593 (0.0591)	0.783 (0.840)	81.25 (73.90)
[80/157]	0.0979 (0.0971)	0.0596 (0.0591)	0.880 (0.839)	68.75 (73.80)
[90/157]	0.0989 (0.0972)	0.0606 (0.0592)	0.817 (0.828)	71.88 (74.24)
[100/157]	0.0992 (0.0972)	0.0600 (0.0593)	0.967 (0.840)	71.88 (73.67)
[110/157]	0.0972 (0.0972)	0.0590 (0.0593)	0.827 (0.842)	75.00 (73.48)
[120/157]	0.0981 (0.0972)	0.0602 (0.0594)	0.858 (0.845)	71.88 (73.45)
[130/157]	0.0982 (0.0973)	0.0603 (0.0594)	0.609 (0.848)	84.38 (73.19)
[140/157]	0.0991 (0.0973)	0.0604 (0.0595)	0.917 (0.855)	75.00 (72.92)
[150/157]	0.0978 (0.0973)	0.0596 (0.0595)	0.738 (0.860)	71.88 (72.85)
[156/157]	0.0802 (0.0972)	0.0556 (0.0595)	0.851 (0.863)	75.00 (72.72)
 * Train Acc 72.720
 * Val Acc 73.000, Total time 0.58
 * Val loss 0.774, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0083 (0.0083)	0.755 (0.755)	81.25 (81.25)
[10/157]	0.0971 (0.0919)	0.0599 (0.0550)	0.555 (0.707)	81.25 (77.56)
[20/157]	0.0973 (0.0948)	0.0598 (0.0576)	0.886 (0.776)	75.00 (75.45)
[30/157]	0.0969 (0.0957)	0.0584 (0.0584)	1.035 (0.785)	71.88 (74.80)
[40/157]	0.1001 (0.0962)	0.0604 (0.0588)	0.770 (0.824)	71.88 (73.55)
[50/157]	0.0989 (0.0965)	0.0592 (0.0590)	0.992 (0.823)	65.62 (73.59)
[60/157]	0.0957 (0.0966)	0.0585 (0.0591)	0.716 (0.837)	84.38 (73.31)
[70/157]	0.0981 (0.0968)	0.0604 (0.0592)	1.020 (0.836)	65.62 (73.15)
[80/157]	0.0984 (0.0969)	0.0604 (0.0593)	1.241 (0.844)	62.50 (73.07)
[90/157]	0.0990 (0.0971)	0.0603 (0.0594)	0.856 (0.850)	71.88 (72.66)
[100/157]	0.0979 (0.0971)	0.0594 (0.0594)	0.779 (0.856)	75.00 (72.59)
[110/157]	0.0987 (0.0971)	0.0608 (0.0594)	0.815 (0.859)	75.00 (72.61)
[120/157]	0.0958 (0.0972)	0.0572 (0.0595)	1.069 (0.868)	75.00 (72.57)
[130/157]	0.0984 (0.0972)	0.0595 (0.0595)	0.787 (0.867)	81.25 (72.47)
[140/157]	0.0968 (0.0972)	0.0591 (0.0595)	0.606 (0.864)	87.50 (72.67)
[150/157]	0.0984 (0.0972)	0.0595 (0.0595)	0.886 (0.860)	75.00 (72.62)
[156/157]	0.0808 (0.0971)	0.0557 (0.0595)	1.156 (0.862)	75.00 (72.60)
 * Train Acc 72.600
 * Val Acc 73.800, Total time 0.58
 * Val loss 0.775, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0086 (0.0086)	0.647 (0.647)	84.38 (84.38)
[10/157]	0.0991 (0.0924)	0.0601 (0.0550)	1.065 (0.892)	68.75 (69.89)
[20/157]	0.0969 (0.0948)	0.0589 (0.0573)	0.645 (0.862)	84.38 (72.17)
[30/157]	0.0971 (0.0955)	0.0592 (0.0582)	0.715 (0.849)	75.00 (73.59)
[40/157]	0.0987 (0.0961)	0.0601 (0.0587)	0.543 (0.823)	90.62 (74.09)
[50/157]	0.0979 (0.0964)	0.0598 (0.0589)	0.948 (0.813)	59.38 (74.20)
[60/157]	0.0987 (0.0966)	0.0602 (0.0591)	0.638 (0.808)	84.38 (74.39)
[70/157]	0.0981 (0.0968)	0.0586 (0.0592)	0.947 (0.827)	62.50 (73.46)
[80/157]	0.0980 (0.0968)	0.0600 (0.0592)	1.101 (0.840)	71.88 (73.26)
[90/157]	0.0986 (0.0969)	0.0602 (0.0593)	0.680 (0.835)	75.00 (73.45)
[100/157]	0.0980 (0.0970)	0.0594 (0.0594)	1.025 (0.852)	75.00 (72.93)
[110/157]	0.0979 (0.0970)	0.0597 (0.0594)	0.869 (0.858)	71.88 (72.66)
[120/157]	0.0974 (0.0971)	0.0593 (0.0594)	0.943 (0.862)	65.62 (72.47)
[130/157]	0.0965 (0.0971)	0.0585 (0.0595)	0.743 (0.857)	78.12 (72.64)
[140/157]	0.0981 (0.0972)	0.0601 (0.0595)	0.992 (0.863)	65.62 (72.54)
[150/157]	0.0982 (0.0972)	0.0604 (0.0595)	0.801 (0.862)	75.00 (72.56)
[156/157]	0.0790 (0.0971)	0.0531 (0.0595)	0.492 (0.860)	100.00 (72.64)
 * Train Acc 72.640
 * Val Acc 73.600, Total time 0.59
 * Val loss 0.778, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0090 (0.0090)	0.777 (0.777)	75.00 (75.00)
[10/157]	0.0973 (0.0918)	0.0594 (0.0546)	0.883 (0.927)	75.00 (70.17)
[20/157]	0.0966 (0.0946)	0.0585 (0.0571)	0.777 (0.941)	71.88 (67.86)
[30/157]	0.0966 (0.0956)	0.0587 (0.0580)	0.935 (0.901)	68.75 (70.16)
[40/157]	0.0984 (0.0962)	0.0590 (0.0584)	0.564 (0.890)	81.25 (71.34)
[50/157]	0.0983 (0.0965)	0.0599 (0.0585)	0.695 (0.888)	78.12 (71.08)
[60/157]	0.0994 (0.0967)	0.0603 (0.0587)	1.273 (0.888)	65.62 (71.21)
[70/157]	0.0972 (0.0968)	0.0586 (0.0588)	0.720 (0.882)	71.88 (71.35)
[80/157]	0.0987 (0.0969)	0.0594 (0.0589)	1.022 (0.880)	68.75 (71.53)
[90/157]	0.0974 (0.0970)	0.0597 (0.0590)	0.791 (0.867)	81.25 (72.08)
[100/157]	0.0961 (0.0971)	0.0586 (0.0591)	0.829 (0.875)	68.75 (71.94)
[110/157]	0.0979 (0.0971)	0.0603 (0.0592)	0.829 (0.867)	75.00 (72.35)
[120/157]	0.0979 (0.0972)	0.0604 (0.0592)	1.129 (0.868)	62.50 (72.29)
[130/157]	0.0960 (0.0970)	0.0581 (0.0591)	0.937 (0.861)	71.88 (72.64)
[140/157]	0.0961 (0.0968)	0.0588 (0.0590)	0.649 (0.861)	81.25 (72.47)
[150/157]	0.0966 (0.0968)	0.0573 (0.0590)	1.047 (0.852)	59.38 (72.81)
[156/157]	0.0771 (0.0966)	0.0524 (0.0589)	1.524 (0.857)	37.50 (72.72)
 * Train Acc 72.720
 * Val Acc 73.300, Total time 0.58
 * Val loss 0.777, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0084 (0.0084)	0.962 (0.962)	65.62 (65.62)
[10/157]	0.0973 (0.0915)	0.0578 (0.0541)	0.797 (0.919)	75.00 (71.88)
[20/157]	0.0932 (0.0930)	0.0558 (0.0556)	0.692 (0.873)	75.00 (73.36)
[30/157]	0.0962 (0.0937)	0.0581 (0.0564)	1.028 (0.857)	68.75 (73.39)
[40/157]	0.0961 (0.0940)	0.0581 (0.0568)	0.752 (0.842)	78.12 (73.70)
[50/157]	0.0968 (0.0942)	0.0585 (0.0571)	0.545 (0.829)	87.50 (73.90)
[60/157]	0.0945 (0.0943)	0.0569 (0.0572)	1.041 (0.842)	65.62 (73.26)
[70/157]	0.0944 (0.0960)	0.0574 (0.0588)	0.924 (0.838)	75.00 (73.37)
[80/157]	0.0954 (0.0959)	0.0580 (0.0587)	0.717 (0.844)	78.12 (73.38)
[90/157]	0.0955 (0.0958)	0.0582 (0.0587)	0.719 (0.839)	81.25 (73.66)
[100/157]	0.0934 (0.0957)	0.0571 (0.0586)	1.056 (0.845)	62.50 (73.39)
[110/157]	0.1023 (0.0967)	0.0654 (0.0595)	0.968 (0.838)	65.62 (73.87)
[120/157]	0.0967 (0.0965)	0.0586 (0.0593)	1.038 (0.845)	62.50 (73.48)
[130/157]	0.0955 (0.0964)	0.0579 (0.0593)	0.714 (0.843)	75.00 (73.43)
[140/157]	0.0970 (0.0963)	0.0592 (0.0592)	1.245 (0.844)	56.25 (73.36)
[150/157]	0.0946 (0.0962)	0.0576 (0.0591)	0.735 (0.847)	81.25 (73.26)
[156/157]	0.0787 (0.0961)	0.0537 (0.0591)	0.805 (0.847)	87.50 (73.28)
 * Train Acc 73.280
 * Val Acc 73.500, Total time 0.57
 * Val loss 0.776, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0086 (0.0086)	1.055 (1.055)	68.75 (68.75)
[10/157]	0.0967 (0.0912)	0.0581 (0.0542)	1.002 (0.843)	59.38 (71.31)
[20/157]	0.0988 (0.0938)	0.0602 (0.0567)	0.964 (0.898)	65.62 (70.54)
[30/157]	0.0970 (0.0947)	0.0593 (0.0576)	1.107 (0.896)	59.38 (70.87)
[40/157]	0.0979 (0.0952)	0.0589 (0.0581)	0.887 (0.866)	78.12 (71.88)
[50/157]	0.0958 (0.0956)	0.0589 (0.0584)	1.251 (0.845)	53.12 (72.37)
[60/157]	0.0980 (0.0958)	0.0594 (0.0586)	0.889 (0.845)	65.62 (72.34)
[70/157]	0.0978 (0.0959)	0.0600 (0.0587)	0.843 (0.843)	81.25 (72.49)
[80/157]	0.0962 (0.0960)	0.0584 (0.0588)	0.706 (0.836)	81.25 (73.30)
[90/157]	0.0985 (0.0961)	0.0596 (0.0588)	0.909 (0.834)	75.00 (73.73)
[100/157]	0.0983 (0.0962)	0.0605 (0.0589)	0.877 (0.842)	75.00 (73.58)
[110/157]	0.0976 (0.0963)	0.0590 (0.0590)	0.796 (0.843)	75.00 (73.65)
[120/157]	0.0971 (0.0963)	0.0589 (0.0590)	1.162 (0.845)	53.12 (73.58)
[130/157]	0.0967 (0.0964)	0.0578 (0.0590)	0.621 (0.844)	81.25 (73.57)
[140/157]	0.0984 (0.0964)	0.0599 (0.0589)	0.690 (0.840)	78.12 (73.65)
[150/157]	0.0974 (0.0964)	0.0586 (0.0590)	1.340 (0.843)	53.12 (73.57)
[156/157]	0.0803 (0.0963)	0.0544 (0.0590)	1.022 (0.850)	62.50 (73.24)
 * Train Acc 73.240
 * Val Acc 72.600, Total time 0.59
 * Val loss 0.775, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0087 (0.0087)	1.014 (1.014)	53.12 (53.12)
[10/157]	0.0980 (0.0910)	0.0603 (0.0541)	0.755 (0.910)	78.12 (69.89)
[20/157]	0.0961 (0.0937)	0.0588 (0.0564)	0.745 (0.856)	71.88 (71.88)
[30/157]	0.0991 (0.0949)	0.0601 (0.0574)	0.974 (0.857)	71.88 (72.58)
[40/157]	0.0958 (0.0954)	0.0580 (0.0577)	0.845 (0.852)	71.88 (72.56)
[50/157]	0.0978 (0.0956)	0.0591 (0.0580)	0.921 (0.844)	71.88 (72.79)
[60/157]	0.0962 (0.0958)	0.0583 (0.0582)	0.773 (0.845)	78.12 (73.05)
[70/157]	0.0977 (0.0960)	0.0578 (0.0584)	1.056 (0.864)	75.00 (72.54)
[80/157]	0.0975 (0.0962)	0.0592 (0.0585)	0.761 (0.858)	81.25 (72.65)
[90/157]	0.0982 (0.0962)	0.0603 (0.0586)	0.738 (0.855)	81.25 (72.97)
[100/157]	0.0962 (0.0963)	0.0585 (0.0587)	1.180 (0.858)	62.50 (72.87)
[110/157]	0.0957 (0.0963)	0.0582 (0.0588)	0.753 (0.861)	75.00 (73.00)
[120/157]	0.0987 (0.0963)	0.0598 (0.0588)	0.768 (0.858)	78.12 (73.01)
[130/157]	0.0966 (0.0964)	0.0595 (0.0589)	0.840 (0.852)	71.88 (73.16)
[140/157]	0.0983 (0.0964)	0.0596 (0.0589)	0.996 (0.850)	68.75 (73.23)
[150/157]	0.0977 (0.0965)	0.0596 (0.0590)	1.164 (0.852)	53.12 (73.10)
[156/157]	0.0784 (0.0964)	0.0539 (0.0590)	0.494 (0.851)	87.50 (73.20)
 * Train Acc 73.200
 * Val Acc 72.900, Total time 0.58
 * Val loss 0.769, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 6/27; cond: 354214.875, radio:6.213232700247318e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0041,  0.1075, -0.1582],
          [-0.1382, -0.0746,  0.0503],
          [ 0.0053,  0.1572, -0.0150]],

         [[ 0.0548, -0.0551, -0.0375],
          [-0.1811, -0.1278, -0.0790],
          [ 0.0137,  0.0788,  0.1180]],

         [[-0.1299, -0.0834,  0.0679],
          [ 0.1596, -0.0408,  0.1444],
          [-0.0282,  0.0217,  0.1758]]],


        [[[-0.1679, -0.1051, -0.0260],
          [-0.0668,  0.1738, -0.1109],
          [-0.0790, -0.1353, -0.1713]],

         [[-0.1063,  0.1791,  0.1033],
          [ 0.0972,  0.0174, -0.0890],
          [ 0.0342, -0.1828, -0.1367]],

         [[-0.0979,  0.1280,  0.1211],
          [-0.0830, -0.0015,  0.1285],
          [ 0.1914,  0.0734,  0.0265]]],


        [[[ 0.1332, -0.1136,  0.0365],
          [-0.1474, -0.1349, -0.0991],
          [ 0.0868,  0.0754, -0.1127]],

         [[ 0.0591,  0.1011, -0.0280],
          [ 0.0059,  0.0401,  0.1155],
          [ 0.1802, -0.1534, -0.0733]],

         [[ 0.0776,  0.1567,  0.1638],
          [ 0.1682,  0.0344, -0.1714],
          [ 0.0119, -0.1272, -0.1841]]],


        ...,


        [[[ 0.0681, -0.0765, -0.1204],
          [ 0.0766,  0.0334, -0.1412],
          [-0.1325,  0.0947,  0.1445]],

         [[ 0.0183, -0.0977,  0.1004],
          [ 0.0360, -0.1678, -0.1244],
          [ 0.0342,  0.1231, -0.0629]],

         [[ 0.0807, -0.1304,  0.1818],
          [ 0.1271,  0.0269,  0.0762],
          [ 0.0457, -0.1185, -0.0130]]],


        [[[ 0.0627, -0.0409,  0.1927],
          [-0.0630, -0.1656, -0.0868],
          [-0.1708,  0.1688, -0.0842]],

         [[-0.1291, -0.0211, -0.1255],
          [-0.0926,  0.1033, -0.1425],
          [ 0.0978,  0.1511,  0.0007]],

         [[ 0.0847, -0.1121,  0.1495],
          [ 0.1868,  0.0453,  0.1724],
          [-0.1951, -0.0196,  0.0319]]],


        [[[-0.1669,  0.0944, -0.0724],
          [-0.0707, -0.1545,  0.0819],
          [-0.1407, -0.0458,  0.0853]],

         [[ 0.1453,  0.0467, -0.0713],
          [-0.0838,  0.1103,  0.0391],
          [ 0.1598,  0.0671,  0.1516]],

         [[-0.1560,  0.1529, -0.0938],
          [-0.0551, -0.0043, -0.0431],
          [ 0.0333,  0.1608, -0.1261]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.0500e+06, 9.9180e+04, 9.7434e+04, 4.9987e+04, 3.6752e+04, 3.0073e+04,
        5.8652e+03, 3.8111e+03, 1.2581e+03, 1.2102e+03, 1.1537e+03, 1.1105e+03,
        4.1990e+02, 2.8894e+02, 2.8712e+02, 2.0266e+02, 1.8567e+02, 1.2968e+02,
        5.7928e+01, 5.2150e+01, 4.4394e+01, 2.9322e+01, 2.5703e+01, 1.0766e+01,
        9.5795e+00, 7.3828e+00, 2.9643e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 6]) 

NULL SPACE BASIS :  tensor([[ 2.0793e-01,  1.9510e-01,  1.4073e-01,  1.3150e-01, -1.2208e-01,
          8.9771e-02],
        [-3.7510e-01,  1.3713e-02, -2.5141e-01,  5.6473e-03,  2.2513e-01,
         -1.6256e-01],
        [ 1.8771e-01, -2.1090e-01,  1.3476e-01, -1.3798e-01, -1.2002e-01,
          8.9641e-02],
        [-1.3187e-02, -3.7854e-01, -2.4879e-03, -2.4443e-01,  2.2359e-01,
         -1.5965e-01],
        [ 3.4622e-03, -2.2861e-04, -6.5069e-03, -1.1690e-03, -4.0966e-01,
          2.8782e-01],
        [ 1.5261e-02,  3.8047e-01,  8.4066e-03,  2.4546e-01,  2.2079e-01,
         -1.5989e-01],
        [-1.9472e-01,  2.1116e-01, -1.3686e-01,  1.3666e-01, -1.2257e-01,
          8.8516e-02],
        [ 3.7437e-01, -1.5248e-02,  2.5693e-01, -4.8141e-03,  2.2551e-01,
         -1.6076e-01],
        [-2.0569e-01, -1.9552e-01, -1.4339e-01, -1.3085e-01, -1.2166e-01,
          8.8519e-02],
        [-2.7597e-03, -7.3308e-03, -2.4913e-01, -2.3987e-01,  6.8535e-05,
         -1.5334e-01],
        [ 3.2809e-03,  8.0478e-04,  4.4093e-01, -9.0952e-03,  4.0071e-04,
          2.7725e-01],
        [ 4.8315e-03,  8.6540e-03, -2.3756e-01,  2.5059e-01, -1.2903e-04,
         -1.5295e-01],
        [ 3.2158e-03,  1.2279e-02,  9.2025e-03,  4.4178e-01,  2.5885e-03,
          2.7140e-01],
        [-5.2896e-03,  5.4041e-05,  3.5973e-03,  3.1751e-04, -8.2885e-03,
         -4.8898e-01],
        [ 2.4274e-03, -1.5710e-02, -1.0512e-02, -4.4227e-01,  4.9446e-03,
          2.7178e-01],
        [ 3.4718e-03, -1.0865e-03,  2.3795e-01, -2.4859e-01, -3.8286e-03,
         -1.5022e-01],
        [-3.4021e-03,  9.4152e-04, -4.4386e-01,  1.0385e-02,  9.2854e-03,
          2.7290e-01],
        [-5.0845e-03,  1.4167e-03,  2.4934e-01,  2.3664e-01, -5.7037e-03,
         -1.5050e-01],
        [-2.3527e-01, -2.1281e-01,  1.2951e-01,  1.2918e-01,  1.3895e-01,
          7.5648e-02],
        [ 4.2516e-01, -1.7256e-02, -2.2672e-01,  4.2525e-03, -2.5672e-01,
         -1.3662e-01],
        [-2.2073e-01,  2.3018e-01,  1.2305e-01, -1.3438e-01,  1.3720e-01,
          7.5328e-02],
        [ 1.6513e-02,  4.1320e-01, -7.2657e-03, -2.3523e-01, -2.5646e-01,
         -1.3325e-01],
        [-6.4194e-03,  2.0216e-03,  2.6908e-03,  9.6179e-04,  4.7376e-01,
          2.4017e-01],
        [-1.4850e-02, -4.1363e-01,  2.4113e-03,  2.3465e-01, -2.5672e-01,
         -1.3332e-01],
        [ 2.1440e-01, -2.3738e-01, -1.2146e-01,  1.3297e-01,  1.4323e-01,
          7.3510e-02],
        [-4.1546e-01,  1.5021e-02,  2.2436e-01, -6.4626e-03, -2.6631e-01,
         -1.3367e-01],
        [ 2.3655e-01,  2.2062e-01, -1.2651e-01, -1.2587e-01,  1.4470e-01,
          7.3746e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0577, -0.0621,  0.0087, -0.0615,  0.0308,  0.0284,  0.0091,  0.0279,
         -0.0390, -0.0336,  0.0353, -0.0047,  0.0353, -0.0177, -0.0157, -0.0048,
         -0.0156,  0.0215, -0.0267,  0.0297, -0.0046,  0.0292, -0.0150, -0.0136,
         -0.0050, -0.0132,  0.0191],
        [-0.0621,  0.1148, -0.0611,  0.0307, -0.0566,  0.0304,  0.0282, -0.0524,
          0.0278,  0.0356, -0.0641,  0.0344, -0.0181,  0.0321, -0.0180, -0.0159,
          0.0288, -0.0148,  0.0296, -0.0565,  0.0298, -0.0147,  0.0283, -0.0145,
         -0.0131,  0.0251, -0.0139],
        [ 0.0087, -0.0611,  0.0569,  0.0284,  0.0306, -0.0616, -0.0391,  0.0275,
          0.0098, -0.0054,  0.0351, -0.0332, -0.0154, -0.0177,  0.0356,  0.0221,
         -0.0158, -0.0054, -0.0039,  0.0289, -0.0264, -0.0138, -0.0150,  0.0291,
          0.0184, -0.0123, -0.0051],
        [-0.0615,  0.0307,  0.0284,  0.1138, -0.0560, -0.0528, -0.0620,  0.0316,
          0.0277,  0.0353, -0.0177, -0.0162, -0.0635,  0.0311,  0.0293,  0.0342,
         -0.0177, -0.0148,  0.0289, -0.0143, -0.0135, -0.0552,  0.0272,  0.0258,
          0.0307, -0.0153, -0.0143],
        [ 0.0308, -0.0566,  0.0306, -0.0560,  0.1024, -0.0559,  0.0309, -0.0568,
          0.0309, -0.0173,  0.0313, -0.0174,  0.0312, -0.0561,  0.0314, -0.0175,
          0.0317, -0.0175, -0.0151,  0.0281, -0.0147,  0.0274, -0.0510,  0.0272,
         -0.0147,  0.0277, -0.0148],
        [ 0.0284,  0.0304, -0.0616, -0.0528, -0.0559,  0.1142,  0.0280,  0.0312,
         -0.0620, -0.0160, -0.0173,  0.0356,  0.0287,  0.0312, -0.0641, -0.0148,
         -0.0173,  0.0341, -0.0135, -0.0146,  0.0288,  0.0263,  0.0274, -0.0553,
         -0.0145, -0.0154,  0.0309],
        [ 0.0091,  0.0282, -0.0391, -0.0620,  0.0309,  0.0280,  0.0583, -0.0628,
          0.0095, -0.0054, -0.0153,  0.0221,  0.0346, -0.0170, -0.0161, -0.0328,
          0.0351, -0.0054, -0.0039, -0.0145,  0.0189,  0.0296, -0.0144, -0.0135,
         -0.0278,  0.0299, -0.0043],
        [ 0.0279, -0.0524,  0.0275,  0.0316, -0.0568,  0.0312, -0.0628,  0.1156,
         -0.0620, -0.0160,  0.0286, -0.0147, -0.0171,  0.0309, -0.0171,  0.0355,
         -0.0642,  0.0343, -0.0135,  0.0266, -0.0143, -0.0152,  0.0271, -0.0148,
          0.0296, -0.0558,  0.0302],
        [-0.0390,  0.0278,  0.0098,  0.0277,  0.0309, -0.0620,  0.0095, -0.0620,
          0.0575,  0.0227, -0.0157, -0.0061, -0.0157, -0.0170,  0.0349, -0.0061,
          0.0350, -0.0321,  0.0181, -0.0135, -0.0040, -0.0135, -0.0147,  0.0295,
         -0.0035,  0.0293, -0.0279],
        [-0.0336,  0.0356, -0.0054,  0.0353, -0.0173, -0.0160, -0.0054, -0.0160,
          0.0227,  0.0585, -0.0613,  0.0092, -0.0612,  0.0302,  0.0274,  0.0095,
          0.0270, -0.0391, -0.0297,  0.0308, -0.0045,  0.0309, -0.0154, -0.0136,
         -0.0048, -0.0134,  0.0197],
        [ 0.0353, -0.0641,  0.0351, -0.0177,  0.0313, -0.0173, -0.0153,  0.0286,
         -0.0157, -0.0613,  0.1108, -0.0610,  0.0307, -0.0547,  0.0305,  0.0268,
         -0.0491,  0.0270,  0.0310, -0.0558,  0.0310, -0.0154,  0.0277, -0.0157,
         -0.0138,  0.0247, -0.0135],
        [-0.0047,  0.0344, -0.0332, -0.0162, -0.0174,  0.0356,  0.0221, -0.0147,
         -0.0061,  0.0092, -0.0610,  0.0583,  0.0274,  0.0302, -0.0612, -0.0391,
          0.0271,  0.0094, -0.0053,  0.0317, -0.0300, -0.0135, -0.0152,  0.0306,
          0.0204, -0.0148, -0.0040],
        [ 0.0353, -0.0181, -0.0154, -0.0635,  0.0312,  0.0287,  0.0346, -0.0171,
         -0.0157, -0.0612,  0.0307,  0.0274,  0.1099, -0.0541, -0.0498, -0.0606,
          0.0305,  0.0269,  0.0309, -0.0150, -0.0144, -0.0554,  0.0273,  0.0252,
          0.0309, -0.0159, -0.0134],
        [-0.0177,  0.0321, -0.0177,  0.0311, -0.0561,  0.0312, -0.0170,  0.0309,
         -0.0170,  0.0302, -0.0547,  0.0302, -0.0541,  0.0977, -0.0543,  0.0303,
         -0.0552,  0.0305, -0.0149,  0.0269, -0.0149,  0.0274, -0.0495,  0.0275,
         -0.0158,  0.0288, -0.0159],
        [-0.0157, -0.0180,  0.0356,  0.0293,  0.0314, -0.0641, -0.0161, -0.0171,
          0.0349,  0.0274,  0.0305, -0.0612, -0.0498, -0.0543,  0.1102,  0.0272,
          0.0303, -0.0605, -0.0141, -0.0149,  0.0307,  0.0246,  0.0274, -0.0551,
         -0.0133, -0.0157,  0.0306],
        [-0.0048, -0.0159,  0.0221,  0.0342, -0.0175, -0.0148, -0.0328,  0.0355,
         -0.0061,  0.0095,  0.0268, -0.0391, -0.0606,  0.0303,  0.0272,  0.0576,
         -0.0609,  0.0094, -0.0056, -0.0131,  0.0203,  0.0316, -0.0153, -0.0148,
         -0.0296,  0.0305, -0.0040],
        [-0.0156,  0.0288, -0.0158, -0.0177,  0.0317, -0.0173,  0.0351, -0.0642,
          0.0350,  0.0270, -0.0491,  0.0271,  0.0305, -0.0552,  0.0303, -0.0609,
          0.1109, -0.0610, -0.0137,  0.0243, -0.0136, -0.0154,  0.0281, -0.0154,
          0.0309, -0.0560,  0.0309],
        [ 0.0215, -0.0148, -0.0054, -0.0148, -0.0175,  0.0341, -0.0054,  0.0343,
         -0.0321, -0.0391,  0.0270,  0.0094,  0.0269,  0.0305, -0.0605,  0.0094,
         -0.0610,  0.0575,  0.0211, -0.0146, -0.0048, -0.0145, -0.0155,  0.0315,
         -0.0050,  0.0319, -0.0303],
        [-0.0267,  0.0296, -0.0039,  0.0289, -0.0151, -0.0135, -0.0039, -0.0135,
          0.0181, -0.0297,  0.0310, -0.0053,  0.0309, -0.0149, -0.0141, -0.0056,
         -0.0137,  0.0211,  0.0650, -0.0699,  0.0107, -0.0689,  0.0349,  0.0312,
          0.0110,  0.0309, -0.0447],
        [ 0.0297, -0.0565,  0.0289, -0.0143,  0.0281, -0.0146, -0.0145,  0.0266,
         -0.0135,  0.0308, -0.0558,  0.0317, -0.0150,  0.0269, -0.0149, -0.0131,
          0.0243, -0.0146, -0.0699,  0.1294, -0.0701,  0.0345, -0.0644,  0.0349,
          0.0312, -0.0576,  0.0317],
        [-0.0046,  0.0298, -0.0264, -0.0135, -0.0147,  0.0288,  0.0189, -0.0143,
         -0.0040, -0.0045,  0.0310, -0.0300, -0.0144, -0.0149,  0.0307,  0.0203,
         -0.0136, -0.0048,  0.0107, -0.0701,  0.0651,  0.0314,  0.0348, -0.0688,
         -0.0447,  0.0314,  0.0103],
        [ 0.0292, -0.0147, -0.0138, -0.0552,  0.0274,  0.0263,  0.0296, -0.0152,
         -0.0135,  0.0309, -0.0154, -0.0135, -0.0554,  0.0274,  0.0246,  0.0316,
         -0.0154, -0.0145, -0.0689,  0.0345,  0.0314,  0.1265, -0.0625, -0.0583,
         -0.0700,  0.0348,  0.0321],
        [-0.0150,  0.0283, -0.0150,  0.0272, -0.0510,  0.0274, -0.0144,  0.0271,
         -0.0147, -0.0154,  0.0277, -0.0152,  0.0273, -0.0495,  0.0274, -0.0153,
          0.0281, -0.0155,  0.0349, -0.0644,  0.0348, -0.0625,  0.1152, -0.0629,
          0.0341, -0.0633,  0.0346],
        [-0.0136, -0.0145,  0.0291,  0.0258,  0.0272, -0.0553, -0.0135, -0.0148,
          0.0295, -0.0136, -0.0157,  0.0306,  0.0252,  0.0275, -0.0551, -0.0148,
         -0.0154,  0.0315,  0.0312,  0.0349, -0.0688, -0.0583, -0.0629,  0.1266,
          0.0324,  0.0348, -0.0700],
        [-0.0050, -0.0131,  0.0184,  0.0307, -0.0147, -0.0145, -0.0278,  0.0296,
         -0.0035, -0.0048, -0.0138,  0.0204,  0.0309, -0.0158, -0.0133, -0.0296,
          0.0309, -0.0050,  0.0110,  0.0312, -0.0447, -0.0700,  0.0341,  0.0324,
          0.0656, -0.0689,  0.0094],
        [-0.0132,  0.0251, -0.0123, -0.0153,  0.0277, -0.0154,  0.0299, -0.0558,
          0.0293, -0.0134,  0.0247, -0.0148, -0.0159,  0.0288, -0.0157,  0.0305,
         -0.0560,  0.0319,  0.0309, -0.0576,  0.0314,  0.0348, -0.0633,  0.0348,
         -0.0689,  0.1274, -0.0698],
        [ 0.0191, -0.0139, -0.0051, -0.0143, -0.0148,  0.0309, -0.0043,  0.0302,
         -0.0279,  0.0197, -0.0135, -0.0040, -0.0134, -0.0159,  0.0306, -0.0040,
          0.0309, -0.0303, -0.0447,  0.0317,  0.0103,  0.0321,  0.0346, -0.0700,
          0.0094, -0.0698,  0.0665]], device='cuda:0') 

reserving basis 75/576; cond: 9591057.0, radio:3.698172076838091e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1387e-02, -3.9928e-02, -7.1375e-03],
          [ 3.8501e-02,  2.6600e-02,  3.1534e-02],
          [ 2.2073e-02, -1.7193e-03,  8.7359e-03]],

         [[-1.4135e-02,  3.7760e-02,  1.8634e-02],
          [-2.9913e-02,  3.6982e-02, -3.3397e-03],
          [-7.0080e-03, -1.7042e-02, -2.0323e-02]],

         [[-3.0181e-03, -2.0893e-02, -3.6732e-02],
          [-2.9195e-02, -1.0244e-02,  3.3236e-02],
          [ 2.0504e-02, -3.1931e-02, -9.3534e-03]],

         ...,

         [[-2.5469e-02,  1.1641e-02, -1.0167e-06],
          [ 1.4883e-02, -6.5767e-03, -9.4300e-03],
          [-2.0622e-03,  8.6980e-03, -3.1126e-02]],

         [[-4.1550e-02,  1.3741e-02, -3.4918e-02],
          [-6.6807e-03,  1.2126e-02,  1.0721e-03],
          [ 2.0794e-02,  4.3203e-05,  4.1699e-03]],

         [[-1.6036e-02,  2.0101e-03,  1.1469e-02],
          [ 2.3213e-02, -3.9582e-02, -1.6235e-02],
          [-4.1602e-02, -1.1472e-02,  2.9657e-02]]],


        [[[-4.2274e-02, -4.7226e-02,  1.4272e-02],
          [-2.3754e-02,  2.0064e-02,  2.7538e-03],
          [ 8.1641e-03,  2.3351e-02, -4.2292e-02]],

         [[ 3.2926e-02,  7.9953e-03, -3.0228e-02],
          [ 1.1824e-02, -1.1383e-02, -1.7508e-02],
          [ 3.4490e-03, -1.1637e-02,  4.0717e-02]],

         [[ 2.6368e-02, -1.6294e-02, -4.4938e-02],
          [-1.7073e-02, -1.1224e-02,  1.0165e-02],
          [-1.2373e-02, -2.2796e-03, -6.4413e-03]],

         ...,

         [[-5.3277e-03, -3.8723e-03,  8.7024e-03],
          [ 1.5609e-02, -3.3912e-02,  1.7254e-02],
          [ 3.3157e-02,  7.4407e-03,  3.0840e-02]],

         [[-2.1761e-02,  6.7552e-03, -4.6461e-04],
          [-1.1127e-02,  2.3802e-02, -4.6492e-02],
          [-9.9333e-04, -1.4425e-02, -2.0109e-02]],

         [[-1.4778e-02, -3.2128e-02,  4.3081e-03],
          [-6.8132e-03, -3.1534e-02, -8.1263e-03],
          [-8.7357e-03,  1.5976e-02, -4.1658e-02]]],


        [[[-3.2245e-03,  1.8439e-02,  2.0699e-02],
          [ 8.0015e-03,  1.3104e-02,  4.1584e-02],
          [-1.3315e-02,  2.5478e-02,  3.2477e-02]],

         [[-3.1278e-02,  1.9904e-02,  2.8958e-02],
          [-2.2763e-03,  3.2329e-02,  6.1863e-03],
          [ 3.5333e-02,  6.7760e-03, -2.0944e-02]],

         [[ 3.8816e-02,  1.5046e-02,  3.4576e-02],
          [ 1.8294e-02,  2.5774e-02,  2.0018e-02],
          [ 1.6680e-02, -5.8853e-03,  9.4894e-03]],

         ...,

         [[ 2.1146e-02, -1.6936e-02,  4.4129e-02],
          [-3.3304e-03,  3.8407e-02, -3.4306e-03],
          [ 2.4491e-02,  3.3287e-02, -3.6213e-02]],

         [[-2.0646e-02, -3.6410e-02,  3.6742e-02],
          [ 4.5241e-02, -1.3519e-02, -1.8864e-02],
          [ 1.1947e-02,  3.1047e-02,  1.3881e-02]],

         [[-1.4906e-02, -2.3189e-02,  3.0055e-02],
          [-2.9713e-02, -4.1749e-02, -2.1314e-02],
          [ 3.5664e-03,  5.9233e-03,  6.1547e-03]]],


        ...,


        [[[-4.4312e-02, -1.8249e-02,  9.4577e-03],
          [ 2.4378e-02, -1.7199e-02, -8.6944e-03],
          [-4.2913e-03,  2.0996e-02, -3.4646e-02]],

         [[-4.1707e-02,  2.7720e-02,  5.7187e-03],
          [-1.3502e-02, -1.1855e-02,  8.7675e-03],
          [-1.9482e-02,  2.8461e-02,  1.2195e-02]],

         [[ 2.4044e-02,  3.2048e-02,  1.0501e-03],
          [ 4.7840e-02, -7.7794e-03, -3.4051e-02],
          [ 3.8587e-02, -1.4883e-02, -3.0806e-03]],

         ...,

         [[-2.5821e-02, -5.3259e-02,  2.3089e-02],
          [-2.0489e-02,  2.2704e-02, -4.7221e-02],
          [ 1.6425e-02,  4.0689e-02,  2.7455e-02]],

         [[-4.6393e-02, -5.5637e-02,  1.6248e-02],
          [-1.3045e-02, -1.4830e-02, -6.4699e-03],
          [-2.0938e-02,  9.7880e-03, -9.3219e-03]],

         [[ 3.6323e-02, -2.2428e-02,  4.1184e-02],
          [ 9.4667e-03, -5.7117e-03,  4.7759e-02],
          [ 3.1297e-02,  2.7819e-02,  4.8170e-02]]],


        [[[ 2.5209e-02,  1.4100e-02, -1.3631e-02],
          [-1.3564e-02, -3.1990e-02, -2.0615e-02],
          [ 1.6032e-02, -5.8669e-02, -4.2064e-02]],

         [[-8.2959e-03,  2.0702e-02,  9.0649e-03],
          [ 1.0089e-02,  1.1227e-03, -4.0757e-02],
          [-9.4504e-03,  3.7889e-02,  1.2389e-02]],

         [[-9.3928e-03,  8.4932e-03,  4.0433e-02],
          [ 4.5760e-04,  5.2701e-03,  3.0365e-02],
          [-3.1444e-02,  2.5873e-02,  3.2508e-02]],

         ...,

         [[ 4.0229e-02,  1.9573e-02,  3.8779e-02],
          [-3.5747e-02,  9.9672e-03,  7.9383e-04],
          [-1.2903e-03, -4.1652e-02, -3.9227e-02]],

         [[ 1.6725e-02,  2.3354e-02, -9.2385e-03],
          [ 2.6068e-02,  8.0689e-03,  1.8203e-02],
          [-4.8345e-02, -2.4963e-02, -5.2705e-03]],

         [[ 2.7800e-02,  2.5210e-02, -3.1746e-02],
          [-3.3105e-02, -2.8779e-02, -1.2071e-02],
          [-8.0740e-03, -3.9894e-02,  2.2211e-02]]],


        [[[ 2.7818e-02,  9.9671e-03, -1.6659e-02],
          [-6.0955e-03,  6.7117e-03, -1.2699e-02],
          [-1.9247e-03, -3.5043e-02, -7.6713e-03]],

         [[-4.3776e-02, -2.8494e-02, -2.9508e-02],
          [ 1.0676e-02, -5.6559e-02, -1.1518e-03],
          [ 2.6805e-02, -3.5934e-02, -6.9826e-03]],

         [[-1.7214e-02,  1.3834e-02,  2.1983e-02],
          [ 1.5374e-02, -1.3707e-02,  1.0841e-02],
          [-2.2012e-02,  3.5966e-02,  3.5426e-03]],

         ...,

         [[-2.1422e-02, -9.0184e-03,  9.9929e-03],
          [ 4.1387e-03,  3.7307e-02, -4.2788e-02],
          [-2.2964e-02,  1.5683e-02,  2.4674e-02]],

         [[-3.1360e-02, -3.8669e-02,  5.5207e-03],
          [-1.9697e-02, -1.0651e-02, -4.8782e-02],
          [-2.8299e-02, -5.1219e-02, -3.9144e-02]],

         [[-4.0457e-03, -1.0372e-02,  3.1078e-02],
          [ 3.4013e-02,  4.4608e-03,  3.5737e-02],
          [-2.8891e-02,  4.5875e-02, -3.2538e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.3023e+07, 1.0191e+06, 8.1034e+05, 7.6637e+05, 5.8232e+05, 4.1389e+05,
        3.9487e+05, 3.6067e+05, 3.0573e+05, 2.5210e+05, 2.0992e+05, 1.8896e+05,
        1.5625e+05, 1.1727e+05, 7.5111e+04, 6.1807e+04, 5.9389e+04, 5.2614e+04,
        4.7187e+04, 4.1416e+04, 3.9800e+04, 3.2881e+04, 2.7036e+04, 2.3834e+04,
        2.3206e+04, 2.2022e+04, 2.0126e+04, 1.9054e+04, 1.8483e+04, 1.7733e+04,
        1.6850e+04, 1.4453e+04, 1.3950e+04, 1.2820e+04, 1.2613e+04, 1.1307e+04,
        1.0443e+04, 9.6790e+03, 9.3872e+03, 8.2990e+03, 8.1058e+03, 7.6556e+03,
        7.4496e+03, 7.0420e+03, 6.9934e+03, 6.5657e+03, 6.1648e+03, 5.9074e+03,
        5.6914e+03, 5.3648e+03, 5.0806e+03, 4.9694e+03, 4.7078e+03, 4.6479e+03,
        4.4391e+03, 4.2381e+03, 4.0985e+03, 3.9476e+03, 3.8679e+03, 3.8302e+03,
        3.6641e+03, 3.5273e+03, 3.4324e+03, 3.3902e+03, 3.2817e+03, 3.1929e+03,
        3.1672e+03, 3.1061e+03, 3.0761e+03, 2.9409e+03, 2.8480e+03, 2.8119e+03,
        2.6984e+03, 2.6398e+03, 2.5524e+03, 2.5070e+03, 2.4418e+03, 2.3589e+03,
        2.2859e+03, 2.2648e+03, 2.1781e+03, 2.1633e+03, 2.0737e+03, 2.0578e+03,
        1.9810e+03, 1.9027e+03, 1.8704e+03, 1.8518e+03, 1.7775e+03, 1.7523e+03,
        1.7409e+03, 1.6882e+03, 1.6521e+03, 1.6151e+03, 1.5145e+03, 1.4874e+03,
        1.4412e+03, 1.4140e+03, 1.3959e+03, 1.3671e+03, 1.3549e+03, 1.3285e+03,
        1.2969e+03, 1.2560e+03, 1.2313e+03, 1.1818e+03, 1.1657e+03, 1.1337e+03,
        1.1281e+03, 1.1050e+03, 1.0817e+03, 1.0408e+03, 1.0108e+03, 1.0077e+03,
        9.9804e+02, 9.6594e+02, 9.5229e+02, 9.4023e+02, 9.1089e+02, 8.9684e+02,
        8.9150e+02, 8.8806e+02, 8.6839e+02, 8.3007e+02, 8.2714e+02, 8.1606e+02,
        7.9931e+02, 7.7273e+02, 7.7037e+02, 7.6498e+02, 7.5175e+02, 7.3588e+02,
        7.2310e+02, 7.1887e+02, 7.0081e+02, 6.9291e+02, 6.8494e+02, 6.6395e+02,
        6.6174e+02, 6.4605e+02, 6.4387e+02, 6.2435e+02, 6.2042e+02, 6.2029e+02,
        6.0028e+02, 5.9937e+02, 5.9062e+02, 5.7949e+02, 5.7544e+02, 5.6717e+02,
        5.5787e+02, 5.4998e+02, 5.4394e+02, 5.3417e+02, 5.2907e+02, 5.1763e+02,
        5.1251e+02, 5.0780e+02, 4.9469e+02, 4.9040e+02, 4.8464e+02, 4.8278e+02,
        4.7134e+02, 4.6661e+02, 4.5502e+02, 4.4824e+02, 4.4436e+02, 4.4061e+02,
        4.3038e+02, 4.2774e+02, 4.1784e+02, 4.1595e+02, 4.0733e+02, 3.9885e+02,
        3.9327e+02, 3.8891e+02, 3.8636e+02, 3.7942e+02, 3.7485e+02, 3.7262e+02,
        3.7135e+02, 3.6556e+02, 3.6115e+02, 3.5760e+02, 3.5275e+02, 3.4838e+02,
        3.4084e+02, 3.3594e+02, 3.3475e+02, 3.3012e+02, 3.2825e+02, 3.2421e+02,
        3.2085e+02, 3.1895e+02, 3.1025e+02, 3.0832e+02, 3.0472e+02, 2.9877e+02,
        2.9751e+02, 2.9495e+02, 2.9030e+02, 2.8787e+02, 2.8597e+02, 2.8154e+02,
        2.7587e+02, 2.7479e+02, 2.7038e+02, 2.6741e+02, 2.6528e+02, 2.6397e+02,
        2.6145e+02, 2.5734e+02, 2.5513e+02, 2.5499e+02, 2.5254e+02, 2.4615e+02,
        2.4361e+02, 2.4074e+02, 2.3917e+02, 2.3748e+02, 2.3464e+02, 2.3383e+02,
        2.3128e+02, 2.2640e+02, 2.2569e+02, 2.2161e+02, 2.2013e+02, 2.1594e+02,
        2.1344e+02, 2.1135e+02, 2.0954e+02, 2.0867e+02, 2.0809e+02, 2.0561e+02,
        2.0499e+02, 1.9940e+02, 1.9799e+02, 1.9497e+02, 1.9390e+02, 1.9366e+02,
        1.9125e+02, 1.8822e+02, 1.8763e+02, 1.8539e+02, 1.8457e+02, 1.8236e+02,
        1.7927e+02, 1.7832e+02, 1.7743e+02, 1.7379e+02, 1.7334e+02, 1.7290e+02,
        1.7008e+02, 1.6876e+02, 1.6657e+02, 1.6569e+02, 1.6345e+02, 1.6270e+02,
        1.6219e+02, 1.5941e+02, 1.5862e+02, 1.5512e+02, 1.5308e+02, 1.5251e+02,
        1.5135e+02, 1.5021e+02, 1.4866e+02, 1.4841e+02, 1.4668e+02, 1.4520e+02,
        1.4494e+02, 1.4353e+02, 1.4248e+02, 1.4105e+02, 1.3981e+02, 1.3790e+02,
        1.3712e+02, 1.3567e+02, 1.3492e+02, 1.3332e+02, 1.3299e+02, 1.3178e+02,
        1.3070e+02, 1.2943e+02, 1.2765e+02, 1.2722e+02, 1.2565e+02, 1.2536e+02,
        1.2438e+02, 1.2215e+02, 1.2177e+02, 1.2053e+02, 1.1934e+02, 1.1772e+02,
        1.1680e+02, 1.1608e+02, 1.1571e+02, 1.1444e+02, 1.1406e+02, 1.1305e+02,
        1.1198e+02, 1.1121e+02, 1.1069e+02, 1.0877e+02, 1.0848e+02, 1.0818e+02,
        1.0745e+02, 1.0635e+02, 1.0603e+02, 1.0464e+02, 1.0370e+02, 1.0312e+02,
        1.0232e+02, 1.0141e+02, 1.0050e+02, 9.9241e+01, 9.8898e+01, 9.8068e+01,
        9.6664e+01, 9.6345e+01, 9.4545e+01, 9.4169e+01, 9.3627e+01, 9.2696e+01,
        9.1883e+01, 9.1750e+01, 9.0968e+01, 9.0570e+01, 9.0185e+01, 8.8880e+01,
        8.8675e+01, 8.7830e+01, 8.7416e+01, 8.6543e+01, 8.5502e+01, 8.5442e+01,
        8.4309e+01, 8.3272e+01, 8.2869e+01, 8.1892e+01, 8.1311e+01, 8.0660e+01,
        8.0276e+01, 7.9711e+01, 7.9182e+01, 7.8610e+01, 7.7825e+01, 7.7170e+01,
        7.7126e+01, 7.6128e+01, 7.5682e+01, 7.4899e+01, 7.4558e+01, 7.4015e+01,
        7.3013e+01, 7.2825e+01, 7.2046e+01, 7.1900e+01, 7.1551e+01, 7.0721e+01,
        7.0103e+01, 6.9807e+01, 6.9452e+01, 6.8639e+01, 6.7716e+01, 6.7264e+01,
        6.6992e+01, 6.6391e+01, 6.6083e+01, 6.5433e+01, 6.5148e+01, 6.4227e+01,
        6.4004e+01, 6.3753e+01, 6.3095e+01, 6.3023e+01, 6.2820e+01, 6.2430e+01,
        6.1943e+01, 6.1542e+01, 6.1146e+01, 6.0776e+01, 6.0164e+01, 5.9694e+01,
        5.8951e+01, 5.8595e+01, 5.8485e+01, 5.8457e+01, 5.7686e+01, 5.7542e+01,
        5.7080e+01, 5.6572e+01, 5.6240e+01, 5.5665e+01, 5.5256e+01, 5.5173e+01,
        5.4407e+01, 5.4016e+01, 5.3607e+01, 5.3202e+01, 5.2862e+01, 5.2404e+01,
        5.2202e+01, 5.1812e+01, 5.1538e+01, 5.1396e+01, 5.0954e+01, 5.0893e+01,
        5.0731e+01, 5.0211e+01, 4.9736e+01, 4.9475e+01, 4.9060e+01, 4.8950e+01,
        4.8420e+01, 4.7535e+01, 4.7349e+01, 4.7125e+01, 4.6882e+01, 4.6413e+01,
        4.6156e+01, 4.5443e+01, 4.5161e+01, 4.4934e+01, 4.4547e+01, 4.4433e+01,
        4.4157e+01, 4.3867e+01, 4.3552e+01, 4.3471e+01, 4.3039e+01, 4.2783e+01,
        4.2037e+01, 4.1559e+01, 4.1352e+01, 4.1054e+01, 4.0715e+01, 4.0537e+01,
        4.0097e+01, 3.9912e+01, 3.9744e+01, 3.9297e+01, 3.9021e+01, 3.8848e+01,
        3.8488e+01, 3.8318e+01, 3.8073e+01, 3.7722e+01, 3.7305e+01, 3.7180e+01,
        3.6978e+01, 3.6855e+01, 3.6450e+01, 3.5868e+01, 3.5644e+01, 3.5593e+01,
        3.5363e+01, 3.5056e+01, 3.4593e+01, 3.4323e+01, 3.4057e+01, 3.3814e+01,
        3.3430e+01, 3.3355e+01, 3.3291e+01, 3.3092e+01, 3.2794e+01, 3.2689e+01,
        3.2227e+01, 3.1975e+01, 3.1590e+01, 3.1316e+01, 3.1096e+01, 3.0814e+01,
        3.0390e+01, 3.0074e+01, 2.9766e+01, 2.9632e+01, 2.9339e+01, 2.9231e+01,
        2.9148e+01, 2.9083e+01, 2.8507e+01, 2.8411e+01, 2.8101e+01, 2.7940e+01,
        2.7527e+01, 2.7399e+01, 2.7324e+01, 2.7143e+01, 2.6806e+01, 2.6507e+01,
        2.6123e+01, 2.5841e+01, 2.5679e+01, 2.5373e+01, 2.5102e+01, 2.4739e+01,
        2.4598e+01, 2.4497e+01, 2.4258e+01, 2.3916e+01, 2.3766e+01, 2.3539e+01,
        2.3425e+01, 2.3071e+01, 2.2900e+01, 2.2728e+01, 2.2509e+01, 2.2492e+01,
        2.1980e+01, 2.1841e+01, 2.1544e+01, 2.1410e+01, 2.1020e+01, 2.0707e+01,
        2.0516e+01, 2.0441e+01, 2.0277e+01, 1.9813e+01, 1.9667e+01, 1.9347e+01,
        1.9045e+01, 1.8839e+01, 1.8617e+01, 1.8569e+01, 1.8069e+01, 1.7769e+01,
        1.7535e+01, 1.7273e+01, 1.7091e+01, 1.6679e+01, 1.6318e+01, 1.6059e+01,
        1.6029e+01, 1.5883e+01, 1.5664e+01, 1.5411e+01, 1.5258e+01, 1.4863e+01,
        1.4686e+01, 1.4473e+01, 1.4204e+01, 1.3730e+01, 1.3642e+01, 1.3459e+01,
        1.3135e+01, 1.2951e+01, 1.2801e+01, 1.2600e+01, 1.2057e+01, 1.1612e+01,
        1.1340e+01, 1.1256e+01, 1.0601e+01, 1.0078e+01, 9.7787e+00, 9.4424e+00,
        8.9416e+00, 8.6890e+00, 8.2496e+00, 8.1842e+00, 7.8299e+00, 7.6654e+00,
        7.3819e+00, 7.2649e+00, 6.9887e+00, 6.4648e+00, 5.7822e+00, 5.5556e+00,
        5.1960e+00, 4.2064e+00, 3.6565e+00, 3.4196e+00, 3.0984e+00, 2.4005e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 75]) 

NULL SPACE BASIS :  tensor([[-0.0287,  0.0376,  0.0010,  ...,  0.0136,  0.0043,  0.0065],
        [ 0.0325,  0.0107,  0.0781,  ..., -0.0044, -0.0022, -0.0060],
        [-0.0214, -0.0571, -0.0819,  ..., -0.0049,  0.0008,  0.0030],
        ...,
        [ 0.0284,  0.0167, -0.0283,  ...,  0.0038,  0.0004, -0.0019],
        [-0.0198,  0.0099, -0.0257,  ..., -0.0039, -0.0005,  0.0017],
        [-0.0029, -0.0159,  0.0305,  ..., -0.0026, -0.0004,  0.0035]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0067, -0.0065,  0.0013,  ..., -0.0004,  0.0010, -0.0006],
        [-0.0065,  0.0145, -0.0087,  ...,  0.0013, -0.0004, -0.0002],
        [ 0.0013, -0.0087,  0.0089,  ..., -0.0010,  0.0001,  0.0007],
        ...,
        [-0.0004,  0.0013, -0.0010,  ...,  0.0018, -0.0012, -0.0002],
        [ 0.0010, -0.0004,  0.0001,  ..., -0.0012,  0.0030, -0.0012],
        [-0.0006, -0.0002,  0.0007,  ..., -0.0002, -0.0012,  0.0016]],
       device='cuda:0') 

reserving basis 210/576; cond: 782168.0625, radio:0.0010952168377116323
PARAMETER       :  Parameter containing:
tensor([[[[-1.3910e-02,  4.2509e-03,  1.4313e-02],
          [ 2.3452e-02,  7.5984e-03,  7.8146e-03],
          [-5.3198e-02, -4.9096e-02, -1.1203e-02]],

         [[ 3.0605e-03, -1.2041e-02, -6.4390e-03],
          [ 2.7158e-02, -2.9426e-02,  2.6366e-02],
          [-6.2651e-02, -7.3160e-03, -5.6727e-02]],

         [[-4.6555e-04,  1.9287e-02,  2.5180e-02],
          [-1.4308e-02, -3.3832e-02,  4.6836e-02],
          [-1.6263e-02, -8.5056e-03,  5.1539e-02]],

         ...,

         [[-3.9948e-02,  2.7505e-02,  2.1439e-02],
          [-1.0897e-02,  7.6016e-03, -7.0198e-03],
          [-3.4995e-02,  1.2650e-02,  2.3769e-02]],

         [[-1.7592e-02,  1.2617e-02,  1.5999e-03],
          [-3.3522e-02,  2.0022e-02, -4.0761e-03],
          [ 1.7508e-02, -3.9689e-02,  3.3688e-02]],

         [[ 2.1721e-02, -1.5337e-03,  2.4336e-02],
          [ 1.0209e-02,  1.0458e-02,  4.0765e-02],
          [-2.9613e-02,  2.0157e-02, -5.3092e-03]]],


        [[[ 3.1723e-02,  1.0909e-02, -2.3054e-02],
          [-9.5790e-03,  1.4582e-03,  1.0951e-02],
          [ 3.6310e-03, -4.6049e-02,  8.4758e-03]],

         [[ 3.3879e-02,  2.5124e-02,  2.2819e-02],
          [-2.0169e-02, -8.7485e-03, -2.4372e-02],
          [ 8.4464e-03, -2.9748e-02,  1.5514e-02]],

         [[-3.0581e-02, -1.4347e-02, -2.7841e-02],
          [-3.9278e-03,  1.2971e-02,  9.9346e-03],
          [ 3.5015e-02,  7.4269e-04, -4.2559e-02]],

         ...,

         [[ 1.6533e-02,  3.3132e-02,  5.6592e-03],
          [ 3.6464e-02,  3.8699e-02, -7.6214e-03],
          [-8.5444e-03, -2.9268e-02,  3.6072e-02]],

         [[-5.6968e-03,  1.2681e-02, -1.7993e-02],
          [-5.0136e-03, -1.9956e-02,  1.5346e-02],
          [-4.0970e-02, -2.3084e-02, -3.4765e-02]],

         [[ 3.4930e-02, -1.6083e-02,  1.0291e-02],
          [ 9.7552e-03,  1.5135e-02,  2.1162e-03],
          [ 2.4125e-02, -1.2647e-02,  2.1744e-02]]],


        [[[ 3.0843e-02, -5.7478e-03, -4.5005e-02],
          [ 3.1841e-02,  3.7011e-02,  3.0079e-02],
          [-1.4575e-02, -7.2361e-03,  3.9108e-02]],

         [[ 1.4312e-02,  3.2858e-03,  9.1619e-03],
          [ 1.8793e-02, -2.8715e-02, -2.8687e-03],
          [ 3.0306e-02,  2.7674e-02, -1.5837e-02]],

         [[ 4.1702e-02,  2.2955e-03,  9.7661e-03],
          [ 1.4315e-02, -1.8423e-02,  3.8958e-02],
          [-3.3669e-02, -3.8209e-02,  7.7191e-03]],

         ...,

         [[-2.0369e-04,  3.0802e-02, -1.2672e-02],
          [-1.5951e-02, -4.5393e-02,  6.3541e-03],
          [-1.0377e-02, -2.5195e-02, -2.5230e-02]],

         [[-8.3701e-03,  2.4658e-02, -4.6119e-02],
          [ 3.3287e-02,  2.4050e-02, -2.2619e-02],
          [-2.7497e-02, -2.4831e-02,  1.1515e-02]],

         [[ 3.7608e-02, -3.5366e-03, -8.5231e-03],
          [-2.5713e-02, -2.6059e-02,  9.4769e-03],
          [ 2.9798e-03,  4.1760e-02,  3.4438e-02]]],


        ...,


        [[[ 3.7559e-02, -3.2069e-02, -3.3874e-02],
          [ 3.5547e-02, -3.4839e-02, -4.7067e-02],
          [-1.5892e-02, -2.1419e-02,  1.4470e-02]],

         [[ 3.8319e-02,  1.6605e-02,  2.8323e-02],
          [-4.1458e-02, -4.2863e-02,  3.0769e-03],
          [-4.8138e-02,  2.4545e-02, -3.8501e-02]],

         [[ 1.3484e-02,  4.4658e-02,  5.8240e-03],
          [-4.4914e-02, -4.2746e-02,  2.1324e-02],
          [-2.1570e-02,  1.7997e-02,  2.7105e-02]],

         ...,

         [[-2.5097e-02, -2.4167e-02,  1.8274e-02],
          [ 2.4302e-03,  2.9587e-02,  3.6708e-02],
          [ 3.4186e-02, -4.9097e-02,  1.2358e-02]],

         [[ 5.7287e-04, -2.2714e-02,  3.2967e-02],
          [-3.8150e-02, -4.7334e-03, -3.7155e-03],
          [-8.4760e-03, -4.3613e-02,  5.5789e-03]],

         [[-3.2382e-03, -1.9901e-02, -8.2974e-03],
          [ 3.1757e-02,  4.8696e-02, -3.2000e-03],
          [-2.9656e-03,  3.1352e-02,  7.2850e-03]]],


        [[[-1.5711e-05, -4.1160e-02,  1.4553e-02],
          [-4.1112e-02,  2.1035e-02,  3.3090e-02],
          [ 1.2828e-02,  2.7889e-02, -1.1791e-02]],

         [[ 1.3016e-02, -2.0922e-02, -1.8931e-02],
          [ 6.9303e-03, -4.4428e-02,  1.7112e-03],
          [-2.1788e-02, -2.0280e-02, -5.1346e-03]],

         [[ 2.6866e-02,  2.5049e-02,  1.5409e-02],
          [-4.8766e-03,  5.8909e-03, -2.0098e-02],
          [ 3.2880e-02, -3.5083e-02, -1.1557e-03]],

         ...,

         [[-2.1392e-02,  4.2699e-02,  4.9515e-03],
          [ 2.4854e-02,  2.3794e-02, -3.0960e-02],
          [-4.2999e-03, -3.8574e-02,  1.4554e-02]],

         [[-2.2801e-02, -1.4412e-02, -3.6342e-02],
          [-3.8767e-03,  2.0837e-02,  2.7659e-03],
          [ 9.7804e-03, -3.1008e-02,  3.3172e-02]],

         [[ 9.4829e-03,  4.3030e-02,  1.2833e-02],
          [-2.6988e-03,  2.2348e-02,  1.8885e-02],
          [-1.2674e-02,  4.7517e-03,  2.2414e-02]]],


        [[[ 8.0781e-03, -9.2961e-03, -7.4946e-03],
          [-1.6028e-02,  1.6981e-02, -8.1971e-03],
          [-4.0039e-02, -4.2627e-02,  4.2613e-02]],

         [[-2.4910e-02,  4.4003e-03, -1.8533e-02],
          [-3.1867e-02, -2.7888e-02,  9.7386e-04],
          [-3.3460e-02,  1.1492e-02,  1.1010e-02]],

         [[ 3.9947e-04,  1.3936e-02, -1.4672e-02],
          [ 7.1303e-03, -3.7204e-02, -8.9357e-03],
          [-2.3785e-02,  2.7998e-02, -4.6853e-02]],

         ...,

         [[ 4.0214e-02, -3.6128e-02, -3.3398e-02],
          [-1.1182e-02,  1.1505e-02,  3.9896e-02],
          [-1.3981e-02,  3.1146e-02, -3.7307e-02]],

         [[-1.2763e-02, -3.1053e-02, -9.6980e-03],
          [ 1.5263e-02,  2.0596e-02, -2.9560e-02],
          [ 3.2399e-02, -3.1458e-02,  2.9222e-02]],

         [[-3.4036e-03, -3.0996e-02, -2.7436e-02],
          [-2.1253e-02,  3.3579e-02,  7.8476e-04],
          [-1.6618e-02,  3.9338e-02,  5.1434e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.9458e+07, 7.3258e+05, 6.2125e+05, 5.8608e+05, 5.6514e+05, 4.6271e+05,
        3.0869e+05, 2.6778e+05, 2.4803e+05, 2.2621e+05, 1.6708e+05, 1.6336e+05,
        1.5357e+05, 1.3363e+05, 1.0850e+05, 8.0748e+04, 7.1303e+04, 6.6554e+04,
        5.8584e+04, 5.2327e+04, 4.7985e+04, 4.0555e+04, 3.7750e+04, 3.6187e+04,
        3.3589e+04, 3.0314e+04, 2.9329e+04, 2.6911e+04, 2.5212e+04, 2.2476e+04,
        2.1530e+04, 1.9988e+04, 1.9339e+04, 1.7327e+04, 1.6285e+04, 1.5691e+04,
        1.5463e+04, 1.4847e+04, 1.4176e+04, 1.2551e+04, 1.2156e+04, 1.1786e+04,
        1.1296e+04, 1.1061e+04, 1.0616e+04, 9.9885e+03, 9.6786e+03, 8.8359e+03,
        8.5060e+03, 8.3504e+03, 8.2903e+03, 7.7877e+03, 7.6404e+03, 7.3668e+03,
        7.2079e+03, 6.9992e+03, 6.6170e+03, 6.5769e+03, 6.1810e+03, 6.0893e+03,
        5.9495e+03, 5.7609e+03, 5.6131e+03, 5.3970e+03, 5.2941e+03, 5.0590e+03,
        5.0380e+03, 4.8594e+03, 4.7365e+03, 4.7230e+03, 4.6439e+03, 4.5336e+03,
        4.3743e+03, 4.2697e+03, 4.1983e+03, 4.1209e+03, 4.0102e+03, 3.9695e+03,
        3.8939e+03, 3.8294e+03, 3.6808e+03, 3.6104e+03, 3.5674e+03, 3.4877e+03,
        3.4735e+03, 3.3536e+03, 3.3169e+03, 3.2435e+03, 3.2278e+03, 3.1853e+03,
        3.1311e+03, 3.0473e+03, 3.0198e+03, 2.9442e+03, 2.9111e+03, 2.8686e+03,
        2.8296e+03, 2.7532e+03, 2.7363e+03, 2.6922e+03, 2.6478e+03, 2.6094e+03,
        2.5886e+03, 2.4925e+03, 2.4535e+03, 2.4234e+03, 2.4046e+03, 2.3879e+03,
        2.3544e+03, 2.3032e+03, 2.2466e+03, 2.2100e+03, 2.1999e+03, 2.1675e+03,
        2.1415e+03, 2.1189e+03, 2.0849e+03, 2.0350e+03, 2.0229e+03, 1.9778e+03,
        1.9551e+03, 1.9242e+03, 1.9098e+03, 1.8812e+03, 1.8426e+03, 1.8213e+03,
        1.7944e+03, 1.7531e+03, 1.7404e+03, 1.7337e+03, 1.7276e+03, 1.6917e+03,
        1.6722e+03, 1.6508e+03, 1.6300e+03, 1.6168e+03, 1.5934e+03, 1.5789e+03,
        1.5668e+03, 1.5286e+03, 1.5207e+03, 1.5071e+03, 1.4538e+03, 1.4472e+03,
        1.4352e+03, 1.4039e+03, 1.3896e+03, 1.3699e+03, 1.3524e+03, 1.3400e+03,
        1.3205e+03, 1.3167e+03, 1.2937e+03, 1.2782e+03, 1.2711e+03, 1.2604e+03,
        1.2450e+03, 1.2340e+03, 1.2130e+03, 1.2051e+03, 1.1931e+03, 1.1848e+03,
        1.1610e+03, 1.1535e+03, 1.1429e+03, 1.1238e+03, 1.1146e+03, 1.0968e+03,
        1.0936e+03, 1.0731e+03, 1.0699e+03, 1.0667e+03, 1.0571e+03, 1.0477e+03,
        1.0296e+03, 1.0147e+03, 1.0050e+03, 1.0021e+03, 9.9161e+02, 9.8209e+02,
        9.7763e+02, 9.6886e+02, 9.5673e+02, 9.4706e+02, 9.3152e+02, 9.2638e+02,
        9.2005e+02, 9.0867e+02, 9.0444e+02, 8.9320e+02, 8.7839e+02, 8.7117e+02,
        8.6785e+02, 8.5646e+02, 8.5399e+02, 8.4485e+02, 8.3784e+02, 8.3423e+02,
        8.2796e+02, 8.1493e+02, 8.1011e+02, 8.0076e+02, 7.9324e+02, 7.7944e+02,
        7.7611e+02, 7.6224e+02, 7.5892e+02, 7.5536e+02, 7.5219e+02, 7.5014e+02,
        7.4298e+02, 7.3849e+02, 7.2810e+02, 7.2193e+02, 7.1213e+02, 7.0385e+02,
        6.9583e+02, 6.8716e+02, 6.8530e+02, 6.7936e+02, 6.7612e+02, 6.7418e+02,
        6.7208e+02, 6.6581e+02, 6.5936e+02, 6.5287e+02, 6.5059e+02, 6.4171e+02,
        6.4131e+02, 6.3310e+02, 6.3069e+02, 6.2467e+02, 6.2219e+02, 6.1630e+02,
        6.1163e+02, 6.0778e+02, 6.0221e+02, 5.9575e+02, 5.8828e+02, 5.8531e+02,
        5.8337e+02, 5.7376e+02, 5.7192e+02, 5.7024e+02, 5.6128e+02, 5.5663e+02,
        5.5526e+02, 5.4614e+02, 5.4208e+02, 5.3565e+02, 5.3526e+02, 5.3054e+02,
        5.2663e+02, 5.2416e+02, 5.2030e+02, 5.1280e+02, 5.0943e+02, 5.0741e+02,
        5.0530e+02, 5.0250e+02, 5.0040e+02, 4.9148e+02, 4.9087e+02, 4.8845e+02,
        4.8574e+02, 4.7925e+02, 4.7438e+02, 4.7275e+02, 4.6979e+02, 4.6689e+02,
        4.6481e+02, 4.6247e+02, 4.5649e+02, 4.5586e+02, 4.5427e+02, 4.4742e+02,
        4.4359e+02, 4.4125e+02, 4.3964e+02, 4.3113e+02, 4.2917e+02, 4.2417e+02,
        4.2326e+02, 4.2145e+02, 4.1940e+02, 4.1798e+02, 4.1559e+02, 4.1167e+02,
        4.1128e+02, 4.0438e+02, 4.0235e+02, 4.0008e+02, 3.9642e+02, 3.9383e+02,
        3.9137e+02, 3.8994e+02, 3.8967e+02, 3.8391e+02, 3.8310e+02, 3.8021e+02,
        3.7708e+02, 3.7544e+02, 3.7279e+02, 3.6931e+02, 3.6678e+02, 3.6175e+02,
        3.6114e+02, 3.5809e+02, 3.5649e+02, 3.5265e+02, 3.5229e+02, 3.5134e+02,
        3.4720e+02, 3.4510e+02, 3.4323e+02, 3.4226e+02, 3.3962e+02, 3.3853e+02,
        3.3482e+02, 3.3312e+02, 3.2979e+02, 3.2850e+02, 3.2645e+02, 3.2518e+02,
        3.2298e+02, 3.2010e+02, 3.1891e+02, 3.1747e+02, 3.1642e+02, 3.1477e+02,
        3.1263e+02, 3.0793e+02, 3.0767e+02, 3.0588e+02, 3.0463e+02, 3.0171e+02,
        2.9956e+02, 2.9832e+02, 2.9571e+02, 2.9232e+02, 2.9185e+02, 2.8899e+02,
        2.8743e+02, 2.8678e+02, 2.8528e+02, 2.8293e+02, 2.8147e+02, 2.7890e+02,
        2.7623e+02, 2.7445e+02, 2.7414e+02, 2.7024e+02, 2.6981e+02, 2.6904e+02,
        2.6855e+02, 2.6707e+02, 2.6490e+02, 2.6388e+02, 2.6231e+02, 2.6098e+02,
        2.6033e+02, 2.5791e+02, 2.5521e+02, 2.5346e+02, 2.5177e+02, 2.5043e+02,
        2.4788e+02, 2.4689e+02, 2.4667e+02, 2.4506e+02, 2.4437e+02, 2.4368e+02,
        2.4018e+02, 2.3931e+02, 2.3805e+02, 2.3786e+02, 2.3562e+02, 2.3520e+02,
        2.3356e+02, 2.3264e+02, 2.3114e+02, 2.2820e+02, 2.2757e+02, 2.2679e+02,
        2.2492e+02, 2.2421e+02, 2.2267e+02, 2.2096e+02, 2.1937e+02, 2.1686e+02,
        2.1548e+02, 2.1424e+02, 2.1245e+02, 2.1124e+02, 2.1015e+02, 2.0893e+02,
        2.0639e+02, 2.0570e+02, 2.0472e+02, 2.0389e+02, 2.0175e+02, 2.0111e+02,
        2.0078e+02, 1.9810e+02, 1.9720e+02, 1.9529e+02, 1.9421e+02, 1.9295e+02,
        1.9188e+02, 1.9080e+02, 1.9010e+02, 1.8837e+02, 1.8816e+02, 1.8546e+02,
        1.8509e+02, 1.8332e+02, 1.8191e+02, 1.8091e+02, 1.8048e+02, 1.7997e+02,
        1.7794e+02, 1.7735e+02, 1.7673e+02, 1.7508e+02, 1.7451e+02, 1.7302e+02,
        1.7203e+02, 1.7149e+02, 1.6960e+02, 1.6897e+02, 1.6696e+02, 1.6678e+02,
        1.6552e+02, 1.6425e+02, 1.6367e+02, 1.6259e+02, 1.6155e+02, 1.6065e+02,
        1.6039e+02, 1.5856e+02, 1.5785e+02, 1.5595e+02, 1.5465e+02, 1.5422e+02,
        1.5328e+02, 1.5245e+02, 1.5098e+02, 1.4961e+02, 1.4876e+02, 1.4669e+02,
        1.4652e+02, 1.4613e+02, 1.4557e+02, 1.4456e+02, 1.4304e+02, 1.4219e+02,
        1.4133e+02, 1.4112e+02, 1.3991e+02, 1.3881e+02, 1.3709e+02, 1.3618e+02,
        1.3549e+02, 1.3464e+02, 1.3436e+02, 1.3386e+02, 1.3342e+02, 1.3206e+02,
        1.3170e+02, 1.3036e+02, 1.2987e+02, 1.2856e+02, 1.2746e+02, 1.2633e+02,
        1.2527e+02, 1.2464e+02, 1.2354e+02, 1.2251e+02, 1.2167e+02, 1.2080e+02,
        1.1951e+02, 1.1926e+02, 1.1860e+02, 1.1770e+02, 1.1712e+02, 1.1615e+02,
        1.1581e+02, 1.1547e+02, 1.1392e+02, 1.1292e+02, 1.1268e+02, 1.1108e+02,
        1.1052e+02, 1.1039e+02, 1.0919e+02, 1.0866e+02, 1.0719e+02, 1.0689e+02,
        1.0671e+02, 1.0518e+02, 1.0382e+02, 1.0370e+02, 1.0269e+02, 1.0215e+02,
        1.0168e+02, 1.0059e+02, 9.9327e+01, 9.8525e+01, 9.8227e+01, 9.6619e+01,
        9.6521e+01, 9.5318e+01, 9.4534e+01, 9.3257e+01, 9.1901e+01, 9.1383e+01,
        9.0486e+01, 8.9156e+01, 8.8131e+01, 8.6817e+01, 8.6444e+01, 8.5675e+01,
        8.5164e+01, 8.3913e+01, 8.3476e+01, 8.2680e+01, 8.2222e+01, 8.0789e+01,
        8.0173e+01, 7.9221e+01, 7.8900e+01, 7.6823e+01, 7.6385e+01, 7.5584e+01,
        7.4035e+01, 7.3432e+01, 7.3178e+01, 7.2041e+01, 7.0904e+01, 7.0106e+01,
        6.9390e+01, 6.8523e+01, 6.7820e+01, 6.6927e+01, 6.5938e+01, 6.4141e+01,
        6.4019e+01, 6.3584e+01, 6.2771e+01, 6.2205e+01, 6.1100e+01, 6.0397e+01,
        5.9275e+01, 5.7659e+01, 5.7300e+01, 5.6489e+01, 5.5239e+01, 5.4888e+01,
        5.2469e+01, 5.1198e+01, 5.0938e+01, 5.0013e+01, 4.9729e+01, 4.7050e+01,
        4.6177e+01, 4.5507e+01, 4.2834e+01, 4.2134e+01, 4.1740e+01, 4.0231e+01,
        3.8398e+01, 3.6603e+01, 3.4542e+01, 3.3438e+01, 3.0779e+01, 2.4877e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 210]) 

NULL SPACE BASIS :  tensor([[-0.0219,  0.0509, -0.0566,  ..., -0.0023,  0.0093, -0.0288],
        [-0.0134,  0.0384, -0.0485,  ...,  0.0111, -0.0045,  0.0472],
        [ 0.0016,  0.0008, -0.0357,  ..., -0.0093,  0.0083, -0.0244],
        ...,
        [-0.0135,  0.0231,  0.0028,  ..., -0.0240, -0.1875, -0.0488],
        [-0.0105, -0.0051,  0.0059,  ...,  0.0750,  0.2998,  0.0539],
        [-0.0413, -0.0300, -0.0058,  ..., -0.0498, -0.1343, -0.0206]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.9364e-02, -1.5902e-02, -6.3909e-03,  ...,  7.1962e-04,
          2.7408e-04,  2.4763e-04],
        [-1.5902e-02,  4.1792e-02, -1.5208e-02,  ..., -1.0728e-04,
         -7.5799e-05,  1.2195e-03],
        [-6.3909e-03, -1.5208e-02,  3.7196e-02,  ..., -4.9945e-04,
          5.7136e-04,  1.0924e-03],
        ...,
        [ 7.1962e-04, -1.0728e-04, -4.9945e-04,  ...,  4.0425e-02,
         -1.8608e-02, -7.9386e-03],
        [ 2.7408e-04, -7.5799e-05,  5.7136e-04,  ..., -1.8608e-02,
          4.4102e-02, -1.6161e-02],
        [ 2.4763e-04,  1.2195e-03,  1.0924e-03,  ..., -7.9386e-03,
         -1.6161e-02,  3.9105e-02]], device='cuda:0') 

reserving basis 233/576; cond: 1019475.25, radio:0.0009090269450098276
PARAMETER       :  Parameter containing:
tensor([[[[ 1.9309e-02,  3.9920e-02,  3.1643e-02],
          [ 3.5652e-03, -2.8246e-03, -9.1020e-03],
          [-3.5040e-02,  2.8449e-02,  1.6801e-02]],

         [[-4.3158e-02, -1.9415e-02, -2.1701e-02],
          [-4.7675e-02, -4.5910e-02, -2.7936e-02],
          [-2.3110e-02,  5.2581e-03, -6.0301e-03]],

         [[ 1.7424e-02,  2.4353e-02, -5.1481e-02],
          [-1.4856e-02, -2.9050e-02, -1.9265e-02],
          [ 1.0739e-02,  1.0612e-02,  1.9119e-02]],

         ...,

         [[-5.5331e-02,  3.1842e-02,  4.5067e-02],
          [-2.9623e-02, -7.1032e-03, -1.5154e-02],
          [ 2.4001e-02, -2.3319e-02, -3.3726e-02]],

         [[ 1.0948e-02, -2.8766e-02,  3.4744e-02],
          [ 4.0511e-02, -2.5867e-02, -3.3620e-03],
          [-1.9036e-02, -3.2932e-02,  1.9866e-02]],

         [[-2.6441e-02, -2.0806e-02,  6.0858e-03],
          [ 1.6682e-02,  1.4915e-02, -2.4448e-02],
          [-2.6021e-02,  3.7783e-03,  4.4310e-03]]],


        [[[-3.0637e-02, -2.0353e-02, -1.0449e-02],
          [ 3.7036e-02,  2.0381e-02,  2.9184e-02],
          [-1.3555e-02,  4.5934e-02,  1.6504e-03]],

         [[-3.8748e-02, -3.2099e-02, -3.1190e-02],
          [-4.8516e-05, -4.0102e-02, -2.4538e-02],
          [-1.4789e-02,  1.9921e-02,  2.2149e-02]],

         [[-9.1399e-03,  1.4883e-02, -2.2800e-02],
          [-3.5101e-02, -3.1080e-02,  1.4424e-02],
          [ 3.1216e-02,  1.1569e-02, -1.7971e-02]],

         ...,

         [[ 2.4711e-02, -4.6267e-02,  1.7285e-02],
          [-1.2248e-02,  1.2320e-02, -2.0104e-02],
          [ 2.6583e-02,  2.5623e-02, -7.1348e-03]],

         [[ 1.1867e-02, -4.0623e-02, -7.4056e-03],
          [-1.3700e-02, -1.8701e-02, -4.1318e-02],
          [-4.0692e-03,  5.2768e-03, -3.7516e-04]],

         [[-1.4898e-03, -2.8220e-02, -1.0834e-02],
          [-3.0154e-02, -2.7843e-02, -1.4998e-02],
          [ 3.4389e-02,  2.1089e-02, -1.8255e-02]]],


        [[[-4.0317e-02, -9.0289e-03, -2.0841e-03],
          [-5.7838e-02, -4.4009e-02, -3.4062e-02],
          [-1.1710e-03,  3.5138e-02,  3.8219e-02]],

         [[-1.8324e-02, -9.7812e-03, -4.2402e-03],
          [-2.3543e-02, -4.0466e-02, -3.9123e-02],
          [-1.6808e-02, -1.8176e-02, -9.5174e-03]],

         [[ 8.3358e-03, -1.4752e-02,  5.2388e-03],
          [-3.6307e-02,  5.1223e-02, -8.3112e-03],
          [-5.7713e-03,  2.6862e-02,  3.6820e-02]],

         ...,

         [[-4.5180e-02,  2.3002e-02,  3.6695e-02],
          [-1.8613e-02, -4.0028e-02,  3.3380e-02],
          [-1.1333e-02, -2.9674e-02,  1.3216e-02]],

         [[ 2.7072e-02,  1.8508e-02,  4.0003e-02],
          [ 2.3046e-02, -3.0273e-02, -2.2055e-02],
          [-3.4203e-02, -4.9985e-02,  1.9882e-02]],

         [[-2.9525e-02,  5.1007e-02,  3.1837e-02],
          [ 2.0540e-02,  8.5669e-03, -1.1580e-02],
          [-1.5350e-02, -1.1840e-03, -3.7421e-02]]],


        ...,


        [[[ 3.4887e-02,  5.7496e-03, -6.6423e-03],
          [ 3.8503e-02,  4.4988e-02,  1.6323e-03],
          [-1.4980e-02,  1.2166e-02, -3.7867e-02]],

         [[-3.1583e-02,  1.8779e-03, -2.8745e-02],
          [-9.9692e-03,  3.4885e-02,  1.4386e-02],
          [-4.6669e-03, -1.8075e-02,  2.1010e-02]],

         [[-4.2492e-02, -4.0597e-02, -3.6208e-02],
          [-2.8109e-02,  3.6501e-02,  4.1986e-02],
          [-1.0934e-02, -3.3412e-02, -2.8073e-02]],

         ...,

         [[ 3.4029e-02, -6.0347e-03,  3.3837e-02],
          [-1.1500e-02,  4.2661e-02, -1.2232e-02],
          [-3.9281e-02,  2.6120e-02, -2.3875e-02]],

         [[-1.8225e-02, -4.2379e-02, -1.4508e-02],
          [-1.1598e-02,  1.8824e-02, -3.5028e-02],
          [ 2.4423e-02,  2.2361e-02,  1.3499e-02]],

         [[ 2.2022e-02, -1.5551e-02,  1.0226e-02],
          [ 3.2858e-02, -3.8256e-03, -2.6353e-02],
          [-4.0583e-02,  1.3971e-02, -1.1377e-02]]],


        [[[-3.7827e-02,  2.4041e-02, -4.8244e-02],
          [ 3.7097e-03, -4.1832e-02,  2.0133e-02],
          [-1.6404e-02, -6.6805e-03,  3.6462e-02]],

         [[ 1.8089e-02, -3.0730e-02,  7.7132e-03],
          [-8.3262e-03,  2.7039e-02,  5.9159e-03],
          [ 2.4519e-02, -1.2557e-02,  1.6765e-02]],

         [[-3.4479e-02, -4.1282e-02, -2.5263e-02],
          [-2.3946e-02, -1.6510e-02,  1.8105e-02],
          [ 7.1652e-03, -3.3318e-02, -3.7533e-02]],

         ...,

         [[-8.8666e-03, -1.5946e-02, -3.0421e-02],
          [-2.0968e-02,  2.8481e-02, -2.2694e-02],
          [-4.0207e-02,  3.3970e-02,  3.1185e-02]],

         [[-3.0506e-02, -1.0683e-02, -2.9302e-02],
          [ 3.7161e-02,  2.3439e-03, -1.6456e-02],
          [ 2.9640e-02,  3.2373e-02, -2.0563e-02]],

         [[ 1.4429e-02, -2.6502e-02, -7.7212e-03],
          [-4.0398e-02, -3.4204e-02,  3.8549e-02],
          [ 3.8899e-02,  2.2799e-02,  3.8611e-02]]],


        [[[-3.8083e-02,  3.1604e-02, -3.5453e-03],
          [-3.3367e-02,  3.4433e-02, -7.6925e-03],
          [-3.7495e-02, -2.3318e-02,  3.0333e-02]],

         [[-3.5817e-03, -2.4776e-02,  8.1743e-03],
          [ 1.8421e-02,  2.8334e-03, -8.9447e-03],
          [-1.4921e-02, -2.3612e-03,  3.1081e-02]],

         [[-2.2613e-02, -3.8005e-02, -6.6141e-03],
          [-1.4845e-03, -1.9269e-03,  2.7424e-03],
          [ 3.5598e-02, -2.5635e-02,  4.1958e-03]],

         ...,

         [[ 1.2607e-02, -1.6722e-02, -2.0076e-02],
          [-2.0034e-02,  3.0893e-02, -4.4222e-02],
          [-2.4653e-04,  1.5894e-02, -3.0117e-02]],

         [[ 3.5490e-02, -6.6284e-03,  3.1827e-02],
          [ 4.9144e-03, -2.0774e-02, -3.8469e-02],
          [-2.0501e-02,  8.2898e-03, -3.6115e-02]],

         [[ 1.0388e-02,  4.0947e-02, -1.7681e-02],
          [-3.1828e-02,  4.1404e-02,  3.6684e-02],
          [ 2.7415e-02,  1.5120e-02,  7.4661e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.0849e+07, 7.8031e+05, 6.9411e+05, 5.8110e+05, 4.7880e+05, 3.3551e+05,
        3.0821e+05, 2.5237e+05, 2.3226e+05, 1.8390e+05, 1.6567e+05, 1.5341e+05,
        1.2869e+05, 9.4629e+04, 5.0748e+04, 4.8932e+04, 4.3953e+04, 4.2555e+04,
        3.9305e+04, 3.4552e+04, 3.2992e+04, 3.0296e+04, 2.7991e+04, 2.5913e+04,
        2.5055e+04, 2.2528e+04, 2.0636e+04, 1.9796e+04, 1.8774e+04, 1.7530e+04,
        1.6989e+04, 1.5568e+04, 1.5318e+04, 1.4496e+04, 1.3649e+04, 1.2441e+04,
        1.2068e+04, 1.1635e+04, 1.1438e+04, 1.0910e+04, 1.0661e+04, 1.0237e+04,
        1.0174e+04, 9.7575e+03, 9.4935e+03, 8.9447e+03, 8.7371e+03, 8.1778e+03,
        8.0247e+03, 7.5279e+03, 7.3794e+03, 7.1749e+03, 6.7502e+03, 6.3191e+03,
        6.2658e+03, 5.8276e+03, 5.5772e+03, 5.4818e+03, 5.3428e+03, 5.1211e+03,
        5.0494e+03, 4.8922e+03, 4.7647e+03, 4.6991e+03, 4.6138e+03, 4.5877e+03,
        4.3170e+03, 4.2449e+03, 4.1925e+03, 4.1142e+03, 4.0097e+03, 3.8503e+03,
        3.7238e+03, 3.6868e+03, 3.6291e+03, 3.5223e+03, 3.3478e+03, 3.2782e+03,
        3.2467e+03, 3.1872e+03, 3.1511e+03, 3.0383e+03, 3.0150e+03, 2.9968e+03,
        2.9235e+03, 2.8915e+03, 2.7566e+03, 2.7098e+03, 2.6823e+03, 2.6350e+03,
        2.6254e+03, 2.5653e+03, 2.5012e+03, 2.4743e+03, 2.4238e+03, 2.3913e+03,
        2.3237e+03, 2.2828e+03, 2.2724e+03, 2.2412e+03, 2.2132e+03, 2.1486e+03,
        2.1003e+03, 2.0602e+03, 2.0116e+03, 1.9783e+03, 1.9404e+03, 1.9078e+03,
        1.8867e+03, 1.8681e+03, 1.8265e+03, 1.8148e+03, 1.7614e+03, 1.7262e+03,
        1.7082e+03, 1.6986e+03, 1.6618e+03, 1.6455e+03, 1.6130e+03, 1.5881e+03,
        1.5789e+03, 1.5470e+03, 1.5103e+03, 1.4817e+03, 1.4625e+03, 1.4394e+03,
        1.4275e+03, 1.4079e+03, 1.3883e+03, 1.3630e+03, 1.3421e+03, 1.3282e+03,
        1.3095e+03, 1.2794e+03, 1.2754e+03, 1.2493e+03, 1.2318e+03, 1.2187e+03,
        1.2053e+03, 1.1784e+03, 1.1511e+03, 1.1480e+03, 1.1252e+03, 1.1217e+03,
        1.1107e+03, 1.0902e+03, 1.0775e+03, 1.0711e+03, 1.0526e+03, 1.0422e+03,
        1.0201e+03, 1.0103e+03, 1.0021e+03, 9.7670e+02, 9.7417e+02, 9.6617e+02,
        9.5374e+02, 9.4333e+02, 9.3881e+02, 9.2421e+02, 9.2058e+02, 9.0682e+02,
        9.0369e+02, 8.9348e+02, 8.7287e+02, 8.6777e+02, 8.5193e+02, 8.4183e+02,
        8.2946e+02, 8.2518e+02, 8.2347e+02, 8.1664e+02, 8.1031e+02, 8.0085e+02,
        7.9759e+02, 7.8500e+02, 7.7322e+02, 7.6557e+02, 7.5229e+02, 7.5114e+02,
        7.4271e+02, 7.3925e+02, 7.2954e+02, 7.2389e+02, 7.1113e+02, 7.0711e+02,
        7.0253e+02, 7.0138e+02, 6.8939e+02, 6.8160e+02, 6.7801e+02, 6.6352e+02,
        6.5935e+02, 6.4847e+02, 6.3878e+02, 6.3412e+02, 6.3173e+02, 6.2439e+02,
        6.1762e+02, 6.1067e+02, 6.0567e+02, 6.0190e+02, 5.9633e+02, 5.9028e+02,
        5.8119e+02, 5.7406e+02, 5.6767e+02, 5.6429e+02, 5.6376e+02, 5.5906e+02,
        5.5191e+02, 5.4716e+02, 5.4238e+02, 5.3662e+02, 5.3300e+02, 5.2795e+02,
        5.2684e+02, 5.1712e+02, 5.1336e+02, 5.1159e+02, 5.0813e+02, 5.0572e+02,
        4.9760e+02, 4.9039e+02, 4.8855e+02, 4.8481e+02, 4.8084e+02, 4.7566e+02,
        4.7030e+02, 4.6562e+02, 4.6242e+02, 4.5871e+02, 4.5664e+02, 4.5643e+02,
        4.5207e+02, 4.5069e+02, 4.4609e+02, 4.4457e+02, 4.4101e+02, 4.3802e+02,
        4.3318e+02, 4.2707e+02, 4.2239e+02, 4.1628e+02, 4.1439e+02, 4.1079e+02,
        4.0857e+02, 4.0642e+02, 4.0255e+02, 3.9751e+02, 3.9669e+02, 3.9243e+02,
        3.9141e+02, 3.8662e+02, 3.8309e+02, 3.8101e+02, 3.8001e+02, 3.7432e+02,
        3.7228e+02, 3.6931e+02, 3.6711e+02, 3.6304e+02, 3.5996e+02, 3.5952e+02,
        3.5534e+02, 3.4801e+02, 3.4668e+02, 3.4467e+02, 3.4236e+02, 3.3998e+02,
        3.3829e+02, 3.3704e+02, 3.3647e+02, 3.3175e+02, 3.2933e+02, 3.2759e+02,
        3.2299e+02, 3.2116e+02, 3.1940e+02, 3.1776e+02, 3.1437e+02, 3.1385e+02,
        3.1243e+02, 3.1084e+02, 3.0795e+02, 3.0712e+02, 3.0216e+02, 3.0145e+02,
        2.9774e+02, 2.9595e+02, 2.9423e+02, 2.9345e+02, 2.9116e+02, 2.8869e+02,
        2.8698e+02, 2.8640e+02, 2.8400e+02, 2.8324e+02, 2.8171e+02, 2.8076e+02,
        2.7732e+02, 2.7519e+02, 2.7214e+02, 2.6947e+02, 2.6922e+02, 2.6626e+02,
        2.6584e+02, 2.6207e+02, 2.6098e+02, 2.5922e+02, 2.5647e+02, 2.5511e+02,
        2.5353e+02, 2.5033e+02, 2.4837e+02, 2.4713e+02, 2.4595e+02, 2.4365e+02,
        2.4305e+02, 2.4151e+02, 2.3853e+02, 2.3691e+02, 2.3629e+02, 2.3489e+02,
        2.3197e+02, 2.3107e+02, 2.2850e+02, 2.2706e+02, 2.2578e+02, 2.2444e+02,
        2.2273e+02, 2.2230e+02, 2.2013e+02, 2.1825e+02, 2.1596e+02, 2.1454e+02,
        2.1389e+02, 2.1115e+02, 2.1058e+02, 2.0902e+02, 2.0729e+02, 2.0632e+02,
        2.0619e+02, 2.0374e+02, 2.0239e+02, 2.0176e+02, 2.0051e+02, 1.9830e+02,
        1.9634e+02, 1.9562e+02, 1.9471e+02, 1.9321e+02, 1.9275e+02, 1.9138e+02,
        1.9094e+02, 1.8899e+02, 1.8797e+02, 1.8709e+02, 1.8624e+02, 1.8490e+02,
        1.8225e+02, 1.8154e+02, 1.8034e+02, 1.7852e+02, 1.7807e+02, 1.7731e+02,
        1.7633e+02, 1.7481e+02, 1.7411e+02, 1.7134e+02, 1.7067e+02, 1.6979e+02,
        1.6844e+02, 1.6672e+02, 1.6593e+02, 1.6474e+02, 1.6379e+02, 1.6313e+02,
        1.6160e+02, 1.6131e+02, 1.6029e+02, 1.5937e+02, 1.5872e+02, 1.5767e+02,
        1.5537e+02, 1.5499e+02, 1.5397e+02, 1.5290e+02, 1.5256e+02, 1.5185e+02,
        1.5049e+02, 1.4974e+02, 1.4824e+02, 1.4701e+02, 1.4621e+02, 1.4614e+02,
        1.4495e+02, 1.4417e+02, 1.4314e+02, 1.4220e+02, 1.4179e+02, 1.4032e+02,
        1.4006e+02, 1.3856e+02, 1.3771e+02, 1.3580e+02, 1.3534e+02, 1.3470e+02,
        1.3377e+02, 1.3323e+02, 1.3295e+02, 1.3239e+02, 1.3144e+02, 1.3031e+02,
        1.2933e+02, 1.2857e+02, 1.2759e+02, 1.2711e+02, 1.2687e+02, 1.2631e+02,
        1.2589e+02, 1.2265e+02, 1.2215e+02, 1.2187e+02, 1.2167e+02, 1.2061e+02,
        1.1942e+02, 1.1857e+02, 1.1842e+02, 1.1729e+02, 1.1666e+02, 1.1624e+02,
        1.1501e+02, 1.1461e+02, 1.1395e+02, 1.1313e+02, 1.1250e+02, 1.1178e+02,
        1.1080e+02, 1.1059e+02, 1.0933e+02, 1.0869e+02, 1.0834e+02, 1.0809e+02,
        1.0689e+02, 1.0673e+02, 1.0502e+02, 1.0464e+02, 1.0439e+02, 1.0364e+02,
        1.0297e+02, 1.0258e+02, 1.0214e+02, 1.0082e+02, 1.0014e+02, 9.9488e+01,
        9.9090e+01, 9.8242e+01, 9.7426e+01, 9.6374e+01, 9.6343e+01, 9.5330e+01,
        9.4215e+01, 9.3670e+01, 9.2856e+01, 9.2079e+01, 9.1373e+01, 9.0790e+01,
        9.0325e+01, 8.9391e+01, 8.8807e+01, 8.8288e+01, 8.7647e+01, 8.7375e+01,
        8.6991e+01, 8.6491e+01, 8.5624e+01, 8.5263e+01, 8.3964e+01, 8.3352e+01,
        8.2875e+01, 8.2542e+01, 8.1872e+01, 8.1582e+01, 8.0740e+01, 8.0360e+01,
        7.9691e+01, 7.9216e+01, 7.8768e+01, 7.8668e+01, 7.8114e+01, 7.6848e+01,
        7.6292e+01, 7.5918e+01, 7.5319e+01, 7.4777e+01, 7.4149e+01, 7.3092e+01,
        7.2098e+01, 7.1915e+01, 7.1395e+01, 7.0849e+01, 7.0305e+01, 6.9824e+01,
        6.9115e+01, 6.8434e+01, 6.7519e+01, 6.7141e+01, 6.6711e+01, 6.6459e+01,
        6.4927e+01, 6.4896e+01, 6.4245e+01, 6.3887e+01, 6.3323e+01, 6.2819e+01,
        6.2413e+01, 6.1675e+01, 6.1112e+01, 6.0657e+01, 6.0256e+01, 5.9705e+01,
        5.8586e+01, 5.7847e+01, 5.7337e+01, 5.6783e+01, 5.6284e+01, 5.5958e+01,
        5.5485e+01, 5.5037e+01, 5.4649e+01, 5.3967e+01, 5.3839e+01, 5.3052e+01,
        5.2223e+01, 5.1529e+01, 5.0866e+01, 5.0463e+01, 5.0241e+01, 4.9538e+01,
        4.9057e+01, 4.8793e+01, 4.8223e+01, 4.7787e+01, 4.6767e+01, 4.6214e+01,
        4.5443e+01, 4.5352e+01, 4.5232e+01, 4.4717e+01, 4.4455e+01, 4.3628e+01,
        4.3048e+01, 4.2944e+01, 4.1654e+01, 4.1277e+01, 4.0669e+01, 3.9865e+01,
        3.9625e+01, 3.8520e+01, 3.7929e+01, 3.7112e+01, 3.6084e+01, 3.5422e+01,
        3.4252e+01, 3.3794e+01, 3.2724e+01, 3.1760e+01, 3.1295e+01, 2.9662e+01,
        2.9053e+01, 2.8239e+01, 2.6033e+01, 2.5924e+01, 2.3892e+01, 2.0450e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 233]) 

NULL SPACE BASIS :  tensor([[-0.0056,  0.0428,  0.0300,  ..., -0.0058, -0.0096,  0.0182],
        [-0.0137, -0.0277, -0.0012,  ...,  0.0067,  0.0141, -0.0235],
        [ 0.0103, -0.0723, -0.0174,  ..., -0.0054, -0.0051,  0.0081],
        ...,
        [ 0.0169,  0.0126,  0.0107,  ..., -0.0074,  0.0140,  0.0070],
        [ 0.0179, -0.0301, -0.0263,  ...,  0.0130, -0.0090, -0.0102],
        [-0.0498, -0.0654,  0.0141,  ..., -0.0089,  0.0074,  0.0029]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0232, -0.0170, -0.0013,  ...,  0.0004, -0.0005, -0.0002],
        [-0.0170,  0.0337, -0.0130,  ..., -0.0013,  0.0008, -0.0012],
        [-0.0013, -0.0130,  0.0192,  ...,  0.0007, -0.0008,  0.0009],
        ...,
        [ 0.0004, -0.0013,  0.0007,  ...,  0.0194, -0.0093, -0.0017],
        [-0.0005,  0.0008, -0.0008,  ..., -0.0093,  0.0262, -0.0098],
        [-0.0002, -0.0012,  0.0009,  ..., -0.0017, -0.0098,  0.0223]],
       device='cuda:0') 

reserving basis 324/576; cond: 246942.3125, radio:0.004740163218230009
PARAMETER       :  Parameter containing:
tensor([[[[-2.7246e-02, -5.1148e-03,  1.9250e-02],
          [-2.3720e-02, -2.6834e-02, -1.2217e-02],
          [ 1.1849e-02,  1.1767e-02,  7.9238e-03]],

         [[ 6.6709e-04,  1.5247e-02,  5.6858e-03],
          [-4.8859e-03,  1.2634e-03,  1.8652e-02],
          [-1.9500e-02, -1.6548e-02, -4.6798e-02]],

         [[ 2.3287e-03,  4.2670e-02,  4.2993e-02],
          [-3.0114e-03, -3.7219e-02, -4.3631e-03],
          [-3.5727e-02, -4.2480e-03, -2.3603e-02]],

         ...,

         [[-1.9489e-02,  4.4002e-02,  3.2639e-03],
          [-2.6548e-02, -8.3920e-03,  4.1826e-02],
          [-1.4607e-02, -3.7680e-02, -1.6843e-02]],

         [[ 2.9350e-02,  1.5817e-02, -4.4012e-02],
          [ 2.8290e-02,  5.4563e-03,  4.6810e-02],
          [-3.3153e-02,  2.1666e-02, -2.1240e-02]],

         [[ 3.4742e-02,  2.2138e-02,  2.1749e-02],
          [-3.6275e-02, -9.8245e-03,  2.5997e-02],
          [ 3.0685e-02, -1.5772e-02, -2.7177e-02]]],


        [[[-2.6910e-02, -2.3639e-02, -4.7337e-02],
          [ 9.5614e-03, -9.7657e-03,  8.8874e-03],
          [-5.2087e-02,  7.1475e-03,  1.8079e-03]],

         [[-3.2329e-02,  1.7745e-02,  5.0961e-02],
          [ 3.5077e-03,  2.2742e-02, -1.1885e-02],
          [-1.2925e-02,  2.9485e-02,  4.6906e-02]],

         [[-1.8853e-03, -1.6684e-02,  3.0703e-02],
          [-9.1465e-05, -3.1176e-02,  3.6603e-02],
          [ 3.4104e-02,  3.2869e-02, -2.7563e-03]],

         ...,

         [[-7.0880e-03,  2.0666e-02, -2.2560e-02],
          [-2.6784e-02,  2.2943e-02,  1.4714e-02],
          [-3.8970e-02,  2.7290e-02, -4.4198e-02]],

         [[-7.8541e-03,  2.2187e-02, -2.3192e-02],
          [-1.8676e-02, -2.9546e-02, -3.6458e-02],
          [-4.4907e-02, -1.0760e-02, -3.8554e-02]],

         [[ 2.3101e-02, -3.8367e-02, -3.5226e-02],
          [-2.0650e-02, -3.4576e-02, -1.6309e-02],
          [-2.2830e-02,  1.1657e-03,  9.7243e-03]]],


        [[[-5.0586e-03, -5.7107e-03, -1.6734e-02],
          [-5.8697e-04, -4.4672e-02, -3.9079e-02],
          [-4.0614e-02, -8.0234e-04, -1.2415e-02]],

         [[-2.5751e-02, -2.3412e-02, -1.9778e-02],
          [ 1.1711e-03, -2.6237e-02, -1.1777e-02],
          [ 1.6306e-02, -2.6130e-02,  5.7975e-04]],

         [[ 4.8327e-02,  4.0854e-02,  2.2494e-03],
          [ 2.9069e-02,  3.8494e-03, -3.9592e-02],
          [ 1.1638e-02,  7.9431e-03, -1.1156e-03]],

         ...,

         [[ 1.3667e-02, -1.3403e-02, -2.6751e-03],
          [ 1.4645e-02,  1.6206e-02, -4.0927e-02],
          [-3.4274e-02,  1.0069e-02, -1.2693e-02]],

         [[ 1.1349e-02,  6.4812e-03, -2.5372e-02],
          [-2.7861e-02,  1.0028e-02,  2.5132e-02],
          [ 5.1355e-03, -2.2715e-02,  2.1307e-04]],

         [[ 1.7715e-02, -1.2170e-02, -3.1481e-02],
          [ 1.1207e-02, -3.3620e-02, -3.3677e-02],
          [ 3.1621e-02, -1.5304e-02,  1.2951e-03]]],


        ...,


        [[[ 1.3487e-02,  3.2611e-02,  1.3954e-02],
          [-2.3267e-03, -1.4943e-02, -1.9492e-02],
          [ 5.0682e-02,  2.2854e-02, -5.4786e-02]],

         [[ 4.1495e-03, -2.6854e-02,  2.9225e-03],
          [-2.9950e-02,  2.4458e-02, -1.9354e-02],
          [ 1.9290e-02, -2.5422e-02,  4.3585e-02]],

         [[ 2.5208e-02, -1.9065e-02,  4.3488e-02],
          [ 3.2215e-02, -3.8283e-02, -3.9267e-02],
          [ 1.7513e-02, -1.3271e-02, -3.5076e-03]],

         ...,

         [[-1.7571e-02, -1.4077e-02,  2.7257e-03],
          [ 1.3013e-02,  8.5857e-03,  1.4455e-02],
          [ 3.2350e-03, -1.1266e-02,  4.2087e-03]],

         [[ 1.4579e-02,  2.2199e-02, -4.6530e-02],
          [-2.9753e-02, -2.0061e-02, -3.7881e-03],
          [ 9.5480e-03,  8.8649e-03,  1.5157e-02]],

         [[ 2.9872e-02,  6.6693e-03,  2.8330e-02],
          [ 1.5946e-02,  4.4185e-02,  1.9440e-03],
          [ 3.8947e-02, -8.5763e-03,  2.0129e-02]]],


        [[[-1.1655e-02,  1.9872e-02,  3.4841e-02],
          [ 1.6870e-02,  2.4172e-02,  7.1184e-03],
          [-2.9705e-03,  2.8716e-02, -5.0830e-02]],

         [[ 3.4914e-02, -1.4300e-02, -2.6871e-02],
          [-3.7096e-02,  4.8276e-03, -3.5778e-02],
          [-3.8947e-02, -3.6080e-03,  4.0711e-02]],

         [[ 1.9966e-02, -4.1878e-02, -2.4159e-02],
          [-3.5568e-02,  2.7096e-02, -1.7577e-02],
          [-3.4872e-02,  1.4533e-02, -3.7434e-03]],

         ...,

         [[-1.8164e-02, -2.8449e-02,  1.4518e-02],
          [ 1.1851e-02,  3.8012e-02,  8.9169e-03],
          [-2.0180e-02, -3.3253e-02, -5.0656e-03]],

         [[-1.9832e-02,  3.7720e-02, -2.5246e-02],
          [ 5.8601e-03, -1.5480e-02,  3.8829e-02],
          [ 4.5533e-02, -1.6035e-03, -9.6500e-03]],

         [[-2.2752e-02,  5.4892e-03,  2.9054e-02],
          [-3.3468e-02, -2.2314e-02,  1.0223e-02],
          [ 1.0244e-02,  1.1220e-02, -2.2305e-03]]],


        [[[ 3.7743e-02,  4.3349e-02,  8.3596e-04],
          [ 2.0103e-02, -1.1083e-02,  2.5357e-02],
          [ 5.8452e-03, -3.4876e-02, -5.1129e-02]],

         [[ 4.6108e-02, -5.4949e-03,  3.3609e-02],
          [ 1.8922e-02,  3.4102e-02,  8.8538e-03],
          [-6.7052e-04, -8.5636e-03,  3.6369e-02]],

         [[-2.3020e-02,  3.5713e-04, -4.1866e-02],
          [-3.8826e-03, -7.7829e-03,  4.0560e-02],
          [ 2.0768e-02, -1.6818e-02,  2.7071e-02]],

         ...,

         [[-7.6933e-03,  3.7559e-02, -3.3606e-02],
          [-9.7941e-03, -3.8732e-02,  2.6783e-02],
          [-3.7296e-02,  6.7910e-03, -2.4524e-04]],

         [[ 2.1455e-03, -3.6509e-02, -1.5311e-02],
          [-3.6642e-02,  2.6217e-02, -2.1005e-02],
          [-2.3701e-02, -3.9994e-02,  7.3284e-03]],

         [[ 4.1409e-02, -5.5362e-03,  3.5315e-02],
          [ 3.3887e-02, -1.4042e-02,  2.4677e-02],
          [ 6.0419e-04,  2.6022e-02,  2.4220e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.3468e+07, 6.3734e+05, 5.4667e+05, 4.8264e+05, 4.1980e+05, 3.0698e+05,
        2.8812e+05, 2.8184e+05, 2.4065e+05, 2.0976e+05, 1.7938e+05, 1.7248e+05,
        1.4405e+05, 1.1467e+05, 8.9606e+04, 7.3270e+04, 6.4222e+04, 5.5489e+04,
        5.2089e+04, 4.8478e+04, 4.4710e+04, 4.3396e+04, 4.1937e+04, 3.7975e+04,
        3.5551e+04, 3.3810e+04, 3.1984e+04, 3.0640e+04, 2.9418e+04, 2.7198e+04,
        2.6476e+04, 2.4060e+04, 2.2738e+04, 2.2071e+04, 1.9625e+04, 1.8749e+04,
        1.8294e+04, 1.7069e+04, 1.6492e+04, 1.5797e+04, 1.5379e+04, 1.4616e+04,
        1.4555e+04, 1.3548e+04, 1.2717e+04, 1.2560e+04, 1.1951e+04, 1.1786e+04,
        1.1307e+04, 1.1257e+04, 1.0874e+04, 1.0261e+04, 1.0204e+04, 9.7863e+03,
        9.5333e+03, 9.1592e+03, 9.1004e+03, 8.8536e+03, 8.5636e+03, 8.2823e+03,
        8.1796e+03, 7.9161e+03, 7.7051e+03, 7.5910e+03, 7.2634e+03, 7.0914e+03,
        6.9950e+03, 6.8121e+03, 6.7360e+03, 6.5008e+03, 6.4491e+03, 6.2377e+03,
        6.0725e+03, 5.9777e+03, 5.9154e+03, 5.8964e+03, 5.8117e+03, 5.7136e+03,
        5.6417e+03, 5.4444e+03, 5.3082e+03, 5.1997e+03, 5.1240e+03, 5.0686e+03,
        4.9666e+03, 4.8872e+03, 4.7798e+03, 4.7102e+03, 4.6430e+03, 4.5651e+03,
        4.4703e+03, 4.4350e+03, 4.4011e+03, 4.3208e+03, 4.2528e+03, 4.1621e+03,
        4.0716e+03, 3.9679e+03, 3.9095e+03, 3.8712e+03, 3.8287e+03, 3.7586e+03,
        3.7442e+03, 3.7081e+03, 3.6417e+03, 3.5800e+03, 3.5328e+03, 3.4832e+03,
        3.4077e+03, 3.3464e+03, 3.3145e+03, 3.2764e+03, 3.1882e+03, 3.1756e+03,
        3.1457e+03, 3.1004e+03, 3.0528e+03, 3.0247e+03, 2.9932e+03, 2.9595e+03,
        2.9408e+03, 2.9077e+03, 2.8545e+03, 2.8164e+03, 2.7727e+03, 2.7579e+03,
        2.7500e+03, 2.7199e+03, 2.6880e+03, 2.6481e+03, 2.6396e+03, 2.6006e+03,
        2.5557e+03, 2.5449e+03, 2.4989e+03, 2.4917e+03, 2.4381e+03, 2.4203e+03,
        2.4047e+03, 2.3708e+03, 2.3456e+03, 2.3140e+03, 2.2828e+03, 2.2718e+03,
        2.2490e+03, 2.2316e+03, 2.2216e+03, 2.1870e+03, 2.1664e+03, 2.1276e+03,
        2.1057e+03, 2.0940e+03, 2.0557e+03, 2.0528e+03, 2.0299e+03, 2.0116e+03,
        1.9951e+03, 1.9595e+03, 1.9545e+03, 1.9394e+03, 1.9324e+03, 1.9058e+03,
        1.8873e+03, 1.8831e+03, 1.8517e+03, 1.8302e+03, 1.8127e+03, 1.8105e+03,
        1.7960e+03, 1.7637e+03, 1.7569e+03, 1.7368e+03, 1.7252e+03, 1.7118e+03,
        1.7059e+03, 1.6859e+03, 1.6776e+03, 1.6575e+03, 1.6552e+03, 1.6351e+03,
        1.6242e+03, 1.6112e+03, 1.6013e+03, 1.5947e+03, 1.5717e+03, 1.5583e+03,
        1.5354e+03, 1.5239e+03, 1.5161e+03, 1.5057e+03, 1.4947e+03, 1.4825e+03,
        1.4775e+03, 1.4616e+03, 1.4565e+03, 1.4310e+03, 1.4156e+03, 1.4018e+03,
        1.3976e+03, 1.3835e+03, 1.3766e+03, 1.3636e+03, 1.3605e+03, 1.3454e+03,
        1.3414e+03, 1.3314e+03, 1.3239e+03, 1.3027e+03, 1.2917e+03, 1.2852e+03,
        1.2734e+03, 1.2554e+03, 1.2521e+03, 1.2393e+03, 1.2317e+03, 1.2192e+03,
        1.2113e+03, 1.2085e+03, 1.2032e+03, 1.1915e+03, 1.1884e+03, 1.1796e+03,
        1.1706e+03, 1.1592e+03, 1.1497e+03, 1.1422e+03, 1.1376e+03, 1.1266e+03,
        1.1170e+03, 1.1071e+03, 1.1000e+03, 1.0985e+03, 1.0834e+03, 1.0740e+03,
        1.0702e+03, 1.0633e+03, 1.0601e+03, 1.0503e+03, 1.0482e+03, 1.0406e+03,
        1.0352e+03, 1.0321e+03, 1.0165e+03, 1.0149e+03, 1.0076e+03, 9.9486e+02,
        9.8931e+02, 9.8512e+02, 9.7419e+02, 9.6771e+02, 9.5573e+02, 9.5529e+02,
        9.5031e+02, 9.4358e+02, 9.3716e+02, 9.3358e+02, 9.2779e+02, 9.1908e+02,
        9.1613e+02, 9.0713e+02, 9.0242e+02, 8.9971e+02, 8.8953e+02, 8.8528e+02,
        8.8190e+02, 8.7505e+02, 8.6993e+02, 8.6799e+02, 8.6296e+02, 8.5006e+02,
        8.4716e+02, 8.3503e+02, 8.2851e+02, 8.2756e+02, 8.2060e+02, 8.1526e+02,
        8.1266e+02, 8.0893e+02, 8.0793e+02, 8.0549e+02, 7.9718e+02, 7.9314e+02,
        7.8738e+02, 7.8144e+02, 7.7858e+02, 7.7531e+02, 7.6554e+02, 7.6088e+02,
        7.5636e+02, 7.5251e+02, 7.5162e+02, 7.4476e+02, 7.3947e+02, 7.3812e+02,
        7.3415e+02, 7.2968e+02, 7.2772e+02, 7.2456e+02, 7.1458e+02, 7.0586e+02,
        7.0394e+02, 6.9985e+02, 6.9566e+02, 6.9286e+02, 6.8479e+02, 6.8328e+02,
        6.8117e+02, 6.7968e+02, 6.7536e+02, 6.7107e+02, 6.6783e+02, 6.6623e+02,
        6.6351e+02, 6.6191e+02, 6.5637e+02, 6.5269e+02, 6.4483e+02, 6.3731e+02,
        6.3337e+02, 6.3267e+02, 6.3007e+02, 6.2406e+02, 6.2186e+02, 6.2042e+02,
        6.1629e+02, 6.1475e+02, 6.0954e+02, 6.0735e+02, 6.0554e+02, 6.0203e+02,
        5.9944e+02, 5.9419e+02, 5.9156e+02, 5.8798e+02, 5.8566e+02, 5.8276e+02,
        5.8007e+02, 5.7709e+02, 5.7436e+02, 5.7357e+02, 5.6819e+02, 5.6551e+02,
        5.6152e+02, 5.5843e+02, 5.5482e+02, 5.5013e+02, 5.4650e+02, 5.4412e+02,
        5.4188e+02, 5.4003e+02, 5.3786e+02, 5.3625e+02, 5.3444e+02, 5.2764e+02,
        5.2623e+02, 5.2309e+02, 5.2051e+02, 5.1473e+02, 5.1212e+02, 5.1129e+02,
        5.0805e+02, 5.0431e+02, 5.0370e+02, 4.9951e+02, 4.9762e+02, 4.9655e+02,
        4.9415e+02, 4.9252e+02, 4.8932e+02, 4.8331e+02, 4.8252e+02, 4.8187e+02,
        4.7689e+02, 4.7449e+02, 4.7190e+02, 4.6762e+02, 4.6640e+02, 4.6634e+02,
        4.6492e+02, 4.6255e+02, 4.5930e+02, 4.5642e+02, 4.5359e+02, 4.5190e+02,
        4.4978e+02, 4.4628e+02, 4.4266e+02, 4.4175e+02, 4.4082e+02, 4.3857e+02,
        4.3454e+02, 4.3340e+02, 4.3060e+02, 4.3012e+02, 4.2878e+02, 4.2496e+02,
        4.1919e+02, 4.1794e+02, 4.1561e+02, 4.1302e+02, 4.1152e+02, 4.1027e+02,
        4.0600e+02, 4.0515e+02, 4.0394e+02, 4.0273e+02, 3.9948e+02, 3.9817e+02,
        3.9444e+02, 3.9346e+02, 3.9187e+02, 3.8862e+02, 3.8797e+02, 3.8478e+02,
        3.8346e+02, 3.8122e+02, 3.7823e+02, 3.7544e+02, 3.7483e+02, 3.7234e+02,
        3.6972e+02, 3.6816e+02, 3.6657e+02, 3.6481e+02, 3.6223e+02, 3.6068e+02,
        3.5961e+02, 3.5793e+02, 3.5618e+02, 3.5373e+02, 3.5270e+02, 3.4949e+02,
        3.4868e+02, 3.4604e+02, 3.4516e+02, 3.4189e+02, 3.3911e+02, 3.3657e+02,
        3.3549e+02, 3.3400e+02, 3.3263e+02, 3.3139e+02, 3.2974e+02, 3.2867e+02,
        3.2269e+02, 3.2253e+02, 3.2034e+02, 3.1934e+02, 3.1735e+02, 3.1694e+02,
        3.1441e+02, 3.1230e+02, 3.1170e+02, 3.1031e+02, 3.0864e+02, 3.0831e+02,
        3.0385e+02, 3.0277e+02, 3.0113e+02, 2.9898e+02, 2.9795e+02, 2.9645e+02,
        2.9623e+02, 2.9314e+02, 2.9150e+02, 2.9106e+02, 2.8925e+02, 2.8768e+02,
        2.8608e+02, 2.8527e+02, 2.8293e+02, 2.7947e+02, 2.7768e+02, 2.7643e+02,
        2.7337e+02, 2.7317e+02, 2.7160e+02, 2.7043e+02, 2.6768e+02, 2.6563e+02,
        2.6314e+02, 2.6197e+02, 2.6069e+02, 2.5916e+02, 2.5770e+02, 2.5631e+02,
        2.5542e+02, 2.5262e+02, 2.5100e+02, 2.4977e+02, 2.4926e+02, 2.4747e+02,
        2.4575e+02, 2.4359e+02, 2.4302e+02, 2.3995e+02, 2.3840e+02, 2.3762e+02,
        2.3666e+02, 2.3387e+02, 2.3331e+02, 2.3140e+02, 2.2973e+02, 2.2738e+02,
        2.2581e+02, 2.2457e+02, 2.2367e+02, 2.2179e+02, 2.2005e+02, 2.1734e+02,
        2.1570e+02, 2.1473e+02, 2.1388e+02, 2.1264e+02, 2.1120e+02, 2.1005e+02,
        2.0705e+02, 2.0534e+02, 2.0469e+02, 2.0427e+02, 2.0086e+02, 1.9914e+02,
        1.9608e+02, 1.9545e+02, 1.9475e+02, 1.9384e+02, 1.8976e+02, 1.8886e+02,
        1.8735e+02, 1.8666e+02, 1.8465e+02, 1.8259e+02, 1.8132e+02, 1.8059e+02,
        1.7901e+02, 1.7733e+02, 1.7494e+02, 1.7453e+02, 1.7221e+02, 1.7184e+02,
        1.7035e+02, 1.6956e+02, 1.6556e+02, 1.6499e+02, 1.6222e+02, 1.6071e+02,
        1.6063e+02, 1.5856e+02, 1.5793e+02, 1.5579e+02, 1.5413e+02, 1.5294e+02,
        1.4960e+02, 1.4842e+02, 1.4701e+02, 1.4378e+02, 1.4228e+02, 1.4080e+02,
        1.3815e+02, 1.3688e+02, 1.3499e+02, 1.3364e+02, 1.3029e+02, 1.3005e+02,
        1.2558e+02, 1.2512e+02, 1.2238e+02, 1.1848e+02, 1.1732e+02, 1.1248e+02,
        1.1236e+02, 1.0917e+02, 1.0784e+02, 1.0214e+02, 9.7183e+01, 9.5035e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 324]) 

NULL SPACE BASIS :  tensor([[ 0.0155,  0.0454, -0.0148,  ..., -0.0027,  0.0006, -0.0012],
        [ 0.0244, -0.0136,  0.0116,  ...,  0.0014, -0.0044,  0.0188],
        [ 0.0652,  0.0358, -0.0097,  ..., -0.0050, -0.0013, -0.0095],
        ...,
        [-0.0361,  0.0315,  0.0201,  ...,  0.0356,  0.0001,  0.0310],
        [ 0.0442, -0.0489,  0.0329,  ..., -0.0402, -0.0038, -0.0536],
        [ 0.0238, -0.0682, -0.0322,  ...,  0.0093,  0.0079,  0.0196]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.8360e-02, -9.0300e-03, -2.0472e-03,  ...,  1.3683e-03,
         -1.0857e-04,  3.3924e-04],
        [-9.0300e-03,  3.1876e-02, -9.5969e-03,  ..., -3.4084e-04,
         -3.5105e-05, -6.3244e-05],
        [-2.0472e-03, -9.5969e-03,  2.4478e-02,  ..., -6.2600e-04,
         -7.7290e-04, -1.1621e-03],
        ...,
        [ 1.3683e-03, -3.4084e-04, -6.2600e-04,  ...,  3.3240e-02,
         -8.1189e-03, -3.9524e-03],
        [-1.0857e-04, -3.5105e-05, -7.7290e-04,  ..., -8.1189e-03,
          3.6786e-02, -7.2701e-03],
        [ 3.3924e-04, -6.3244e-05, -1.1621e-03,  ..., -3.9524e-03,
         -7.2701e-03,  3.0566e-02]], device='cuda:0') 

reserving basis 292/576; cond: 464376.9375, radio:0.0023899090010672808
PARAMETER       :  Parameter containing:
tensor([[[[-2.7102e-02, -1.2570e-02, -2.4811e-02],
          [ 3.3850e-02, -1.6293e-02, -2.4094e-02],
          [ 1.6075e-02, -7.4047e-03,  4.4898e-02]],

         [[ 3.4901e-02, -7.4857e-03, -2.6494e-02],
          [ 3.2770e-02,  6.1949e-03,  3.6278e-02],
          [ 8.6219e-03, -2.1205e-02,  2.2353e-03]],

         [[-6.8372e-03, -2.2935e-02, -1.7111e-02],
          [ 2.5991e-02, -4.6531e-02,  2.0035e-02],
          [-3.2330e-02,  2.8679e-03, -1.0873e-02]],

         ...,

         [[-1.7366e-02,  1.5318e-02,  2.5449e-02],
          [ 1.4663e-02, -1.3154e-02, -1.0207e-02],
          [ 1.5456e-02, -1.4824e-02, -3.2890e-02]],

         [[ 3.1429e-03,  3.2081e-02,  4.3323e-02],
          [ 4.5490e-03,  2.3823e-02,  2.3634e-02],
          [ 5.3236e-03,  9.9242e-03,  3.3972e-02]],

         [[-4.0446e-02, -2.3106e-02, -1.2874e-02],
          [-2.1283e-02, -2.2773e-02,  2.2754e-02],
          [-3.5384e-02, -2.2600e-03, -3.7619e-02]]],


        [[[ 5.5098e-03, -1.7644e-02, -5.0146e-02],
          [-1.1578e-02, -2.3147e-02, -4.4860e-02],
          [-5.7862e-03, -1.8934e-02,  1.2399e-02]],

         [[-1.4223e-02, -5.2395e-03,  9.3841e-03],
          [ 2.0726e-02,  4.0102e-02,  2.5799e-02],
          [-1.2891e-02, -4.6638e-03,  7.4673e-03]],

         [[-2.1659e-02,  9.8657e-03, -1.6712e-02],
          [ 2.7865e-02, -3.3973e-02,  3.8135e-02],
          [-1.2058e-02,  7.6651e-03, -7.5320e-04]],

         ...,

         [[ 3.7692e-03,  4.2371e-02, -1.3847e-02],
          [-1.4573e-03, -8.9307e-04, -2.0278e-02],
          [-2.5856e-02, -1.0490e-02, -1.2021e-02]],

         [[-5.1224e-02,  3.3693e-03,  1.4983e-02],
          [ 1.3829e-02, -1.4822e-03, -1.1271e-02],
          [ 4.8483e-02, -9.4773e-03, -9.8933e-06]],

         [[-4.1829e-02,  7.4382e-03,  1.5036e-02],
          [-1.4044e-02,  2.6819e-02, -9.2579e-03],
          [ 2.3385e-02,  3.5990e-02,  3.7430e-02]]],


        [[[-3.9726e-02,  2.2443e-02,  1.4022e-02],
          [ 3.1213e-02,  1.7779e-02,  2.9989e-02],
          [ 3.3195e-02,  1.6031e-02,  2.2630e-02]],

         [[-5.6655e-03,  4.0950e-03, -8.1791e-03],
          [ 2.4372e-02, -2.1181e-02,  2.0673e-02],
          [-9.0942e-03, -3.6620e-02, -2.5858e-02]],

         [[ 4.3187e-02, -1.5150e-02, -9.6615e-03],
          [ 1.0615e-02,  3.6216e-02, -4.9954e-03],
          [-2.2457e-02,  1.2611e-02,  3.8376e-02]],

         ...,

         [[ 1.9345e-02, -2.4325e-02,  2.3639e-02],
          [ 4.3239e-02,  1.2428e-02, -2.0587e-02],
          [ 3.4161e-02,  2.5180e-02, -1.3390e-02]],

         [[ 2.0435e-02, -4.0470e-02, -1.7287e-02],
          [-2.9440e-02,  2.7387e-02,  5.4472e-03],
          [ 3.4226e-03,  2.9576e-02,  2.0970e-02]],

         [[-2.3734e-02,  8.4186e-03,  7.5322e-03],
          [-2.4646e-02,  3.9970e-03, -3.4515e-02],
          [ 3.8602e-03,  1.6699e-02,  5.1224e-03]]],


        ...,


        [[[-1.4960e-02, -2.8618e-02, -3.5279e-02],
          [ 1.3781e-04,  1.2096e-02, -4.9372e-02],
          [-9.9006e-03, -9.7599e-03, -3.8507e-02]],

         [[-2.0758e-02,  2.7082e-02,  3.9272e-02],
          [-1.5353e-02, -3.0046e-02, -3.1629e-02],
          [-1.2810e-02,  3.7866e-02,  4.3056e-02]],

         [[-2.9022e-03, -1.8364e-02,  4.0463e-02],
          [-5.5533e-03,  2.9505e-02,  3.8857e-02],
          [ 2.6966e-03,  4.2170e-02,  1.8523e-02]],

         ...,

         [[ 3.0204e-02, -4.2997e-02, -3.5696e-02],
          [ 1.6477e-02,  2.1376e-02, -1.0052e-02],
          [ 1.5602e-03, -4.7343e-03,  8.7169e-03]],

         [[ 2.2739e-02, -9.5526e-04,  9.2100e-03],
          [ 2.6386e-02,  8.4561e-03, -3.9734e-02],
          [ 4.5753e-02,  3.7729e-02, -3.8962e-02]],

         [[ 2.8266e-03,  4.2452e-02, -2.3045e-04],
          [ 3.0325e-02, -3.4551e-02, -7.0304e-03],
          [ 3.6416e-03, -3.3050e-02,  1.8497e-02]]],


        [[[ 3.6234e-02, -1.2149e-02,  4.5706e-02],
          [-4.7189e-02,  2.0285e-02, -1.4167e-02],
          [ 3.0656e-02, -1.3978e-02, -2.0752e-02]],

         [[ 2.6482e-02, -2.2211e-02, -1.2944e-02],
          [ 2.4130e-02,  8.1798e-03, -2.2694e-02],
          [ 5.2041e-02, -2.3666e-02,  4.6234e-02]],

         [[-3.0852e-02, -3.4969e-02,  2.7522e-02],
          [ 1.6695e-02, -3.7046e-02, -5.4419e-02],
          [-3.7935e-02,  2.7478e-04, -1.2345e-02]],

         ...,

         [[-2.0033e-02,  1.9393e-02,  3.1987e-02],
          [-3.5252e-02,  9.5567e-03, -9.3488e-03],
          [ 3.1521e-02, -3.3901e-02,  3.5933e-02]],

         [[-5.0106e-03, -1.8555e-02, -3.2381e-02],
          [ 2.4170e-02, -1.0512e-02,  1.3366e-02],
          [-5.7846e-04,  3.4836e-02,  1.2956e-02]],

         [[-3.0203e-02, -1.4347e-02, -4.1589e-02],
          [ 2.6399e-02, -4.2595e-02, -2.9807e-02],
          [-3.6214e-02,  4.3950e-02,  1.6789e-02]]],


        [[[-1.2937e-03, -4.7172e-03,  1.5381e-02],
          [-3.8752e-02, -6.3409e-03, -7.7147e-03],
          [ 2.5126e-02,  4.3601e-03,  3.7352e-02]],

         [[-2.2713e-02,  2.8587e-02,  1.5326e-02],
          [ 3.8340e-02,  2.5940e-02,  1.8472e-02],
          [-5.1124e-03,  2.3988e-02,  1.1935e-02]],

         [[-1.5005e-02, -2.7191e-02,  3.5307e-02],
          [-3.7210e-02,  2.0087e-02,  1.7200e-02],
          [ 3.5718e-03, -2.7232e-02, -3.3014e-02]],

         ...,

         [[ 5.1451e-02, -2.0260e-02,  2.7489e-02],
          [ 3.3380e-02, -3.8830e-02,  2.7706e-02],
          [-8.0181e-03, -4.1999e-02, -1.9599e-02]],

         [[-1.6172e-02, -5.1307e-04, -1.8806e-02],
          [-3.0934e-02, -4.8305e-02, -3.9100e-02],
          [ 2.2196e-02,  1.8911e-02, -1.4342e-02]],

         [[-8.6767e-03, -1.3427e-02,  4.7366e-02],
          [ 2.3503e-02,  1.8196e-02,  2.3303e-03],
          [ 1.1669e-02,  3.0245e-02, -5.6106e-04]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.8894e+06, 2.4069e+05, 2.1699e+05, 1.3586e+05, 9.9003e+04, 5.8319e+04,
        5.5437e+04, 3.4883e+04, 3.3697e+04, 2.5590e+04, 1.9575e+04, 1.6051e+04,
        1.3290e+04, 1.2221e+04, 1.1067e+04, 1.0411e+04, 9.9225e+03, 9.6453e+03,
        8.9477e+03, 8.0855e+03, 7.6227e+03, 6.9684e+03, 6.1739e+03, 5.9660e+03,
        5.7204e+03, 5.5283e+03, 5.2248e+03, 4.8844e+03, 4.6748e+03, 4.3508e+03,
        4.1843e+03, 4.0596e+03, 3.8225e+03, 3.4742e+03, 3.4067e+03, 3.2864e+03,
        2.9495e+03, 2.9269e+03, 2.7452e+03, 2.5713e+03, 2.3966e+03, 2.2570e+03,
        2.1693e+03, 2.1457e+03, 2.1059e+03, 1.9981e+03, 1.9266e+03, 1.8745e+03,
        1.8191e+03, 1.7999e+03, 1.7040e+03, 1.5872e+03, 1.5364e+03, 1.4949e+03,
        1.4711e+03, 1.3878e+03, 1.3763e+03, 1.3527e+03, 1.3195e+03, 1.2745e+03,
        1.2689e+03, 1.2247e+03, 1.2006e+03, 1.1815e+03, 1.1571e+03, 1.1392e+03,
        1.1197e+03, 1.0714e+03, 1.0640e+03, 1.0443e+03, 1.0225e+03, 1.0097e+03,
        9.7026e+02, 9.5201e+02, 9.3079e+02, 8.9060e+02, 8.8395e+02, 8.5850e+02,
        8.5051e+02, 8.3650e+02, 8.2243e+02, 7.9785e+02, 7.8832e+02, 7.5985e+02,
        7.4391e+02, 7.2882e+02, 7.1741e+02, 7.1254e+02, 7.0030e+02, 6.9020e+02,
        6.7431e+02, 6.6153e+02, 6.5728e+02, 6.4122e+02, 6.3682e+02, 6.2465e+02,
        6.0887e+02, 5.9148e+02, 5.8688e+02, 5.8169e+02, 5.7397e+02, 5.6119e+02,
        5.5496e+02, 5.3865e+02, 5.2752e+02, 5.2583e+02, 5.1778e+02, 5.1122e+02,
        5.0435e+02, 4.9974e+02, 4.9371e+02, 4.8559e+02, 4.7481e+02, 4.7053e+02,
        4.6644e+02, 4.5696e+02, 4.5169e+02, 4.4498e+02, 4.3611e+02, 4.3485e+02,
        4.3159e+02, 4.2313e+02, 4.1542e+02, 4.1469e+02, 4.0619e+02, 3.9644e+02,
        3.9226e+02, 3.8724e+02, 3.8551e+02, 3.8289e+02, 3.7823e+02, 3.7251e+02,
        3.6871e+02, 3.6292e+02, 3.6094e+02, 3.5713e+02, 3.5447e+02, 3.4947e+02,
        3.4821e+02, 3.4508e+02, 3.4309e+02, 3.3551e+02, 3.3207e+02, 3.2429e+02,
        3.2217e+02, 3.2066e+02, 3.1566e+02, 3.1230e+02, 3.0943e+02, 3.0856e+02,
        3.0579e+02, 3.0036e+02, 2.9599e+02, 2.9405e+02, 2.9055e+02, 2.8767e+02,
        2.8514e+02, 2.8221e+02, 2.8013e+02, 2.7469e+02, 2.7088e+02, 2.6893e+02,
        2.6690e+02, 2.6580e+02, 2.6102e+02, 2.6013e+02, 2.5731e+02, 2.5494e+02,
        2.5103e+02, 2.5039e+02, 2.4985e+02, 2.4773e+02, 2.4594e+02, 2.4322e+02,
        2.4201e+02, 2.3862e+02, 2.3694e+02, 2.3149e+02, 2.3123e+02, 2.3050e+02,
        2.2723e+02, 2.2462e+02, 2.2217e+02, 2.2127e+02, 2.2016e+02, 2.1524e+02,
        2.1372e+02, 2.1280e+02, 2.0946e+02, 2.0835e+02, 2.0631e+02, 2.0588e+02,
        2.0532e+02, 2.0225e+02, 2.0010e+02, 1.9885e+02, 1.9614e+02, 1.9553e+02,
        1.9468e+02, 1.9325e+02, 1.8942e+02, 1.8866e+02, 1.8676e+02, 1.8600e+02,
        1.8543e+02, 1.8474e+02, 1.8206e+02, 1.8074e+02, 1.7922e+02, 1.7759e+02,
        1.7560e+02, 1.7361e+02, 1.7285e+02, 1.7112e+02, 1.7097e+02, 1.6928e+02,
        1.6771e+02, 1.6760e+02, 1.6504e+02, 1.6453e+02, 1.6410e+02, 1.6322e+02,
        1.6223e+02, 1.6090e+02, 1.5905e+02, 1.5882e+02, 1.5693e+02, 1.5556e+02,
        1.5463e+02, 1.5366e+02, 1.5226e+02, 1.5159e+02, 1.4997e+02, 1.4908e+02,
        1.4724e+02, 1.4653e+02, 1.4549e+02, 1.4446e+02, 1.4336e+02, 1.4265e+02,
        1.4161e+02, 1.4073e+02, 1.3967e+02, 1.3835e+02, 1.3766e+02, 1.3547e+02,
        1.3537e+02, 1.3369e+02, 1.3253e+02, 1.3206e+02, 1.3000e+02, 1.2999e+02,
        1.2809e+02, 1.2758e+02, 1.2682e+02, 1.2620e+02, 1.2544e+02, 1.2420e+02,
        1.2329e+02, 1.2312e+02, 1.2295e+02, 1.2121e+02, 1.2004e+02, 1.1962e+02,
        1.1908e+02, 1.1854e+02, 1.1729e+02, 1.1643e+02, 1.1604e+02, 1.1568e+02,
        1.1466e+02, 1.1379e+02, 1.1344e+02, 1.1254e+02, 1.1171e+02, 1.1080e+02,
        1.0983e+02, 1.0972e+02, 1.0885e+02, 1.0847e+02, 1.0789e+02, 1.0767e+02,
        1.0643e+02, 1.0631e+02, 1.0498e+02, 1.0433e+02, 1.0365e+02, 1.0289e+02,
        1.0187e+02, 1.0155e+02, 1.0113e+02, 1.0099e+02, 9.9930e+01, 9.9114e+01,
        9.8418e+01, 9.7968e+01, 9.7398e+01, 9.6383e+01, 9.6115e+01, 9.5700e+01,
        9.5294e+01, 9.5008e+01, 9.4885e+01, 9.3455e+01, 9.3221e+01, 9.2993e+01,
        9.2438e+01, 9.1751e+01, 9.0612e+01, 9.0340e+01, 9.0119e+01, 8.9677e+01,
        8.8548e+01, 8.8222e+01, 8.7627e+01, 8.7028e+01, 8.6891e+01, 8.6463e+01,
        8.5739e+01, 8.4858e+01, 8.4194e+01, 8.4049e+01, 8.3874e+01, 8.3691e+01,
        8.2087e+01, 8.1834e+01, 8.1585e+01, 8.1191e+01, 8.0344e+01, 8.0337e+01,
        7.9745e+01, 7.9613e+01, 7.8930e+01, 7.8630e+01, 7.7955e+01, 7.7292e+01,
        7.7167e+01, 7.6401e+01, 7.6233e+01, 7.6024e+01, 7.4971e+01, 7.4777e+01,
        7.4424e+01, 7.4018e+01, 7.3402e+01, 7.3099e+01, 7.2667e+01, 7.2569e+01,
        7.2352e+01, 7.2065e+01, 7.1290e+01, 7.0557e+01, 7.0022e+01, 6.9848e+01,
        6.9426e+01, 6.9196e+01, 6.8764e+01, 6.8469e+01, 6.8025e+01, 6.7565e+01,
        6.7272e+01, 6.6671e+01, 6.6101e+01, 6.6059e+01, 6.5887e+01, 6.5441e+01,
        6.5258e+01, 6.4878e+01, 6.4240e+01, 6.3973e+01, 6.3664e+01, 6.3148e+01,
        6.3014e+01, 6.2796e+01, 6.2444e+01, 6.1990e+01, 6.1759e+01, 6.1352e+01,
        6.1010e+01, 6.0811e+01, 6.0284e+01, 5.9966e+01, 5.9623e+01, 5.9077e+01,
        5.8871e+01, 5.8539e+01, 5.8329e+01, 5.8211e+01, 5.7868e+01, 5.7583e+01,
        5.7398e+01, 5.6530e+01, 5.6367e+01, 5.6253e+01, 5.5833e+01, 5.5570e+01,
        5.5356e+01, 5.5068e+01, 5.4398e+01, 5.4186e+01, 5.3938e+01, 5.3612e+01,
        5.3294e+01, 5.3040e+01, 5.2891e+01, 5.2252e+01, 5.2133e+01, 5.2023e+01,
        5.1466e+01, 5.1195e+01, 5.0767e+01, 5.0621e+01, 5.0477e+01, 5.0163e+01,
        4.9958e+01, 4.9764e+01, 4.9501e+01, 4.9221e+01, 4.9156e+01, 4.8994e+01,
        4.8530e+01, 4.8307e+01, 4.8064e+01, 4.7689e+01, 4.7487e+01, 4.7134e+01,
        4.6981e+01, 4.6620e+01, 4.6387e+01, 4.6243e+01, 4.5768e+01, 4.5601e+01,
        4.5154e+01, 4.5002e+01, 4.4559e+01, 4.4170e+01, 4.4031e+01, 4.3788e+01,
        4.3471e+01, 4.3289e+01, 4.3156e+01, 4.2859e+01, 4.2551e+01, 4.2469e+01,
        4.2353e+01, 4.2169e+01, 4.1959e+01, 4.1914e+01, 4.1356e+01, 4.1219e+01,
        4.1142e+01, 4.0966e+01, 4.0827e+01, 4.0320e+01, 4.0103e+01, 3.9819e+01,
        3.9622e+01, 3.9419e+01, 3.9122e+01, 3.8937e+01, 3.8734e+01, 3.8437e+01,
        3.8135e+01, 3.7822e+01, 3.7737e+01, 3.7540e+01, 3.7140e+01, 3.7006e+01,
        3.6669e+01, 3.6455e+01, 3.6166e+01, 3.6035e+01, 3.5922e+01, 3.5678e+01,
        3.5511e+01, 3.5429e+01, 3.5283e+01, 3.4819e+01, 3.4637e+01, 3.4184e+01,
        3.4030e+01, 3.3645e+01, 3.3431e+01, 3.3333e+01, 3.3179e+01, 3.2990e+01,
        3.2698e+01, 3.2445e+01, 3.2326e+01, 3.2167e+01, 3.1746e+01, 3.1628e+01,
        3.1446e+01, 3.1127e+01, 3.0872e+01, 3.0704e+01, 3.0620e+01, 3.0491e+01,
        3.0328e+01, 2.9876e+01, 2.9548e+01, 2.9442e+01, 2.9152e+01, 2.8983e+01,
        2.8884e+01, 2.8730e+01, 2.8533e+01, 2.8399e+01, 2.8197e+01, 2.7747e+01,
        2.7568e+01, 2.7287e+01, 2.7202e+01, 2.7002e+01, 2.6825e+01, 2.6660e+01,
        2.6514e+01, 2.6172e+01, 2.6082e+01, 2.6002e+01, 2.5653e+01, 2.5363e+01,
        2.5217e+01, 2.5037e+01, 2.4887e+01, 2.4537e+01, 2.4471e+01, 2.4374e+01,
        2.4010e+01, 2.3691e+01, 2.3646e+01, 2.3519e+01, 2.3317e+01, 2.3217e+01,
        2.2822e+01, 2.2727e+01, 2.2634e+01, 2.2509e+01, 2.2074e+01, 2.1827e+01,
        2.1722e+01, 2.1417e+01, 2.1075e+01, 2.1040e+01, 2.0698e+01, 2.0595e+01,
        2.0538e+01, 2.0385e+01, 2.0148e+01, 1.9954e+01, 1.9650e+01, 1.9460e+01,
        1.8839e+01, 1.8770e+01, 1.8276e+01, 1.8072e+01, 1.7793e+01, 1.7686e+01,
        1.7571e+01, 1.7306e+01, 1.7156e+01, 1.6920e+01, 1.6519e+01, 1.6168e+01,
        1.5482e+01, 1.5185e+01, 1.4955e+01, 1.4766e+01, 1.4586e+01, 1.4133e+01,
        1.4036e+01, 1.3704e+01, 1.3116e+01, 1.2695e+01, 1.2344e+01, 1.0529e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 292]) 

NULL SPACE BASIS :  tensor([[-0.0437, -0.0176, -0.0937,  ...,  0.0025,  0.0010,  0.0162],
        [-0.0242, -0.0495, -0.0137,  ..., -0.0137,  0.0035, -0.0181],
        [ 0.0626,  0.0774,  0.0269,  ...,  0.0152,  0.0009,  0.0048],
        ...,
        [-0.0225, -0.0301,  0.0433,  ...,  0.0070, -0.0047,  0.0007],
        [-0.0340,  0.0100, -0.0393,  ..., -0.0088,  0.0096,  0.0171],
        [ 0.0150,  0.0249,  0.0498,  ...,  0.0002, -0.0044, -0.0138]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.3909e-02, -1.7192e-02, -2.2834e-04,  ..., -7.9929e-04,
          1.9502e-04,  5.1506e-04],
        [-1.7192e-02,  3.2496e-02, -1.4661e-02,  ..., -2.8260e-04,
          4.6138e-04, -1.1769e-03],
        [-2.2834e-04, -1.4661e-02,  1.9515e-02,  ...,  4.2756e-05,
          1.9305e-05,  5.0448e-04],
        ...,
        [-7.9929e-04, -2.8260e-04,  4.2756e-05,  ...,  1.5687e-02,
         -8.3190e-03, -3.1108e-04],
        [ 1.9502e-04,  4.6138e-04,  1.9305e-05,  ..., -8.3190e-03,
          2.0008e-02, -7.3696e-03],
        [ 5.1506e-04, -1.1769e-03,  5.0448e-04,  ..., -3.1108e-04,
         -7.3696e-03,  1.4703e-02]], device='cuda:0') 

reserving basis 856/1152; cond: 233573.046875, radio:0.011058760806918144
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0167,  0.0306, -0.0183],
          [ 0.0004, -0.0163,  0.0093],
          [ 0.0198,  0.0196, -0.0029]],

         [[-0.0182, -0.0232, -0.0256],
          [-0.0213, -0.0221,  0.0071],
          [-0.0144,  0.0097, -0.0281]],

         [[ 0.0014, -0.0143,  0.0040],
          [ 0.0083,  0.0149, -0.0156],
          [-0.0145, -0.0251,  0.0270]],

         ...,

         [[-0.0128,  0.0015,  0.0050],
          [-0.0309, -0.0219, -0.0020],
          [ 0.0057,  0.0146,  0.0282]],

         [[ 0.0095, -0.0338, -0.0183],
          [ 0.0235,  0.0160,  0.0009],
          [ 0.0150, -0.0149,  0.0347]],

         [[ 0.0185,  0.0053,  0.0151],
          [-0.0229, -0.0341, -0.0261],
          [-0.0291,  0.0185, -0.0151]]],


        [[[ 0.0252,  0.0005,  0.0072],
          [-0.0156, -0.0137,  0.0212],
          [-0.0279, -0.0302, -0.0161]],

         [[ 0.0079, -0.0242, -0.0224],
          [ 0.0099, -0.0122,  0.0026],
          [-0.0250,  0.0076, -0.0117]],

         [[ 0.0003, -0.0173, -0.0187],
          [-0.0050,  0.0136, -0.0293],
          [ 0.0256,  0.0185,  0.0028]],

         ...,

         [[ 0.0042,  0.0132, -0.0045],
          [-0.0213,  0.0256,  0.0267],
          [ 0.0151,  0.0208,  0.0212]],

         [[ 0.0255,  0.0014,  0.0067],
          [-0.0080, -0.0215, -0.0274],
          [ 0.0415, -0.0190, -0.0295]],

         [[-0.0158, -0.0188, -0.0330],
          [-0.0234, -0.0134, -0.0082],
          [ 0.0071, -0.0111, -0.0014]]],


        [[[-0.0169,  0.0192, -0.0281],
          [-0.0043,  0.0179, -0.0023],
          [ 0.0006,  0.0280,  0.0164]],

         [[ 0.0047, -0.0123,  0.0228],
          [-0.0040, -0.0090,  0.0053],
          [-0.0218,  0.0049,  0.0089]],

         [[-0.0187, -0.0157, -0.0134],
          [-0.0255, -0.0268, -0.0314],
          [-0.0126, -0.0179,  0.0170]],

         ...,

         [[ 0.0268,  0.0081, -0.0177],
          [-0.0309,  0.0139,  0.0037],
          [ 0.0198, -0.0169, -0.0322]],

         [[-0.0407,  0.0023, -0.0112],
          [-0.0166,  0.0178,  0.0086],
          [-0.0144,  0.0081, -0.0148]],

         [[-0.0133, -0.0193,  0.0111],
          [-0.0035, -0.0101,  0.0048],
          [ 0.0108, -0.0178,  0.0304]]],


        ...,


        [[[ 0.0274,  0.0002,  0.0304],
          [ 0.0245,  0.0175, -0.0306],
          [ 0.0046,  0.0006,  0.0276]],

         [[ 0.0100, -0.0071,  0.0132],
          [ 0.0051, -0.0252,  0.0050],
          [-0.0159, -0.0072,  0.0164]],

         [[ 0.0158,  0.0120, -0.0109],
          [-0.0139,  0.0277,  0.0056],
          [ 0.0065, -0.0067, -0.0148]],

         ...,

         [[-0.0287,  0.0143, -0.0140],
          [ 0.0069, -0.0290, -0.0152],
          [-0.0191,  0.0022,  0.0087]],

         [[ 0.0190,  0.0171,  0.0285],
          [-0.0220,  0.0004,  0.0165],
          [-0.0056,  0.0108,  0.0055]],

         [[-0.0146, -0.0033,  0.0235],
          [ 0.0154,  0.0089,  0.0313],
          [ 0.0051,  0.0089,  0.0194]]],


        [[[-0.0048, -0.0121,  0.0050],
          [-0.0296, -0.0090,  0.0201],
          [ 0.0097, -0.0276, -0.0221]],

         [[-0.0124, -0.0159,  0.0238],
          [-0.0185, -0.0352, -0.0048],
          [-0.0240, -0.0007, -0.0227]],

         [[-0.0159, -0.0024, -0.0207],
          [-0.0118,  0.0146,  0.0226],
          [ 0.0090, -0.0194,  0.0203]],

         ...,

         [[-0.0149,  0.0110,  0.0048],
          [-0.0296, -0.0212, -0.0144],
          [-0.0233, -0.0496, -0.0175]],

         [[-0.0034, -0.0072, -0.0137],
          [-0.0305,  0.0304,  0.0293],
          [-0.0364,  0.0222, -0.0087]],

         [[ 0.0214, -0.0221, -0.0087],
          [ 0.0305, -0.0155,  0.0322],
          [-0.0048,  0.0186,  0.0012]]],


        [[[ 0.0041, -0.0267,  0.0293],
          [-0.0237,  0.0053, -0.0358],
          [-0.0012, -0.0281, -0.0184]],

         [[ 0.0144,  0.0144, -0.0164],
          [ 0.0029,  0.0127,  0.0085],
          [-0.0174, -0.0282,  0.0223]],

         [[-0.0085, -0.0081,  0.0078],
          [-0.0180,  0.0021,  0.0267],
          [-0.0164,  0.0243, -0.0041]],

         ...,

         [[ 0.0266,  0.0119, -0.0067],
          [-0.0177,  0.0321,  0.0156],
          [ 0.0280, -0.0097, -0.0005]],

         [[-0.0162,  0.0090, -0.0049],
          [-0.0227,  0.0319,  0.0286],
          [ 0.0164,  0.0133,  0.0082]],

         [[ 0.0113,  0.0388, -0.0173],
          [-0.0143,  0.0126,  0.0326],
          [ 0.0056, -0.0247, -0.0171]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([9.2092e+06, 3.4899e+05, 3.3606e+05,  ..., 4.3902e+01, 4.0836e+01,
        3.9428e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 856]) 

NULL SPACE BASIS :  tensor([[-9.4781e-03, -1.7809e-02, -3.0289e-03,  ...,  7.4678e-03,
          6.6657e-02,  8.4465e-03],
        [ 1.6424e-02,  8.7746e-03, -1.1343e-02,  ...,  3.3272e-02,
         -1.3962e-01, -2.3511e-02],
        [-2.5481e-03,  1.4506e-02, -1.0147e-02,  ..., -4.3375e-02,
          6.5317e-02,  1.5301e-02],
        ...,
        [ 7.2786e-02, -1.8305e-02,  6.3296e-03,  ..., -2.2113e-02,
          2.6161e-02, -9.3405e-05],
        [ 7.2814e-02,  3.3768e-02,  8.5817e-03,  ...,  2.3785e-02,
         -3.0596e-02, -1.4035e-02],
        [ 1.1840e-02,  8.0410e-03, -3.9712e-02,  ..., -1.5316e-02,
          1.2328e-02,  1.6105e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.9192e-02, -9.4551e-04, -5.4124e-04,  ..., -2.8060e-04,
          2.7499e-04,  3.6261e-05],
        [-9.4551e-04,  2.9889e-02, -6.1837e-04,  ...,  1.8969e-04,
          1.4270e-04,  1.2298e-04],
        [-5.4124e-04, -6.1837e-04,  2.9497e-02,  ..., -1.4193e-04,
          1.4578e-04,  1.2779e-04],
        ...,
        [-2.8060e-04,  1.8969e-04, -1.4193e-04,  ...,  2.4378e-02,
         -2.3493e-03, -9.8309e-04],
        [ 2.7499e-04,  1.4270e-04,  1.4578e-04,  ..., -2.3493e-03,
          2.5505e-02, -2.5593e-03],
        [ 3.6261e-05,  1.2298e-04,  1.2779e-04,  ..., -9.8309e-04,
         -2.5593e-03,  2.5478e-02]], device='cuda:0') 

reserving basis 44/64; cond: 7057.5576171875, radio:0.017404742538928986
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0142]],

         [[ 0.0921]],

         [[ 0.0557]],

         ...,

         [[-0.0806]],

         [[-0.0929]],

         [[-0.0553]]],


        [[[ 0.0247]],

         [[-0.0298]],

         [[-0.0957]],

         ...,

         [[-0.0407]],

         [[ 0.0156]],

         [[ 0.0220]]],


        [[[ 0.0263]],

         [[-0.0599]],

         [[ 0.0070]],

         ...,

         [[-0.1479]],

         [[ 0.0968]],

         [[ 0.0153]]],


        ...,


        [[[ 0.0307]],

         [[-0.0795]],

         [[-0.1012]],

         ...,

         [[-0.0168]],

         [[ 0.0574]],

         [[-0.0271]]],


        [[[ 0.0144]],

         [[-0.0751]],

         [[ 0.1172]],

         ...,

         [[-0.0049]],

         [[ 0.0383]],

         [[ 0.0715]]],


        [[[ 0.0090]],

         [[ 0.0031]],

         [[ 0.0827]],

         ...,

         [[-0.0120]],

         [[ 0.0741]],

         [[ 0.0955]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([6.1494e+05, 3.3205e+04, 2.8103e+04, 1.2422e+04, 8.9282e+03, 7.2762e+03,
        4.9281e+03, 4.1466e+03, 3.0939e+03, 2.5815e+03, 2.2675e+03, 2.0467e+03,
        1.6558e+03, 1.6025e+03, 1.4205e+03, 1.3558e+03, 1.2015e+03, 1.1909e+03,
        9.9248e+02, 9.8264e+02, 8.0017e+02, 7.2868e+02, 6.3764e+02, 6.2574e+02,
        5.9679e+02, 5.6674e+02, 5.4475e+02, 4.8613e+02, 4.6392e+02, 4.2319e+02,
        3.9276e+02, 3.8139e+02, 3.5907e+02, 3.4795e+02, 3.3083e+02, 3.1418e+02,
        3.0749e+02, 2.9473e+02, 2.7941e+02, 2.6552e+02, 2.6000e+02, 2.5107e+02,
        2.2540e+02, 2.2062e+02, 2.0409e+02, 2.0237e+02, 1.8958e+02, 1.7852e+02,
        1.7565e+02, 1.7130e+02, 1.6782e+02, 1.6328e+02, 1.5484e+02, 1.4331e+02,
        1.4116e+02, 1.3290e+02, 1.2831e+02, 1.2254e+02, 1.1687e+02, 1.1542e+02,
        1.0691e+02, 1.0326e+02, 9.8001e+01, 8.7132e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 44]) 

NULL SPACE BASIS :  tensor([[-0.2466, -0.0349,  0.1518,  ..., -0.1433, -0.0800, -0.1206],
        [-0.0784, -0.1182, -0.0707,  ...,  0.0154,  0.0828, -0.0272],
        [-0.0472, -0.1481, -0.0613,  ...,  0.2105, -0.1060,  0.0266],
        ...,
        [-0.0670,  0.1083, -0.0438,  ...,  0.0247, -0.0073,  0.0178],
        [ 0.0607,  0.0937, -0.0203,  ..., -0.0680,  0.0085, -0.0150],
        [ 0.2122, -0.0356, -0.0703,  ...,  0.0095, -0.0352,  0.0174]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.1036,  0.0022,  0.0172,  ...,  0.0007, -0.0042, -0.0110],
        [ 0.0022,  0.1201, -0.0124,  ...,  0.0116, -0.0109, -0.0044],
        [ 0.0172, -0.0124,  0.0860,  ..., -0.0046,  0.0046,  0.0063],
        ...,
        [ 0.0007,  0.0116, -0.0046,  ...,  0.0341, -0.0324, -0.0030],
        [-0.0042, -0.0109,  0.0046,  ..., -0.0324,  0.1104,  0.0041],
        [-0.0110, -0.0044,  0.0063,  ..., -0.0030,  0.0041,  0.0845]],
       device='cuda:0') 

reserving basis 804/1152; cond: 283450.75, radio:0.009788327850401402
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1664e-02, -3.0397e-03,  9.9772e-03],
          [-3.1613e-03,  1.6595e-02,  1.1467e-02],
          [-1.0278e-02, -1.2081e-02, -1.3186e-02]],

         [[ 3.5770e-03, -1.8671e-02, -2.2796e-02],
          [ 2.4751e-02, -2.8385e-03, -1.0630e-04],
          [ 6.0765e-03,  1.8320e-02, -1.6938e-02]],

         [[ 6.3441e-03, -5.0420e-04,  6.8314e-03],
          [ 1.7041e-02,  1.7744e-02,  1.7024e-02],
          [-2.4160e-02, -1.3958e-02,  1.2922e-02]],

         ...,

         [[ 1.6730e-02,  6.7287e-03,  2.1451e-04],
          [-1.1788e-02,  2.7245e-02,  6.9257e-03],
          [ 1.4632e-02, -1.6471e-03, -6.5821e-03]],

         [[ 9.6191e-03,  1.1489e-02,  6.0269e-03],
          [ 2.6446e-03,  2.2514e-02,  2.0186e-02],
          [ 2.4216e-02,  2.8458e-02, -5.7267e-03]],

         [[ 1.4879e-03,  2.6456e-02,  4.6785e-02],
          [-1.9380e-02,  1.4250e-02,  2.1108e-02],
          [ 5.1965e-03, -2.0148e-02,  1.7550e-02]]],


        [[[-1.0724e-02,  7.9746e-03, -2.0714e-02],
          [ 1.4285e-02, -6.2652e-03,  5.8764e-03],
          [ 2.6525e-03, -2.3750e-02,  1.9625e-02]],

         [[-1.6971e-03,  1.9304e-02,  2.6597e-02],
          [ 2.1021e-02,  1.6526e-02,  7.5875e-04],
          [-9.5308e-03,  1.6781e-02,  1.8987e-02]],

         [[ 1.9348e-02,  1.8684e-02,  1.0143e-02],
          [ 2.7187e-02, -4.3455e-03,  2.2446e-02],
          [-1.6585e-02,  1.0500e-03,  9.4770e-03]],

         ...,

         [[-4.2246e-03,  1.5095e-02, -1.8736e-02],
          [-2.7410e-02, -2.2812e-02, -3.1172e-02],
          [-2.0941e-02, -2.1601e-02, -2.3708e-02]],

         [[-2.2805e-02,  2.3226e-03,  3.2558e-02],
          [-1.5088e-02, -1.7654e-02, -1.5197e-03],
          [ 1.5336e-02, -1.0330e-02, -1.7206e-02]],

         [[ 2.2618e-02, -2.3758e-02,  1.3181e-02],
          [-7.7474e-03, -1.3569e-02, -2.8453e-02],
          [ 2.7788e-03,  1.6586e-02, -3.1712e-02]]],


        [[[ 1.9021e-02,  2.0975e-02,  1.4542e-02],
          [-6.0627e-03, -2.9029e-02,  2.7932e-02],
          [-1.1912e-02,  1.1927e-05, -2.4645e-02]],

         [[-1.0405e-02,  5.4200e-03, -1.6252e-02],
          [ 3.2604e-02,  6.5692e-03, -4.8262e-03],
          [-7.1525e-03, -1.8355e-02,  1.9027e-02]],

         [[-3.0100e-02, -1.5889e-02, -2.2823e-02],
          [-1.4999e-02, -1.5108e-02,  2.2602e-03],
          [ 2.5170e-02, -1.3281e-02, -7.6004e-03]],

         ...,

         [[-3.5165e-02, -6.6405e-03, -2.1359e-02],
          [-7.2130e-03, -3.4714e-02, -2.7170e-02],
          [-3.5680e-03, -2.7512e-02, -9.6909e-04]],

         [[ 2.5092e-02, -1.3176e-02,  5.4947e-03],
          [ 2.0060e-02, -1.6674e-02, -2.0409e-02],
          [-2.9817e-03, -2.9977e-02,  1.0122e-02]],

         [[ 1.0972e-02,  1.4044e-02, -4.7886e-05],
          [ 6.1010e-03,  1.0914e-02, -1.0973e-02],
          [ 4.3752e-03,  2.3603e-02, -1.2009e-02]]],


        ...,


        [[[-1.4510e-02, -2.3661e-02, -3.3587e-02],
          [-1.0405e-02, -1.5233e-02,  2.1859e-03],
          [ 1.5305e-02,  7.4356e-03,  2.4158e-02]],

         [[-3.1655e-02, -7.6501e-03, -2.5680e-02],
          [-1.2832e-02,  1.4822e-02,  1.0636e-02],
          [-1.2155e-02,  3.1623e-02,  2.8013e-02]],

         [[ 3.0220e-02,  5.6618e-03,  3.9317e-04],
          [-1.6375e-02,  2.1488e-03, -2.7639e-02],
          [ 3.1249e-02,  2.3588e-02,  8.7405e-03]],

         ...,

         [[-2.1896e-02, -2.7240e-02,  7.1394e-03],
          [ 1.6672e-02, -1.4104e-02,  1.6233e-02],
          [-1.1399e-02,  3.4066e-02, -6.6961e-03]],

         [[ 1.1715e-02,  3.5039e-02,  1.1809e-02],
          [-1.1313e-02, -1.9447e-02,  2.5833e-02],
          [ 1.6163e-02, -2.3979e-03, -4.0140e-03]],

         [[ 3.7763e-02,  7.7345e-03,  1.7020e-02],
          [ 2.1013e-02, -1.4838e-02,  1.6160e-02],
          [-1.1352e-02, -3.4422e-02, -3.8429e-02]]],


        [[[-2.7078e-02, -1.3028e-03,  1.4147e-02],
          [ 2.1639e-02,  1.4386e-02, -2.0165e-02],
          [ 2.8901e-02,  2.5045e-02, -1.5982e-03]],

         [[ 7.8730e-03, -3.7202e-03,  1.7356e-03],
          [ 2.0062e-02,  3.8667e-02,  1.8409e-02],
          [ 2.2030e-02,  3.7085e-02, -1.2038e-02]],

         [[-1.9933e-02, -1.6009e-02, -2.7583e-02],
          [-1.7238e-02, -8.0347e-04, -9.3520e-03],
          [-2.1813e-03,  5.7111e-03, -2.7928e-02]],

         ...,

         [[-2.6303e-02,  1.6486e-02, -2.5282e-02],
          [ 1.6200e-02, -1.1975e-02, -6.7729e-03],
          [-1.0560e-02,  2.7123e-02,  1.2175e-02]],

         [[-2.8180e-02,  2.4816e-03,  1.4816e-02],
          [-2.4533e-03, -5.0049e-03,  3.8099e-03],
          [ 2.1920e-02,  2.2655e-02,  1.8570e-03]],

         [[-2.8237e-02, -6.9294e-03,  9.1655e-03],
          [-2.6871e-02,  1.2003e-02,  2.8070e-02],
          [-2.8799e-02,  9.1056e-04,  1.5202e-02]]],


        [[[-1.6076e-02,  1.4826e-02, -3.0205e-03],
          [-2.8991e-03,  1.1851e-02,  5.1029e-03],
          [-4.3482e-03, -1.7720e-03,  1.9587e-02]],

         [[-1.6089e-03, -2.2077e-02, -1.9120e-02],
          [-2.4841e-02, -1.3023e-02, -3.8639e-02],
          [ 3.8105e-02,  1.6235e-02,  3.5663e-03]],

         [[ 2.8855e-02,  1.0507e-03, -9.2811e-03],
          [-1.6607e-02, -1.2649e-02, -1.4965e-02],
          [ 2.7093e-02, -6.8489e-03, -2.0644e-02]],

         ...,

         [[ 9.8610e-03, -4.2381e-03,  9.5986e-03],
          [-2.0046e-02, -9.8796e-03,  4.6860e-03],
          [ 1.3990e-02,  6.7948e-04, -1.3807e-02]],

         [[-1.6009e-02, -9.6493e-03,  3.3491e-03],
          [ 1.6972e-02,  1.0124e-02, -3.1728e-02],
          [-1.6519e-02, -6.5039e-03, -8.1788e-03]],

         [[-2.4341e-03, -2.9762e-02, -1.6743e-02],
          [ 3.2999e-03,  2.4118e-02, -2.7304e-02],
          [ 2.6777e-02,  1.2820e-02,  2.0675e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([9.0482e+06, 3.4570e+05, 3.3001e+05,  ..., 3.8951e+01, 3.8718e+01,
        3.1922e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 804]) 

NULL SPACE BASIS :  tensor([[-0.0143,  0.0298,  0.0010,  ...,  0.0183,  0.0004,  0.0183],
        [-0.0344, -0.0456,  0.0081,  ..., -0.0341, -0.0060, -0.0133],
        [ 0.0062, -0.0282,  0.0169,  ...,  0.0099,  0.0101,  0.0031],
        ...,
        [-0.0179,  0.0377, -0.0238,  ..., -0.0092, -0.0056, -0.0006],
        [-0.0488, -0.0037,  0.0204,  ...,  0.0012,  0.0010,  0.0028],
        [-0.0430, -0.0394,  0.0065,  ...,  0.0064,  0.0029,  0.0004]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.3524e-02, -5.0750e-03, -2.1556e-03,  ..., -2.7086e-04,
         -4.5397e-04,  3.1142e-04],
        [-5.0750e-03,  2.4535e-02, -4.8374e-03,  ..., -8.8572e-05,
          3.0030e-04, -1.8075e-04],
        [-2.1556e-03, -4.8374e-03,  2.3071e-02,  ..., -1.0032e-04,
         -8.8009e-05, -8.1803e-05],
        ...,
        [-2.7086e-04, -8.8572e-05, -1.0032e-04,  ...,  2.0358e-02,
         -6.0583e-03, -2.2425e-03],
        [-4.5397e-04,  3.0030e-04, -8.8009e-05,  ..., -6.0583e-03,
          2.1225e-02, -6.0540e-03],
        [ 3.1142e-04, -1.8075e-04, -8.1803e-05,  ..., -2.2425e-03,
         -6.0540e-03,  2.0561e-02]], device='cuda:0') 

reserving basis 664/1152; cond: 368964.6875, radio:0.0072599551640450954
PARAMETER       :  Parameter containing:
tensor([[[[ 1.8253e-03, -1.3335e-02, -7.1527e-03],
          [ 1.8573e-02,  1.7629e-02,  3.7787e-02],
          [-1.1807e-02, -6.3281e-03,  1.5580e-02]],

         [[-1.6281e-02,  1.8968e-02, -1.0252e-02],
          [-1.1679e-02, -2.3754e-02, -2.5473e-02],
          [-6.4078e-04, -1.6029e-02, -2.8460e-03]],

         [[-2.0915e-02, -1.3851e-02,  3.0085e-02],
          [ 3.7787e-02,  2.5767e-02, -9.3725e-04],
          [ 5.0558e-03,  7.6541e-03, -7.5788e-03]],

         ...,

         [[ 3.8119e-02,  1.2792e-02, -6.4110e-03],
          [ 3.9475e-02,  1.4799e-02,  1.5576e-02],
          [-1.5660e-02,  1.7147e-03, -1.5742e-02]],

         [[ 4.2283e-02,  5.2389e-03,  2.8858e-02],
          [ 3.0003e-02,  1.3186e-02,  1.9375e-02],
          [-2.2920e-02, -3.3232e-03,  1.8766e-02]],

         [[ 1.1212e-02, -1.3753e-02, -1.4072e-02],
          [ 2.1000e-02,  1.9152e-02,  3.8292e-02],
          [-1.5416e-02,  1.4822e-02,  4.3612e-04]]],


        [[[ 1.9698e-02,  3.6534e-02, -2.6364e-02],
          [ 2.4441e-02, -3.7393e-03, -2.7366e-02],
          [ 9.3593e-03, -2.5003e-02, -1.8476e-02]],

         [[-3.1701e-02, -3.4297e-02, -4.2139e-02],
          [-1.9328e-02, -2.3281e-02, -6.9510e-03],
          [ 1.8161e-02, -1.4187e-02,  6.1958e-03]],

         [[ 1.1069e-02,  1.8179e-02,  2.8331e-02],
          [-4.0543e-03,  1.3825e-02, -5.4835e-03],
          [ 1.2873e-02, -9.2423e-03, -2.9163e-02]],

         ...,

         [[-1.0837e-02,  3.0489e-02, -1.6954e-02],
          [ 3.1017e-02, -7.9715e-03,  1.2321e-02],
          [ 8.8479e-03,  1.8715e-02, -2.0425e-02]],

         [[-2.6403e-02,  1.8650e-02, -1.8483e-02],
          [-1.6720e-02, -3.2028e-03, -2.0494e-02],
          [ 2.8387e-02, -1.0133e-02,  2.0388e-02]],

         [[ 1.2953e-02, -2.6239e-02,  3.3477e-02],
          [-1.9957e-02,  2.2412e-02,  4.0030e-04],
          [ 1.5059e-02, -1.0145e-03, -8.1839e-03]]],


        [[[-1.6575e-03, -1.7833e-03,  9.0877e-03],
          [-2.0239e-02,  6.4316e-03, -1.7486e-02],
          [-1.6551e-02,  1.7323e-03, -5.8037e-03]],

         [[ 1.3913e-02, -1.7367e-02, -1.5234e-02],
          [ 2.6080e-02,  6.3436e-05,  3.0847e-02],
          [ 3.3758e-02,  1.9882e-02, -1.4822e-02]],

         [[-1.4599e-02,  1.0381e-02, -2.4805e-02],
          [ 6.5293e-03,  1.7222e-02,  1.7274e-03],
          [ 4.7654e-05, -6.2540e-03,  3.1071e-02]],

         ...,

         [[ 1.8615e-02,  2.3637e-02,  1.6499e-02],
          [-1.6418e-02, -2.5394e-02,  4.0240e-03],
          [ 2.8881e-03,  1.7417e-02, -9.9348e-03]],

         [[ 1.5740e-02,  1.6897e-02,  8.8229e-03],
          [ 8.6167e-03, -9.3320e-03, -2.0097e-02],
          [ 4.4798e-03,  1.0741e-02,  3.2110e-02]],

         [[-1.2282e-02, -1.4779e-02, -4.7007e-03],
          [-2.0321e-02,  2.2291e-03,  5.2272e-04],
          [-2.4486e-03,  6.4499e-03,  4.1049e-02]]],


        ...,


        [[[-1.0172e-02, -2.4474e-02,  1.1545e-02],
          [ 1.9405e-02, -7.3120e-03,  1.7006e-02],
          [ 2.8551e-03, -1.5314e-02,  2.4913e-02]],

         [[-2.2479e-02, -2.3873e-02, -9.2383e-03],
          [-1.8643e-02, -1.7667e-02,  1.9694e-03],
          [-3.2471e-02,  2.3442e-02, -5.6575e-03]],

         [[-6.8596e-03,  3.2179e-03, -6.3691e-03],
          [-7.6982e-03,  1.9043e-02,  3.8018e-03],
          [ 3.0498e-02,  9.9889e-03,  1.9551e-02]],

         ...,

         [[ 2.4080e-04, -1.2114e-02,  3.9399e-02],
          [-2.0914e-03,  2.0490e-02,  2.1262e-02],
          [ 1.4841e-02,  9.8606e-03,  8.8925e-03]],

         [[-2.0921e-02,  2.1747e-03, -5.4617e-03],
          [-1.7116e-02,  1.2557e-02,  4.9127e-03],
          [ 1.1693e-02, -1.8913e-03,  1.9616e-02]],

         [[-4.4242e-03, -5.4889e-03, -9.0105e-03],
          [ 5.5883e-03,  2.3339e-02, -1.7677e-02],
          [-1.7813e-03, -1.3109e-02, -5.8251e-03]]],


        [[[-8.6633e-03,  3.8370e-02,  1.0477e-02],
          [-2.4596e-03, -5.4361e-04, -8.1301e-04],
          [-2.3333e-02,  2.3449e-02,  2.2298e-02]],

         [[ 1.6802e-02, -8.3944e-03,  3.8016e-04],
          [-3.3943e-03,  3.2208e-02,  1.1679e-02],
          [ 5.3937e-03, -8.2363e-03, -2.4993e-02]],

         [[-8.7682e-03, -1.4105e-02,  6.5057e-03],
          [-1.3191e-02, -3.1862e-02, -3.5237e-03],
          [ 1.5526e-02, -1.4562e-02,  2.7852e-02]],

         ...,

         [[-2.2762e-03,  1.1840e-02, -2.0024e-02],
          [ 1.5917e-02,  8.3444e-04,  1.6798e-02],
          [ 3.0335e-02,  1.2535e-02,  2.3844e-02]],

         [[ 9.4005e-03, -2.8562e-02,  2.4102e-03],
          [-1.6847e-02,  8.4536e-04, -1.9967e-02],
          [ 1.7747e-02,  1.9343e-02,  2.4782e-02]],

         [[-4.4973e-03, -3.0600e-02, -1.9025e-02],
          [ 1.2295e-02,  1.0290e-02,  2.9158e-03],
          [ 1.5796e-02,  5.1478e-03, -2.7758e-02]]],


        [[[ 2.1810e-02, -2.6494e-02,  2.4087e-04],
          [ 1.1414e-02,  7.5722e-03, -2.8190e-02],
          [ 2.0723e-02,  2.5141e-02,  1.1513e-02]],

         [[-4.1465e-02, -1.7436e-02, -3.0298e-02],
          [ 1.8121e-04,  1.2870e-02, -1.2255e-03],
          [ 4.1475e-03, -2.7137e-02,  1.0018e-02]],

         [[-9.3251e-03,  1.4780e-03, -8.4805e-03],
          [-2.5837e-02, -1.6721e-02, -2.2357e-02],
          [ 1.9800e-03, -1.1288e-02,  1.5434e-02]],

         ...,

         [[-1.4881e-02, -2.8269e-02, -2.8508e-02],
          [ 1.0857e-02,  1.3107e-03,  1.2968e-03],
          [ 1.8865e-02, -1.5798e-02, -1.5179e-02]],

         [[-3.2554e-03, -1.7688e-02, -6.6412e-03],
          [ 2.6956e-03,  4.2761e-03,  1.9433e-02],
          [ 1.4232e-03,  2.0573e-03,  3.2377e-03]],

         [[ 2.0697e-02,  1.2566e-02,  1.1928e-03],
          [ 2.0715e-02, -7.4182e-03,  2.0719e-02],
          [-1.4755e-02, -1.5175e-02,  1.5225e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([9.3510e+06, 3.7371e+05, 3.4768e+05,  ..., 3.4065e+01, 2.9943e+01,
        2.5344e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 664]) 

NULL SPACE BASIS :  tensor([[-1.6202e-02,  4.6049e-02,  3.7720e-03,  ...,  3.7561e-03,
          1.4890e-03, -6.7447e-03],
        [-3.0992e-02, -1.4037e-02, -8.0549e-03,  ...,  7.1287e-03,
          4.9429e-03, -3.8211e-05],
        [ 8.3675e-03,  4.6018e-04,  2.1073e-02,  ..., -3.5750e-03,
         -5.9064e-03,  3.7138e-03],
        ...,
        [-1.6100e-02, -3.7870e-03, -1.4156e-02,  ..., -6.0295e-03,
         -2.6147e-03,  1.1921e-02],
        [-4.4104e-02, -1.6243e-02, -8.9106e-03,  ...,  5.2731e-03,
          2.1012e-03, -7.6615e-03],
        [ 2.0462e-04, -2.2245e-02, -1.2583e-02,  ...,  7.4649e-03,
         -1.1727e-04,  2.2530e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.4457e-02, -2.8898e-03, -1.0294e-03,  ...,  2.4912e-04,
          6.4814e-04,  5.5928e-04],
        [-2.8898e-03,  1.5104e-02, -2.4071e-03,  ..., -7.9498e-05,
          3.3408e-04,  4.5980e-04],
        [-1.0294e-03, -2.4071e-03,  1.4430e-02,  ...,  3.6257e-04,
         -2.7631e-04,  5.0651e-05],
        ...,
        [ 2.4912e-04, -7.9498e-05,  3.6257e-04,  ...,  2.7433e-02,
         -5.0698e-03, -1.8884e-03],
        [ 6.4814e-04,  3.3408e-04, -2.7631e-04,  ..., -5.0698e-03,
          2.7036e-02, -4.7199e-03],
        [ 5.5928e-04,  4.5980e-04,  5.0651e-05,  ..., -1.8884e-03,
         -4.7199e-03,  2.6477e-02]], device='cuda:0') 

reserving basis 820/1152; cond: 290993.8125, radio:0.009355494752526283
PARAMETER       :  Parameter containing:
tensor([[[[-2.0582e-02, -1.4959e-02,  4.1978e-02],
          [ 2.1788e-02,  8.0553e-03, -1.8049e-03],
          [ 1.6461e-02,  2.1420e-02, -2.7392e-02]],

         [[ 6.7656e-03,  5.6211e-04,  3.2911e-02],
          [-2.6596e-03,  1.6596e-02,  2.9793e-02],
          [ 2.6365e-02, -2.4323e-02, -7.0233e-03]],

         [[ 1.7352e-02,  6.0661e-03,  8.4159e-03],
          [-5.6431e-03, -8.2159e-03,  2.5900e-02],
          [ 1.1137e-02,  2.0031e-02,  2.5525e-02]],

         ...,

         [[ 7.3685e-03,  1.6600e-02,  2.6145e-02],
          [ 3.3852e-02, -2.0209e-02,  2.3558e-02],
          [ 1.0307e-03, -1.3505e-02, -1.3706e-02]],

         [[-8.0253e-03, -7.2926e-03, -1.9253e-02],
          [-2.9866e-03,  3.3163e-03,  4.9497e-03],
          [ 1.6283e-03, -2.5061e-04, -7.1492e-03]],

         [[-3.8408e-02,  8.8675e-03, -1.7477e-02],
          [ 9.6423e-03,  2.4171e-03,  1.7462e-02],
          [ 2.6719e-02, -4.6487e-03,  2.3728e-02]]],


        [[[ 5.1057e-03, -3.1024e-03,  1.5751e-02],
          [ 2.1268e-02,  1.2995e-02, -1.8295e-03],
          [-3.3497e-03, -1.0216e-02, -3.2001e-03]],

         [[ 2.7518e-03,  1.2162e-02,  1.6901e-02],
          [-2.5830e-02, -2.5283e-03, -2.4702e-02],
          [-1.3572e-02,  1.8702e-02, -2.4467e-03]],

         [[-1.8904e-02, -6.2947e-03,  2.7579e-02],
          [ 3.2865e-03, -1.1197e-02, -1.1362e-02],
          [ 1.2469e-04,  1.2517e-02, -1.3022e-02]],

         ...,

         [[-2.0068e-02, -1.5830e-02, -2.1523e-02],
          [-2.5770e-02,  7.7289e-03, -2.1165e-02],
          [ 3.2716e-02,  2.2403e-03, -3.4695e-02]],

         [[ 1.7620e-02,  2.6319e-02,  3.4402e-02],
          [-1.4213e-02,  1.9486e-02, -9.9216e-03],
          [ 2.2699e-02,  1.9038e-02,  4.0184e-02]],

         [[ 1.0155e-03,  3.3422e-03,  8.4475e-04],
          [-5.0403e-03, -9.5446e-03,  2.0491e-02],
          [ 7.1081e-03,  4.7222e-02,  4.2670e-02]]],


        [[[-2.1009e-02, -2.0569e-02,  4.2063e-03],
          [-2.2708e-02,  4.3159e-03,  1.3723e-03],
          [ 5.9009e-03,  3.1060e-03,  5.0778e-03]],

         [[-2.7771e-04,  8.6772e-03,  1.0109e-02],
          [ 6.2085e-04,  2.2588e-02,  6.7472e-03],
          [ 2.4969e-03,  3.4018e-02, -1.9206e-02]],

         [[-4.6582e-03, -1.7443e-02, -2.2078e-02],
          [-2.2889e-02, -2.9562e-02, -5.8262e-03],
          [-7.9874e-03, -2.8685e-02, -1.0975e-02]],

         ...,

         [[ 1.3607e-02,  1.1015e-02,  4.2751e-02],
          [-2.1548e-02,  8.3043e-03, -1.6534e-02],
          [-1.8476e-02,  2.8824e-02, -2.2455e-02]],

         [[-3.7116e-02, -1.1846e-02, -2.8919e-03],
          [ 6.3683e-03, -1.1230e-02,  8.2044e-03],
          [-8.7437e-03,  1.7567e-02, -2.6639e-02]],

         [[ 1.8897e-03, -9.4814e-03, -1.5837e-02],
          [-2.9355e-02,  4.2824e-03,  2.4271e-02],
          [-2.0354e-02,  1.1044e-02,  1.9103e-02]]],


        ...,


        [[[-2.0708e-03,  1.0339e-02, -2.0892e-02],
          [-1.9747e-02, -1.4142e-02, -2.3688e-02],
          [-3.9065e-03, -1.5829e-02, -2.3685e-02]],

         [[ 3.1165e-02,  3.1142e-03,  7.3762e-03],
          [-8.4143e-03,  5.7778e-03, -2.3539e-02],
          [-3.5658e-02, -3.9505e-02, -1.6974e-02]],

         [[ 9.4346e-03,  1.1933e-02, -2.3184e-03],
          [-5.0858e-03, -1.6142e-02,  3.5010e-03],
          [-4.7144e-03,  3.6582e-02,  2.2469e-02]],

         ...,

         [[ 1.9560e-02, -6.8752e-03,  8.6195e-03],
          [-1.3067e-02, -2.3096e-02,  2.0531e-02],
          [ 1.2131e-02, -2.5829e-02, -1.8232e-02]],

         [[-8.1470e-03, -1.1936e-02,  2.9566e-02],
          [-3.5554e-02,  2.5979e-02,  1.5719e-02],
          [-1.8879e-02,  4.6800e-03,  1.6013e-02]],

         [[-2.0850e-02, -2.0018e-02,  4.4456e-03],
          [-2.7440e-03, -1.9151e-02, -2.3824e-02],
          [-5.5496e-03,  1.3921e-02, -1.8863e-02]]],


        [[[-3.0094e-02,  8.9740e-03,  1.0005e-02],
          [-8.3676e-03, -1.5369e-02, -2.5285e-02],
          [-2.5304e-02, -1.0116e-02, -2.9533e-04]],

         [[ 1.0466e-02,  1.7460e-02,  1.2655e-02],
          [-1.7542e-02, -2.1048e-02, -2.2637e-02],
          [-8.0184e-03, -8.0585e-03, -1.7314e-02]],

         [[-2.7919e-02, -2.0354e-02, -1.7889e-02],
          [ 1.9116e-02,  2.2120e-02,  3.0959e-02],
          [ 1.7524e-02, -1.6926e-02, -1.6424e-02]],

         ...,

         [[-3.6916e-02,  1.5131e-02,  1.0145e-02],
          [-3.2304e-02,  1.3529e-03, -5.6027e-03],
          [-2.4936e-03,  1.8372e-03,  1.1936e-02]],

         [[ 3.0954e-03, -1.2605e-02, -3.0790e-05],
          [ 4.7586e-03, -2.9873e-02, -7.2711e-04],
          [ 6.5085e-03, -5.2229e-03,  9.4848e-03]],

         [[-3.0235e-03,  7.1011e-03,  4.1749e-03],
          [-1.1439e-02,  7.8304e-04, -2.2077e-02],
          [ 5.2393e-03,  9.3470e-03, -1.8063e-02]]],


        [[[ 6.9849e-03, -7.5949e-03,  1.8660e-02],
          [ 1.2481e-02, -2.0307e-02,  9.9515e-03],
          [ 7.3483e-03,  2.0775e-02,  4.6509e-03]],

         [[ 3.1186e-02,  5.9528e-03,  9.7720e-03],
          [-1.0070e-02,  3.1384e-02,  1.8045e-02],
          [ 9.3399e-03,  1.6763e-03,  1.4201e-02]],

         [[ 6.6724e-03, -1.2917e-02,  4.5065e-03],
          [-7.0071e-03, -3.6056e-02, -1.8468e-02],
          [-3.6309e-02, -1.2102e-02,  4.4913e-03]],

         ...,

         [[ 8.8775e-03, -3.2216e-02,  1.2682e-02],
          [-3.6477e-02,  1.7354e-02,  8.7409e-04],
          [-1.5836e-02,  5.5057e-03, -1.3177e-02]],

         [[ 1.5744e-02, -2.2048e-03,  7.6205e-04],
          [ 2.5978e-03,  2.8073e-02,  2.7561e-02],
          [ 8.4761e-03,  1.5090e-02,  2.4275e-02]],

         [[ 2.0889e-02,  3.1580e-02,  2.2299e-02],
          [-6.2078e-03,  7.5893e-03, -6.0722e-03],
          [-2.1904e-02,  7.7753e-03, -1.5962e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.3806e+06, 1.3872e+05, 1.3086e+05,  ..., 9.0363e+00, 8.7746e+00,
        8.1810e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 820]) 

NULL SPACE BASIS :  tensor([[ 0.0612,  0.0371,  0.0600,  ..., -0.0074, -0.0182, -0.0075],
        [-0.0242, -0.0023,  0.0184,  ...,  0.0051,  0.0231,  0.0042],
        [ 0.0128,  0.0571,  0.0224,  ..., -0.0019, -0.0035,  0.0110],
        ...,
        [ 0.0261,  0.0233,  0.0142,  ..., -0.0035, -0.0104,  0.0033],
        [ 0.0104,  0.0675,  0.0762,  ...,  0.0097,  0.0168, -0.0026],
        [ 0.0002,  0.0574, -0.0109,  ..., -0.0095, -0.0087, -0.0136]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.5921e-02, -3.5593e-03, -1.3838e-03,  ..., -2.3325e-04,
         -9.7477e-05,  2.2476e-05],
        [-3.5593e-03,  2.6240e-02, -3.7779e-03,  ...,  1.4562e-04,
         -1.7427e-04,  4.9963e-05],
        [-1.3838e-03, -3.7779e-03,  2.5570e-02,  ..., -1.2099e-04,
          1.1037e-04, -1.4037e-04],
        ...,
        [-2.3325e-04,  1.4562e-04, -1.2099e-04,  ...,  2.3545e-02,
         -5.9372e-03, -2.4827e-03],
        [-9.7477e-05, -1.7427e-04,  1.1037e-04,  ..., -5.9372e-03,
          2.2973e-02, -6.3060e-03],
        [ 2.2476e-05,  4.9963e-05, -1.4037e-04,  ..., -2.4827e-03,
         -6.3060e-03,  2.1509e-02]], device='cuda:0') 

reserving basis 1700/2304; cond: 455055.15625, radio:0.010871823877096176
PARAMETER       :  Parameter containing:
tensor([[[[ 4.0050e-03, -4.5841e-03,  3.3923e-04],
          [-2.3476e-02, -6.8043e-03,  1.4098e-02],
          [-2.4393e-02, -6.0610e-03, -1.7260e-02]],

         [[ 6.4802e-03,  2.0929e-02,  2.0324e-03],
          [-6.6316e-03, -1.9012e-02,  2.0068e-02],
          [-1.1874e-02, -2.2384e-03,  1.2914e-02]],

         [[-1.8082e-02, -1.7024e-02, -1.1622e-02],
          [-2.2514e-03, -1.5673e-02, -2.0836e-02],
          [ 5.9968e-03,  7.9787e-03, -1.1662e-02]],

         ...,

         [[ 1.4148e-02,  1.1281e-02,  1.4720e-02],
          [-1.7817e-02, -1.5141e-02,  9.0116e-03],
          [ 2.2483e-03, -8.6370e-04,  8.3841e-04]],

         [[ 2.1394e-02,  6.3212e-04,  1.4714e-02],
          [-1.3814e-02,  6.7221e-04,  2.3722e-02],
          [ 2.0881e-02, -1.7408e-03,  8.4743e-03]],

         [[ 7.9362e-03,  8.8620e-03, -2.0624e-02],
          [ 6.7936e-03, -3.0417e-03,  3.4891e-03],
          [ 2.4972e-02,  6.6095e-03,  1.2665e-02]]],


        [[[ 9.2009e-03, -5.2761e-03, -2.3227e-03],
          [ 5.6092e-03,  7.5671e-03, -9.8283e-03],
          [ 2.3309e-02, -1.0393e-02, -1.4657e-02]],

         [[-1.0093e-02,  1.1757e-02, -1.6244e-03],
          [-1.8369e-02,  2.2138e-02,  1.1619e-02],
          [ 1.7709e-03, -5.0665e-03,  2.4189e-04]],

         [[-1.5676e-02, -7.1517e-04,  1.1979e-02],
          [ 2.4952e-02,  5.6385e-03,  8.5784e-03],
          [-6.7906e-03,  2.3303e-02, -1.2119e-02]],

         ...,

         [[-1.2174e-02,  2.2984e-02,  2.6759e-03],
          [ 2.0905e-02, -1.3873e-02,  1.5739e-02],
          [ 2.9251e-03, -1.9626e-02, -1.2126e-02]],

         [[-1.5154e-02, -2.6214e-02, -5.5489e-03],
          [-9.0886e-03,  3.4635e-03, -5.8574e-03],
          [-1.5327e-02,  1.6760e-02,  1.8190e-02]],

         [[ 7.5236e-03, -5.1986e-03, -2.0655e-02],
          [-9.2115e-03, -1.4618e-02,  5.9639e-03],
          [-1.9648e-02,  9.4109e-03, -6.9493e-03]]],


        [[[ 1.6677e-02, -2.2195e-02, -1.2095e-02],
          [ 1.6438e-02,  1.8666e-02, -2.7945e-03],
          [ 1.7573e-02,  2.1710e-02, -2.4869e-02]],

         [[-8.4745e-04,  1.1989e-02,  1.7154e-03],
          [-1.0638e-02, -1.4056e-02, -2.0019e-02],
          [-7.8283e-03,  2.2891e-02,  1.3514e-03]],

         [[-3.7214e-03, -1.0425e-02,  1.2838e-03],
          [-5.1867e-03, -5.0288e-03,  1.0761e-02],
          [-3.6348e-04, -1.4655e-02,  9.1700e-03]],

         ...,

         [[-1.6881e-02,  2.2077e-02, -7.6346e-03],
          [ 1.7843e-02, -1.5267e-02, -3.4740e-03],
          [-1.6182e-02, -1.2898e-02, -2.1379e-02]],

         [[-3.6672e-03,  1.8581e-02, -1.1928e-02],
          [-1.7376e-02, -2.1412e-02, -2.0681e-02],
          [ 1.8412e-02, -1.9087e-02, -1.0575e-02]],

         [[ 8.0891e-03,  2.7593e-03, -2.0023e-02],
          [ 2.3656e-03, -4.7066e-03,  1.0557e-02],
          [-9.7171e-03, -1.1446e-04, -9.3115e-03]]],


        ...,


        [[[ 8.0213e-03,  1.7775e-02,  7.5770e-03],
          [ 2.0473e-02, -1.5560e-02, -1.6005e-02],
          [-3.7174e-03, -1.3446e-02, -1.1771e-02]],

         [[-1.5559e-02, -9.8421e-03,  3.0474e-02],
          [ 8.2603e-05,  9.5835e-03, -2.5772e-03],
          [ 2.4319e-02, -1.9804e-02,  1.0719e-02]],

         [[-1.7040e-02, -9.8394e-04, -1.5123e-02],
          [ 4.5313e-03,  2.5208e-02, -2.5562e-02],
          [ 9.3220e-03,  3.1047e-02,  1.2876e-02]],

         ...,

         [[-6.7880e-03,  1.1096e-02,  1.6030e-03],
          [-1.3005e-02, -1.0860e-02,  7.1835e-03],
          [ 1.5742e-03,  1.7697e-03, -2.9245e-02]],

         [[-1.1973e-02,  2.1062e-03, -1.1567e-02],
          [-1.6117e-02, -1.0159e-02, -1.8509e-03],
          [-8.3695e-03,  4.7953e-03,  2.0604e-02]],

         [[-2.4247e-02, -4.0239e-02, -2.6568e-02],
          [ 1.1651e-02, -2.6963e-02, -1.8438e-02],
          [ 1.2836e-02, -1.0678e-02,  1.0625e-02]]],


        [[[-9.3332e-03,  1.6066e-02,  1.0087e-03],
          [-1.2538e-02, -1.9819e-02,  3.6814e-03],
          [ 1.0716e-02, -3.8938e-03, -1.4099e-02]],

         [[-4.0538e-03,  5.8146e-03, -3.9711e-03],
          [ 4.9315e-03, -1.4694e-02,  1.4976e-02],
          [ 8.3036e-03, -1.3716e-02, -1.6294e-02]],

         [[-2.0580e-02, -2.7214e-02, -1.9556e-02],
          [ 2.6215e-02,  1.8639e-02,  7.7093e-04],
          [-3.4405e-03,  9.2875e-03, -4.6529e-03]],

         ...,

         [[ 7.3384e-03, -1.4210e-02, -3.1267e-02],
          [ 1.4287e-02,  1.3946e-02, -2.4704e-02],
          [-1.6115e-02, -2.3519e-02,  1.1822e-02]],

         [[ 4.6657e-03,  6.9594e-04,  1.0598e-02],
          [-1.4065e-02, -8.5333e-03, -1.0839e-02],
          [-8.5307e-03,  7.7697e-03,  4.1502e-03]],

         [[-2.2515e-02,  1.7990e-02,  1.8062e-02],
          [-2.5596e-02, -8.8974e-03,  2.6620e-02],
          [-1.4620e-02,  3.7444e-02,  4.0974e-02]]],


        [[[-1.9851e-03, -3.0936e-02,  8.6495e-03],
          [-1.5323e-02, -8.0532e-03,  2.7125e-02],
          [-2.0023e-02, -8.7355e-03,  9.4648e-03]],

         [[-1.9558e-02, -1.6174e-02,  1.4077e-02],
          [-1.0957e-02,  7.0899e-03, -6.2422e-03],
          [ 5.2043e-03,  1.2247e-02,  1.6788e-03]],

         [[ 9.8255e-03, -7.1468e-03, -5.8649e-03],
          [-1.1576e-02,  2.2447e-02, -3.4895e-03],
          [-2.1787e-02,  1.4297e-03, -5.2785e-03]],

         ...,

         [[-1.8130e-03, -2.7872e-02, -6.7943e-04],
          [-2.2153e-02, -2.5610e-02,  8.5152e-03],
          [ 1.3242e-02, -1.3442e-02,  1.9205e-02]],

         [[ 9.2839e-03,  2.7905e-03,  1.7504e-02],
          [ 6.7182e-03, -1.2476e-02, -1.1659e-03],
          [-2.2702e-02,  1.4251e-02, -2.6711e-02]],

         [[-2.4604e-02,  8.7934e-03, -3.3672e-02],
          [ 1.0580e-02, -1.4032e-02, -5.3490e-03],
          [ 1.3003e-02, -2.5069e-02, -1.9132e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.4573e+06, 2.1029e+05, 1.9711e+05,  ..., 7.8638e+00, 7.6446e+00,
        7.5976e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1700]) 

NULL SPACE BASIS :  tensor([[-0.0064,  0.0245, -0.0397,  ..., -0.0072,  0.0174, -0.0067],
        [-0.0031, -0.0018,  0.0103,  ...,  0.0036, -0.0026,  0.0110],
        [ 0.0095,  0.0136,  0.0663,  ..., -0.0037, -0.0019, -0.0036],
        ...,
        [ 0.0056, -0.0039,  0.0002,  ...,  0.0116, -0.0381,  0.0006],
        [ 0.0103, -0.0030, -0.0034,  ..., -0.0126,  0.0250, -0.0106],
        [ 0.0144, -0.0070,  0.0094,  ...,  0.0051, -0.0143,  0.0073]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.8263e-02, -3.1000e-04, -1.2910e-04,  ...,  4.2532e-06,
         -1.6404e-04,  5.6231e-05],
        [-3.1000e-04,  1.8049e-02, -4.3665e-04,  ..., -7.5632e-05,
         -3.6990e-05, -1.6583e-04],
        [-1.2910e-04, -4.3665e-04,  1.8612e-02,  ...,  8.9336e-05,
         -1.3026e-04, -1.6140e-04],
        ...,
        [ 4.2532e-06, -7.5632e-05,  8.9336e-05,  ...,  1.9687e-02,
         -3.2107e-05, -1.3493e-04],
        [-1.6404e-04, -3.6990e-05, -1.3026e-04,  ..., -3.2107e-05,
          1.9394e-02, -7.6916e-05],
        [ 5.6231e-05, -1.6583e-04, -1.6140e-04,  ..., -1.3493e-04,
         -7.6916e-05,  1.9898e-02]], device='cuda:0') 

reserving basis 100/128; cond: 11193.4306640625, radio:0.02279159612953663
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0218]],

         [[-0.0357]],

         [[-0.0309]],

         ...,

         [[-0.0323]],

         [[-0.0706]],

         [[ 0.0765]]],


        [[[-0.0458]],

         [[ 0.0602]],

         [[ 0.0698]],

         ...,

         [[-0.0468]],

         [[-0.0039]],

         [[-0.0164]]],


        [[[ 0.0581]],

         [[-0.0273]],

         [[-0.0385]],

         ...,

         [[ 0.0570]],

         [[ 0.0201]],

         [[-0.0221]]],


        ...,


        [[[-0.0654]],

         [[ 0.0267]],

         [[ 0.0801]],

         ...,

         [[-0.0825]],

         [[-0.0640]],

         [[ 0.0278]]],


        [[[-0.0299]],

         [[-0.0726]],

         [[-0.0035]],

         ...,

         [[ 0.0289]],

         [[ 0.0515]],

         [[ 0.0237]]],


        [[[ 0.0425]],

         [[-0.0345]],

         [[-0.0664]],

         ...,

         [[-0.0537]],

         [[ 0.0645]],

         [[-0.0357]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.3312e+05, 1.7766e+04, 1.4840e+04, 5.4339e+03, 3.6586e+03, 3.5713e+03,
        2.9347e+03, 2.3426e+03, 1.6921e+03, 1.6271e+03, 1.4756e+03, 1.1199e+03,
        9.7930e+02, 8.5162e+02, 7.8831e+02, 7.1221e+02, 6.0754e+02, 5.7422e+02,
        5.1146e+02, 4.8119e+02, 4.5563e+02, 4.3305e+02, 4.2769e+02, 3.8828e+02,
        3.7344e+02, 3.5789e+02, 3.2767e+02, 3.0560e+02, 2.8712e+02, 2.7171e+02,
        2.5431e+02, 2.3987e+02, 2.3161e+02, 2.2091e+02, 2.1348e+02, 2.0562e+02,
        1.9564e+02, 1.8256e+02, 1.7410e+02, 1.7281e+02, 1.6675e+02, 1.6278e+02,
        1.5964e+02, 1.5642e+02, 1.4830e+02, 1.4522e+02, 1.3964e+02, 1.3582e+02,
        1.3392e+02, 1.2570e+02, 1.2240e+02, 1.1985e+02, 1.1827e+02, 1.1716e+02,
        1.1416e+02, 1.1180e+02, 1.1034e+02, 1.0617e+02, 1.0228e+02, 1.0102e+02,
        9.8935e+01, 9.8000e+01, 9.6197e+01, 9.3558e+01, 9.2225e+01, 8.9995e+01,
        8.6914e+01, 8.6051e+01, 8.4802e+01, 8.3733e+01, 8.2692e+01, 8.2058e+01,
        7.8781e+01, 7.8491e+01, 7.6931e+01, 7.5899e+01, 7.2670e+01, 7.1295e+01,
        7.1229e+01, 6.9873e+01, 6.8406e+01, 6.7655e+01, 6.6315e+01, 6.5407e+01,
        6.4111e+01, 6.3029e+01, 6.2145e+01, 6.1306e+01, 6.0993e+01, 5.9964e+01,
        5.8208e+01, 5.7861e+01, 5.7514e+01, 5.7115e+01, 5.6336e+01, 5.5516e+01,
        5.4587e+01, 5.3960e+01, 5.3363e+01, 5.2511e+01, 5.2153e+01, 5.0149e+01,
        4.9938e+01, 4.9451e+01, 4.8637e+01, 4.8070e+01, 4.7491e+01, 4.6544e+01,
        4.6464e+01, 4.5319e+01, 4.4749e+01, 4.4090e+01, 4.3133e+01, 4.2296e+01,
        4.1403e+01, 4.0588e+01, 4.0040e+01, 3.9722e+01, 3.8707e+01, 3.8533e+01,
        3.7813e+01, 3.6681e+01, 3.6322e+01, 3.5079e+01, 3.4210e+01, 3.3092e+01,
        3.1870e+01, 2.9760e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 100]) 

NULL SPACE BASIS :  tensor([[ 0.0892, -0.1300,  0.0083,  ..., -0.0359, -0.0288,  0.0725],
        [-0.1292, -0.1215,  0.0581,  ...,  0.1052,  0.1063, -0.0500],
        [ 0.0173, -0.0358, -0.0645,  ..., -0.1086,  0.1563, -0.0900],
        ...,
        [ 0.0368, -0.0222, -0.0054,  ...,  0.1185, -0.0468, -0.0013],
        [-0.0356, -0.0914,  0.0105,  ..., -0.0318, -0.0559,  0.0632],
        [-0.0265,  0.0623, -0.0400,  ..., -0.0719, -0.0407,  0.0423]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.9388e-02,  1.5381e-03, -3.5431e-04,  ..., -4.1613e-03,
          4.0724e-03, -1.0165e-02],
        [ 1.5381e-03,  6.9012e-02,  4.2046e-05,  ..., -1.7917e-03,
         -2.8540e-03,  7.1928e-03],
        [-3.5431e-04,  4.2046e-05,  8.5121e-02,  ...,  1.3810e-03,
         -9.3794e-04, -3.5020e-03],
        ...,
        [-4.1613e-03, -1.7917e-03,  1.3810e-03,  ...,  7.5898e-02,
          1.2957e-03, -6.6762e-03],
        [ 4.0724e-03, -2.8540e-03, -9.3794e-04,  ...,  1.2957e-03,
          7.8117e-02, -1.7235e-03],
        [-1.0165e-02,  7.1928e-03, -3.5020e-03,  ..., -6.6762e-03,
         -1.7235e-03,  6.8420e-02]], device='cuda:0') 

reserving basis 1686/2304; cond: 441303.0625, radio:0.010894279927015305
PARAMETER       :  Parameter containing:
tensor([[[[ 5.6871e-03,  1.9666e-03, -1.0324e-02],
          [ 4.6205e-03, -2.0232e-02,  7.4080e-04],
          [-6.1704e-03,  2.1092e-02, -6.1293e-03]],

         [[-1.9790e-03,  1.2234e-02,  1.7973e-03],
          [ 2.6595e-02, -6.0168e-03, -2.1164e-02],
          [-3.6967e-03,  5.8567e-03,  9.8151e-03]],

         [[ 1.5836e-02,  2.7132e-02, -1.2780e-02],
          [ 1.4295e-02,  1.0346e-02,  1.2000e-02],
          [-4.2343e-03,  9.2154e-03,  4.9832e-03]],

         ...,

         [[-1.0725e-03,  2.0552e-02,  3.3152e-03],
          [-2.8612e-02, -6.1921e-03, -3.0958e-02],
          [-4.3355e-05, -1.5799e-02, -2.2958e-03]],

         [[-2.3151e-02,  1.4426e-02,  9.8992e-03],
          [ 4.5291e-03, -2.5617e-02, -1.8809e-02],
          [-1.9364e-02, -1.5093e-02, -1.5530e-02]],

         [[ 1.0417e-03, -1.5174e-03, -2.1718e-02],
          [ 7.5993e-03,  1.1747e-02,  5.1094e-03],
          [ 2.3231e-02,  2.0329e-02, -1.3629e-02]]],


        [[[-5.7976e-03,  2.2881e-02,  3.1146e-02],
          [ 1.8845e-03, -1.9157e-02,  1.6083e-02],
          [ 6.5268e-03,  1.7770e-02, -2.5126e-02]],

         [[-1.3896e-02, -2.5466e-03, -3.9895e-02],
          [ 3.3109e-04, -5.6054e-03,  8.1163e-03],
          [ 2.0311e-03,  1.8831e-02,  2.5639e-02]],

         [[-7.4181e-03, -2.2833e-02,  1.5499e-02],
          [-1.0168e-02,  6.7335e-04, -1.3831e-02],
          [ 7.0643e-03,  7.4455e-03,  1.9149e-02]],

         ...,

         [[-1.1753e-02, -2.6059e-02, -1.1153e-02],
          [-8.7368e-03,  6.5708e-03,  1.7269e-02],
          [ 3.9921e-03,  1.4103e-02, -8.0761e-03]],

         [[ 1.4312e-03, -1.7334e-02, -7.1680e-04],
          [ 1.0121e-02,  1.7155e-02,  2.2839e-02],
          [-1.6491e-02, -1.9467e-02,  1.9947e-02]],

         [[ 6.6231e-03, -1.1903e-02, -2.9183e-02],
          [-7.7177e-03,  5.3667e-03,  3.8116e-03],
          [-1.3390e-02, -2.5381e-02, -1.2973e-02]]],


        [[[-1.3815e-02,  4.1152e-04,  3.3518e-03],
          [ 1.2387e-02, -2.0401e-02, -1.6946e-02],
          [-1.0414e-02,  2.1268e-02,  3.8922e-03]],

         [[-1.6289e-02,  7.4465e-03,  3.2342e-03],
          [-1.1356e-02, -4.3133e-03,  1.8732e-02],
          [-1.8326e-02,  1.5931e-02,  1.4828e-03]],

         [[ 2.1500e-02, -9.3202e-03,  1.9253e-02],
          [-1.5559e-02, -3.7161e-03,  2.6028e-02],
          [-1.6407e-02, -1.1329e-02,  2.2791e-02]],

         ...,

         [[ 1.7195e-02, -5.8872e-03, -1.6828e-02],
          [ 1.0708e-02, -2.9697e-02, -3.9065e-03],
          [ 7.6348e-03, -2.9817e-02, -3.3816e-02]],

         [[-8.3283e-03,  8.3321e-04, -1.4526e-02],
          [ 1.3837e-02,  5.7782e-03,  4.6531e-03],
          [ 2.4822e-02, -3.1689e-02, -8.7406e-03]],

         [[-1.8529e-03, -1.1306e-02, -1.0753e-02],
          [-1.0585e-02, -1.4665e-02,  3.6020e-03],
          [ 1.7181e-02,  3.0268e-02,  1.7870e-02]]],


        ...,


        [[[-1.9894e-02,  1.1734e-02, -2.7210e-02],
          [ 4.2500e-03, -1.1259e-02, -2.9914e-02],
          [ 1.0342e-02,  4.0233e-04, -5.8277e-03]],

         [[-9.7062e-03, -2.4696e-02, -3.0327e-03],
          [ 1.4493e-02,  1.6251e-02,  7.2577e-03],
          [-1.6727e-02, -2.4802e-02, -1.1905e-02]],

         [[ 3.7833e-03, -1.8089e-02,  1.1629e-02],
          [-1.9679e-02,  1.3061e-04,  2.0988e-02],
          [ 1.7066e-02,  5.3743e-03, -1.6328e-02]],

         ...,

         [[ 1.5543e-02, -3.6734e-02,  1.2357e-02],
          [-1.2997e-02, -1.7661e-03,  5.9501e-03],
          [-1.4490e-02, -2.3795e-02,  2.9692e-03]],

         [[ 1.4736e-02, -2.2007e-03,  2.4180e-03],
          [-1.5001e-02,  1.1060e-02,  7.4313e-04],
          [-2.0966e-02,  1.7176e-03,  3.0857e-03]],

         [[-1.9725e-02, -1.1503e-02, -2.4786e-02],
          [ 9.4127e-03,  1.3247e-02, -4.9253e-03],
          [-2.2044e-02,  2.5470e-02,  3.4884e-03]]],


        [[[ 7.8281e-03, -5.3870e-03,  3.5949e-02],
          [ 6.4282e-03,  1.8919e-02,  1.1259e-02],
          [ 2.6362e-02, -5.3973e-03,  3.4646e-02]],

         [[ 4.7379e-03,  2.0565e-02, -1.7525e-02],
          [-1.3231e-02,  3.8952e-03,  2.1865e-02],
          [ 3.4350e-03,  9.7756e-03,  2.6963e-02]],

         [[-1.2921e-02,  2.7157e-03,  1.4366e-02],
          [-1.4585e-02,  7.8431e-03, -2.3098e-02],
          [ 3.2982e-03,  1.8886e-02,  9.8933e-03]],

         ...,

         [[ 9.8566e-03,  1.2454e-02, -1.4322e-02],
          [ 1.3970e-02,  1.7964e-02, -1.7852e-02],
          [ 1.0569e-04,  1.8260e-02,  3.0875e-02]],

         [[ 4.7189e-03, -9.4288e-03,  1.9306e-02],
          [ 1.0052e-02,  1.6176e-02,  2.0818e-02],
          [-1.3827e-02,  7.7715e-03,  1.4444e-02]],

         [[-1.2936e-02, -9.4064e-03,  4.6946e-03],
          [ 7.2765e-03, -4.2489e-03,  2.0950e-03],
          [-1.5669e-02, -2.6596e-03,  1.8499e-02]]],


        [[[ 2.8620e-02,  3.8367e-03, -6.9841e-03],
          [ 8.9332e-03, -4.7505e-03,  7.9477e-03],
          [-2.4441e-03, -1.2269e-02, -7.7692e-03]],

         [[-1.0613e-02, -7.2526e-03,  1.3472e-02],
          [ 1.7957e-02, -1.8977e-03,  2.3803e-02],
          [-1.5356e-03,  2.6204e-02,  1.0641e-02]],

         [[-1.2604e-02, -2.1988e-02, -4.1777e-03],
          [-1.3517e-02, -2.7543e-02, -1.4088e-02],
          [ 1.0892e-03,  4.3282e-05,  4.1633e-03]],

         ...,

         [[ 1.0127e-02,  8.2470e-03,  1.7919e-02],
          [ 1.5788e-02,  1.4229e-02, -1.2154e-02],
          [-5.5490e-04, -1.3326e-02, -1.5400e-02]],

         [[ 1.4155e-02, -1.0483e-02, -1.3601e-02],
          [-6.2660e-04, -2.3713e-02, -1.5051e-02],
          [ 2.4437e-04, -2.1730e-02, -2.0616e-02]],

         [[ 6.1547e-03, -2.9046e-03, -1.1953e-02],
          [-6.4253e-05, -8.1258e-03, -1.6077e-02],
          [ 2.2307e-02,  3.9022e-03,  1.5018e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.9662e+06, 1.9827e+05, 1.9015e+05,  ..., 7.1631e+00, 7.0128e+00,
        6.7214e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1686]) 

NULL SPACE BASIS :  tensor([[ 7.7847e-03,  3.9855e-03, -2.9879e-02,  ..., -1.0726e-02,
          6.4099e-04, -1.3087e-02],
        [ 4.1714e-04, -4.0362e-02, -2.9172e-02,  ..., -2.4699e-03,
         -8.5099e-03,  1.3104e-02],
        [-2.9204e-04,  5.7766e-03, -1.1761e-02,  ...,  4.9552e-03,
         -4.7954e-03,  2.9590e-03],
        ...,
        [ 2.3533e-05, -1.6068e-03, -1.4921e-02,  ..., -3.5343e-03,
          1.9739e-02,  1.1071e-02],
        [-1.7812e-02,  4.8013e-03,  1.0445e-02,  ...,  6.1579e-03,
         -3.6413e-02, -1.6294e-02],
        [ 1.7863e-03, -1.1298e-02,  1.0134e-02,  ..., -1.5755e-02,
          2.1980e-02,  3.2844e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.5565e-02, -3.6936e-03, -9.0300e-04,  ...,  1.1945e-04,
         -2.9008e-05, -3.0272e-05],
        [-3.6936e-03,  1.5920e-02, -3.7512e-03,  ..., -3.8610e-05,
          4.1315e-05,  3.5745e-05],
        [-9.0300e-04, -3.7512e-03,  1.5438e-02,  ..., -2.9520e-04,
         -3.0726e-05,  3.7950e-04],
        ...,
        [ 1.1945e-04, -3.8610e-05, -2.9520e-04,  ...,  2.0021e-02,
         -4.2228e-04, -2.8105e-04],
        [-2.9008e-05,  4.1315e-05, -3.0726e-05,  ..., -4.2228e-04,
          1.9607e-02, -3.1731e-04],
        [-3.0272e-05,  3.5745e-05,  3.7950e-04,  ..., -2.8105e-04,
         -3.1731e-04,  1.9777e-02]], device='cuda:0') 

reserving basis 1416/2304; cond: 634924.5, radio:0.006772081833332777
PARAMETER       :  Parameter containing:
tensor([[[[-1.3385e-02, -5.6633e-03, -1.4342e-02],
          [-4.2366e-03, -2.9065e-02,  7.3314e-03],
          [-1.4521e-02, -1.2255e-02, -4.0833e-03]],

         [[ 2.2985e-02,  2.6963e-02,  2.8722e-02],
          [-1.1410e-02,  1.1983e-02, -1.9576e-02],
          [-8.2922e-03,  2.4632e-02,  2.5770e-02]],

         [[-5.2863e-03, -1.2589e-02, -2.8403e-02],
          [-1.4420e-02, -1.7740e-02, -3.7626e-03],
          [ 1.3859e-02, -2.2141e-02, -1.6225e-02]],

         ...,

         [[-1.7761e-02,  2.2198e-02, -1.8973e-02],
          [ 5.4885e-03,  2.6661e-03, -2.2835e-02],
          [ 6.8287e-04,  2.3510e-03,  8.9987e-03]],

         [[ 2.0529e-02,  2.1390e-03, -2.0325e-02],
          [-1.2059e-02,  2.3043e-03, -8.7191e-03],
          [-2.7200e-02,  1.5332e-02,  1.0143e-02]],

         [[ 3.9496e-03,  2.5480e-02,  7.5078e-03],
          [-1.0479e-02,  1.4239e-02, -1.7847e-02],
          [ 9.1216e-03, -4.8543e-03,  1.5900e-02]]],


        [[[ 1.5624e-03,  2.0813e-02,  3.3263e-02],
          [-2.4054e-02,  1.8890e-02,  8.2914e-03],
          [ 3.9092e-03, -4.8917e-03,  7.5188e-03]],

         [[ 2.1404e-02,  1.6839e-02, -1.9664e-02],
          [ 1.2396e-02,  2.9677e-03, -4.3805e-03],
          [-2.9733e-02, -1.1745e-02, -1.8047e-02]],

         [[ 2.6040e-03, -3.0100e-03,  9.4459e-03],
          [ 4.5405e-03, -2.6109e-02, -1.6829e-02],
          [ 1.5173e-02, -8.4019e-03,  1.6978e-02]],

         ...,

         [[-1.8580e-02, -3.6937e-03, -1.4444e-02],
          [ 8.3935e-03,  1.2977e-02, -2.5033e-05],
          [-1.8807e-03,  1.4246e-02, -9.1531e-03]],

         [[ 2.7415e-03, -7.2305e-03,  2.0971e-02],
          [-1.0732e-03, -4.2749e-03,  2.0983e-02],
          [ 5.9590e-03,  1.0087e-02,  1.4911e-02]],

         [[-2.3173e-02, -1.5816e-02, -2.2148e-02],
          [-1.8801e-02, -1.5387e-02, -1.7342e-02],
          [-1.9276e-02, -1.6785e-02,  2.6344e-03]]],


        [[[-2.7837e-02, -1.9755e-02, -1.6217e-02],
          [ 1.8155e-03,  1.7449e-02,  7.1104e-03],
          [ 1.0838e-02, -8.1067e-03, -1.6204e-02]],

         [[-1.4725e-02,  1.6730e-02,  5.0948e-03],
          [-8.8503e-03,  2.9880e-03,  2.8798e-03],
          [-3.6095e-03, -1.4711e-02,  1.8285e-02]],

         [[-8.1591e-03,  7.6883e-03,  5.6101e-04],
          [-1.2517e-03, -1.6641e-02, -2.4680e-02],
          [ 9.0435e-03, -5.5658e-03,  2.5553e-03]],

         ...,

         [[-5.1166e-03,  2.7465e-02, -9.0557e-03],
          [-1.8119e-02,  1.5302e-03, -1.5835e-02],
          [-2.0621e-02, -2.3491e-02,  8.1079e-03]],

         [[-1.3273e-04,  2.3739e-03, -6.9489e-04],
          [-2.5095e-03,  1.3204e-02,  9.1314e-03],
          [-1.8334e-02, -1.6990e-02,  1.5119e-03]],

         [[-1.9910e-02, -1.0405e-02, -3.4689e-04],
          [ 1.0773e-02, -8.0999e-03,  1.9279e-02],
          [-6.4989e-03, -1.4286e-03, -1.2914e-02]]],


        ...,


        [[[-1.4130e-02, -1.6791e-02,  1.7132e-03],
          [-1.2452e-03,  1.0876e-02,  1.1393e-02],
          [-1.2597e-02, -1.2621e-02,  3.8516e-03]],

         [[ 6.1583e-04,  2.1133e-02, -2.7701e-03],
          [ 1.4445e-02, -2.6378e-03,  2.8557e-02],
          [-2.7821e-03, -1.3843e-02, -1.0767e-02]],

         [[-2.4983e-03, -2.0094e-02,  6.4358e-03],
          [-6.2076e-04,  1.9783e-02, -4.7339e-03],
          [-3.6395e-02, -6.0446e-03,  2.2760e-02]],

         ...,

         [[ 2.4070e-02,  4.0577e-03,  1.3919e-02],
          [ 9.0506e-03,  1.7554e-02,  1.6116e-02],
          [-1.5389e-03, -1.0831e-02,  6.5313e-03]],

         [[ 3.1470e-03, -1.4500e-02, -2.3105e-02],
          [-4.3752e-03,  1.8531e-02, -6.0331e-03],
          [ 1.1194e-02, -1.3260e-02,  1.6759e-02]],

         [[ 1.2392e-02,  1.4105e-02, -6.4158e-03],
          [-1.1934e-02, -2.1728e-03,  1.7486e-02],
          [ 1.2593e-02, -1.8306e-03,  7.3771e-03]]],


        [[[ 9.0142e-03, -1.8070e-02,  7.1795e-03],
          [ 2.6874e-02,  4.3937e-03, -9.8712e-03],
          [-2.5450e-03,  2.1811e-03, -1.7104e-02]],

         [[ 2.5053e-03, -9.3852e-03,  1.4603e-02],
          [ 1.7867e-02,  1.9195e-02,  3.5194e-02],
          [ 2.9856e-02,  2.0102e-02,  3.3360e-02]],

         [[-1.3459e-02,  1.6018e-02, -7.8110e-03],
          [ 2.7627e-02,  3.0386e-02,  1.1441e-02],
          [-1.7326e-02,  6.0834e-04,  2.3851e-02]],

         ...,

         [[-1.3867e-02,  6.8996e-03,  2.0975e-02],
          [ 1.6976e-02,  9.4974e-03,  1.1859e-02],
          [-1.3619e-02,  9.7064e-03, -1.4994e-02]],

         [[-2.1670e-02, -2.0413e-02,  8.1601e-03],
          [-1.0985e-02,  9.0958e-03, -7.5877e-04],
          [ 1.7078e-02, -1.4906e-02, -8.6594e-03]],

         [[-1.8511e-02,  2.0064e-03,  2.2542e-02],
          [ 1.4113e-02,  2.0406e-02, -1.1542e-02],
          [ 1.3355e-02,  1.1505e-02, -7.0077e-03]]],


        [[[ 1.5206e-02, -1.1938e-02, -2.5033e-03],
          [-6.1638e-03, -1.0804e-02, -2.3993e-03],
          [-2.1078e-02, -1.9197e-02, -1.0195e-03]],

         [[ 5.0653e-03, -6.4618e-03, -1.3182e-02],
          [-6.2343e-03, -2.3337e-03, -1.7980e-02],
          [ 4.6137e-04, -2.1380e-02, -9.6746e-03]],

         [[ 3.1082e-02,  1.7808e-03,  2.4223e-03],
          [ 2.4707e-02,  4.4508e-03,  9.8389e-03],
          [-2.9770e-03, -5.7272e-03, -1.3742e-02]],

         ...,

         [[ 1.4594e-02,  1.2486e-02,  2.2173e-03],
          [ 1.9890e-02, -7.7253e-03,  5.9900e-03],
          [-6.2735e-03, -3.4618e-03,  3.3402e-04]],

         [[-6.6168e-03,  3.5532e-03, -1.9957e-02],
          [ 2.0433e-02,  1.8533e-02,  1.2596e-02],
          [ 3.4982e-04,  3.8100e-02,  3.2845e-02]],

         [[ 2.0373e-02, -7.7966e-04, -1.1289e-02],
          [ 7.5752e-03,  7.5602e-03, -1.2554e-02],
          [-3.0588e-02, -3.4345e-03, -1.7221e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.6994e+06, 2.1769e+05, 1.8257e+05,  ..., 4.9601e+00, 4.7907e+00,
        4.2515e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1416]) 

NULL SPACE BASIS :  tensor([[-3.8374e-02,  2.4638e-03, -1.5352e-02,  ..., -8.1115e-03,
         -3.3552e-03,  8.3715e-03],
        [ 1.3405e-02,  2.7413e-02, -1.2518e-02,  ..., -4.3540e-03,
          1.1506e-02, -7.7917e-05],
        [ 2.5111e-02, -1.6559e-02,  2.9857e-03,  ...,  1.2573e-02,
         -1.4325e-02, -1.0400e-02],
        ...,
        [ 1.2551e-02,  3.1920e-03,  1.3784e-02,  ..., -1.3501e-02,
         -4.2701e-03, -7.9363e-03],
        [-5.6203e-04, -2.8515e-02, -4.6307e-02,  ...,  1.0496e-02,
         -3.6054e-03,  2.7782e-03],
        [ 3.2248e-02,  3.6948e-03,  2.4340e-02,  ..., -3.8857e-05,
          2.2661e-03,  4.3340e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.8065e-02, -2.5237e-03, -6.3095e-04,  ...,  9.8884e-05,
         -2.2370e-04, -2.5379e-05],
        [-2.5237e-03,  1.7806e-02, -2.9386e-03,  ..., -2.1835e-05,
          2.5916e-04, -3.0437e-06],
        [-6.3095e-04, -2.9386e-03,  1.9481e-02,  ..., -1.4781e-04,
          9.9098e-05,  4.7553e-04],
        ...,
        [ 9.8884e-05, -2.1835e-05, -1.4781e-04,  ...,  1.3572e-02,
         -2.1063e-03, -1.6424e-04],
        [-2.2370e-04,  2.5916e-04,  9.9098e-05,  ..., -2.1063e-03,
          1.3053e-02, -2.2005e-03],
        [-2.5379e-05, -3.0437e-06,  4.7553e-04,  ..., -1.6424e-04,
         -2.2005e-03,  1.3156e-02]], device='cuda:0') 

reserving basis 935/2304; cond: 1766553.25, radio:0.0017639833968132734
PARAMETER       :  Parameter containing:
tensor([[[[-0.0056, -0.0091, -0.0069],
          [-0.0081,  0.0163,  0.0129],
          [-0.0241, -0.0042, -0.0029]],

         [[-0.0003,  0.0020, -0.0127],
          [-0.0083,  0.0121, -0.0046],
          [ 0.0165, -0.0250,  0.0050]],

         [[ 0.0150, -0.0052,  0.0086],
          [ 0.0127, -0.0102,  0.0018],
          [-0.0179, -0.0114, -0.0071]],

         ...,

         [[ 0.0147, -0.0149,  0.0015],
          [ 0.0099,  0.0105,  0.0144],
          [-0.0015, -0.0255,  0.0124]],

         [[ 0.0205,  0.0115, -0.0024],
          [-0.0104,  0.0026, -0.0107],
          [-0.0027,  0.0180, -0.0168]],

         [[-0.0057, -0.0044,  0.0080],
          [-0.0314,  0.0028, -0.0002],
          [ 0.0055, -0.0103,  0.0203]]],


        [[[-0.0118,  0.0082,  0.0076],
          [ 0.0107, -0.0074,  0.0152],
          [ 0.0160, -0.0058, -0.0012]],

         [[ 0.0174, -0.0014, -0.0195],
          [ 0.0014, -0.0045,  0.0082],
          [ 0.0133, -0.0213, -0.0157]],

         [[ 0.0154, -0.0266,  0.0023],
          [-0.0222,  0.0005,  0.0087],
          [-0.0108, -0.0181,  0.0042]],

         ...,

         [[-0.0207, -0.0176, -0.0079],
          [ 0.0007, -0.0118, -0.0258],
          [ 0.0043, -0.0044, -0.0003]],

         [[-0.0088,  0.0061, -0.0063],
          [ 0.0090,  0.0242, -0.0098],
          [-0.0150,  0.0119, -0.0092]],

         [[ 0.0107,  0.0213,  0.0038],
          [ 0.0013,  0.0059, -0.0088],
          [-0.0081,  0.0260, -0.0028]]],


        [[[ 0.0036, -0.0243, -0.0042],
          [-0.0032,  0.0106, -0.0045],
          [ 0.0071,  0.0104,  0.0034]],

         [[ 0.0217,  0.0196,  0.0215],
          [ 0.0206,  0.0093,  0.0061],
          [ 0.0092, -0.0036,  0.0098]],

         [[-0.0176, -0.0137,  0.0032],
          [ 0.0058, -0.0197, -0.0055],
          [-0.0131, -0.0084, -0.0305]],

         ...,

         [[ 0.0014, -0.0040, -0.0321],
          [-0.0052,  0.0032, -0.0085],
          [ 0.0188,  0.0014, -0.0028]],

         [[ 0.0128,  0.0037, -0.0045],
          [-0.0061, -0.0013,  0.0059],
          [-0.0228, -0.0226, -0.0146]],

         [[ 0.0004,  0.0045, -0.0066],
          [ 0.0007,  0.0163, -0.0003],
          [ 0.0107,  0.0107, -0.0098]]],


        ...,


        [[[-0.0006,  0.0011, -0.0144],
          [-0.0145,  0.0026,  0.0002],
          [ 0.0075, -0.0107, -0.0080]],

         [[ 0.0157, -0.0044, -0.0121],
          [ 0.0216,  0.0147, -0.0147],
          [-0.0067,  0.0026, -0.0143]],

         [[-0.0207, -0.0022,  0.0078],
          [-0.0060, -0.0147,  0.0069],
          [ 0.0047,  0.0031, -0.0251]],

         ...,

         [[ 0.0085, -0.0126,  0.0074],
          [ 0.0015,  0.0051, -0.0058],
          [-0.0051, -0.0072, -0.0298]],

         [[-0.0154,  0.0072, -0.0030],
          [ 0.0079, -0.0235, -0.0211],
          [-0.0077, -0.0136, -0.0119]],

         [[ 0.0101,  0.0189,  0.0065],
          [-0.0102,  0.0120, -0.0100],
          [-0.0023,  0.0100, -0.0062]]],


        [[[-0.0189, -0.0038, -0.0014],
          [-0.0102,  0.0024,  0.0075],
          [ 0.0022,  0.0114,  0.0079]],

         [[-0.0128,  0.0218,  0.0108],
          [-0.0031,  0.0028,  0.0115],
          [ 0.0086, -0.0103,  0.0119]],

         [[-0.0082,  0.0047,  0.0183],
          [ 0.0135, -0.0056,  0.0140],
          [-0.0052, -0.0103, -0.0125]],

         ...,

         [[ 0.0121, -0.0182, -0.0017],
          [-0.0030, -0.0153,  0.0106],
          [-0.0156, -0.0126,  0.0037]],

         [[-0.0028,  0.0072, -0.0266],
          [-0.0159, -0.0061, -0.0338],
          [-0.0150, -0.0202, -0.0057]],

         [[ 0.0117,  0.0175,  0.0274],
          [ 0.0146,  0.0028,  0.0027],
          [ 0.0078, -0.0013,  0.0126]]],


        [[[ 0.0019,  0.0081,  0.0230],
          [-0.0040, -0.0029, -0.0169],
          [ 0.0064,  0.0052, -0.0080]],

         [[ 0.0002, -0.0045, -0.0105],
          [-0.0103, -0.0108, -0.0168],
          [ 0.0065, -0.0262,  0.0036]],

         [[ 0.0014, -0.0182,  0.0081],
          [-0.0131,  0.0099,  0.0050],
          [-0.0210, -0.0002, -0.0092]],

         ...,

         [[-0.0149, -0.0161,  0.0201],
          [-0.0117, -0.0120, -0.0228],
          [ 0.0089, -0.0138,  0.0188]],

         [[ 0.0216,  0.0090, -0.0085],
          [ 0.0141,  0.0109,  0.0166],
          [-0.0097, -0.0214, -0.0013]],

         [[-0.0100, -0.0095,  0.0017],
          [-0.0174,  0.0278,  0.0160],
          [-0.0205, -0.0041, -0.0181]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([7.9889e+05, 8.9244e+04, 8.6031e+04,  ..., 4.6153e-01, 4.5868e-01,
        4.5223e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 935]) 

NULL SPACE BASIS :  tensor([[-0.0213,  0.0019, -0.0143,  ...,  0.0025,  0.0227, -0.0012],
        [ 0.0147, -0.0160,  0.0089,  ..., -0.0015, -0.0106, -0.0086],
        [-0.0141, -0.0267, -0.0098,  ..., -0.0024,  0.0153, -0.0114],
        ...,
        [-0.0303,  0.0278, -0.0256,  ..., -0.0297, -0.0042,  0.0332],
        [-0.0195,  0.0018, -0.0189,  ..., -0.0044,  0.0192, -0.0256],
        [ 0.0149,  0.0085,  0.0010,  ..., -0.0097, -0.0029,  0.0014]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 8.3220e-03, -3.3041e-03,  3.2325e-04,  ...,  4.5615e-04,
          1.6114e-04, -1.3674e-05],
        [-3.3041e-03,  7.9865e-03, -3.2468e-03,  ..., -1.7308e-04,
         -2.8112e-04, -4.0836e-05],
        [ 3.2325e-04, -3.2468e-03,  6.1604e-03,  ...,  8.8620e-05,
          4.1294e-04,  8.8722e-06],
        ...,
        [ 4.5615e-04, -1.7308e-04,  8.8620e-05,  ...,  1.6112e-02,
         -3.1198e-03, -6.1492e-04],
        [ 1.6114e-04, -2.8112e-04,  4.1294e-04,  ..., -3.1198e-03,
          1.4598e-02, -3.4415e-03],
        [-1.3674e-05, -4.0836e-05,  8.8722e-06,  ..., -6.1492e-04,
         -3.4415e-03,  1.3080e-02]], device='cuda:0') 

reserving basis 5/4608; cond: 2324721664.0, radio:7.200558371778243e-09
PARAMETER       :  Parameter containing:
tensor([[[[ 5.6966e-03,  4.3113e-03,  1.3618e-02],
          [-7.4745e-03, -8.3634e-03,  1.3038e-02],
          [-4.6928e-03, -5.2487e-03,  9.5846e-03]],

         [[ 4.1137e-03, -6.0026e-03,  1.9685e-03],
          [ 5.4075e-03,  9.6618e-03,  2.2747e-03],
          [ 1.7836e-02, -7.0006e-03,  8.3898e-03]],

         [[-2.7339e-03, -1.3981e-02,  4.4896e-05],
          [-5.8201e-03, -6.3349e-03, -2.3115e-02],
          [-1.1647e-02,  1.0212e-02, -2.0821e-02]],

         ...,

         [[-4.3780e-03,  1.7393e-03,  4.3884e-03],
          [-2.6151e-03, -2.4857e-03, -1.8380e-02],
          [-1.0031e-02, -1.1749e-03, -2.5735e-02]],

         [[-4.2887e-03,  9.7986e-03,  1.3226e-02],
          [ 2.4993e-03,  1.6517e-02,  3.9232e-03],
          [ 3.2239e-04,  5.1875e-03,  5.0068e-03]],

         [[-1.4520e-02, -3.2768e-02, -2.8027e-02],
          [-6.5761e-03, -9.1114e-04, -2.0037e-02],
          [ 7.1262e-03,  4.6405e-04, -7.6104e-04]]],


        [[[ 8.3911e-03, -1.6139e-02,  8.7930e-03],
          [-3.8688e-03,  5.5320e-03,  6.9675e-03],
          [ 4.7946e-03, -1.2976e-02, -1.1883e-02]],

         [[-1.5054e-02,  7.9671e-03, -9.4542e-03],
          [-3.7103e-03, -2.5860e-03, -3.2864e-03],
          [ 7.2750e-03,  4.8959e-03, -1.0459e-02]],

         [[-9.9038e-04,  2.3824e-04,  1.1376e-02],
          [ 8.0689e-04,  1.3113e-02,  1.3807e-03],
          [ 1.0825e-02,  2.3257e-02,  1.8281e-02]],

         ...,

         [[-2.2955e-03,  3.1587e-03, -8.9362e-03],
          [ 3.7045e-04, -6.7021e-03, -2.3708e-03],
          [-1.6669e-03,  6.8753e-03,  1.3687e-02]],

         [[ 5.3627e-03, -4.3792e-03, -1.2869e-02],
          [-4.3609e-03, -3.4763e-03,  3.1588e-03],
          [-5.0011e-03,  5.8586e-04, -1.2193e-02]],

         [[-1.3375e-04,  2.9848e-04, -5.0519e-03],
          [-1.5665e-02, -2.0137e-03, -1.6961e-04],
          [ 1.0711e-02, -6.7571e-03, -4.5027e-03]]],


        [[[ 3.5055e-03,  1.0597e-02,  3.5565e-03],
          [ 8.0760e-03,  1.5240e-02,  1.9637e-02],
          [-7.1138e-03, -2.2342e-03,  4.4899e-03]],

         [[ 1.1003e-03,  1.2981e-02,  8.3501e-03],
          [-1.2171e-02, -7.1074e-03,  3.5892e-03],
          [-1.0202e-02, -7.5346e-03, -5.7858e-03]],

         [[-4.2025e-03, -7.0402e-03,  6.3494e-03],
          [-6.6342e-03, -1.1438e-02,  9.4668e-03],
          [-1.3703e-03, -4.8988e-03, -1.2271e-03]],

         ...,

         [[ 8.4403e-03,  2.9333e-03, -9.0794e-03],
          [ 7.0942e-03, -1.5228e-03, -1.1866e-02],
          [ 9.1092e-03,  4.2439e-03,  1.7820e-03]],

         [[ 9.3737e-03,  5.7953e-03,  2.0705e-02],
          [-7.1104e-03, -5.1052e-03,  1.8964e-02],
          [-4.3492e-03,  3.4167e-03, -3.1941e-03]],

         [[-4.8300e-03,  4.4020e-03, -1.2457e-02],
          [-1.1855e-02,  2.9973e-03, -6.8337e-03],
          [-7.2900e-03,  1.0558e-02,  8.4656e-03]]],


        ...,


        [[[ 6.1422e-03, -3.8700e-03, -1.6444e-02],
          [-1.0227e-02,  5.2153e-03, -3.6514e-03],
          [ 2.0519e-03,  1.0998e-02, -1.3954e-02]],

         [[-6.5557e-03,  6.4850e-04,  4.5179e-03],
          [-7.6887e-03,  1.5470e-02, -3.5699e-04],
          [-6.3926e-03,  9.0503e-03, -8.2761e-03]],

         [[ 1.0382e-03, -9.7317e-03,  1.0847e-03],
          [-3.0090e-03, -2.6176e-02, -1.1131e-02],
          [ 1.2692e-02, -2.0092e-02, -1.7556e-02]],

         ...,

         [[-1.8393e-02,  6.0145e-03, -7.0415e-03],
          [-7.8484e-03, -5.2907e-03, -1.2353e-02],
          [-6.6929e-03,  1.0583e-02,  6.2062e-03]],

         [[-1.3094e-03, -3.9181e-03, -3.7239e-03],
          [-6.1152e-03,  5.3418e-03,  1.2668e-02],
          [ 9.3077e-03, -5.6667e-03, -9.7824e-03]],

         [[ 1.3864e-02,  1.3889e-02,  5.4040e-03],
          [-9.7725e-04,  4.7102e-03,  2.2845e-03],
          [-4.0148e-04,  3.7648e-04, -1.3723e-03]]],


        [[[-1.3868e-02, -3.8462e-03,  3.3292e-03],
          [-1.2718e-02,  8.5689e-03, -1.8656e-02],
          [-5.8280e-03, -2.3582e-03,  3.0118e-03]],

         [[ 2.0648e-03, -1.3925e-02, -2.2614e-02],
          [ 1.0026e-02,  4.2348e-03, -1.6429e-02],
          [ 9.1123e-03,  4.5374e-03,  5.0324e-03]],

         [[-6.6853e-03,  8.8430e-04,  1.0823e-02],
          [ 1.8002e-03, -1.4420e-03, -5.9587e-03],
          [-6.9115e-03, -5.0123e-03, -7.1931e-03]],

         ...,

         [[ 2.1360e-03,  1.0733e-03, -7.7634e-03],
          [-2.3910e-02, -1.7749e-02, -1.7115e-02],
          [-7.8363e-04, -2.9737e-03,  1.3888e-02]],

         [[ 3.8215e-03, -3.4437e-03,  3.2102e-03],
          [-3.7538e-03,  1.6178e-05, -8.1843e-04],
          [ 1.0101e-02,  9.7964e-03,  4.8149e-03]],

         [[ 7.6535e-03, -1.3827e-02, -1.3638e-02],
          [ 1.9120e-03, -2.9417e-03,  1.4059e-02],
          [-1.3402e-02,  3.4932e-03, -4.9119e-03]]],


        [[[ 9.4735e-03, -5.9686e-03, -3.4122e-03],
          [-1.4733e-02, -6.3221e-04,  4.8137e-03],
          [ 1.5804e-03, -9.2553e-03, -1.1787e-02]],

         [[-5.3039e-03,  1.3382e-02,  2.7095e-03],
          [ 5.7266e-03, -1.0226e-02,  3.6330e-04],
          [ 4.5992e-03,  6.9571e-03, -5.9453e-03]],

         [[-1.2602e-02, -3.1146e-03,  1.6543e-02],
          [-2.3390e-03,  4.6586e-03,  1.8167e-02],
          [-8.5690e-03, -1.1854e-03,  2.8805e-03]],

         ...,

         [[ 1.3538e-02,  5.6555e-03,  2.1007e-04],
          [ 1.1003e-02, -1.3657e-02,  5.9128e-03],
          [ 2.1178e-03, -1.5065e-02,  1.0271e-02]],

         [[ 1.8756e-02,  2.4200e-02,  2.9734e-03],
          [ 1.7370e-02,  1.9557e-03, -5.2152e-03],
          [-9.5132e-03,  1.3050e-02, -1.2137e-02]],

         [[-4.7518e-03, -1.3304e-02,  7.7283e-03],
          [-1.1125e-02, -5.7072e-03, -5.0054e-03],
          [ 1.1017e-02, -3.0942e-03,  2.1881e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([9.6576e+05, 1.6003e+05, 1.5331e+05,  ..., 2.5291e-03, 2.0556e-03,
        4.1543e-04], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 5]) 

NULL SPACE BASIS :  tensor([[ 0.0200, -0.0242,  0.0165, -0.0152, -0.0442],
        [ 0.0165,  0.0075,  0.0164,  0.0102, -0.0036],
        [ 0.0023, -0.0315,  0.0089, -0.0076, -0.0231],
        ...,
        [ 0.0123,  0.0094, -0.0048, -0.0051, -0.0086],
        [-0.0084,  0.0180, -0.0175,  0.0119,  0.0107],
        [ 0.0012,  0.0100,  0.0044, -0.0141, -0.0024]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.5333e-03,  1.8916e-04,  9.3236e-04,  ...,  1.7642e-04,
         -6.9010e-04,  7.8466e-05],
        [ 1.8916e-04,  3.1905e-04, -2.0886e-05,  ...,  7.7236e-05,
         -9.3505e-05,  1.4371e-05],
        [ 9.3236e-04, -2.0886e-05,  7.4413e-04,  ..., -3.2304e-05,
         -4.8117e-04, -4.9189e-05],
        ...,
        [ 1.7642e-04,  7.7236e-05, -3.2304e-05,  ...,  1.6101e-04,
         -1.2647e-06,  7.9829e-05],
        [-6.9010e-04, -9.3505e-05, -4.8117e-04,  ..., -1.2647e-06,
          4.2720e-04, -4.5550e-05],
        [ 7.8466e-05,  1.4371e-05, -4.9189e-05,  ...,  7.9829e-05,
         -4.5550e-05,  1.4515e-04]], device='cuda:0') 

reserving basis 200/256; cond: 41156.078125, radio:0.013918323442339897
PARAMETER       :  Parameter containing:
tensor([[[[-0.0194]],

         [[-0.0360]],

         [[-0.0420]],

         ...,

         [[ 0.0081]],

         [[-0.0443]],

         [[-0.0525]]],


        [[[ 0.0657]],

         [[-0.0590]],

         [[ 0.0164]],

         ...,

         [[ 0.0048]],

         [[ 0.0063]],

         [[ 0.0189]]],


        [[[ 0.0413]],

         [[ 0.0281]],

         [[-0.0047]],

         ...,

         [[ 0.0188]],

         [[ 0.0373]],

         [[-0.0343]]],


        ...,


        [[[-0.0274]],

         [[ 0.0056]],

         [[ 0.0482]],

         ...,

         [[ 0.0180]],

         [[-0.0177]],

         [[-0.0087]]],


        [[[ 0.0183]],

         [[ 0.0233]],

         [[-0.0238]],

         ...,

         [[ 0.0422]],

         [[ 0.0122]],

         [[ 0.0437]]],


        [[[-0.0374]],

         [[ 0.0257]],

         [[ 0.0420]],

         ...,

         [[ 0.0502]],

         [[-0.0640]],

         [[ 0.0397]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.5958e+05, 9.2800e+03, 7.7761e+03, 4.0097e+03, 2.9098e+03, 2.1585e+03,
        1.4960e+03, 7.9041e+02, 6.6916e+02, 5.6968e+02, 4.5119e+02, 4.3033e+02,
        3.3668e+02, 3.1506e+02, 2.8657e+02, 2.6295e+02, 2.5036e+02, 2.3892e+02,
        2.2022e+02, 2.1167e+02, 1.9029e+02, 1.8443e+02, 1.6877e+02, 1.6061e+02,
        1.4245e+02, 1.3567e+02, 1.2674e+02, 1.1488e+02, 1.0739e+02, 1.0354e+02,
        9.5507e+01, 9.4512e+01, 8.9336e+01, 8.3486e+01, 8.1721e+01, 7.9297e+01,
        7.6894e+01, 7.3013e+01, 7.0884e+01, 6.8882e+01, 6.7696e+01, 6.3261e+01,
        6.0453e+01, 5.7640e+01, 5.6425e+01, 5.4921e+01, 5.2708e+01, 5.0519e+01,
        4.8720e+01, 4.7514e+01, 4.6848e+01, 4.4630e+01, 4.3839e+01, 4.2545e+01,
        4.1703e+01, 4.0813e+01, 3.8563e+01, 3.7371e+01, 3.6843e+01, 3.6371e+01,
        3.4883e+01, 3.4413e+01, 3.3853e+01, 3.2892e+01, 3.2433e+01, 3.1594e+01,
        3.1357e+01, 3.0362e+01, 2.9750e+01, 2.9363e+01, 2.9169e+01, 2.8192e+01,
        2.7508e+01, 2.6948e+01, 2.6801e+01, 2.6677e+01, 2.6366e+01, 2.5737e+01,
        2.5499e+01, 2.5125e+01, 2.4493e+01, 2.4048e+01, 2.3778e+01, 2.3650e+01,
        2.3097e+01, 2.2810e+01, 2.2694e+01, 2.2500e+01, 2.2385e+01, 2.2073e+01,
        2.1574e+01, 2.1527e+01, 2.1186e+01, 2.0802e+01, 2.0744e+01, 2.0370e+01,
        2.0308e+01, 1.9931e+01, 1.9471e+01, 1.9332e+01, 1.9059e+01, 1.8743e+01,
        1.8684e+01, 1.8484e+01, 1.8248e+01, 1.7832e+01, 1.7555e+01, 1.7414e+01,
        1.7247e+01, 1.7133e+01, 1.6974e+01, 1.6804e+01, 1.6712e+01, 1.6455e+01,
        1.6318e+01, 1.6118e+01, 1.6042e+01, 1.5922e+01, 1.5791e+01, 1.5643e+01,
        1.5397e+01, 1.5290e+01, 1.4950e+01, 1.4852e+01, 1.4820e+01, 1.4508e+01,
        1.4345e+01, 1.4295e+01, 1.4195e+01, 1.4059e+01, 1.3988e+01, 1.3796e+01,
        1.3679e+01, 1.3519e+01, 1.3475e+01, 1.3368e+01, 1.3282e+01, 1.3087e+01,
        1.3053e+01, 1.2989e+01, 1.2821e+01, 1.2676e+01, 1.2577e+01, 1.2442e+01,
        1.2278e+01, 1.2151e+01, 1.2099e+01, 1.2009e+01, 1.1885e+01, 1.1769e+01,
        1.1748e+01, 1.1635e+01, 1.1575e+01, 1.1488e+01, 1.1421e+01, 1.1369e+01,
        1.1227e+01, 1.1160e+01, 1.1105e+01, 1.0969e+01, 1.0860e+01, 1.0778e+01,
        1.0695e+01, 1.0613e+01, 1.0501e+01, 1.0474e+01, 1.0332e+01, 1.0324e+01,
        1.0231e+01, 1.0165e+01, 1.0069e+01, 1.0017e+01, 9.8626e+00, 9.8043e+00,
        9.7572e+00, 9.7192e+00, 9.5958e+00, 9.5346e+00, 9.4170e+00, 9.3621e+00,
        9.3050e+00, 9.2511e+00, 9.2120e+00, 9.1250e+00, 9.0549e+00, 8.9061e+00,
        8.8894e+00, 8.7319e+00, 8.7078e+00, 8.6710e+00, 8.5921e+00, 8.5098e+00,
        8.4804e+00, 8.3816e+00, 8.3547e+00, 8.2255e+00, 8.1528e+00, 8.1170e+00,
        8.0590e+00, 7.9790e+00, 7.9270e+00, 7.8856e+00, 7.8350e+00, 7.7127e+00,
        7.6887e+00, 7.5577e+00, 7.4488e+00, 7.4314e+00, 7.3978e+00, 7.3605e+00,
        7.2492e+00, 7.2156e+00, 7.1668e+00, 7.0872e+00, 7.0253e+00, 6.9672e+00,
        6.8866e+00, 6.7922e+00, 6.7721e+00, 6.7382e+00, 6.6848e+00, 6.6147e+00,
        6.6037e+00, 6.4692e+00, 6.3898e+00, 6.3621e+00, 6.3095e+00, 6.2603e+00,
        6.2501e+00, 6.1582e+00, 6.0648e+00, 6.0396e+00, 5.9436e+00, 5.8959e+00,
        5.8713e+00, 5.7759e+00, 5.7392e+00, 5.6813e+00, 5.5914e+00, 5.5344e+00,
        5.4268e+00, 5.3574e+00, 5.3223e+00, 5.2901e+00, 5.2443e+00, 5.1507e+00,
        5.0656e+00, 5.0336e+00, 4.8948e+00, 4.8794e+00, 4.7115e+00, 4.6683e+00,
        4.5663e+00, 4.4703e+00, 4.1968e+00, 3.8774e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 200]) 

NULL SPACE BASIS :  tensor([[ 0.0837,  0.0689, -0.0175,  ..., -0.0309, -0.0082, -0.0121],
        [ 0.0151, -0.0424, -0.0227,  ..., -0.0148, -0.0085, -0.0423],
        [ 0.0016,  0.0148, -0.0085,  ...,  0.0166, -0.0707, -0.0139],
        ...,
        [-0.0114, -0.0027,  0.1415,  ...,  0.0417,  0.0628, -0.0184],
        [ 0.0615,  0.0476,  0.1197,  ..., -0.0163, -0.0077, -0.0035],
        [-0.0024,  0.0090, -0.0431,  ...,  0.1212,  0.0010, -0.0361]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0363,  0.0034, -0.0005,  ...,  0.0010, -0.0024,  0.0021],
        [ 0.0034,  0.0566, -0.0028,  ..., -0.0055,  0.0033,  0.0017],
        [-0.0005, -0.0028,  0.0605,  ..., -0.0026,  0.0034, -0.0012],
        ...,
        [ 0.0010, -0.0055, -0.0026,  ...,  0.0565, -0.0003,  0.0013],
        [-0.0024,  0.0033,  0.0034,  ..., -0.0003,  0.0500, -0.0003],
        [ 0.0021,  0.0017, -0.0012,  ...,  0.0013, -0.0003,  0.0609]],
       device='cuda:0') 

reserving basis 1/4608; cond: 9550946304.0, radio:5.955371612520466e-11
PARAMETER       :  Parameter containing:
tensor([[[[-3.7023e-03,  5.1903e-03, -1.1996e-02],
          [-7.7703e-03,  5.5355e-03, -1.6139e-02],
          [ 9.6902e-03, -3.0844e-03,  6.9753e-03]],

         [[-7.7860e-03, -3.0109e-03,  3.8305e-03],
          [ 8.3978e-03,  4.5517e-03, -3.5945e-03],
          [ 4.5373e-03, -5.3020e-03,  5.5888e-03]],

         [[ 7.4041e-04, -1.4147e-02, -1.4707e-02],
          [-3.8934e-03, -1.7763e-03, -1.2357e-02],
          [ 1.6136e-03,  1.4239e-02, -4.7692e-03]],

         ...,

         [[-3.7279e-03,  9.4630e-03, -1.1227e-03],
          [ 1.0349e-02,  4.4335e-04,  5.7420e-03],
          [ 4.8349e-03,  1.0927e-02, -4.7687e-03]],

         [[-1.2721e-02,  9.8154e-04, -1.1414e-02],
          [ 2.1906e-03, -7.3027e-03, -3.0096e-03],
          [-2.9844e-04, -1.0536e-02,  9.6542e-03]],

         [[-6.1538e-04,  1.0508e-02,  7.8429e-03],
          [ 7.3126e-03,  9.8682e-03,  1.3014e-02],
          [-1.2724e-02,  3.6670e-03, -9.7585e-03]]],


        [[[ 6.0135e-03, -4.0007e-03,  7.6601e-03],
          [-1.3559e-02, -8.9038e-03,  5.4310e-03],
          [-2.3109e-02,  1.0761e-03,  6.8115e-03]],

         [[-7.3953e-03,  1.4589e-03,  2.7247e-03],
          [-6.5455e-03,  1.6450e-02, -2.0716e-03],
          [ 1.1788e-02,  8.2158e-03,  1.6302e-02]],

         [[ 5.3482e-03, -6.9939e-03,  5.7252e-03],
          [ 1.0176e-03, -1.0909e-02,  5.2086e-03],
          [ 1.1728e-02,  1.9301e-04,  7.4194e-03]],

         ...,

         [[ 9.3162e-03,  3.6791e-03,  8.2358e-03],
          [ 1.2017e-02, -3.4015e-03, -4.1523e-03],
          [-6.3687e-03, -1.0951e-03,  2.6912e-03]],

         [[ 2.1205e-03, -1.9635e-03, -9.9812e-03],
          [ 7.7980e-04, -5.3143e-03, -2.3435e-03],
          [-3.3045e-03,  1.2719e-02,  2.9996e-03]],

         [[ 1.2114e-02, -7.3520e-03, -9.5679e-03],
          [ 1.2816e-02, -5.4521e-03,  1.6034e-03],
          [-1.8556e-03,  5.9102e-03, -1.0510e-02]]],


        [[[-5.7563e-03, -1.8147e-02, -8.0833e-03],
          [-2.6817e-03,  7.5400e-03, -1.5026e-02],
          [ 1.5184e-02,  1.7838e-02,  5.7424e-03]],

         [[ 6.9220e-03, -1.0614e-02,  1.2948e-03],
          [-5.8052e-03, -3.3377e-03, -2.1316e-02],
          [-4.0771e-03, -4.1974e-03, -2.0990e-02]],

         [[-1.6312e-03, -2.2565e-03,  5.1728e-03],
          [ 1.6507e-03,  2.3378e-03, -2.5348e-04],
          [-6.1793e-03,  1.0279e-02, -2.0739e-03]],

         ...,

         [[ 7.3284e-03,  1.3960e-02,  8.5039e-03],
          [-5.4006e-03,  9.5240e-03, -8.2218e-03],
          [ 7.3407e-04, -1.2592e-02, -1.9864e-03]],

         [[ 5.6405e-03, -1.1309e-02, -5.5853e-03],
          [ 4.2353e-03, -6.6588e-03, -1.9348e-02],
          [-3.0562e-03,  1.8698e-03, -1.3348e-02]],

         [[-1.2500e-02, -1.8956e-02, -3.8949e-03],
          [ 7.7157e-03,  6.1149e-03, -1.1315e-02],
          [-5.1291e-03, -1.1264e-02,  6.4449e-03]]],


        ...,


        [[[-1.1493e-02,  8.8214e-03, -8.6031e-03],
          [ 9.3539e-03,  7.5748e-03,  1.0084e-02],
          [-1.2371e-02, -1.7535e-02,  1.0595e-03]],

         [[-1.6105e-02, -1.6021e-03, -2.6671e-03],
          [ 4.6709e-03,  3.3730e-03, -1.7189e-03],
          [-9.5684e-03,  5.0171e-03, -3.1562e-03]],

         [[-1.5077e-02,  1.4842e-03,  1.6918e-03],
          [-1.3741e-02, -5.6171e-03, -1.6266e-02],
          [-7.4436e-03, -1.4015e-02, -2.7327e-03]],

         ...,

         [[ 1.0044e-02, -1.5195e-02,  5.4634e-03],
          [-3.3885e-03,  3.7219e-03, -1.0848e-02],
          [-8.6425e-03, -1.4176e-03,  5.2966e-03]],

         [[-1.9398e-03, -2.4746e-03, -1.8354e-03],
          [-1.2710e-02, -8.6307e-03, -1.8977e-02],
          [-1.5477e-02, -4.8452e-03, -3.5338e-03]],

         [[-3.3971e-03, -4.2211e-03, -1.0451e-02],
          [ 8.4951e-03,  9.9833e-03, -1.6362e-03],
          [ 1.0673e-02,  5.9208e-03,  9.7636e-03]]],


        [[[-1.4580e-02, -2.7416e-03,  9.7619e-03],
          [ 6.5331e-03, -1.4720e-03,  2.2915e-02],
          [-6.3979e-03, -7.7392e-03,  2.0872e-02]],

         [[ 1.0687e-02, -7.6372e-03, -1.5753e-03],
          [-4.4752e-03, -5.1163e-03,  8.3441e-03],
          [-1.9446e-02, -7.4151e-03, -1.9202e-02]],

         [[-1.5980e-02, -1.4619e-02, -1.3613e-02],
          [ 1.4162e-02,  1.3938e-03, -1.5213e-02],
          [ 7.9517e-03, -1.2400e-02, -3.2485e-03]],

         ...,

         [[ 1.2808e-02,  7.6745e-03, -2.5229e-03],
          [ 1.3129e-02,  1.0097e-02,  1.5793e-02],
          [-5.7313e-03,  9.7504e-03, -1.5142e-02]],

         [[-8.1574e-03,  1.9800e-03, -4.5521e-03],
          [ 5.3603e-03, -4.0865e-03,  1.4015e-03],
          [ 1.6539e-02,  5.7412e-03,  6.3839e-03]],

         [[ 3.6655e-03,  1.2139e-02,  1.0603e-02],
          [-4.6800e-04,  4.3159e-03,  3.5088e-03],
          [-1.8144e-03,  1.6837e-02,  2.5931e-03]]],


        [[[ 1.9379e-02,  4.6378e-03, -1.7886e-03],
          [ 2.8719e-03, -5.4574e-03, -7.2175e-03],
          [-1.0393e-02,  2.5169e-03,  3.4418e-03]],

         [[ 1.5791e-02,  5.5097e-04, -2.9421e-03],
          [-1.2704e-03,  1.7110e-02, -2.5547e-03],
          [ 6.7285e-03,  2.6735e-03,  1.4476e-02]],

         [[ 1.8284e-02,  2.5396e-03,  6.0463e-03],
          [-5.4709e-03, -9.6146e-03, -9.3298e-03],
          [-1.0664e-02, -3.9638e-04, -9.1246e-03]],

         ...,

         [[ 1.0631e-02, -1.4455e-02,  1.9820e-03],
          [ 3.7716e-03,  9.3256e-03,  4.6018e-03],
          [ 1.9481e-02,  2.3541e-02,  2.6604e-02]],

         [[-1.6016e-03,  1.1556e-02, -8.0511e-03],
          [-1.3667e-02,  9.7941e-04, -1.7712e-02],
          [ 7.1877e-03,  7.3381e-04, -1.4074e-03]],

         [[ 9.7482e-05,  3.9873e-03, -1.3773e-02],
          [-1.0413e-02,  6.3557e-03, -1.1027e-02],
          [-5.0203e-03, -1.1824e-02,  1.0431e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.1929e+06, 2.4237e+05, 2.3729e+05,  ..., 1.3550e-03, 1.2811e-03,
        1.2490e-04], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1]) 

NULL SPACE BASIS :  tensor([[-0.0112],
        [-0.0032],
        [-0.0105],
        ...,
        [ 0.0024],
        [-0.0052],
        [-0.0186]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.2455e-04,  3.5549e-05,  1.1724e-04,  ..., -2.7248e-05,
          5.8282e-05,  2.0743e-04],
        [ 3.5549e-05,  1.0146e-05,  3.3463e-05,  ..., -7.7772e-06,
          1.6635e-05,  5.9206e-05],
        [ 1.1724e-04,  3.3463e-05,  1.1036e-04,  ..., -2.5649e-05,
          5.4862e-05,  1.9526e-04],
        ...,
        [-2.7248e-05, -7.7772e-06, -2.5649e-05,  ...,  5.9612e-06,
         -1.2751e-05, -4.5381e-05],
        [ 5.8282e-05,  1.6635e-05,  5.4862e-05,  ..., -1.2751e-05,
          2.7273e-05,  9.7067e-05],
        [ 2.0743e-04,  5.9206e-05,  1.9526e-04,  ..., -4.5381e-05,
          9.7067e-05,  3.4547e-04]], device='cuda:0') 

reserving basis 6/4608; cond: 26605420544.0, radio:6.161462451359512e-10
PARAMETER       :  Parameter containing:
tensor([[[[ 9.1330e-03,  2.5422e-03, -6.0381e-03],
          [ 1.3366e-03, -9.0422e-03, -2.8988e-03],
          [ 3.0827e-03, -7.7899e-03,  4.7357e-03]],

         [[-5.8089e-03, -1.1172e-02,  5.7089e-04],
          [-4.8304e-03, -3.6694e-04, -2.4464e-03],
          [ 2.6631e-03,  9.1362e-03, -2.6100e-03]],

         [[-1.0536e-02,  5.9996e-03,  1.1368e-02],
          [ 1.5779e-02,  1.2381e-02,  2.9729e-03],
          [-1.7403e-03,  1.7713e-02,  5.3918e-03]],

         ...,

         [[ 1.1526e-02,  4.5926e-03, -9.6273e-04],
          [ 2.8982e-03, -9.1833e-03, -1.7563e-04],
          [ 6.2613e-03, -5.8507e-03,  7.5364e-03]],

         [[ 6.9791e-03, -9.4157e-04,  2.7615e-03],
          [ 3.1403e-03,  1.4814e-03,  7.5320e-03],
          [ 8.1566e-03,  8.5628e-03, -4.5448e-04]],

         [[-8.1289e-03,  1.2782e-02, -2.1323e-04],
          [-1.0169e-02, -1.2917e-02,  1.7672e-03],
          [-1.2671e-02,  5.3003e-03, -1.3636e-02]]],


        [[[-3.3636e-04,  7.5197e-03, -3.1183e-03],
          [ 2.0686e-03, -3.9372e-03, -8.3256e-03],
          [-4.6143e-03, -1.5126e-02,  7.3529e-03]],

         [[ 3.2706e-03,  5.8264e-03,  1.9039e-03],
          [ 9.5998e-03,  1.4040e-02, -1.2494e-02],
          [ 1.7363e-02,  7.4855e-03, -9.7224e-03]],

         [[ 6.7593e-03,  5.9541e-03,  2.3023e-03],
          [ 3.6885e-03, -4.7340e-03,  6.5266e-03],
          [-2.4352e-03, -7.6050e-03,  1.0148e-04]],

         ...,

         [[-2.0302e-03,  9.6352e-03,  4.4798e-03],
          [-8.9398e-03, -8.1856e-03,  6.1233e-03],
          [-6.6492e-03,  5.8713e-03, -2.4344e-03]],

         [[ 4.8300e-03, -3.0472e-03,  8.9261e-03],
          [-3.2090e-03, -1.4580e-02, -4.4631e-03],
          [ 3.8182e-03, -1.1741e-03, -1.4284e-02]],

         [[ 1.4772e-02,  1.9584e-02, -2.1202e-03],
          [ 3.9570e-04, -1.3811e-04,  5.7973e-03],
          [ 3.9163e-03, -2.1900e-03, -7.6744e-04]]],


        [[[-4.0432e-03, -5.0287e-03, -6.8444e-03],
          [-2.2861e-03, -1.4315e-02, -6.7687e-03],
          [-8.1816e-03, -9.7089e-04, -6.6428e-04]],

         [[-4.2434e-03,  5.4040e-03, -1.1065e-02],
          [ 1.0322e-03, -9.9035e-03, -1.2816e-02],
          [-4.8457e-03, -3.1478e-03, -9.3558e-03]],

         [[-1.1988e-03, -1.3177e-02,  4.9639e-03],
          [-4.9636e-03, -7.2044e-03, -1.3294e-02],
          [-4.8539e-03,  1.3422e-02, -6.4835e-03]],

         ...,

         [[ 2.9708e-03, -1.8604e-02, -9.5984e-04],
          [ 1.5842e-02,  6.2832e-04, -6.2164e-03],
          [-4.8614e-03,  7.5626e-03, -2.7517e-03]],

         [[ 4.0392e-03,  1.8064e-02,  4.8320e-03],
          [-2.5527e-03, -5.7038e-03,  9.3066e-03],
          [ 3.9639e-04, -3.4816e-03, -4.2602e-04]],

         [[-1.2651e-02, -1.5566e-02, -1.6166e-02],
          [-1.2476e-02, -4.6816e-04,  6.8190e-04],
          [-1.2160e-02, -1.1809e-02, -1.7028e-02]]],


        ...,


        [[[-1.4695e-03, -1.4659e-04,  9.4006e-03],
          [ 6.8093e-04,  8.5179e-03, -8.9492e-03],
          [ 9.1435e-03,  3.6149e-04,  4.4595e-03]],

         [[-8.2304e-03, -4.7921e-03,  8.9986e-03],
          [ 8.1228e-03,  7.3500e-03, -7.4573e-03],
          [ 1.1837e-02, -4.0284e-03, -2.9026e-03]],

         [[-1.4348e-02, -1.4388e-03, -1.1748e-02],
          [-1.2058e-02, -1.1621e-02, -3.5449e-03],
          [ 1.1498e-02,  1.6324e-02, -6.9422e-03]],

         ...,

         [[-4.2746e-03,  7.1026e-03,  5.7162e-03],
          [ 9.6210e-03,  1.7383e-03,  5.3173e-03],
          [-5.0298e-03, -3.7583e-03, -2.5489e-03]],

         [[-8.5960e-03,  1.1450e-02, -9.1628e-03],
          [ 9.7308e-03, -1.2519e-02, -8.0383e-03],
          [-1.0180e-04,  1.3044e-03, -1.6959e-02]],

         [[ 1.7025e-02,  2.0343e-02,  2.4224e-02],
          [ 1.1658e-02, -1.6185e-03,  3.7622e-03],
          [ 1.1024e-02,  1.5953e-02,  6.8770e-03]]],


        [[[-4.9887e-04, -3.1407e-03, -2.0827e-03],
          [-4.8125e-03, -1.2306e-02, -9.3361e-03],
          [-1.0558e-02,  1.1149e-03, -2.2832e-03]],

         [[-4.5433e-03,  9.1442e-03,  2.5740e-03],
          [-7.1199e-04,  7.1678e-03,  1.3378e-02],
          [-6.9276e-03, -1.0714e-02,  3.2592e-04]],

         [[ 4.4429e-03,  4.0466e-05,  4.7191e-03],
          [-1.2338e-02, -8.2279e-03,  9.7755e-03],
          [ 3.5396e-03,  9.3345e-03, -7.4456e-03]],

         ...,

         [[ 4.6867e-03,  5.7636e-03, -4.0353e-03],
          [ 9.0960e-03,  4.4007e-03,  2.3425e-03],
          [ 1.9026e-03,  1.9459e-03, -1.3458e-02]],

         [[-7.9227e-03, -3.1507e-03, -5.8113e-03],
          [ 1.9156e-03,  1.8725e-03, -1.4868e-02],
          [-1.2491e-03, -8.3789e-03,  2.5720e-03]],

         [[-1.3469e-02, -3.4829e-05,  3.0863e-04],
          [ 2.4781e-03, -4.0479e-03,  4.0045e-03],
          [-1.1646e-02, -1.1475e-02,  1.2519e-03]]],


        [[[-8.0974e-03, -9.7211e-03, -9.3302e-03],
          [ 2.5892e-03, -6.5829e-03,  1.5086e-03],
          [ 5.2155e-03,  3.4034e-03, -4.9817e-03]],

         [[-6.3700e-03,  8.4450e-03,  8.4091e-04],
          [ 1.7703e-03,  8.4258e-03,  2.1061e-03],
          [ 1.7662e-03,  1.1290e-02,  1.8836e-03]],

         [[-1.9769e-03,  5.9750e-03,  6.2516e-03],
          [-7.1285e-03,  1.0300e-02, -1.3149e-02],
          [ 9.5807e-05,  1.8600e-03,  2.3287e-03]],

         ...,

         [[ 3.7984e-03,  3.0226e-04,  1.4158e-02],
          [ 1.0464e-02,  2.9568e-03,  1.4278e-02],
          [ 6.9976e-03,  1.1513e-02,  4.2317e-03]],

         [[-7.8055e-03, -1.9285e-02, -1.5122e-03],
          [-1.1722e-02, -1.1453e-02,  3.4788e-03],
          [ 5.4370e-03, -6.7395e-05,  4.9772e-03]],

         [[-7.2556e-03,  4.4310e-03, -1.4952e-02],
          [-5.3018e-03,  1.0183e-02, -1.2231e-02],
          [ 6.9179e-03, -3.5831e-03, -5.5031e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.1583e+05, 9.3001e+04, 8.2099e+04,  ..., 1.1560e-04, 2.4350e-05,
        1.5630e-05], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 6]) 

NULL SPACE BASIS :  tensor([[ 0.0086,  0.0120, -0.0421,  0.0157, -0.0292,  0.0366],
        [ 0.0245,  0.0111, -0.0050,  0.0408, -0.0131, -0.0152],
        [ 0.0103,  0.0029,  0.0166,  0.0390, -0.0199,  0.0301],
        ...,
        [ 0.0137,  0.0028, -0.0073,  0.0250,  0.0014, -0.0002],
        [-0.0136, -0.0064,  0.0075, -0.0194, -0.0069, -0.0083],
        [-0.0005, -0.0109,  0.0086, -0.0018, -0.0046,  0.0080]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.8028e-03,  4.1660e-04,  6.9906e-04,  ...,  3.2749e-04,
         -3.7466e-04, -3.9193e-05],
        [ 4.1660e-04,  1.1468e-03,  6.4958e-04,  ...,  5.7288e-04,
         -4.1342e-04, -1.2681e-04],
        [ 6.9906e-04,  6.4958e-04,  1.3080e-03,  ...,  3.9516e-04,
         -3.6746e-04,  1.5007e-04],
        ...,
        [ 3.2749e-04,  5.7288e-04,  3.9516e-04,  ...,  3.5638e-04,
         -3.0612e-04, -6.2315e-05],
        [-3.7466e-04, -4.1342e-04, -3.6746e-04,  ..., -3.0612e-04,
          3.1565e-04,  5.7747e-05],
        [-3.9193e-05, -1.2681e-04,  1.5007e-04,  ..., -6.2315e-05,
          5.7747e-05,  1.1443e-04]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 83.900, Total time 0.55
 * Val loss 0.836, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 98.160, Total time 3.12
 * Val loss 0.054, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 72.900, Total time 0.56
 * Val loss 0.769, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 75.680, Total time 3.13
 * Val loss 0.715, Total time 0.00
**************************************************
====================== 3 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0080 (0.0080)	2.510 (2.510)	0.00 (0.00)
[10/157]	0.0944 (0.0896)	0.0560 (0.0534)	2.153 (2.284)	31.25 (21.02)
[20/157]	0.0946 (0.0917)	0.0581 (0.0555)	1.942 (2.180)	43.75 (28.12)
[30/157]	0.0942 (0.0923)	0.0576 (0.0561)	1.807 (2.066)	46.88 (34.48)
[40/157]	0.0928 (0.0926)	0.0561 (0.0565)	1.690 (1.992)	50.00 (36.20)
[50/157]	0.0976 (0.0930)	0.0571 (0.0568)	1.549 (1.929)	53.12 (38.66)
[60/157]	0.0967 (0.0932)	0.0588 (0.0570)	1.597 (1.868)	43.75 (40.93)
[70/157]	0.0965 (0.0933)	0.0596 (0.0572)	1.623 (1.823)	50.00 (42.78)
[80/157]	0.0935 (0.0934)	0.0572 (0.0573)	1.664 (1.792)	46.88 (43.36)
[90/157]	0.0954 (0.0935)	0.0586 (0.0575)	1.383 (1.758)	62.50 (44.64)
[100/157]	0.0968 (0.0935)	0.0584 (0.0576)	1.232 (1.727)	59.38 (45.88)
[110/157]	0.0952 (0.0936)	0.0578 (0.0576)	1.730 (1.704)	34.38 (46.79)
[120/157]	0.0963 (0.0936)	0.0589 (0.0577)	1.512 (1.683)	46.88 (47.21)
[130/157]	0.0954 (0.0937)	0.0589 (0.0578)	1.360 (1.665)	56.25 (47.76)
[140/157]	0.0948 (0.0937)	0.0583 (0.0578)	1.728 (1.652)	46.88 (48.32)
[150/157]	0.0943 (0.0937)	0.0576 (0.0578)	1.138 (1.640)	71.88 (48.65)
[156/157]	0.0761 (0.0936)	0.0527 (0.0578)	1.772 (1.630)	37.50 (49.00)
 * Train Acc 49.000
 * Val Acc 54.700, Total time 0.57
 * Val loss 1.320, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0406 (0.0406)	0.0081 (0.0081)	1.353 (1.353)	53.12 (53.12)
[10/157]	0.0948 (0.0902)	0.0586 (0.0543)	1.334 (1.386)	56.25 (51.99)
[20/157]	0.0950 (0.0922)	0.0589 (0.0563)	1.243 (1.395)	59.38 (53.27)
[30/157]	0.0925 (0.0929)	0.0563 (0.0569)	1.427 (1.382)	53.12 (55.14)
[40/157]	0.0958 (0.0933)	0.0580 (0.0572)	1.361 (1.381)	53.12 (55.34)
[50/157]	0.0959 (0.0935)	0.0581 (0.0574)	0.955 (1.375)	71.88 (55.33)
[60/157]	0.0962 (0.0936)	0.0585 (0.0575)	1.318 (1.371)	65.62 (55.64)
[70/157]	0.0957 (0.0937)	0.0587 (0.0576)	1.841 (1.375)	37.50 (55.63)
[80/157]	0.0934 (0.0938)	0.0566 (0.0576)	1.190 (1.374)	65.62 (55.63)
[90/157]	0.0956 (0.0939)	0.0581 (0.0577)	0.855 (1.363)	78.12 (56.28)
[100/157]	0.0952 (0.0939)	0.0586 (0.0578)	1.347 (1.354)	53.12 (56.87)
[110/157]	0.0935 (0.0940)	0.0569 (0.0579)	1.362 (1.355)	56.25 (56.95)
[120/157]	0.0931 (0.0940)	0.0566 (0.0579)	1.493 (1.355)	50.00 (56.97)
[130/157]	0.0940 (0.0940)	0.0575 (0.0580)	1.231 (1.349)	62.50 (57.04)
[140/157]	0.0949 (0.0941)	0.0579 (0.0580)	1.527 (1.352)	50.00 (56.91)
[150/157]	0.0958 (0.0941)	0.0589 (0.0580)	1.427 (1.355)	46.88 (56.77)
[156/157]	0.0761 (0.0940)	0.0522 (0.0580)	1.345 (1.352)	62.50 (56.86)
 * Train Acc 56.860
 * Val Acc 58.600, Total time 0.57
 * Val loss 1.231, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0080 (0.0080)	1.123 (1.123)	62.50 (62.50)
[10/157]	0.0937 (0.0905)	0.0574 (0.0535)	1.443 (1.353)	53.12 (58.81)
[20/157]	0.0945 (0.0925)	0.0571 (0.0557)	1.518 (1.345)	50.00 (56.70)
[30/157]	0.0965 (0.0932)	0.0587 (0.0565)	1.364 (1.318)	46.88 (57.76)
[40/157]	0.0959 (0.0935)	0.0583 (0.0569)	1.046 (1.298)	68.75 (58.69)
[50/157]	0.0956 (0.0936)	0.0586 (0.0571)	1.213 (1.309)	59.38 (58.09)
[60/157]	0.0951 (0.0937)	0.0582 (0.0573)	1.425 (1.330)	53.12 (57.22)
[70/157]	0.0941 (0.0938)	0.0569 (0.0574)	1.136 (1.319)	56.25 (57.97)
[80/157]	0.0936 (0.0939)	0.0574 (0.0575)	1.431 (1.315)	50.00 (58.10)
[90/157]	0.0957 (0.0939)	0.0585 (0.0576)	1.394 (1.311)	62.50 (58.34)
[100/157]	0.0953 (0.0940)	0.0589 (0.0577)	1.183 (1.317)	62.50 (58.11)
[110/157]	0.0942 (0.0940)	0.0576 (0.0577)	1.187 (1.302)	59.38 (58.50)
[120/157]	0.0946 (0.0940)	0.0578 (0.0578)	1.138 (1.295)	59.38 (58.94)
[130/157]	0.0957 (0.0941)	0.0580 (0.0578)	1.217 (1.292)	62.50 (59.06)
[140/157]	0.0946 (0.0941)	0.0582 (0.0578)	1.231 (1.296)	56.25 (58.89)
[150/157]	0.0939 (0.0942)	0.0571 (0.0579)	1.124 (1.294)	65.62 (58.94)
[156/157]	0.0776 (0.0941)	0.0540 (0.0579)	1.610 (1.295)	50.00 (58.80)
 * Train Acc 58.800
 * Val Acc 61.000, Total time 0.57
 * Val loss 1.182, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0407 (0.0407)	0.0085 (0.0085)	1.183 (1.183)	50.00 (50.00)
[10/157]	0.0941 (0.0908)	0.0581 (0.0542)	1.008 (1.290)	68.75 (58.24)
[20/157]	0.0959 (0.0925)	0.0586 (0.0560)	1.566 (1.295)	53.12 (59.23)
[30/157]	0.0954 (0.0931)	0.0589 (0.0568)	1.077 (1.268)	68.75 (60.18)
[40/157]	0.0952 (0.0935)	0.0581 (0.0572)	1.645 (1.303)	46.88 (58.84)
[50/157]	0.0974 (0.0937)	0.0582 (0.0574)	0.960 (1.282)	62.50 (59.93)
[60/157]	0.0946 (0.0938)	0.0573 (0.0575)	1.012 (1.268)	75.00 (60.55)
[70/157]	0.0956 (0.0939)	0.0583 (0.0576)	1.393 (1.272)	56.25 (60.34)
[80/157]	0.0942 (0.0940)	0.0566 (0.0577)	1.332 (1.276)	62.50 (60.11)
[90/157]	0.0945 (0.0940)	0.0572 (0.0578)	1.567 (1.270)	56.25 (60.61)
[100/157]	0.0959 (0.0941)	0.0582 (0.0578)	1.089 (1.252)	65.62 (61.20)
[110/157]	0.0954 (0.0941)	0.0574 (0.0578)	1.149 (1.247)	75.00 (61.68)
[120/157]	0.0954 (0.0942)	0.0577 (0.0579)	1.070 (1.246)	71.88 (61.62)
[130/157]	0.0963 (0.0942)	0.0594 (0.0579)	1.403 (1.255)	62.50 (61.35)
[140/157]	0.0959 (0.0943)	0.0561 (0.0579)	1.266 (1.255)	50.00 (61.15)
[150/157]	0.0969 (0.0943)	0.0598 (0.0579)	1.143 (1.254)	65.62 (61.05)
[156/157]	0.0768 (0.0941)	0.0532 (0.0579)	1.975 (1.252)	50.00 (61.04)
 * Train Acc 61.040
 * Val Acc 62.700, Total time 0.56
 * Val loss 1.127, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0083 (0.0083)	1.347 (1.347)	59.38 (59.38)
[10/157]	0.0956 (0.0909)	0.0582 (0.0546)	1.398 (1.215)	46.88 (61.65)
[20/157]	0.0961 (0.0926)	0.0595 (0.0564)	1.034 (1.248)	65.62 (59.23)
[30/157]	0.0937 (0.0931)	0.0563 (0.0570)	1.298 (1.234)	53.12 (59.98)
[40/157]	0.0947 (0.0935)	0.0580 (0.0574)	1.151 (1.239)	56.25 (59.83)
[50/157]	0.0956 (0.0937)	0.0585 (0.0576)	1.325 (1.237)	59.38 (60.36)
[60/157]	0.0946 (0.0938)	0.0574 (0.0577)	1.650 (1.242)	37.50 (60.19)
[70/157]	0.0941 (0.0940)	0.0575 (0.0578)	1.583 (1.240)	53.12 (59.99)
[80/157]	0.0958 (0.0941)	0.0591 (0.0579)	1.274 (1.234)	56.25 (60.42)
[90/157]	0.0942 (0.0941)	0.0574 (0.0580)	1.181 (1.227)	68.75 (60.92)
[100/157]	0.0943 (0.0942)	0.0575 (0.0580)	0.921 (1.219)	75.00 (61.23)
[110/157]	0.0957 (0.0942)	0.0581 (0.0581)	0.915 (1.203)	81.25 (61.66)
[120/157]	0.0937 (0.0943)	0.0567 (0.0581)	1.331 (1.210)	65.62 (61.42)
[130/157]	0.0962 (0.0943)	0.0586 (0.0581)	1.135 (1.211)	71.88 (61.40)
[140/157]	0.0940 (0.0944)	0.0571 (0.0582)	1.209 (1.207)	59.38 (61.57)
[150/157]	0.0952 (0.0944)	0.0574 (0.0582)	1.226 (1.206)	65.62 (61.78)
[156/157]	0.0778 (0.0943)	0.0516 (0.0581)	1.413 (1.212)	50.00 (61.60)
 * Train Acc 61.600
 * Val Acc 63.400, Total time 0.57
 * Val loss 1.110, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0087 (0.0087)	1.275 (1.275)	53.12 (53.12)
[10/157]	0.0938 (0.0911)	0.0564 (0.0537)	1.353 (1.276)	50.00 (59.09)
[20/157]	0.0961 (0.0929)	0.0586 (0.0560)	1.250 (1.219)	65.62 (61.46)
[30/157]	0.0957 (0.0934)	0.0589 (0.0568)	1.225 (1.201)	62.50 (62.60)
[40/157]	0.0965 (0.0939)	0.0574 (0.0571)	0.896 (1.190)	84.38 (63.41)
[50/157]	0.0955 (0.0941)	0.0573 (0.0572)	1.140 (1.190)	62.50 (63.24)
[60/157]	0.0967 (0.0942)	0.0586 (0.0573)	1.103 (1.210)	68.75 (62.30)
[70/157]	0.0943 (0.0944)	0.0577 (0.0575)	1.115 (1.205)	68.75 (62.37)
[80/157]	0.0957 (0.0944)	0.0584 (0.0576)	1.182 (1.203)	56.25 (62.23)
[90/157]	0.0934 (0.0945)	0.0565 (0.0577)	1.256 (1.207)	56.25 (61.88)
[100/157]	0.0956 (0.0946)	0.0587 (0.0577)	1.313 (1.204)	59.38 (62.25)
[110/157]	0.0964 (0.0946)	0.0582 (0.0578)	1.336 (1.208)	53.12 (61.97)
[120/157]	0.0963 (0.0946)	0.0588 (0.0578)	1.171 (1.204)	62.50 (62.27)
[130/157]	0.0976 (0.0947)	0.0593 (0.0579)	1.232 (1.205)	56.25 (62.26)
[140/157]	0.0936 (0.0947)	0.0568 (0.0579)	1.012 (1.207)	68.75 (62.28)
[150/157]	0.0973 (0.0947)	0.0584 (0.0579)	1.100 (1.203)	59.38 (62.52)
[156/157]	0.0769 (0.0946)	0.0523 (0.0579)	1.546 (1.202)	62.50 (62.42)
 * Train Acc 62.420
 * Val Acc 61.800, Total time 0.57
 * Val loss 1.101, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0084 (0.0084)	1.341 (1.341)	59.38 (59.38)
[10/157]	0.0947 (0.0903)	0.0576 (0.0538)	1.244 (1.248)	65.62 (61.93)
[20/157]	0.0932 (0.0926)	0.0561 (0.0561)	1.048 (1.212)	62.50 (62.50)
[30/157]	0.0969 (0.0933)	0.0595 (0.0568)	1.035 (1.196)	68.75 (63.41)
[40/157]	0.0924 (0.0936)	0.0561 (0.0571)	1.241 (1.175)	71.88 (64.79)
[50/157]	0.0932 (0.0938)	0.0567 (0.0573)	1.271 (1.175)	59.38 (64.89)
[60/157]	0.0954 (0.0939)	0.0580 (0.0574)	1.123 (1.166)	56.25 (64.75)
[70/157]	0.0953 (0.0940)	0.0584 (0.0576)	1.372 (1.180)	43.75 (64.26)
[80/157]	0.0969 (0.0942)	0.0585 (0.0577)	0.835 (1.170)	78.12 (64.35)
[90/157]	0.0967 (0.0943)	0.0589 (0.0579)	1.051 (1.169)	68.75 (64.18)
[100/157]	0.0948 (0.0943)	0.0566 (0.0579)	0.897 (1.165)	71.88 (64.29)
[110/157]	0.0960 (0.0943)	0.0581 (0.0579)	1.126 (1.171)	68.75 (63.96)
[120/157]	0.0963 (0.0944)	0.0579 (0.0579)	1.148 (1.168)	59.38 (64.20)
[130/157]	0.0971 (0.0944)	0.0586 (0.0579)	1.191 (1.165)	62.50 (64.27)
[140/157]	0.0946 (0.0945)	0.0568 (0.0579)	1.066 (1.164)	53.12 (64.27)
[150/157]	0.0969 (0.0945)	0.0595 (0.0579)	1.069 (1.168)	65.62 (64.09)
[156/157]	0.0773 (0.0944)	0.0521 (0.0579)	1.693 (1.168)	50.00 (64.08)
 * Train Acc 64.080
 * Val Acc 65.500, Total time 0.56
 * Val loss 1.051, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0082 (0.0082)	1.029 (1.029)	71.88 (71.88)
[10/157]	0.0954 (0.0911)	0.0580 (0.0539)	0.855 (1.230)	81.25 (62.50)
[20/157]	0.0947 (0.0928)	0.0571 (0.0559)	0.981 (1.161)	68.75 (62.50)
[30/157]	0.0974 (0.0935)	0.0598 (0.0567)	0.871 (1.127)	68.75 (64.21)
[40/157]	0.0940 (0.0937)	0.0569 (0.0571)	1.074 (1.123)	62.50 (64.41)
[50/157]	0.0945 (0.0939)	0.0574 (0.0573)	0.959 (1.106)	68.75 (65.07)
[60/157]	0.0940 (0.0939)	0.0574 (0.0575)	1.308 (1.103)	56.25 (65.06)
[70/157]	0.0959 (0.0940)	0.0585 (0.0575)	1.153 (1.096)	65.62 (65.49)
[80/157]	0.0960 (0.0941)	0.0581 (0.0576)	0.917 (1.107)	71.88 (65.51)
[90/157]	0.0950 (0.0942)	0.0575 (0.0577)	1.189 (1.110)	59.38 (65.14)
[100/157]	0.0943 (0.0942)	0.0570 (0.0576)	1.125 (1.118)	62.50 (64.82)
[110/157]	0.0959 (0.0943)	0.0572 (0.0577)	1.123 (1.128)	59.38 (64.41)
[120/157]	0.0953 (0.0943)	0.0575 (0.0577)	1.416 (1.133)	50.00 (64.36)
[130/157]	0.0949 (0.0944)	0.0577 (0.0578)	1.089 (1.129)	59.38 (64.43)
[140/157]	0.0971 (0.0944)	0.0593 (0.0578)	1.398 (1.129)	46.88 (64.67)
[150/157]	0.0939 (0.0945)	0.0570 (0.0578)	1.004 (1.127)	62.50 (64.94)
[156/157]	0.0789 (0.0944)	0.0528 (0.0578)	1.110 (1.129)	75.00 (64.86)
 * Train Acc 64.860
 * Val Acc 65.000, Total time 0.57
 * Val loss 1.037, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0087 (0.0087)	1.097 (1.097)	68.75 (68.75)
[10/157]	0.0954 (0.0906)	0.0582 (0.0539)	0.766 (1.113)	84.38 (65.34)
[20/157]	0.0945 (0.0928)	0.0573 (0.0561)	1.382 (1.106)	62.50 (65.48)
[30/157]	0.0969 (0.0935)	0.0597 (0.0568)	1.254 (1.097)	56.25 (65.02)
[40/157]	0.0957 (0.0938)	0.0580 (0.0571)	1.282 (1.106)	56.25 (64.41)
[50/157]	0.0956 (0.0940)	0.0582 (0.0574)	1.042 (1.113)	65.62 (64.58)
[60/157]	0.0945 (0.0941)	0.0573 (0.0575)	1.037 (1.109)	59.38 (64.65)
[70/157]	0.0961 (0.0942)	0.0585 (0.0576)	1.098 (1.108)	68.75 (64.96)
[80/157]	0.0954 (0.0943)	0.0582 (0.0577)	1.350 (1.106)	56.25 (65.24)
[90/157]	0.0939 (0.0943)	0.0574 (0.0578)	1.066 (1.103)	65.62 (65.38)
[100/157]	0.0965 (0.0943)	0.0591 (0.0579)	0.985 (1.111)	68.75 (65.01)
[110/157]	0.0954 (0.0944)	0.0581 (0.0579)	1.149 (1.119)	71.88 (64.72)
[120/157]	0.0950 (0.0944)	0.0576 (0.0579)	1.145 (1.117)	56.25 (64.85)
[130/157]	0.0952 (0.0944)	0.0578 (0.0580)	0.843 (1.115)	84.38 (65.20)
[140/157]	0.0935 (0.0945)	0.0571 (0.0580)	1.149 (1.120)	68.75 (64.92)
[150/157]	0.0972 (0.0945)	0.0594 (0.0580)	1.394 (1.127)	62.50 (64.67)
[156/157]	0.0757 (0.0944)	0.0514 (0.0580)	1.486 (1.130)	37.50 (64.52)
 * Train Acc 64.520
 * Val Acc 65.400, Total time 0.56
 * Val loss 1.032, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0084 (0.0084)	1.244 (1.244)	53.12 (53.12)
[10/157]	0.0941 (0.0906)	0.0580 (0.0542)	1.052 (1.104)	71.88 (61.08)
[20/157]	0.0947 (0.0926)	0.0575 (0.0563)	1.252 (1.130)	53.12 (62.50)
[30/157]	0.0964 (0.0932)	0.0584 (0.0570)	0.981 (1.120)	71.88 (63.61)
[40/157]	0.0971 (0.0936)	0.0590 (0.0573)	1.114 (1.113)	68.75 (64.10)
[50/157]	0.0948 (0.0938)	0.0576 (0.0575)	1.172 (1.113)	59.38 (64.09)
[60/157]	0.0969 (0.0940)	0.0594 (0.0576)	1.235 (1.110)	59.38 (65.27)
[70/157]	0.0968 (0.0941)	0.0589 (0.0577)	1.022 (1.102)	68.75 (65.36)
[80/157]	0.0961 (0.0942)	0.0587 (0.0578)	1.185 (1.092)	68.75 (65.97)
[90/157]	0.0945 (0.0943)	0.0573 (0.0579)	1.245 (1.101)	53.12 (65.45)
[100/157]	0.0960 (0.0943)	0.0586 (0.0580)	1.229 (1.105)	53.12 (65.22)
[110/157]	0.0945 (0.0943)	0.0575 (0.0580)	1.599 (1.110)	56.25 (65.23)
[120/157]	0.0937 (0.0944)	0.0569 (0.0580)	0.995 (1.109)	78.12 (65.06)
[130/157]	0.0955 (0.0944)	0.0582 (0.0581)	1.115 (1.112)	62.50 (64.98)
[140/157]	0.0929 (0.0945)	0.0567 (0.0581)	0.964 (1.108)	68.75 (65.34)
[150/157]	0.0954 (0.0945)	0.0581 (0.0581)	1.073 (1.109)	71.88 (65.48)
[156/157]	0.0756 (0.0944)	0.0513 (0.0581)	1.021 (1.108)	75.00 (65.62)
 * Train Acc 65.620
 * Val Acc 67.000, Total time 0.56
 * Val loss 1.014, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0414 (0.0414)	0.0085 (0.0085)	1.089 (1.089)	65.62 (65.62)
[10/157]	0.0962 (0.0910)	0.0583 (0.0542)	0.983 (1.070)	62.50 (67.05)
[20/157]	0.0943 (0.0927)	0.0575 (0.0561)	1.296 (1.092)	56.25 (65.77)
[30/157]	0.0961 (0.0934)	0.0585 (0.0568)	1.171 (1.077)	62.50 (65.73)
[40/157]	0.0956 (0.0936)	0.0585 (0.0571)	1.166 (1.082)	59.38 (65.78)
[50/157]	0.0945 (0.0938)	0.0570 (0.0574)	1.123 (1.107)	68.75 (65.26)
[60/157]	0.0954 (0.0940)	0.0578 (0.0575)	1.122 (1.124)	75.00 (65.01)
[70/157]	0.0942 (0.0941)	0.0574 (0.0576)	1.231 (1.120)	68.75 (65.45)
[80/157]	0.0967 (0.0942)	0.0590 (0.0577)	1.192 (1.115)	65.62 (65.43)
[90/157]	0.0957 (0.0942)	0.0588 (0.0578)	1.189 (1.112)	59.38 (65.25)
[100/157]	0.0950 (0.0943)	0.0572 (0.0578)	0.924 (1.101)	75.00 (65.72)
[110/157]	0.0957 (0.0943)	0.0581 (0.0579)	1.145 (1.100)	62.50 (65.88)
[120/157]	0.0970 (0.0944)	0.0578 (0.0579)	1.385 (1.109)	46.88 (65.34)
[130/157]	0.0969 (0.0944)	0.0584 (0.0579)	0.913 (1.107)	75.00 (65.46)
[140/157]	0.0972 (0.0945)	0.0591 (0.0579)	1.340 (1.110)	62.50 (65.51)
[150/157]	0.0960 (0.0945)	0.0583 (0.0579)	0.941 (1.109)	75.00 (65.36)
[156/157]	0.0771 (0.0944)	0.0513 (0.0579)	1.586 (1.108)	37.50 (65.40)
 * Train Acc 65.400
 * Val Acc 67.200, Total time 0.57
 * Val loss 0.998, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0412 (0.0412)	0.0084 (0.0084)	1.150 (1.150)	65.62 (65.62)
[10/157]	0.0968 (0.0915)	0.0587 (0.0544)	1.090 (1.021)	68.75 (68.47)
[20/157]	0.0943 (0.0930)	0.0560 (0.0560)	1.064 (1.072)	59.38 (66.22)
[30/157]	0.0970 (0.0936)	0.0588 (0.0568)	1.086 (1.067)	62.50 (67.44)
[40/157]	0.0965 (0.0938)	0.0590 (0.0571)	2.126 (1.106)	37.50 (65.40)
[50/157]	0.0951 (0.0940)	0.0574 (0.0573)	0.651 (1.119)	78.12 (64.95)
[60/157]	0.0956 (0.0942)	0.0576 (0.0574)	1.307 (1.121)	59.38 (64.91)
[70/157]	0.0932 (0.0943)	0.0570 (0.0576)	1.126 (1.114)	59.38 (65.18)
[80/157]	0.0996 (0.0944)	0.0588 (0.0577)	1.112 (1.107)	65.62 (65.24)
[90/157]	0.0943 (0.0944)	0.0571 (0.0577)	0.978 (1.100)	71.88 (65.52)
[100/157]	0.0952 (0.0945)	0.0581 (0.0578)	0.969 (1.092)	65.62 (65.93)
[110/157]	0.0960 (0.0946)	0.0572 (0.0578)	1.461 (1.096)	59.38 (65.93)
[120/157]	0.0956 (0.0946)	0.0572 (0.0578)	1.146 (1.096)	68.75 (65.91)
[130/157]	0.0940 (0.0946)	0.0564 (0.0578)	1.194 (1.090)	56.25 (66.17)
[140/157]	0.0956 (0.0947)	0.0579 (0.0578)	0.837 (1.089)	75.00 (66.16)
[150/157]	0.0965 (0.0947)	0.0584 (0.0578)	0.808 (1.084)	81.25 (66.06)
[156/157]	0.0765 (0.0946)	0.0517 (0.0578)	1.068 (1.088)	75.00 (65.98)
 * Train Acc 65.980
 * Val Acc 65.800, Total time 0.57
 * Val loss 1.017, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0089 (0.0089)	1.066 (1.066)	68.75 (68.75)
[10/157]	0.0923 (0.0906)	0.0561 (0.0539)	0.948 (1.105)	71.88 (65.34)
[20/157]	0.0960 (0.0928)	0.0588 (0.0562)	1.075 (1.106)	65.62 (66.52)
[30/157]	0.0948 (0.0933)	0.0578 (0.0568)	1.208 (1.104)	56.25 (66.33)
[40/157]	0.0948 (0.0937)	0.0573 (0.0572)	1.384 (1.114)	56.25 (65.47)
[50/157]	0.0947 (0.0939)	0.0569 (0.0573)	1.064 (1.094)	59.38 (66.12)
[60/157]	0.0933 (0.0940)	0.0567 (0.0574)	1.168 (1.099)	62.50 (65.93)
[70/157]	0.0956 (0.0942)	0.0579 (0.0575)	1.015 (1.087)	75.00 (66.24)
[80/157]	0.0943 (0.0943)	0.0576 (0.0576)	0.910 (1.089)	71.88 (65.97)
[90/157]	0.0964 (0.0943)	0.0579 (0.0576)	1.077 (1.082)	65.62 (66.48)
[100/157]	0.0973 (0.0944)	0.0591 (0.0577)	1.069 (1.085)	68.75 (66.34)
[110/157]	0.0938 (0.0944)	0.0566 (0.0577)	1.085 (1.080)	62.50 (66.50)
[120/157]	0.0968 (0.0945)	0.0585 (0.0577)	1.122 (1.082)	65.62 (66.63)
[130/157]	0.0916 (0.0945)	0.0538 (0.0578)	1.415 (1.090)	59.38 (66.44)
[140/157]	0.1002 (0.0945)	0.0590 (0.0578)	0.999 (1.086)	65.62 (66.64)
[150/157]	0.0958 (0.0945)	0.0577 (0.0578)	1.169 (1.090)	59.38 (66.51)
[156/157]	0.0779 (0.0944)	0.0536 (0.0578)	1.756 (1.089)	37.50 (66.56)
 * Train Acc 66.560
 * Val Acc 66.900, Total time 0.57
 * Val loss 0.991, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0082 (0.0082)	0.843 (0.843)	78.12 (78.12)
[10/157]	0.0958 (0.0914)	0.0587 (0.0543)	1.007 (1.047)	71.88 (66.48)
[20/157]	0.0948 (0.0929)	0.0577 (0.0563)	1.184 (1.107)	68.75 (66.07)
[30/157]	0.0969 (0.0936)	0.0589 (0.0571)	0.862 (1.103)	71.88 (66.23)
[40/157]	0.0951 (0.0938)	0.0576 (0.0573)	1.107 (1.109)	59.38 (66.08)
[50/157]	0.0959 (0.0940)	0.0583 (0.0575)	1.072 (1.101)	65.62 (65.69)
[60/157]	0.0966 (0.0941)	0.0583 (0.0576)	0.937 (1.089)	59.38 (66.34)
[70/157]	0.0944 (0.0942)	0.0574 (0.0577)	1.080 (1.096)	65.62 (65.93)
[80/157]	0.0943 (0.0943)	0.0571 (0.0578)	0.991 (1.095)	62.50 (65.90)
[90/157]	0.0959 (0.0943)	0.0582 (0.0578)	0.911 (1.088)	78.12 (66.07)
[100/157]	0.0959 (0.0944)	0.0587 (0.0578)	1.127 (1.086)	59.38 (66.21)
[110/157]	0.0952 (0.0944)	0.0573 (0.0578)	1.434 (1.081)	50.00 (66.33)
[120/157]	0.0926 (0.0944)	0.0558 (0.0578)	0.948 (1.079)	75.00 (66.32)
[130/157]	0.0959 (0.0945)	0.0592 (0.0578)	1.181 (1.078)	62.50 (66.29)
[140/157]	0.0948 (0.0946)	0.0571 (0.0579)	0.867 (1.082)	84.38 (66.36)
[150/157]	0.0939 (0.0946)	0.0556 (0.0579)	0.864 (1.072)	71.88 (66.83)
[156/157]	0.0780 (0.0945)	0.0536 (0.0579)	0.875 (1.067)	62.50 (66.92)
 * Train Acc 66.920
 * Val Acc 68.100, Total time 0.57
 * Val loss 0.991, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0088 (0.0088)	0.908 (0.908)	65.62 (65.62)
[10/157]	0.0936 (0.0912)	0.0566 (0.0541)	1.324 (1.133)	62.50 (64.77)
[20/157]	0.0962 (0.0930)	0.0582 (0.0559)	1.234 (1.098)	68.75 (66.52)
[30/157]	0.0945 (0.0937)	0.0572 (0.0566)	0.971 (1.084)	75.00 (66.23)
[40/157]	0.0977 (0.0941)	0.0592 (0.0569)	0.927 (1.076)	71.88 (65.78)
[50/157]	0.0952 (0.0942)	0.0576 (0.0572)	1.118 (1.071)	62.50 (66.73)
[60/157]	0.0978 (0.0943)	0.0592 (0.0574)	0.911 (1.060)	75.00 (67.16)
[70/157]	0.0926 (0.0944)	0.0550 (0.0574)	0.897 (1.051)	78.12 (67.25)
[80/157]	0.0975 (0.0945)	0.0589 (0.0575)	1.296 (1.047)	68.75 (67.48)
[90/157]	0.0935 (0.0945)	0.0565 (0.0575)	1.455 (1.052)	43.75 (67.20)
[100/157]	0.0965 (0.0946)	0.0589 (0.0576)	1.249 (1.060)	62.50 (67.14)
[110/157]	0.0953 (0.0946)	0.0574 (0.0576)	1.005 (1.066)	65.62 (66.81)
[120/157]	0.0940 (0.0946)	0.0564 (0.0576)	1.140 (1.068)	62.50 (66.81)
[130/157]	0.0958 (0.0946)	0.0581 (0.0577)	1.002 (1.068)	65.62 (67.03)
[140/157]	0.0970 (0.0947)	0.0586 (0.0577)	1.049 (1.064)	65.62 (67.18)
[150/157]	0.0963 (0.0947)	0.0586 (0.0577)	1.010 (1.062)	65.62 (67.07)
[156/157]	0.0781 (0.0946)	0.0530 (0.0577)	1.087 (1.063)	62.50 (67.00)
 * Train Acc 67.000
 * Val Acc 68.100, Total time 0.57
 * Val loss 0.952, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0086 (0.0086)	0.880 (0.880)	75.00 (75.00)
[10/157]	0.0964 (0.0918)	0.0581 (0.0541)	0.882 (0.957)	75.00 (70.45)
[20/157]	0.0949 (0.0932)	0.0581 (0.0560)	1.177 (1.014)	59.38 (68.01)
[30/157]	0.0971 (0.0939)	0.0582 (0.0567)	0.981 (1.032)	78.12 (66.63)
[40/157]	0.0941 (0.0941)	0.0568 (0.0571)	1.178 (1.050)	65.62 (65.62)
[50/157]	0.0967 (0.0943)	0.0581 (0.0573)	0.829 (1.071)	75.00 (64.89)
[60/157]	0.0955 (0.0945)	0.0573 (0.0573)	0.919 (1.071)	68.75 (65.37)
[70/157]	0.0966 (0.0946)	0.0588 (0.0574)	0.886 (1.072)	68.75 (65.40)
[80/157]	0.0964 (0.0947)	0.0574 (0.0575)	1.182 (1.072)	59.38 (65.32)
[90/157]	0.0957 (0.0947)	0.0586 (0.0576)	0.837 (1.075)	78.12 (65.38)
[100/157]	0.0962 (0.0947)	0.0584 (0.0576)	1.077 (1.067)	68.75 (65.90)
[110/157]	0.0937 (0.0948)	0.0563 (0.0577)	1.030 (1.063)	68.75 (66.13)
[120/157]	0.0961 (0.0948)	0.0583 (0.0577)	1.580 (1.068)	56.25 (66.09)
[130/157]	0.0946 (0.0948)	0.0573 (0.0577)	1.196 (1.062)	62.50 (66.39)
[140/157]	0.0956 (0.0948)	0.0583 (0.0577)	0.959 (1.062)	68.75 (66.49)
[150/157]	0.0957 (0.0948)	0.0585 (0.0578)	0.771 (1.057)	81.25 (66.58)
[156/157]	0.0763 (0.0946)	0.0516 (0.0578)	0.683 (1.056)	87.50 (66.48)
 * Train Acc 66.480
 * Val Acc 69.900, Total time 0.57
 * Val loss 0.933, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0401 (0.0401)	0.0076 (0.0076)	1.685 (1.685)	56.25 (56.25)
[10/157]	0.0944 (0.0910)	0.0570 (0.0539)	1.113 (1.100)	62.50 (66.48)
[20/157]	0.0956 (0.0929)	0.0578 (0.0558)	1.336 (1.100)	56.25 (66.67)
[30/157]	0.0963 (0.0937)	0.0574 (0.0565)	1.004 (1.075)	65.62 (67.24)
[40/157]	0.0958 (0.0939)	0.0581 (0.0567)	1.013 (1.032)	71.88 (68.98)
[50/157]	0.0954 (0.0941)	0.0577 (0.0570)	1.018 (1.038)	62.50 (68.44)
[60/157]	0.0926 (0.0942)	0.0552 (0.0571)	1.054 (1.033)	59.38 (68.24)
[70/157]	0.0972 (0.0944)	0.0587 (0.0573)	0.985 (1.048)	71.88 (67.96)
[80/157]	0.0964 (0.0945)	0.0587 (0.0573)	0.812 (1.050)	78.12 (67.94)
[90/157]	0.0944 (0.0945)	0.0572 (0.0574)	1.120 (1.056)	68.75 (67.75)
[100/157]	0.0951 (0.0945)	0.0574 (0.0575)	0.807 (1.049)	71.88 (67.88)
[110/157]	0.0941 (0.0946)	0.0568 (0.0576)	0.933 (1.048)	65.62 (67.88)
[120/157]	0.0957 (0.0946)	0.0582 (0.0576)	1.203 (1.051)	56.25 (67.54)
[130/157]	0.0934 (0.0946)	0.0563 (0.0577)	1.166 (1.049)	65.62 (67.82)
[140/157]	0.0947 (0.0946)	0.0572 (0.0577)	0.684 (1.041)	81.25 (68.02)
[150/157]	0.0962 (0.0946)	0.0582 (0.0578)	0.974 (1.045)	71.88 (67.76)
[156/157]	0.0793 (0.0945)	0.0544 (0.0577)	1.520 (1.045)	50.00 (67.80)
 * Train Acc 67.800
 * Val Acc 66.700, Total time 0.56
 * Val loss 0.993, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0406 (0.0406)	0.0085 (0.0085)	0.901 (0.901)	84.38 (84.38)
[10/157]	0.0952 (0.0911)	0.0582 (0.0542)	1.062 (0.984)	75.00 (70.45)
[20/157]	0.0924 (0.0927)	0.0557 (0.0561)	1.015 (1.034)	65.62 (67.26)
[30/157]	0.0961 (0.0935)	0.0581 (0.0568)	0.957 (1.019)	75.00 (68.75)
[40/157]	0.0961 (0.0937)	0.0586 (0.0572)	0.917 (1.045)	75.00 (67.00)
[50/157]	0.0959 (0.0939)	0.0580 (0.0573)	1.058 (1.041)	71.88 (67.59)
[60/157]	0.0980 (0.0940)	0.0581 (0.0575)	1.013 (1.035)	68.75 (68.14)
[70/157]	0.0956 (0.0942)	0.0579 (0.0576)	0.744 (1.018)	78.12 (68.71)
[80/157]	0.0946 (0.0942)	0.0571 (0.0577)	1.024 (1.016)	62.50 (68.71)
[90/157]	0.0963 (0.0943)	0.0587 (0.0577)	0.862 (1.019)	71.88 (68.41)
[100/157]	0.0961 (0.0944)	0.0569 (0.0577)	0.846 (1.018)	78.12 (68.72)
[110/157]	0.0958 (0.0944)	0.0582 (0.0577)	0.781 (1.010)	78.12 (69.17)
[120/157]	0.0947 (0.0945)	0.0568 (0.0577)	1.048 (1.011)	62.50 (69.24)
[130/157]	0.0963 (0.0945)	0.0584 (0.0577)	0.920 (1.023)	81.25 (68.85)
[140/157]	0.0958 (0.0945)	0.0584 (0.0577)	0.795 (1.022)	71.88 (68.77)
[150/157]	0.0944 (0.0945)	0.0568 (0.0577)	0.884 (1.022)	75.00 (68.73)
[156/157]	0.0788 (0.0944)	0.0541 (0.0577)	0.934 (1.026)	75.00 (68.52)
 * Train Acc 68.520
 * Val Acc 70.000, Total time 0.57
 * Val loss 0.934, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0419 (0.0419)	0.0086 (0.0086)	1.278 (1.278)	59.38 (59.38)
[10/157]	0.0933 (0.0910)	0.0552 (0.0538)	0.797 (0.994)	78.12 (69.89)
[20/157]	0.0944 (0.0926)	0.0575 (0.0559)	1.061 (0.985)	56.25 (69.79)
[30/157]	0.0950 (0.0932)	0.0580 (0.0567)	1.599 (1.033)	46.88 (68.35)
[40/157]	0.0967 (0.0936)	0.0584 (0.0570)	0.991 (1.039)	62.50 (68.29)
[50/157]	0.0940 (0.0937)	0.0571 (0.0572)	0.899 (1.035)	78.12 (68.57)
[60/157]	0.0940 (0.0939)	0.0574 (0.0574)	1.069 (1.033)	62.50 (68.49)
[70/157]	0.0954 (0.0940)	0.0575 (0.0575)	1.214 (1.042)	56.25 (67.78)
[80/157]	0.0941 (0.0941)	0.0573 (0.0576)	1.081 (1.029)	75.00 (68.52)
[90/157]	0.0942 (0.0942)	0.0579 (0.0577)	0.992 (1.029)	65.62 (68.27)
[100/157]	0.0940 (0.0943)	0.0571 (0.0577)	1.172 (1.021)	78.12 (68.69)
[110/157]	0.0974 (0.0943)	0.0595 (0.0578)	1.080 (1.025)	65.62 (68.41)
[120/157]	0.0947 (0.0943)	0.0571 (0.0578)	1.120 (1.023)	59.38 (68.36)
[130/157]	0.0944 (0.0944)	0.0566 (0.0578)	0.757 (1.019)	84.38 (68.73)
[140/157]	0.0959 (0.0944)	0.0579 (0.0578)	1.321 (1.021)	68.75 (68.73)
[150/157]	0.0932 (0.0944)	0.0563 (0.0578)	0.891 (1.019)	71.88 (68.79)
[156/157]	0.0793 (0.0943)	0.0529 (0.0578)	1.387 (1.019)	50.00 (68.72)
 * Train Acc 68.720
 * Val Acc 70.300, Total time 0.56
 * Val loss 0.922, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0084 (0.0084)	1.261 (1.261)	68.75 (68.75)
[10/157]	0.0962 (0.0911)	0.0586 (0.0538)	0.965 (1.038)	71.88 (70.17)
[20/157]	0.0927 (0.0929)	0.0557 (0.0557)	1.030 (1.038)	75.00 (68.75)
[30/157]	0.0956 (0.0936)	0.0577 (0.0564)	1.015 (1.019)	68.75 (69.25)
[40/157]	0.0961 (0.0939)	0.0578 (0.0568)	0.912 (1.038)	68.75 (68.22)
[50/157]	0.0977 (0.0941)	0.0596 (0.0571)	0.990 (1.032)	68.75 (68.81)
[60/157]	0.0941 (0.0942)	0.0563 (0.0572)	1.405 (1.032)	50.00 (69.06)
[70/157]	0.0956 (0.0944)	0.0584 (0.0574)	0.769 (1.033)	81.25 (69.01)
[80/157]	0.0939 (0.0944)	0.0567 (0.0575)	1.080 (1.031)	68.75 (69.21)
[90/157]	0.0966 (0.0945)	0.0582 (0.0576)	0.688 (1.024)	78.12 (69.33)
[100/157]	0.0958 (0.0945)	0.0576 (0.0576)	1.164 (1.025)	71.88 (69.40)
[110/157]	0.0961 (0.0946)	0.0584 (0.0577)	0.953 (1.026)	78.12 (69.37)
[120/157]	0.0925 (0.0946)	0.0555 (0.0577)	1.018 (1.016)	71.88 (69.52)
[130/157]	0.0966 (0.0946)	0.0584 (0.0577)	1.001 (1.013)	62.50 (69.42)
[140/157]	0.0959 (0.0946)	0.0581 (0.0577)	0.841 (1.009)	84.38 (69.84)
[150/157]	0.0952 (0.0947)	0.0573 (0.0577)	0.926 (1.009)	75.00 (69.74)
[156/157]	0.0753 (0.0945)	0.0517 (0.0577)	0.881 (1.011)	75.00 (69.76)
 * Train Acc 69.760
 * Val Acc 68.800, Total time 0.57
 * Val loss 0.964, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0093 (0.0093)	0.723 (0.723)	84.38 (84.38)
[10/157]	0.0955 (0.0915)	0.0582 (0.0543)	1.005 (1.012)	68.75 (68.75)
[20/157]	0.0938 (0.0931)	0.0567 (0.0560)	0.960 (0.991)	62.50 (69.05)
[30/157]	0.0961 (0.0936)	0.0578 (0.0566)	1.156 (1.005)	68.75 (68.25)
[40/157]	0.0950 (0.0938)	0.0579 (0.0570)	1.051 (1.016)	65.62 (68.06)
[50/157]	0.0944 (0.0940)	0.0570 (0.0571)	1.093 (1.025)	59.38 (67.71)
[60/157]	0.0971 (0.0941)	0.0584 (0.0573)	1.337 (1.021)	65.62 (68.14)
[70/157]	0.0962 (0.0943)	0.0581 (0.0574)	1.241 (1.029)	59.38 (67.56)
[80/157]	0.0977 (0.0943)	0.0591 (0.0575)	1.144 (1.020)	68.75 (68.06)
[90/157]	0.0937 (0.0943)	0.0563 (0.0576)	0.879 (1.011)	78.12 (68.54)
[100/157]	0.0938 (0.0944)	0.0563 (0.0576)	0.769 (1.002)	75.00 (68.66)
[110/157]	0.0961 (0.0945)	0.0581 (0.0576)	0.939 (0.994)	81.25 (69.06)
[120/157]	0.0960 (0.0945)	0.0583 (0.0576)	1.210 (1.002)	65.62 (68.93)
[130/157]	0.0969 (0.0945)	0.0593 (0.0576)	1.022 (1.009)	68.75 (68.54)
[140/157]	0.0935 (0.0946)	0.0565 (0.0576)	1.429 (1.007)	50.00 (68.68)
[150/157]	0.0971 (0.0946)	0.0584 (0.0577)	1.060 (1.007)	68.75 (68.87)
[156/157]	0.0781 (0.0944)	0.0526 (0.0576)	1.469 (1.008)	50.00 (68.86)
 * Train Acc 68.860
 * Val Acc 69.300, Total time 0.58
 * Val loss 0.908, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0089 (0.0089)	0.957 (0.957)	68.75 (68.75)
[10/157]	0.0963 (0.0914)	0.0583 (0.0540)	1.264 (0.935)	62.50 (72.16)
[20/157]	0.0923 (0.0930)	0.0556 (0.0560)	1.021 (0.968)	65.62 (72.32)
[30/157]	0.0960 (0.0936)	0.0585 (0.0567)	1.064 (0.996)	65.62 (70.06)
[40/157]	0.0942 (0.0938)	0.0576 (0.0569)	0.965 (1.015)	75.00 (68.98)
[50/157]	0.0944 (0.0940)	0.0571 (0.0572)	1.132 (1.008)	65.62 (68.81)
[60/157]	0.0959 (0.0941)	0.0584 (0.0575)	0.955 (0.999)	65.62 (69.06)
[70/157]	0.0949 (0.0943)	0.0578 (0.0576)	0.747 (1.007)	87.50 (68.53)
[80/157]	0.0963 (0.0943)	0.0590 (0.0577)	1.346 (1.011)	59.38 (68.71)
[90/157]	0.0952 (0.0944)	0.0569 (0.0577)	0.919 (1.005)	68.75 (68.96)
[100/157]	0.0949 (0.0944)	0.0575 (0.0577)	0.900 (1.008)	75.00 (68.69)
[110/157]	0.0974 (0.0944)	0.0597 (0.0578)	1.145 (0.999)	65.62 (69.12)
[120/157]	0.0931 (0.0945)	0.0560 (0.0578)	1.011 (1.002)	71.88 (69.14)
[130/157]	0.0971 (0.0945)	0.0579 (0.0578)	0.678 (1.001)	84.38 (69.30)
[140/157]	0.0938 (0.0945)	0.0556 (0.0577)	1.155 (1.003)	68.75 (69.24)
[150/157]	0.0951 (0.0945)	0.0577 (0.0578)	0.975 (1.001)	71.88 (69.14)
[156/157]	0.0762 (0.0944)	0.0525 (0.0577)	1.084 (1.002)	75.00 (69.18)
 * Train Acc 69.180
 * Val Acc 69.000, Total time 0.56
 * Val loss 0.931, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0412 (0.0412)	0.0082 (0.0082)	1.298 (1.298)	56.25 (56.25)
[10/157]	0.0940 (0.0910)	0.0570 (0.0542)	0.848 (1.058)	65.62 (66.48)
[20/157]	0.0946 (0.0928)	0.0574 (0.0562)	1.317 (1.058)	62.50 (66.37)
[30/157]	0.0967 (0.0935)	0.0593 (0.0569)	1.079 (1.030)	62.50 (67.44)
[40/157]	0.0948 (0.0938)	0.0572 (0.0572)	1.140 (1.028)	62.50 (68.29)
[50/157]	0.0957 (0.0940)	0.0575 (0.0575)	0.848 (0.996)	68.75 (69.36)
[60/157]	0.0961 (0.0940)	0.0584 (0.0575)	0.973 (0.993)	71.88 (69.67)
[70/157]	0.0939 (0.0942)	0.0555 (0.0576)	1.084 (0.997)	65.62 (69.45)
[80/157]	0.0970 (0.0943)	0.0583 (0.0577)	0.884 (1.001)	78.12 (68.94)
[90/157]	0.0942 (0.0943)	0.0572 (0.0577)	1.030 (0.999)	75.00 (69.33)
[100/157]	0.0975 (0.0944)	0.0595 (0.0578)	1.163 (1.003)	71.88 (69.12)
[110/157]	0.0944 (0.0944)	0.0580 (0.0579)	1.128 (1.012)	71.88 (68.92)
[120/157]	0.0957 (0.0945)	0.0578 (0.0579)	1.424 (1.003)	62.50 (69.45)
[130/157]	0.0948 (0.0945)	0.0565 (0.0579)	0.987 (1.009)	71.88 (69.20)
[140/157]	0.0962 (0.0945)	0.0575 (0.0580)	0.879 (1.002)	81.25 (69.41)
[150/157]	0.0943 (0.0946)	0.0563 (0.0579)	0.985 (1.003)	75.00 (69.41)
[156/157]	0.0792 (0.0945)	0.0534 (0.0579)	1.398 (1.000)	50.00 (69.38)
 * Train Acc 69.380
 * Val Acc 70.200, Total time 0.57
 * Val loss 0.921, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0086 (0.0086)	0.961 (0.961)	71.88 (71.88)
[10/157]	0.0941 (0.0912)	0.0575 (0.0543)	1.180 (1.031)	62.50 (68.18)
[20/157]	0.0938 (0.0929)	0.0567 (0.0560)	0.814 (0.988)	78.12 (69.35)
[30/157]	0.0963 (0.0935)	0.0579 (0.0567)	0.676 (0.997)	84.38 (69.96)
[40/157]	0.0953 (0.0937)	0.0583 (0.0570)	0.946 (1.005)	68.75 (68.90)
[50/157]	0.0920 (0.0939)	0.0550 (0.0571)	0.699 (1.007)	75.00 (68.75)
[60/157]	0.0968 (0.0941)	0.0577 (0.0572)	0.872 (1.004)	68.75 (68.39)
[70/157]	0.0949 (0.0942)	0.0573 (0.0573)	0.887 (0.994)	75.00 (68.75)
[80/157]	0.0960 (0.0943)	0.0579 (0.0574)	0.947 (1.000)	68.75 (68.40)
[90/157]	0.0946 (0.0943)	0.0564 (0.0574)	1.089 (0.997)	65.62 (68.75)
[100/157]	0.0945 (0.0944)	0.0573 (0.0575)	0.927 (0.996)	62.50 (68.63)
[110/157]	0.0950 (0.0944)	0.0578 (0.0576)	0.865 (0.987)	68.75 (68.86)
[120/157]	0.0948 (0.0944)	0.0568 (0.0576)	1.232 (0.987)	68.75 (69.32)
[130/157]	0.0977 (0.0945)	0.0591 (0.0577)	1.162 (0.993)	65.62 (69.04)
[140/157]	0.0943 (0.0945)	0.0577 (0.0577)	0.914 (0.991)	65.62 (69.15)
[150/157]	0.0968 (0.0945)	0.0589 (0.0578)	0.948 (0.987)	68.75 (69.39)
[156/157]	0.0776 (0.0944)	0.0533 (0.0578)	0.705 (0.986)	87.50 (69.44)
 * Train Acc 69.440
 * Val Acc 71.000, Total time 0.57
 * Val loss 0.902, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0409 (0.0409)	0.0083 (0.0083)	1.077 (1.077)	65.62 (65.62)
[10/157]	0.0949 (0.0911)	0.0573 (0.0543)	0.838 (0.970)	62.50 (67.05)
[20/157]	0.0938 (0.0926)	0.0564 (0.0560)	0.908 (0.966)	81.25 (69.35)
[30/157]	0.0926 (0.0934)	0.0549 (0.0567)	0.783 (0.942)	81.25 (70.87)
[40/157]	0.0964 (0.0937)	0.0580 (0.0572)	0.900 (0.940)	68.75 (70.50)
[50/157]	0.0944 (0.0939)	0.0567 (0.0572)	1.057 (0.942)	62.50 (70.40)
[60/157]	0.0962 (0.0940)	0.0581 (0.0573)	0.822 (0.942)	68.75 (70.80)
[70/157]	0.0935 (0.0942)	0.0568 (0.0574)	0.900 (0.947)	75.00 (70.38)
[80/157]	0.0960 (0.0942)	0.0578 (0.0574)	0.900 (0.950)	75.00 (70.83)
[90/157]	0.0948 (0.0943)	0.0572 (0.0575)	0.973 (0.945)	68.75 (71.02)
[100/157]	0.0946 (0.0943)	0.0569 (0.0575)	0.845 (0.948)	75.00 (70.88)
[110/157]	0.0954 (0.0944)	0.0579 (0.0575)	0.997 (0.958)	75.00 (70.66)
[120/157]	0.0939 (0.0944)	0.0566 (0.0576)	1.116 (0.965)	59.38 (70.38)
[130/157]	0.0950 (0.0945)	0.0573 (0.0576)	1.046 (0.968)	65.62 (70.18)
[140/157]	0.0952 (0.0945)	0.0577 (0.0576)	0.747 (0.971)	75.00 (70.08)
[150/157]	0.0986 (0.0945)	0.0581 (0.0577)	0.873 (0.972)	68.75 (70.05)
[156/157]	0.0764 (0.0944)	0.0518 (0.0577)	1.052 (0.973)	75.00 (70.04)
 * Train Acc 70.040
 * Val Acc 71.200, Total time 0.57
 * Val loss 0.895, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0081 (0.0081)	0.792 (0.792)	81.25 (81.25)
[10/157]	0.0940 (0.0912)	0.0574 (0.0540)	0.746 (0.935)	68.75 (68.75)
[20/157]	0.0954 (0.0929)	0.0584 (0.0560)	0.756 (0.988)	81.25 (67.41)
[30/157]	0.0940 (0.0935)	0.0576 (0.0567)	0.798 (0.977)	78.12 (68.75)
[40/157]	0.0970 (0.0939)	0.0589 (0.0571)	0.809 (0.985)	78.12 (68.60)
[50/157]	0.0969 (0.0940)	0.0587 (0.0573)	0.955 (0.998)	78.12 (68.14)
[60/157]	0.0969 (0.0942)	0.0582 (0.0573)	1.056 (0.993)	62.50 (68.44)
[70/157]	0.0987 (0.0943)	0.0595 (0.0574)	0.879 (0.977)	71.88 (69.06)
[80/157]	0.0930 (0.0943)	0.0568 (0.0575)	0.944 (0.973)	75.00 (69.25)
[90/157]	0.0947 (0.0944)	0.0572 (0.0576)	0.664 (0.968)	81.25 (69.75)
[100/157]	0.0944 (0.0944)	0.0578 (0.0576)	0.622 (0.968)	84.38 (69.77)
[110/157]	0.0945 (0.0944)	0.0571 (0.0577)	0.922 (0.965)	68.75 (70.05)
[120/157]	0.0967 (0.0944)	0.0583 (0.0578)	1.079 (0.971)	62.50 (69.76)
[130/157]	0.0947 (0.0944)	0.0577 (0.0578)	0.707 (0.969)	78.12 (69.82)
[140/157]	0.0953 (0.0945)	0.0581 (0.0578)	1.231 (0.972)	59.38 (69.75)
[150/157]	0.0950 (0.0945)	0.0586 (0.0579)	1.327 (0.975)	56.25 (69.58)
[156/157]	0.0775 (0.0944)	0.0521 (0.0578)	2.119 (0.977)	37.50 (69.64)
 * Train Acc 69.640
 * Val Acc 71.000, Total time 0.58
 * Val loss 0.882, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0415 (0.0415)	0.0085 (0.0085)	1.018 (1.018)	68.75 (68.75)
[10/157]	0.0959 (0.0912)	0.0583 (0.0545)	0.971 (1.019)	68.75 (68.47)
[20/157]	0.0944 (0.0931)	0.0571 (0.0562)	0.859 (0.966)	71.88 (70.24)
[30/157]	0.0960 (0.0936)	0.0579 (0.0568)	1.034 (1.012)	62.50 (68.55)
[40/157]	0.0949 (0.0939)	0.0569 (0.0570)	0.924 (0.990)	71.88 (69.28)
[50/157]	0.0965 (0.0941)	0.0598 (0.0573)	0.887 (0.967)	75.00 (69.98)
[60/157]	0.0976 (0.0941)	0.0590 (0.0574)	1.028 (0.980)	71.88 (70.13)
[70/157]	0.0957 (0.0943)	0.0580 (0.0576)	0.954 (0.985)	71.88 (70.11)
[80/157]	0.0964 (0.0943)	0.0592 (0.0577)	1.047 (0.992)	71.88 (69.79)
[90/157]	0.0950 (0.0944)	0.0574 (0.0577)	1.037 (0.987)	65.62 (70.02)
[100/157]	0.0965 (0.0944)	0.0590 (0.0578)	0.825 (0.988)	75.00 (69.83)
[110/157]	0.0956 (0.0944)	0.0581 (0.0579)	1.044 (0.978)	65.62 (70.19)
[120/157]	0.0944 (0.0945)	0.0568 (0.0579)	1.175 (0.971)	68.75 (70.61)
[130/157]	0.0960 (0.0945)	0.0592 (0.0579)	0.885 (0.968)	81.25 (70.80)
[140/157]	0.0937 (0.0945)	0.0570 (0.0579)	0.921 (0.971)	75.00 (70.63)
[150/157]	0.0962 (0.0945)	0.0586 (0.0580)	0.965 (0.984)	65.62 (69.99)
[156/157]	0.0774 (0.0944)	0.0534 (0.0579)	1.032 (0.983)	87.50 (70.04)
 * Train Acc 70.040
 * Val Acc 71.100, Total time 0.57
 * Val loss 0.888, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0409 (0.0409)	0.0085 (0.0085)	1.003 (1.003)	75.00 (75.00)
[10/157]	0.0936 (0.0911)	0.0562 (0.0545)	0.982 (1.012)	71.88 (69.03)
[20/157]	0.0958 (0.0929)	0.0583 (0.0565)	0.705 (0.981)	75.00 (70.09)
[30/157]	0.0962 (0.0935)	0.0590 (0.0572)	0.981 (0.945)	75.00 (71.37)
[40/157]	0.0960 (0.0939)	0.0580 (0.0575)	0.914 (0.958)	75.00 (70.96)
[50/157]	0.0964 (0.0940)	0.0588 (0.0577)	0.918 (0.977)	71.88 (70.40)
[60/157]	0.0960 (0.0942)	0.0583 (0.0578)	1.046 (0.980)	65.62 (70.44)
[70/157]	0.0961 (0.0942)	0.0584 (0.0578)	0.834 (0.965)	75.00 (70.60)
[80/157]	0.0949 (0.0943)	0.0582 (0.0579)	1.202 (0.968)	62.50 (70.22)
[90/157]	0.0955 (0.0944)	0.0580 (0.0580)	0.873 (0.959)	75.00 (70.43)
[100/157]	0.0951 (0.0944)	0.0581 (0.0580)	0.818 (0.958)	81.25 (70.51)
[110/157]	0.0932 (0.0944)	0.0561 (0.0580)	1.256 (0.957)	59.38 (70.69)
[120/157]	0.0959 (0.0945)	0.0578 (0.0580)	1.002 (0.954)	65.62 (70.56)
[130/157]	0.0936 (0.0945)	0.0564 (0.0580)	0.977 (0.958)	71.88 (70.52)
[140/157]	0.0960 (0.0945)	0.0586 (0.0580)	1.083 (0.958)	62.50 (70.63)
[150/157]	0.0955 (0.0945)	0.0586 (0.0580)	0.995 (0.963)	59.38 (70.34)
[156/157]	0.0779 (0.0944)	0.0536 (0.0580)	2.202 (0.969)	37.50 (70.18)
 * Train Acc 70.180
 * Val Acc 71.500, Total time 0.57
 * Val loss 0.875, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0416 (0.0416)	0.0088 (0.0088)	1.074 (1.074)	65.62 (65.62)
[10/157]	0.0975 (0.0914)	0.0584 (0.0539)	1.130 (0.979)	59.38 (70.17)
[20/157]	0.0946 (0.0931)	0.0575 (0.0558)	1.055 (0.933)	62.50 (71.58)
[30/157]	0.0960 (0.0937)	0.0574 (0.0565)	0.444 (0.948)	87.50 (70.77)
[40/157]	0.0958 (0.0941)	0.0566 (0.0569)	1.019 (0.942)	71.88 (71.11)
[50/157]	0.0949 (0.0942)	0.0579 (0.0571)	0.953 (0.978)	81.25 (69.91)
[60/157]	0.0944 (0.0943)	0.0574 (0.0573)	0.825 (0.959)	65.62 (70.39)
[70/157]	0.0962 (0.0944)	0.0580 (0.0574)	1.108 (0.981)	59.38 (69.98)
[80/157]	0.0948 (0.0945)	0.0571 (0.0575)	0.703 (0.987)	84.38 (69.64)
[90/157]	0.0963 (0.0945)	0.0581 (0.0575)	0.943 (0.981)	78.12 (69.92)
[100/157]	0.0957 (0.0945)	0.0579 (0.0576)	0.752 (0.981)	78.12 (69.83)
[110/157]	0.0921 (0.0945)	0.0549 (0.0576)	0.819 (0.970)	78.12 (70.24)
[120/157]	0.0960 (0.0945)	0.0588 (0.0576)	1.073 (0.970)	59.38 (70.20)
[130/157]	0.0941 (0.0945)	0.0576 (0.0577)	1.008 (0.973)	78.12 (70.06)
[140/157]	0.0941 (0.0945)	0.0571 (0.0577)	1.164 (0.969)	59.38 (70.37)
[150/157]	0.0942 (0.0945)	0.0573 (0.0578)	1.077 (0.969)	62.50 (70.20)
[156/157]	0.0780 (0.0944)	0.0539 (0.0577)	1.032 (0.969)	62.50 (70.16)
 * Train Acc 70.160
 * Val Acc 73.100, Total time 0.57
 * Val loss 0.866, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0083 (0.0083)	0.774 (0.774)	84.38 (84.38)
[10/157]	0.0947 (0.0909)	0.0579 (0.0545)	0.830 (0.986)	71.88 (71.88)
[20/157]	0.0935 (0.0928)	0.0569 (0.0564)	0.696 (0.977)	81.25 (70.68)
[30/157]	0.0962 (0.0934)	0.0585 (0.0571)	0.960 (0.974)	59.38 (68.85)
[40/157]	0.0964 (0.0937)	0.0592 (0.0574)	1.230 (0.976)	56.25 (68.90)
[50/157]	0.0951 (0.0939)	0.0578 (0.0575)	1.189 (0.972)	62.50 (69.12)
[60/157]	0.0954 (0.0941)	0.0580 (0.0576)	1.486 (0.968)	62.50 (69.26)
[70/157]	0.0933 (0.0942)	0.0557 (0.0576)	1.182 (0.969)	56.25 (69.23)
[80/157]	0.0968 (0.0943)	0.0583 (0.0576)	0.995 (0.976)	68.75 (69.17)
[90/157]	0.0963 (0.0943)	0.0593 (0.0576)	1.085 (0.989)	62.50 (68.65)
[100/157]	0.0956 (0.0944)	0.0583 (0.0576)	0.758 (0.975)	71.88 (69.37)
[110/157]	0.0953 (0.0944)	0.0578 (0.0576)	0.934 (0.967)	75.00 (69.93)
[120/157]	0.0946 (0.0944)	0.0571 (0.0577)	0.747 (0.961)	78.12 (70.30)
[130/157]	0.0958 (0.0945)	0.0581 (0.0577)	0.924 (0.965)	68.75 (70.16)
[140/157]	0.0949 (0.0945)	0.0573 (0.0577)	1.034 (0.962)	62.50 (70.01)
[150/157]	0.0943 (0.0945)	0.0571 (0.0577)	0.710 (0.957)	75.00 (70.34)
[156/157]	0.0795 (0.0944)	0.0546 (0.0577)	1.209 (0.957)	50.00 (70.24)
 * Train Acc 70.240
 * Val Acc 72.700, Total time 0.57
 * Val loss 0.865, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0083 (0.0083)	1.054 (1.054)	75.00 (75.00)
[10/157]	0.0964 (0.0910)	0.0589 (0.0543)	0.897 (0.960)	71.88 (71.88)
[20/157]	0.0940 (0.0925)	0.0570 (0.0559)	1.128 (0.957)	65.62 (71.58)
[30/157]	0.0938 (0.0933)	0.0566 (0.0566)	0.775 (0.915)	75.00 (72.88)
[40/157]	0.0968 (0.0936)	0.0586 (0.0570)	0.890 (0.930)	71.88 (72.03)
[50/157]	0.0954 (0.0938)	0.0582 (0.0571)	0.659 (0.918)	84.38 (72.55)
[60/157]	0.0958 (0.0939)	0.0580 (0.0573)	0.884 (0.930)	71.88 (72.08)
[70/157]	0.0945 (0.0940)	0.0569 (0.0574)	0.696 (0.931)	78.12 (71.88)
[80/157]	0.0947 (0.0941)	0.0581 (0.0576)	0.991 (0.936)	65.62 (71.80)
[90/157]	0.0944 (0.0941)	0.0576 (0.0577)	1.143 (0.928)	62.50 (71.98)
[100/157]	0.0944 (0.0942)	0.0572 (0.0577)	0.652 (0.926)	84.38 (72.22)
[110/157]	0.0953 (0.0942)	0.0583 (0.0578)	0.947 (0.933)	71.88 (72.02)
[120/157]	0.0956 (0.0942)	0.0585 (0.0578)	0.902 (0.942)	75.00 (71.67)
[130/157]	0.0967 (0.0943)	0.0590 (0.0579)	0.797 (0.937)	78.12 (71.78)
[140/157]	0.0945 (0.0943)	0.0573 (0.0579)	1.088 (0.941)	68.75 (71.50)
[150/157]	0.0956 (0.0944)	0.0577 (0.0579)	0.605 (0.940)	84.38 (71.38)
[156/157]	0.0778 (0.0943)	0.0526 (0.0578)	1.300 (0.942)	50.00 (71.30)
 * Train Acc 71.300
 * Val Acc 73.600, Total time 0.57
 * Val loss 0.850, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0083 (0.0083)	1.066 (1.066)	71.88 (71.88)
[10/157]	0.0941 (0.0905)	0.0575 (0.0540)	0.872 (0.887)	71.88 (75.28)
[20/157]	0.0935 (0.0925)	0.0568 (0.0561)	0.887 (0.946)	75.00 (72.77)
[30/157]	0.0947 (0.0931)	0.0577 (0.0568)	0.989 (0.998)	68.75 (70.56)
[40/157]	0.0946 (0.0934)	0.0577 (0.0571)	0.880 (0.963)	75.00 (71.11)
[50/157]	0.0960 (0.0936)	0.0584 (0.0574)	0.737 (0.968)	84.38 (70.71)
[60/157]	0.0935 (0.0938)	0.0560 (0.0575)	0.947 (0.961)	68.75 (70.95)
[70/157]	0.0958 (0.0940)	0.0572 (0.0576)	1.079 (0.963)	68.75 (70.69)
[80/157]	0.0940 (0.0941)	0.0568 (0.0576)	0.979 (0.965)	68.75 (70.45)
[90/157]	0.0946 (0.0941)	0.0571 (0.0576)	0.802 (0.958)	78.12 (70.50)
[100/157]	0.0961 (0.0942)	0.0587 (0.0577)	0.958 (0.949)	62.50 (70.88)
[110/157]	0.0946 (0.0942)	0.0573 (0.0577)	0.847 (0.947)	75.00 (70.89)
[120/157]	0.0974 (0.0943)	0.0590 (0.0577)	0.764 (0.942)	81.25 (71.02)
[130/157]	0.0950 (0.0944)	0.0577 (0.0578)	0.951 (0.941)	71.88 (70.99)
[140/157]	0.0961 (0.0944)	0.0578 (0.0578)	0.898 (0.937)	71.88 (71.14)
[150/157]	0.0953 (0.0944)	0.0581 (0.0577)	0.939 (0.941)	68.75 (71.11)
[156/157]	0.0762 (0.0943)	0.0517 (0.0577)	0.774 (0.938)	87.50 (71.30)
 * Train Acc 71.300
 * Val Acc 72.200, Total time 0.57
 * Val loss 0.875, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0084 (0.0084)	0.794 (0.794)	71.88 (71.88)
[10/157]	0.0921 (0.0904)	0.0560 (0.0539)	1.048 (0.943)	65.62 (71.02)
[20/157]	0.0944 (0.0923)	0.0576 (0.0560)	0.892 (0.939)	78.12 (72.17)
[30/157]	0.0967 (0.0930)	0.0590 (0.0567)	1.092 (0.926)	65.62 (72.08)
[40/157]	0.0943 (0.0934)	0.0571 (0.0570)	0.841 (0.941)	75.00 (70.88)
[50/157]	0.0938 (0.0936)	0.0568 (0.0573)	0.989 (0.941)	62.50 (71.08)
[60/157]	0.0956 (0.0939)	0.0575 (0.0574)	0.694 (0.940)	84.38 (71.06)
[70/157]	0.0970 (0.0940)	0.0588 (0.0575)	0.694 (0.926)	75.00 (71.39)
[80/157]	0.0961 (0.0941)	0.0582 (0.0576)	1.144 (0.928)	65.62 (71.64)
[90/157]	0.0946 (0.0941)	0.0571 (0.0576)	1.066 (0.937)	71.88 (71.36)
[100/157]	0.0948 (0.0942)	0.0575 (0.0576)	0.742 (0.933)	71.88 (71.41)
[110/157]	0.0959 (0.0943)	0.0586 (0.0577)	0.760 (0.936)	78.12 (71.40)
[120/157]	0.0939 (0.0943)	0.0562 (0.0577)	0.948 (0.928)	71.88 (71.54)
[130/157]	0.0956 (0.0943)	0.0579 (0.0577)	0.821 (0.924)	71.88 (71.49)
[140/157]	0.0979 (0.0944)	0.0599 (0.0577)	0.905 (0.930)	75.00 (71.25)
[150/157]	0.0945 (0.0944)	0.0574 (0.0577)	1.089 (0.929)	59.38 (71.50)
[156/157]	0.0785 (0.0943)	0.0540 (0.0577)	0.985 (0.929)	62.50 (71.44)
 * Train Acc 71.440
 * Val Acc 73.400, Total time 0.57
 * Val loss 0.841, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0084 (0.0084)	0.781 (0.781)	81.25 (81.25)
[10/157]	0.0937 (0.0912)	0.0566 (0.0536)	0.658 (0.943)	78.12 (70.17)
[20/157]	0.0955 (0.0930)	0.0574 (0.0556)	0.957 (0.937)	53.12 (69.05)
[30/157]	0.0937 (0.0935)	0.0566 (0.0565)	1.035 (0.969)	62.50 (68.55)
[40/157]	0.0954 (0.0937)	0.0581 (0.0569)	0.720 (0.976)	75.00 (68.75)
[50/157]	0.0945 (0.0939)	0.0570 (0.0571)	1.022 (0.969)	65.62 (69.24)
[60/157]	0.0965 (0.0940)	0.0592 (0.0573)	1.075 (0.968)	62.50 (69.52)
[70/157]	0.0945 (0.0941)	0.0568 (0.0574)	0.820 (0.954)	78.12 (70.11)
[80/157]	0.0953 (0.0942)	0.0589 (0.0575)	0.879 (0.946)	71.88 (70.56)
[90/157]	0.0959 (0.0943)	0.0586 (0.0576)	1.020 (0.958)	68.75 (70.30)
[100/157]	0.0949 (0.0943)	0.0580 (0.0577)	0.841 (0.952)	71.88 (70.54)
[110/157]	0.0959 (0.0944)	0.0586 (0.0578)	0.707 (0.950)	87.50 (70.92)
[120/157]	0.0938 (0.0944)	0.0570 (0.0578)	0.808 (0.940)	75.00 (71.41)
[130/157]	0.0960 (0.0944)	0.0587 (0.0579)	0.691 (0.940)	75.00 (71.40)
[140/157]	0.0950 (0.0945)	0.0578 (0.0579)	0.741 (0.936)	71.88 (71.45)
[150/157]	0.0962 (0.0945)	0.0584 (0.0579)	0.740 (0.931)	78.12 (71.67)
[156/157]	0.0765 (0.0944)	0.0527 (0.0579)	1.114 (0.929)	62.50 (71.70)
 * Train Acc 71.700
 * Val Acc 73.300, Total time 0.56
 * Val loss 0.855, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0414 (0.0414)	0.0083 (0.0083)	1.246 (1.246)	65.62 (65.62)
[10/157]	0.0946 (0.0914)	0.0571 (0.0545)	0.632 (0.911)	84.38 (71.31)
[20/157]	0.0960 (0.0932)	0.0577 (0.0563)	0.932 (0.930)	81.25 (71.28)
[30/157]	0.0967 (0.0937)	0.0583 (0.0568)	0.986 (0.932)	75.00 (71.27)
[40/157]	0.0939 (0.0939)	0.0568 (0.0571)	0.731 (0.926)	78.12 (71.04)
[50/157]	0.0954 (0.0941)	0.0578 (0.0573)	1.011 (0.916)	65.62 (70.96)
[60/157]	0.0939 (0.0942)	0.0568 (0.0574)	0.734 (0.909)	81.25 (71.36)
[70/157]	0.0961 (0.0943)	0.0582 (0.0575)	1.139 (0.927)	65.62 (70.91)
[80/157]	0.0945 (0.0943)	0.0575 (0.0576)	0.843 (0.914)	75.00 (71.60)
[90/157]	0.0947 (0.0943)	0.0572 (0.0577)	1.125 (0.923)	56.25 (71.57)
[100/157]	0.0944 (0.0944)	0.0575 (0.0578)	0.686 (0.920)	84.38 (71.72)
[110/157]	0.0942 (0.0944)	0.0575 (0.0579)	1.140 (0.926)	68.75 (71.57)
[120/157]	0.0927 (0.0944)	0.0559 (0.0579)	1.033 (0.924)	65.62 (71.51)
[130/157]	0.0961 (0.0945)	0.0573 (0.0579)	1.057 (0.929)	59.38 (71.23)
[140/157]	0.0949 (0.0945)	0.0574 (0.0579)	1.012 (0.931)	62.50 (71.14)
[150/157]	0.0966 (0.0945)	0.0577 (0.0579)	0.682 (0.926)	81.25 (71.23)
[156/157]	0.0764 (0.0944)	0.0526 (0.0579)	1.019 (0.926)	75.00 (71.30)
 * Train Acc 71.300
 * Val Acc 72.900, Total time 0.56
 * Val loss 0.861, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0455 (0.0455)	0.0088 (0.0088)	0.907 (0.907)	71.88 (71.88)
[10/157]	0.0947 (0.0911)	0.0569 (0.0537)	0.911 (0.993)	62.50 (68.47)
[20/157]	0.0950 (0.0926)	0.0580 (0.0557)	0.591 (0.974)	90.62 (69.94)
[30/157]	0.0962 (0.0932)	0.0587 (0.0566)	0.968 (0.960)	68.75 (70.36)
[40/157]	0.0936 (0.0935)	0.0567 (0.0570)	0.925 (0.964)	71.88 (70.58)
[50/157]	0.0925 (0.0937)	0.0563 (0.0573)	0.891 (0.957)	71.88 (71.26)
[60/157]	0.0966 (0.0939)	0.0586 (0.0574)	0.894 (0.962)	75.00 (70.85)
[70/157]	0.0966 (0.0941)	0.0577 (0.0576)	1.000 (0.960)	71.88 (70.64)
[80/157]	0.0968 (0.0941)	0.0588 (0.0577)	0.555 (0.959)	87.50 (70.76)
[90/157]	0.0962 (0.0942)	0.0590 (0.0577)	0.862 (0.953)	71.88 (70.74)
[100/157]	0.0929 (0.0942)	0.0568 (0.0577)	0.685 (0.942)	78.12 (70.85)
[110/157]	0.0945 (0.0943)	0.0579 (0.0577)	0.660 (0.940)	90.62 (70.83)
[120/157]	0.0965 (0.0943)	0.0584 (0.0578)	0.927 (0.935)	68.75 (71.02)
[130/157]	0.0933 (0.0943)	0.0568 (0.0578)	0.771 (0.934)	68.75 (70.90)
[140/157]	0.0960 (0.0944)	0.0587 (0.0578)	0.657 (0.931)	78.12 (71.23)
[150/157]	0.0943 (0.0944)	0.0566 (0.0578)	1.146 (0.930)	62.50 (71.19)
[156/157]	0.0787 (0.0943)	0.0541 (0.0578)	0.711 (0.933)	75.00 (71.22)
 * Train Acc 71.220
 * Val Acc 72.600, Total time 0.55
 * Val loss 0.853, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0412 (0.0412)	0.0083 (0.0083)	0.836 (0.836)	78.12 (78.12)
[10/157]	0.0936 (0.0909)	0.0559 (0.0538)	1.024 (0.833)	65.62 (74.15)
[20/157]	0.0943 (0.0926)	0.0578 (0.0560)	0.874 (0.829)	68.75 (74.26)
[30/157]	0.0951 (0.0931)	0.0581 (0.0567)	1.045 (0.864)	62.50 (73.59)
[40/157]	0.0961 (0.0935)	0.0585 (0.0571)	0.864 (0.904)	75.00 (72.18)
[50/157]	0.0954 (0.0937)	0.0581 (0.0574)	1.062 (0.900)	71.88 (72.37)
[60/157]	0.0951 (0.0938)	0.0584 (0.0576)	1.119 (0.911)	65.62 (71.98)
[70/157]	0.0961 (0.0939)	0.0586 (0.0577)	0.652 (0.907)	81.25 (72.05)
[80/157]	0.0946 (0.0940)	0.0580 (0.0578)	1.002 (0.898)	68.75 (72.72)
[90/157]	0.0939 (0.0940)	0.0575 (0.0578)	0.661 (0.894)	84.38 (72.94)
[100/157]	0.0938 (0.0941)	0.0572 (0.0579)	1.129 (0.898)	62.50 (72.80)
[110/157]	0.0946 (0.0942)	0.0569 (0.0579)	0.822 (0.898)	75.00 (72.86)
[120/157]	0.0950 (0.0942)	0.0570 (0.0579)	0.763 (0.900)	84.38 (72.83)
[130/157]	0.0948 (0.0943)	0.0580 (0.0579)	1.227 (0.912)	65.62 (72.21)
[140/157]	0.0952 (0.0943)	0.0571 (0.0579)	0.721 (0.910)	81.25 (72.30)
[150/157]	0.0958 (0.0944)	0.0585 (0.0580)	1.048 (0.913)	65.62 (72.08)
[156/157]	0.0763 (0.0942)	0.0527 (0.0579)	0.795 (0.912)	75.00 (72.20)
 * Train Acc 72.200
 * Val Acc 74.800, Total time 0.57
 * Val loss 0.831, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0085 (0.0085)	0.600 (0.600)	87.50 (87.50)
[10/157]	0.0961 (0.0912)	0.0586 (0.0542)	1.066 (0.909)	62.50 (72.73)
[20/157]	0.0957 (0.0928)	0.0577 (0.0561)	1.042 (0.921)	71.88 (73.81)
[30/157]	0.0943 (0.0933)	0.0583 (0.0568)	0.765 (0.921)	78.12 (73.29)
[40/157]	0.0944 (0.0936)	0.0580 (0.0573)	0.703 (0.921)	84.38 (72.48)
[50/157]	0.0954 (0.0938)	0.0581 (0.0574)	0.830 (0.920)	68.75 (72.49)
[60/157]	0.0942 (0.0939)	0.0570 (0.0575)	1.089 (0.919)	62.50 (72.18)
[70/157]	0.0937 (0.0939)	0.0568 (0.0576)	1.059 (0.922)	62.50 (71.57)
[80/157]	0.0968 (0.0941)	0.0583 (0.0577)	0.846 (0.920)	75.00 (71.76)
[90/157]	0.0949 (0.0941)	0.0579 (0.0577)	0.976 (0.910)	68.75 (72.29)
[100/157]	0.0935 (0.0942)	0.0566 (0.0578)	0.626 (0.917)	84.38 (71.91)
[110/157]	0.0959 (0.0942)	0.0589 (0.0578)	1.083 (0.924)	65.62 (71.73)
[120/157]	0.0954 (0.0943)	0.0574 (0.0578)	0.785 (0.915)	81.25 (72.18)
[130/157]	0.0943 (0.0943)	0.0570 (0.0579)	1.265 (0.921)	59.38 (71.85)
[140/157]	0.0956 (0.0943)	0.0581 (0.0579)	0.766 (0.919)	75.00 (71.96)
[150/157]	0.0960 (0.0943)	0.0592 (0.0579)	0.620 (0.917)	93.75 (72.16)
[156/157]	0.0757 (0.0942)	0.0519 (0.0579)	1.464 (0.921)	75.00 (72.12)
 * Train Acc 72.120
 * Val Acc 73.300, Total time 0.58
 * Val loss 0.849, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0404 (0.0404)	0.0080 (0.0080)	0.927 (0.927)	75.00 (75.00)
[10/157]	0.0912 (0.0904)	0.0552 (0.0543)	0.974 (1.035)	68.75 (67.61)
[20/157]	0.0961 (0.0924)	0.0591 (0.0564)	0.731 (0.964)	78.12 (70.24)
[30/157]	0.0956 (0.0930)	0.0594 (0.0570)	0.836 (0.940)	81.25 (71.98)
[40/157]	0.0950 (0.0934)	0.0583 (0.0573)	0.839 (0.929)	71.88 (72.18)
[50/157]	0.0939 (0.0936)	0.0575 (0.0575)	0.804 (0.919)	75.00 (71.88)
[60/157]	0.0959 (0.0938)	0.0588 (0.0577)	0.716 (0.913)	81.25 (72.08)
[70/157]	0.0956 (0.0939)	0.0586 (0.0578)	0.898 (0.906)	71.88 (72.40)
[80/157]	0.0953 (0.0939)	0.0562 (0.0578)	0.896 (0.911)	68.75 (71.95)
[90/157]	0.0950 (0.0940)	0.0578 (0.0579)	1.433 (0.922)	56.25 (71.29)
[100/157]	0.0960 (0.0940)	0.0585 (0.0579)	1.020 (0.924)	68.75 (71.26)
[110/157]	0.0963 (0.0941)	0.0592 (0.0580)	0.987 (0.928)	65.62 (71.09)
[120/157]	0.0948 (0.0941)	0.0575 (0.0580)	0.783 (0.921)	75.00 (71.23)
[130/157]	0.0961 (0.0942)	0.0588 (0.0581)	0.710 (0.922)	81.25 (71.25)
[140/157]	0.0967 (0.0942)	0.0594 (0.0581)	1.027 (0.920)	71.88 (71.37)
[150/157]	0.0956 (0.0942)	0.0581 (0.0581)	0.761 (0.916)	75.00 (71.36)
[156/157]	0.0773 (0.0941)	0.0532 (0.0581)	1.824 (0.927)	37.50 (71.10)
 * Train Acc 71.100
 * Val Acc 73.500, Total time 0.58
 * Val loss 0.833, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0094 (0.0094)	0.888 (0.888)	78.12 (78.12)
[10/157]	0.0961 (0.0910)	0.0590 (0.0539)	0.826 (0.908)	81.25 (73.86)
[20/157]	0.0938 (0.0929)	0.0567 (0.0561)	0.979 (0.924)	81.25 (72.02)
[30/157]	0.0952 (0.0934)	0.0576 (0.0568)	0.910 (0.939)	71.88 (72.38)
[40/157]	0.0944 (0.0937)	0.0577 (0.0570)	0.798 (0.926)	78.12 (73.02)
[50/157]	0.0944 (0.0940)	0.0572 (0.0572)	0.797 (0.906)	75.00 (73.53)
[60/157]	0.0955 (0.0941)	0.0585 (0.0574)	0.808 (0.913)	84.38 (73.51)
[70/157]	0.0949 (0.0941)	0.0580 (0.0575)	0.920 (0.907)	65.62 (73.50)
[80/157]	0.0944 (0.0942)	0.0573 (0.0576)	0.762 (0.909)	75.00 (73.11)
[90/157]	0.0962 (0.0942)	0.0587 (0.0577)	1.066 (0.917)	75.00 (72.77)
[100/157]	0.0964 (0.0943)	0.0593 (0.0578)	0.989 (0.917)	68.75 (72.99)
[110/157]	0.0943 (0.0943)	0.0570 (0.0578)	0.935 (0.920)	75.00 (73.09)
[120/157]	0.0962 (0.0943)	0.0575 (0.0578)	1.262 (0.925)	50.00 (72.96)
[130/157]	0.0951 (0.0944)	0.0584 (0.0578)	0.881 (0.919)	71.88 (73.21)
[140/157]	0.0953 (0.0944)	0.0578 (0.0578)	0.962 (0.915)	62.50 (73.29)
[150/157]	0.0966 (0.0944)	0.0594 (0.0579)	0.876 (0.916)	71.88 (73.10)
[156/157]	0.0772 (0.0943)	0.0528 (0.0579)	0.998 (0.921)	62.50 (73.00)
 * Train Acc 73.000
 * Val Acc 72.500, Total time 0.57
 * Val loss 0.842, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0415 (0.0415)	0.0083 (0.0083)	1.164 (1.164)	59.38 (59.38)
[10/157]	0.0943 (0.0907)	0.0568 (0.0537)	0.764 (0.897)	78.12 (73.86)
[20/157]	0.0941 (0.0926)	0.0567 (0.0557)	0.924 (0.936)	65.62 (71.73)
[30/157]	0.0969 (0.0933)	0.0584 (0.0564)	1.018 (0.943)	71.88 (71.17)
[40/157]	0.0994 (0.0936)	0.0594 (0.0568)	0.792 (0.950)	75.00 (70.35)
[50/157]	0.0952 (0.0937)	0.0576 (0.0570)	1.182 (0.932)	62.50 (70.71)
[60/157]	0.0937 (0.0938)	0.0573 (0.0572)	0.943 (0.929)	71.88 (70.90)
[70/157]	0.0976 (0.0940)	0.0591 (0.0574)	0.718 (0.914)	71.88 (71.52)
[80/157]	0.0944 (0.0940)	0.0563 (0.0574)	0.841 (0.916)	75.00 (71.60)
[90/157]	0.0939 (0.0941)	0.0559 (0.0575)	1.012 (0.915)	65.62 (71.63)
[100/157]	0.0960 (0.0942)	0.0582 (0.0575)	0.856 (0.911)	81.25 (71.88)
[110/157]	0.0951 (0.0942)	0.0581 (0.0575)	0.702 (0.899)	81.25 (72.02)
[120/157]	0.0941 (0.0942)	0.0566 (0.0576)	0.788 (0.899)	81.25 (72.11)
[130/157]	0.0949 (0.0943)	0.0569 (0.0576)	0.830 (0.896)	75.00 (72.11)
[140/157]	0.0957 (0.0943)	0.0571 (0.0577)	0.850 (0.892)	75.00 (72.32)
[150/157]	0.0950 (0.0943)	0.0578 (0.0577)	1.096 (0.900)	62.50 (71.98)
[156/157]	0.0783 (0.0942)	0.0534 (0.0576)	1.066 (0.900)	75.00 (72.14)
 * Train Acc 72.140
 * Val Acc 73.100, Total time 0.56
 * Val loss 0.830, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0087 (0.0087)	0.831 (0.831)	71.88 (71.88)
[10/157]	0.0957 (0.0911)	0.0586 (0.0542)	1.015 (1.005)	71.88 (67.61)
[20/157]	0.0949 (0.0927)	0.0578 (0.0561)	0.929 (0.927)	71.88 (71.88)
[30/157]	0.0957 (0.0933)	0.0572 (0.0567)	0.883 (0.926)	78.12 (71.98)
[40/157]	0.0951 (0.0936)	0.0587 (0.0570)	0.744 (0.912)	75.00 (72.41)
[50/157]	0.0966 (0.0938)	0.0589 (0.0573)	1.077 (0.904)	68.75 (72.86)
[60/157]	0.0947 (0.0939)	0.0576 (0.0574)	0.714 (0.900)	78.12 (73.16)
[70/157]	0.0958 (0.0940)	0.0588 (0.0576)	0.736 (0.882)	71.88 (73.46)
[80/157]	0.0964 (0.0941)	0.0587 (0.0577)	0.903 (0.882)	75.00 (73.53)
[90/157]	0.0936 (0.0941)	0.0555 (0.0578)	1.036 (0.889)	68.75 (73.49)
[100/157]	0.0950 (0.0942)	0.0579 (0.0578)	0.939 (0.889)	75.00 (73.33)
[110/157]	0.0967 (0.0942)	0.0593 (0.0579)	0.732 (0.891)	75.00 (73.31)
[120/157]	0.0960 (0.0942)	0.0582 (0.0579)	0.699 (0.889)	84.38 (73.35)
[130/157]	0.0944 (0.0942)	0.0577 (0.0579)	0.946 (0.895)	71.88 (72.95)
[140/157]	0.0942 (0.0943)	0.0571 (0.0579)	0.868 (0.905)	75.00 (72.63)
[150/157]	0.0948 (0.0943)	0.0574 (0.0579)	0.814 (0.899)	71.88 (72.72)
[156/157]	0.0764 (0.0942)	0.0520 (0.0579)	1.047 (0.901)	75.00 (72.62)
 * Train Acc 72.620
 * Val Acc 73.000, Total time 0.57
 * Val loss 0.835, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0406 (0.0406)	0.0081 (0.0081)	0.718 (0.718)	71.88 (71.88)
[10/157]	0.0963 (0.0901)	0.0594 (0.0541)	0.963 (0.811)	75.00 (75.28)
[20/157]	0.0944 (0.0922)	0.0571 (0.0561)	0.666 (0.825)	78.12 (76.19)
[30/157]	0.0933 (0.0929)	0.0562 (0.0568)	0.699 (0.845)	81.25 (75.30)
[40/157]	0.0976 (0.0934)	0.0586 (0.0572)	0.874 (0.869)	65.62 (74.47)
[50/157]	0.0964 (0.0935)	0.0597 (0.0574)	0.729 (0.871)	84.38 (74.39)
[60/157]	0.0944 (0.0937)	0.0573 (0.0576)	0.727 (0.866)	81.25 (74.49)
[70/157]	0.0950 (0.0938)	0.0579 (0.0577)	1.115 (0.884)	59.38 (73.81)
[80/157]	0.0966 (0.0939)	0.0580 (0.0578)	1.170 (0.896)	65.62 (73.26)
[90/157]	0.0966 (0.0941)	0.0574 (0.0579)	0.711 (0.884)	78.12 (73.63)
[100/157]	0.0943 (0.0941)	0.0573 (0.0579)	0.692 (0.887)	75.00 (73.36)
[110/157]	0.0943 (0.0942)	0.0568 (0.0579)	1.109 (0.892)	71.88 (73.25)
[120/157]	0.0968 (0.0942)	0.0584 (0.0579)	0.829 (0.893)	65.62 (73.19)
[130/157]	0.0954 (0.0943)	0.0578 (0.0579)	1.042 (0.902)	68.75 (72.83)
[140/157]	0.0960 (0.0943)	0.0581 (0.0580)	0.730 (0.901)	81.25 (72.65)
[150/157]	0.0932 (0.0944)	0.0568 (0.0580)	0.747 (0.902)	68.75 (72.50)
[156/157]	0.0775 (0.0943)	0.0534 (0.0579)	1.502 (0.903)	62.50 (72.52)
 * Train Acc 72.520
 * Val Acc 73.000, Total time 0.57
 * Val loss 0.837, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0087 (0.0087)	1.173 (1.173)	62.50 (62.50)
[10/157]	0.0940 (0.0909)	0.0571 (0.0540)	1.034 (0.983)	59.38 (71.31)
[20/157]	0.0953 (0.0928)	0.0581 (0.0560)	0.658 (0.909)	78.12 (72.62)
[30/157]	0.0941 (0.0934)	0.0570 (0.0566)	0.831 (0.905)	78.12 (72.78)
[40/157]	0.0965 (0.0938)	0.0588 (0.0570)	0.757 (0.904)	75.00 (72.79)
[50/157]	0.0940 (0.0940)	0.0567 (0.0572)	0.877 (0.925)	68.75 (72.00)
[60/157]	0.0942 (0.0941)	0.0568 (0.0573)	1.029 (0.928)	62.50 (71.21)
[70/157]	0.0970 (0.0941)	0.0585 (0.0574)	1.028 (0.923)	62.50 (71.35)
[80/157]	0.0956 (0.0942)	0.0583 (0.0575)	0.742 (0.915)	81.25 (71.68)
[90/157]	0.0952 (0.0943)	0.0572 (0.0575)	1.107 (0.918)	62.50 (71.57)
[100/157]	0.0962 (0.0943)	0.0584 (0.0576)	0.619 (0.912)	87.50 (71.78)
[110/157]	0.0952 (0.0943)	0.0581 (0.0576)	0.693 (0.911)	81.25 (71.82)
[120/157]	0.0963 (0.0944)	0.0590 (0.0577)	0.972 (0.907)	78.12 (72.06)
[130/157]	0.0943 (0.0945)	0.0571 (0.0577)	1.227 (0.921)	53.12 (71.56)
[140/157]	0.0955 (0.0945)	0.0581 (0.0578)	0.776 (0.923)	75.00 (71.41)
[150/157]	0.0939 (0.0945)	0.0568 (0.0578)	0.646 (0.917)	78.12 (71.73)
[156/157]	0.0786 (0.0944)	0.0550 (0.0578)	1.307 (0.918)	62.50 (71.74)
 * Train Acc 71.740
 * Val Acc 73.900, Total time 0.56
 * Val loss 0.829, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0412 (0.0412)	0.0082 (0.0082)	0.738 (0.738)	75.00 (75.00)
[10/157]	0.0944 (0.0908)	0.0577 (0.0541)	0.678 (1.028)	81.25 (70.45)
[20/157]	0.0954 (0.0926)	0.0583 (0.0562)	0.718 (0.947)	81.25 (72.32)
[30/157]	0.0938 (0.0934)	0.0566 (0.0568)	0.943 (0.955)	62.50 (70.97)
[40/157]	0.0952 (0.0936)	0.0584 (0.0571)	0.989 (0.938)	71.88 (71.19)
[50/157]	0.0962 (0.0939)	0.0585 (0.0573)	0.959 (0.933)	71.88 (71.94)
[60/157]	0.0942 (0.0940)	0.0576 (0.0574)	1.193 (0.926)	68.75 (72.39)
[70/157]	0.0966 (0.0941)	0.0590 (0.0575)	0.696 (0.907)	78.12 (72.71)
[80/157]	0.0950 (0.0942)	0.0579 (0.0576)	1.044 (0.903)	62.50 (72.65)
[90/157]	0.0945 (0.0942)	0.0578 (0.0577)	0.957 (0.919)	68.75 (72.15)
[100/157]	0.0978 (0.0942)	0.0596 (0.0578)	0.757 (0.912)	78.12 (72.46)
[110/157]	0.0946 (0.0943)	0.0578 (0.0578)	0.763 (0.909)	75.00 (72.35)
[120/157]	0.0949 (0.0943)	0.0574 (0.0579)	0.845 (0.914)	71.88 (72.13)
[130/157]	0.0975 (0.0943)	0.0590 (0.0579)	0.688 (0.911)	87.50 (72.40)
[140/157]	0.0945 (0.0944)	0.0576 (0.0580)	0.662 (0.907)	81.25 (72.58)
[150/157]	0.0965 (0.0944)	0.0589 (0.0580)	0.747 (0.909)	81.25 (72.64)
[156/157]	0.0761 (0.0943)	0.0522 (0.0579)	1.181 (0.910)	62.50 (72.60)
 * Train Acc 72.600
 * Val Acc 74.500, Total time 0.56
 * Val loss 0.821, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0405 (0.0405)	0.0082 (0.0082)	0.904 (0.904)	68.75 (68.75)
[10/157]	0.0945 (0.0909)	0.0575 (0.0546)	1.110 (0.970)	62.50 (70.17)
[20/157]	0.0937 (0.0926)	0.0568 (0.0564)	0.759 (0.948)	81.25 (71.13)
[30/157]	0.0951 (0.0933)	0.0581 (0.0571)	0.634 (0.918)	71.88 (71.47)
[40/157]	0.0961 (0.0937)	0.0593 (0.0575)	0.791 (0.920)	81.25 (71.65)
[50/157]	0.0957 (0.0939)	0.0576 (0.0576)	1.060 (0.917)	68.75 (72.24)
[60/157]	0.0952 (0.0941)	0.0578 (0.0577)	0.956 (0.917)	65.62 (72.59)
[70/157]	0.0957 (0.0942)	0.0576 (0.0577)	0.871 (0.916)	68.75 (72.49)
[80/157]	0.0956 (0.0943)	0.0580 (0.0577)	0.696 (0.905)	75.00 (72.99)
[90/157]	0.0941 (0.0943)	0.0568 (0.0577)	0.674 (0.904)	84.38 (72.97)
[100/157]	0.0943 (0.0944)	0.0564 (0.0577)	0.953 (0.909)	65.62 (72.52)
[110/157]	0.0947 (0.0944)	0.0577 (0.0578)	1.023 (0.908)	62.50 (72.55)
[120/157]	0.0939 (0.0944)	0.0565 (0.0578)	0.911 (0.910)	68.75 (72.57)
[130/157]	0.0958 (0.0944)	0.0588 (0.0579)	1.285 (0.907)	65.62 (72.50)
[140/157]	0.0955 (0.0944)	0.0589 (0.0579)	0.813 (0.907)	78.12 (72.50)
[150/157]	0.0944 (0.0944)	0.0579 (0.0580)	0.990 (0.901)	68.75 (72.60)
[156/157]	0.0776 (0.0943)	0.0540 (0.0579)	0.799 (0.903)	75.00 (72.58)
 * Train Acc 72.580
 * Val Acc 73.400, Total time 0.56
 * Val loss 0.829, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0413 (0.0413)	0.0086 (0.0086)	1.123 (1.123)	62.50 (62.50)
[10/157]	0.0926 (0.0907)	0.0562 (0.0540)	0.716 (0.892)	84.38 (74.43)
[20/157]	0.0943 (0.0926)	0.0571 (0.0560)	0.754 (0.913)	78.12 (72.92)
[30/157]	0.0952 (0.0931)	0.0585 (0.0568)	0.577 (0.895)	81.25 (72.88)
[40/157]	0.0942 (0.0934)	0.0568 (0.0572)	0.568 (0.871)	84.38 (74.09)
[50/157]	0.0939 (0.0936)	0.0571 (0.0574)	1.006 (0.882)	65.62 (73.47)
[60/157]	0.0961 (0.0938)	0.0580 (0.0575)	1.058 (0.885)	71.88 (73.31)
[70/157]	0.0953 (0.0940)	0.0573 (0.0575)	1.156 (0.899)	68.75 (73.06)
[80/157]	0.0954 (0.0940)	0.0576 (0.0576)	0.749 (0.895)	75.00 (73.23)
[90/157]	0.0952 (0.0941)	0.0579 (0.0576)	0.839 (0.904)	75.00 (72.87)
[100/157]	0.0940 (0.0942)	0.0571 (0.0576)	0.846 (0.911)	78.12 (72.34)
[110/157]	0.0950 (0.0942)	0.0571 (0.0577)	1.100 (0.901)	62.50 (72.75)
[120/157]	0.0954 (0.0942)	0.0583 (0.0577)	0.914 (0.901)	75.00 (72.65)
[130/157]	0.0962 (0.0942)	0.0586 (0.0577)	1.147 (0.906)	65.62 (72.47)
[140/157]	0.0943 (0.0942)	0.0572 (0.0577)	0.790 (0.905)	68.75 (72.56)
[150/157]	0.0941 (0.0943)	0.0574 (0.0578)	0.927 (0.901)	71.88 (72.93)
[156/157]	0.0782 (0.0941)	0.0543 (0.0577)	1.215 (0.900)	50.00 (72.92)
 * Train Acc 72.920
 * Val Acc 74.400, Total time 0.56
 * Val loss 0.814, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0414 (0.0414)	0.0084 (0.0084)	0.926 (0.926)	75.00 (75.00)
[10/157]	0.0925 (0.0903)	0.0563 (0.0540)	0.727 (0.822)	84.38 (75.00)
[20/157]	0.0984 (0.0925)	0.0589 (0.0561)	0.919 (0.869)	71.88 (74.26)
[30/157]	0.0961 (0.0930)	0.0585 (0.0567)	0.872 (0.849)	62.50 (74.19)
[40/157]	0.0954 (0.0934)	0.0583 (0.0571)	0.601 (0.865)	87.50 (73.09)
[50/157]	0.0971 (0.0936)	0.0594 (0.0573)	0.845 (0.878)	68.75 (72.92)
[60/157]	0.0943 (0.0937)	0.0574 (0.0575)	0.566 (0.886)	90.62 (72.90)
[70/157]	0.0968 (0.0938)	0.0593 (0.0576)	0.878 (0.880)	75.00 (73.33)
[80/157]	0.0956 (0.0939)	0.0580 (0.0577)	0.880 (0.874)	65.62 (73.23)
[90/157]	0.0957 (0.0940)	0.0583 (0.0578)	0.846 (0.883)	68.75 (72.70)
[100/157]	0.0955 (0.0941)	0.0576 (0.0578)	0.846 (0.887)	65.62 (72.46)
[110/157]	0.0939 (0.0941)	0.0566 (0.0578)	0.919 (0.886)	68.75 (72.49)
[120/157]	0.0925 (0.0941)	0.0561 (0.0579)	0.786 (0.887)	71.88 (72.52)
[130/157]	0.0959 (0.0942)	0.0582 (0.0579)	0.584 (0.885)	81.25 (72.66)
[140/157]	0.0961 (0.0942)	0.0581 (0.0579)	1.077 (0.892)	65.62 (72.43)
[150/157]	0.0944 (0.0943)	0.0575 (0.0579)	1.163 (0.898)	75.00 (72.39)
[156/157]	0.0777 (0.0941)	0.0531 (0.0579)	1.163 (0.895)	62.50 (72.34)
 * Train Acc 72.340
 * Val Acc 73.100, Total time 0.57
 * Val loss 0.827, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0086 (0.0086)	1.033 (1.033)	62.50 (62.50)
[10/157]	0.0948 (0.0907)	0.0575 (0.0542)	0.887 (0.868)	71.88 (72.16)
[20/157]	0.0950 (0.0925)	0.0569 (0.0560)	0.900 (0.873)	78.12 (72.32)
[30/157]	0.0955 (0.0933)	0.0576 (0.0565)	0.854 (0.890)	78.12 (72.68)
[40/157]	0.0955 (0.0936)	0.0591 (0.0569)	0.833 (0.899)	75.00 (72.94)
[50/157]	0.0929 (0.0938)	0.0560 (0.0572)	0.707 (0.895)	71.88 (72.30)
[60/157]	0.0956 (0.0940)	0.0580 (0.0574)	1.001 (0.896)	71.88 (72.69)
[70/157]	0.0955 (0.0941)	0.0575 (0.0574)	1.145 (0.895)	71.88 (73.02)
[80/157]	0.0931 (0.0941)	0.0566 (0.0575)	1.048 (0.888)	71.88 (73.42)
[90/157]	0.0963 (0.0942)	0.0584 (0.0576)	1.013 (0.894)	68.75 (73.15)
[100/157]	0.0949 (0.0942)	0.0580 (0.0577)	1.222 (0.887)	62.50 (73.14)
[110/157]	0.0956 (0.0942)	0.0583 (0.0577)	1.130 (0.887)	56.25 (72.94)
[120/157]	0.0947 (0.0943)	0.0575 (0.0578)	0.719 (0.886)	84.38 (72.99)
[130/157]	0.0956 (0.0943)	0.0582 (0.0578)	1.060 (0.892)	65.62 (73.12)
[140/157]	0.0936 (0.0943)	0.0568 (0.0579)	0.968 (0.889)	65.62 (73.20)
[150/157]	0.0934 (0.0943)	0.0570 (0.0579)	0.671 (0.887)	78.12 (73.14)
[156/157]	0.0783 (0.0942)	0.0543 (0.0579)	0.699 (0.889)	75.00 (73.06)
 * Train Acc 73.060
 * Val Acc 73.500, Total time 0.56
 * Val loss 0.834, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0090 (0.0090)	1.130 (1.130)	65.62 (65.62)
[10/157]	0.0925 (0.0898)	0.0560 (0.0538)	0.955 (0.837)	68.75 (75.00)
[20/157]	0.0938 (0.0921)	0.0569 (0.0560)	1.212 (0.868)	62.50 (73.36)
[30/157]	0.0956 (0.0929)	0.0587 (0.0567)	0.907 (0.877)	68.75 (73.59)
[40/157]	0.0954 (0.0932)	0.0586 (0.0570)	1.128 (0.891)	65.62 (72.79)
[50/157]	0.0962 (0.0935)	0.0592 (0.0573)	1.028 (0.897)	65.62 (72.79)
[60/157]	0.0950 (0.0936)	0.0578 (0.0575)	1.482 (0.918)	56.25 (71.98)
[70/157]	0.0953 (0.0938)	0.0580 (0.0576)	1.054 (0.905)	75.00 (72.36)
[80/157]	0.0927 (0.0939)	0.0558 (0.0577)	0.961 (0.899)	68.75 (72.22)
[90/157]	0.0947 (0.0940)	0.0578 (0.0578)	1.272 (0.906)	56.25 (71.77)
[100/157]	0.0957 (0.0940)	0.0586 (0.0579)	0.973 (0.908)	75.00 (71.66)
[110/157]	0.0960 (0.0941)	0.0582 (0.0579)	0.843 (0.910)	75.00 (71.85)
[120/157]	0.0948 (0.0941)	0.0570 (0.0579)	0.840 (0.904)	65.62 (72.00)
[130/157]	0.0965 (0.0942)	0.0581 (0.0580)	0.689 (0.899)	78.12 (72.33)
[140/157]	0.0954 (0.0942)	0.0585 (0.0580)	1.125 (0.896)	59.38 (72.41)
[150/157]	0.0943 (0.0942)	0.0578 (0.0580)	0.530 (0.895)	93.75 (72.39)
[156/157]	0.0780 (0.0941)	0.0542 (0.0580)	0.517 (0.892)	87.50 (72.48)
 * Train Acc 72.480
 * Val Acc 75.000, Total time 0.57
 * Val loss 0.808, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0405 (0.0405)	0.0079 (0.0079)	0.845 (0.845)	78.12 (78.12)
[10/157]	0.0950 (0.0903)	0.0581 (0.0540)	0.663 (0.953)	84.38 (71.31)
[20/157]	0.0942 (0.0924)	0.0579 (0.0562)	1.191 (0.961)	56.25 (70.68)
[30/157]	0.0957 (0.0930)	0.0586 (0.0569)	0.928 (0.963)	71.88 (70.56)
[40/157]	0.0955 (0.0934)	0.0592 (0.0572)	1.025 (0.963)	71.88 (69.66)
[50/157]	0.0948 (0.0937)	0.0577 (0.0574)	0.701 (0.953)	78.12 (69.91)
[60/157]	0.0958 (0.0938)	0.0588 (0.0576)	0.930 (0.941)	75.00 (70.34)
[70/157]	0.0932 (0.0940)	0.0567 (0.0577)	0.906 (0.941)	65.62 (70.69)
[80/157]	0.0928 (0.0940)	0.0567 (0.0578)	0.907 (0.941)	71.88 (70.79)
[90/157]	0.0956 (0.0941)	0.0582 (0.0578)	1.134 (0.939)	68.75 (71.26)
[100/157]	0.0952 (0.0941)	0.0583 (0.0579)	0.899 (0.929)	65.62 (71.53)
[110/157]	0.0936 (0.0941)	0.0568 (0.0579)	1.007 (0.929)	65.62 (71.54)
[120/157]	0.0940 (0.0941)	0.0574 (0.0579)	1.281 (0.930)	62.50 (71.64)
[130/157]	0.0959 (0.0942)	0.0578 (0.0580)	0.575 (0.928)	84.38 (71.71)
[140/157]	0.0961 (0.0942)	0.0562 (0.0580)	0.774 (0.916)	81.25 (71.99)
[150/157]	0.0942 (0.0943)	0.0574 (0.0580)	0.919 (0.914)	65.62 (71.92)
[156/157]	0.0784 (0.0942)	0.0538 (0.0580)	1.010 (0.907)	62.50 (72.14)
 * Train Acc 72.140
 * Val Acc 74.600, Total time 0.58
 * Val loss 0.824, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0400 (0.0400)	0.0085 (0.0085)	0.795 (0.795)	75.00 (75.00)
[10/157]	0.0942 (0.0907)	0.0577 (0.0543)	0.797 (0.914)	75.00 (70.17)
[20/157]	0.0948 (0.0927)	0.0574 (0.0562)	0.755 (0.876)	75.00 (72.47)
[30/157]	0.0967 (0.0933)	0.0586 (0.0568)	1.196 (0.889)	71.88 (72.38)
[40/157]	0.0950 (0.0936)	0.0579 (0.0571)	0.825 (0.893)	68.75 (72.48)
[50/157]	0.0923 (0.0938)	0.0559 (0.0574)	0.781 (0.896)	81.25 (72.61)
[60/157]	0.0962 (0.0939)	0.0586 (0.0576)	1.008 (0.904)	68.75 (72.59)
[70/157]	0.0935 (0.0940)	0.0564 (0.0576)	0.734 (0.893)	75.00 (73.20)
[80/157]	0.0952 (0.0941)	0.0579 (0.0577)	1.028 (0.896)	71.88 (72.69)
[90/157]	0.0963 (0.0942)	0.0580 (0.0577)	0.890 (0.900)	78.12 (72.87)
[100/157]	0.0964 (0.0942)	0.0586 (0.0577)	0.747 (0.893)	78.12 (73.08)
[110/157]	0.0936 (0.0942)	0.0567 (0.0578)	0.791 (0.893)	68.75 (72.97)
[120/157]	0.0970 (0.0943)	0.0585 (0.0578)	0.890 (0.896)	75.00 (72.73)
[130/157]	0.0949 (0.0944)	0.0576 (0.0578)	0.844 (0.896)	68.75 (72.76)
[140/157]	0.0954 (0.0944)	0.0583 (0.0578)	0.718 (0.895)	87.50 (72.89)
[150/157]	0.0946 (0.0944)	0.0575 (0.0578)	0.785 (0.895)	75.00 (72.68)
[156/157]	0.0769 (0.0943)	0.0518 (0.0578)	1.089 (0.898)	75.00 (72.60)
 * Train Acc 72.600
 * Val Acc 73.600, Total time 0.57
 * Val loss 0.822, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0415 (0.0415)	0.0084 (0.0084)	0.702 (0.702)	75.00 (75.00)
[10/157]	0.0948 (0.0908)	0.0567 (0.0535)	0.900 (0.844)	65.62 (73.01)
[20/157]	0.0956 (0.0926)	0.0582 (0.0558)	0.733 (0.863)	84.38 (74.11)
[30/157]	0.0934 (0.0932)	0.0560 (0.0565)	1.137 (0.883)	62.50 (73.39)
[40/157]	0.0935 (0.0936)	0.0566 (0.0569)	0.681 (0.886)	87.50 (73.63)
[50/157]	0.0955 (0.0938)	0.0581 (0.0572)	0.682 (0.869)	81.25 (74.33)
[60/157]	0.0963 (0.0939)	0.0585 (0.0573)	1.232 (0.875)	65.62 (74.44)
[70/157]	0.0955 (0.0940)	0.0583 (0.0574)	0.648 (0.869)	84.38 (74.52)
[80/157]	0.0966 (0.0941)	0.0593 (0.0575)	0.871 (0.874)	75.00 (74.19)
[90/157]	0.0955 (0.0942)	0.0577 (0.0576)	0.858 (0.874)	62.50 (73.83)
[100/157]	0.0954 (0.0942)	0.0583 (0.0577)	0.865 (0.871)	75.00 (73.95)
[110/157]	0.0974 (0.0943)	0.0593 (0.0577)	1.092 (0.870)	59.38 (73.85)
[120/157]	0.0949 (0.0943)	0.0574 (0.0577)	1.271 (0.876)	59.38 (73.48)
[130/157]	0.0951 (0.0943)	0.0575 (0.0578)	0.750 (0.877)	78.12 (73.45)
[140/157]	0.0947 (0.0943)	0.0566 (0.0578)	0.732 (0.883)	78.12 (73.29)
[150/157]	0.0969 (0.0943)	0.0593 (0.0579)	0.741 (0.882)	75.00 (73.26)
[156/157]	0.0765 (0.0942)	0.0517 (0.0578)	1.542 (0.887)	37.50 (73.06)
 * Train Acc 73.060
 * Val Acc 74.200, Total time 0.57
 * Val loss 0.816, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0085 (0.0085)	0.479 (0.479)	87.50 (87.50)
[10/157]	0.0983 (0.0910)	0.0583 (0.0540)	0.887 (0.839)	68.75 (73.86)
[20/157]	0.0960 (0.0925)	0.0588 (0.0559)	0.804 (0.878)	71.88 (72.77)
[30/157]	0.0956 (0.0932)	0.0568 (0.0567)	0.676 (0.867)	84.38 (73.89)
[40/157]	0.0975 (0.0936)	0.0599 (0.0571)	0.887 (0.869)	71.88 (73.78)
[50/157]	0.0949 (0.0938)	0.0575 (0.0572)	0.870 (0.879)	71.88 (73.59)
[60/157]	0.0939 (0.0939)	0.0572 (0.0575)	0.897 (0.896)	71.88 (72.80)
[70/157]	0.0956 (0.0940)	0.0582 (0.0576)	0.712 (0.895)	78.12 (73.11)
[80/157]	0.0959 (0.0941)	0.0588 (0.0577)	0.848 (0.890)	71.88 (73.07)
[90/157]	0.0941 (0.0941)	0.0572 (0.0577)	1.030 (0.894)	78.12 (73.01)
[100/157]	0.0924 (0.0941)	0.0560 (0.0578)	0.947 (0.898)	68.75 (72.99)
[110/157]	0.0932 (0.0942)	0.0565 (0.0578)	0.913 (0.895)	71.88 (72.92)
[120/157]	0.0948 (0.0942)	0.0572 (0.0578)	0.903 (0.891)	68.75 (72.99)
[130/157]	0.0953 (0.0942)	0.0579 (0.0578)	1.065 (0.887)	71.88 (73.21)
[140/157]	0.0954 (0.0942)	0.0587 (0.0579)	1.285 (0.895)	50.00 (72.76)
[150/157]	0.0936 (0.0942)	0.0568 (0.0579)	0.932 (0.896)	71.88 (72.62)
[156/157]	0.0778 (0.0941)	0.0538 (0.0579)	1.062 (0.899)	50.00 (72.48)
 * Train Acc 72.480
 * Val Acc 74.300, Total time 0.57
 * Val loss 0.821, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0084 (0.0084)	0.859 (0.859)	75.00 (75.00)
[10/157]	0.0960 (0.0914)	0.0581 (0.0540)	1.046 (0.815)	59.38 (75.85)
[20/157]	0.0943 (0.0929)	0.0569 (0.0560)	0.994 (0.876)	68.75 (73.07)
[30/157]	0.0942 (0.0934)	0.0575 (0.0567)	0.654 (0.847)	81.25 (74.70)
[40/157]	0.0959 (0.0936)	0.0582 (0.0571)	0.932 (0.855)	75.00 (75.00)
[50/157]	0.0967 (0.0938)	0.0589 (0.0572)	0.956 (0.871)	68.75 (74.26)
[60/157]	0.0943 (0.0939)	0.0567 (0.0574)	0.696 (0.873)	78.12 (74.18)
[70/157]	0.0954 (0.0940)	0.0575 (0.0575)	0.755 (0.869)	78.12 (74.56)
[80/157]	0.0958 (0.0941)	0.0575 (0.0575)	0.902 (0.865)	75.00 (74.92)
[90/157]	0.0940 (0.0942)	0.0562 (0.0575)	1.414 (0.865)	56.25 (74.55)
[100/157]	0.0938 (0.0942)	0.0569 (0.0576)	0.742 (0.862)	81.25 (74.75)
[110/157]	0.0957 (0.0942)	0.0585 (0.0576)	0.894 (0.872)	65.62 (74.24)
[120/157]	0.0951 (0.0943)	0.0567 (0.0577)	0.638 (0.865)	84.38 (74.33)
[130/157]	0.0970 (0.0943)	0.0590 (0.0578)	1.045 (0.868)	65.62 (74.14)
[140/157]	0.0954 (0.0943)	0.0581 (0.0578)	0.766 (0.871)	75.00 (74.00)
[150/157]	0.0936 (0.0943)	0.0571 (0.0579)	0.609 (0.874)	87.50 (74.11)
[156/157]	0.0768 (0.0942)	0.0534 (0.0578)	1.509 (0.874)	50.00 (74.06)
 * Train Acc 74.060
 * Val Acc 73.000, Total time 0.57
 * Val loss 0.830, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0409 (0.0409)	0.0082 (0.0082)	0.910 (0.910)	78.12 (78.12)
[10/157]	0.0930 (0.0905)	0.0564 (0.0540)	0.832 (0.940)	71.88 (71.88)
[20/157]	0.0953 (0.0924)	0.0584 (0.0561)	1.000 (0.901)	78.12 (73.51)
[30/157]	0.0959 (0.0931)	0.0590 (0.0569)	0.679 (0.877)	87.50 (73.89)
[40/157]	0.0961 (0.0934)	0.0591 (0.0572)	0.977 (0.872)	65.62 (74.31)
[50/157]	0.0938 (0.0936)	0.0570 (0.0575)	1.100 (0.892)	68.75 (73.59)
[60/157]	0.0966 (0.0938)	0.0590 (0.0577)	0.597 (0.884)	87.50 (73.87)
[70/157]	0.0951 (0.0939)	0.0583 (0.0578)	0.676 (0.875)	78.12 (74.21)
[80/157]	0.0943 (0.0940)	0.0570 (0.0578)	1.174 (0.879)	59.38 (73.96)
[90/157]	0.0940 (0.0940)	0.0573 (0.0579)	1.095 (0.881)	59.38 (73.83)
[100/157]	0.0959 (0.0941)	0.0585 (0.0580)	1.437 (0.889)	68.75 (73.51)
[110/157]	0.0951 (0.0941)	0.0578 (0.0580)	0.741 (0.888)	81.25 (73.25)
[120/157]	0.0963 (0.0941)	0.0586 (0.0580)	0.846 (0.891)	71.88 (73.24)
[130/157]	0.0946 (0.0942)	0.0575 (0.0580)	1.094 (0.889)	62.50 (73.16)
[140/157]	0.0970 (0.0942)	0.0597 (0.0581)	0.985 (0.888)	71.88 (73.12)
[150/157]	0.0944 (0.0942)	0.0574 (0.0581)	1.357 (0.885)	53.12 (73.16)
[156/157]	0.0772 (0.0941)	0.0535 (0.0581)	1.340 (0.884)	62.50 (73.16)
 * Train Acc 73.160
 * Val Acc 74.600, Total time 0.57
 * Val loss 0.806, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0090 (0.0090)	0.971 (0.971)	68.75 (68.75)
[10/157]	0.0953 (0.0914)	0.0581 (0.0544)	0.899 (0.930)	75.00 (74.43)
[20/157]	0.0950 (0.0929)	0.0580 (0.0561)	1.219 (0.932)	59.38 (72.32)
[30/157]	0.0952 (0.0935)	0.0581 (0.0569)	1.115 (0.934)	71.88 (72.18)
[40/157]	0.0958 (0.0938)	0.0585 (0.0571)	1.146 (0.928)	53.12 (71.72)
[50/157]	0.0936 (0.0939)	0.0566 (0.0573)	0.686 (0.924)	84.38 (71.88)
[60/157]	0.0964 (0.0941)	0.0583 (0.0575)	1.149 (0.910)	59.38 (72.54)
[70/157]	0.0966 (0.0942)	0.0596 (0.0576)	0.513 (0.902)	87.50 (72.84)
[80/157]	0.0932 (0.0943)	0.0560 (0.0577)	0.770 (0.892)	78.12 (73.19)
[90/157]	0.0952 (0.0943)	0.0584 (0.0578)	0.771 (0.891)	75.00 (73.39)
[100/157]	0.0960 (0.0943)	0.0590 (0.0578)	0.875 (0.893)	71.88 (73.36)
[110/157]	0.0934 (0.0944)	0.0569 (0.0579)	0.970 (0.884)	68.75 (73.85)
[120/157]	0.0957 (0.0944)	0.0581 (0.0579)	0.754 (0.882)	81.25 (73.84)
[130/157]	0.0937 (0.0944)	0.0567 (0.0580)	0.807 (0.893)	71.88 (73.45)
[140/157]	0.0966 (0.0944)	0.0590 (0.0580)	0.774 (0.892)	78.12 (73.49)
[150/157]	0.0959 (0.0945)	0.0588 (0.0580)	0.826 (0.887)	75.00 (73.68)
[156/157]	0.0778 (0.0944)	0.0539 (0.0580)	0.558 (0.887)	87.50 (73.68)
 * Train Acc 73.680
 * Val Acc 74.600, Total time 0.57
 * Val loss 0.811, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0410 (0.0410)	0.0082 (0.0082)	1.050 (1.050)	68.75 (68.75)
[10/157]	0.0957 (0.0908)	0.0580 (0.0545)	1.089 (0.929)	62.50 (73.30)
[20/157]	0.0925 (0.0925)	0.0554 (0.0561)	0.856 (0.922)	71.88 (72.77)
[30/157]	0.0947 (0.0932)	0.0584 (0.0569)	0.749 (0.893)	81.25 (73.39)
[40/157]	0.0955 (0.0935)	0.0580 (0.0573)	0.897 (0.901)	71.88 (72.79)
[50/157]	0.0968 (0.0937)	0.0589 (0.0575)	0.703 (0.907)	84.38 (72.37)
[60/157]	0.0951 (0.0938)	0.0580 (0.0576)	1.020 (0.885)	62.50 (73.10)
[70/157]	0.0949 (0.0940)	0.0569 (0.0578)	1.020 (0.880)	65.62 (73.15)
[80/157]	0.0973 (0.0941)	0.0591 (0.0579)	1.001 (0.885)	75.00 (73.11)
[90/157]	0.0966 (0.0941)	0.0597 (0.0579)	0.979 (0.880)	71.88 (73.18)
[100/157]	0.0958 (0.0941)	0.0578 (0.0580)	1.431 (0.884)	56.25 (72.80)
[110/157]	0.0953 (0.0942)	0.0571 (0.0580)	0.847 (0.874)	65.62 (73.09)
[120/157]	0.0934 (0.0942)	0.0564 (0.0580)	0.783 (0.872)	65.62 (73.22)
[130/157]	0.0958 (0.0943)	0.0585 (0.0580)	0.725 (0.872)	90.62 (73.28)
[140/157]	0.0947 (0.0943)	0.0573 (0.0580)	0.663 (0.878)	84.38 (73.29)
[150/157]	0.0945 (0.0944)	0.0576 (0.0580)	0.682 (0.880)	81.25 (73.16)
[156/157]	0.0792 (0.0942)	0.0546 (0.0580)	0.875 (0.881)	62.50 (72.98)
 * Train Acc 72.980
 * Val Acc 74.200, Total time 0.57
 * Val loss 0.811, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0086 (0.0086)	1.103 (1.103)	56.25 (56.25)
[10/157]	0.0939 (0.0911)	0.0571 (0.0535)	0.887 (0.885)	81.25 (73.58)
[20/157]	0.0929 (0.0928)	0.0561 (0.0556)	0.909 (0.842)	71.88 (73.66)
[30/157]	0.0934 (0.0934)	0.0573 (0.0566)	0.863 (0.864)	65.62 (72.88)
[40/157]	0.0967 (0.0936)	0.0593 (0.0570)	0.983 (0.854)	65.62 (73.86)
[50/157]	0.0963 (0.0938)	0.0591 (0.0572)	0.817 (0.862)	71.88 (73.90)
[60/157]	0.0944 (0.0939)	0.0571 (0.0574)	0.928 (0.885)	68.75 (73.05)
[70/157]	0.0970 (0.0941)	0.0582 (0.0575)	1.044 (0.889)	68.75 (72.62)
[80/157]	0.0961 (0.0941)	0.0582 (0.0575)	0.920 (0.898)	87.50 (72.42)
[90/157]	0.0948 (0.0942)	0.0567 (0.0576)	1.360 (0.895)	56.25 (72.53)
[100/157]	0.0959 (0.0942)	0.0579 (0.0576)	0.918 (0.889)	71.88 (72.52)
[110/157]	0.0953 (0.0943)	0.0579 (0.0576)	0.822 (0.888)	75.00 (72.55)
[120/157]	0.0945 (0.0943)	0.0540 (0.0577)	0.817 (0.884)	84.38 (72.93)
[130/157]	0.0965 (0.0943)	0.0591 (0.0577)	0.977 (0.887)	62.50 (72.78)
[140/157]	0.0942 (0.0943)	0.0572 (0.0577)	0.634 (0.886)	87.50 (72.98)
[150/157]	0.0955 (0.0944)	0.0578 (0.0577)	1.010 (0.886)	68.75 (72.97)
[156/157]	0.0773 (0.0942)	0.0524 (0.0577)	1.303 (0.889)	62.50 (72.78)
 * Train Acc 72.780
 * Val Acc 74.900, Total time 0.57
 * Val loss 0.798, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0085 (0.0085)	1.008 (1.008)	65.62 (65.62)
[10/157]	0.0929 (0.0910)	0.0553 (0.0537)	0.974 (0.919)	71.88 (69.03)
[20/157]	0.0938 (0.0927)	0.0571 (0.0557)	0.802 (0.880)	78.12 (71.88)
[30/157]	0.0974 (0.0934)	0.0596 (0.0564)	0.682 (0.875)	84.38 (72.08)
[40/157]	0.0962 (0.0936)	0.0588 (0.0567)	0.803 (0.868)	71.88 (72.64)
[50/157]	0.0952 (0.0939)	0.0574 (0.0569)	0.707 (0.873)	71.88 (72.86)
[60/157]	0.0960 (0.0940)	0.0578 (0.0571)	1.010 (0.873)	68.75 (73.21)
[70/157]	0.0954 (0.0940)	0.0581 (0.0572)	0.930 (0.876)	68.75 (73.02)
[80/157]	0.0948 (0.0941)	0.0570 (0.0572)	0.806 (0.871)	75.00 (73.07)
[90/157]	0.0960 (0.0942)	0.0585 (0.0573)	0.835 (0.876)	78.12 (72.80)
[100/157]	0.0959 (0.0942)	0.0579 (0.0574)	1.442 (0.883)	50.00 (72.71)
[110/157]	0.0938 (0.0943)	0.0573 (0.0575)	0.755 (0.882)	81.25 (72.92)
[120/157]	0.0979 (0.0943)	0.0598 (0.0576)	0.958 (0.881)	81.25 (73.24)
[130/157]	0.0952 (0.0943)	0.0574 (0.0576)	0.994 (0.881)	78.12 (73.21)
[140/157]	0.0943 (0.0943)	0.0574 (0.0576)	1.037 (0.879)	62.50 (73.34)
[150/157]	0.0956 (0.0944)	0.0580 (0.0576)	0.717 (0.884)	84.38 (73.24)
[156/157]	0.0781 (0.0942)	0.0539 (0.0576)	1.190 (0.885)	50.00 (73.12)
 * Train Acc 73.120
 * Val Acc 73.700, Total time 0.56
 * Val loss 0.810, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0087 (0.0087)	0.694 (0.694)	84.38 (84.38)
[10/157]	0.0954 (0.0909)	0.0580 (0.0544)	0.953 (0.906)	65.62 (72.73)
[20/157]	0.0940 (0.0926)	0.0569 (0.0562)	0.825 (0.874)	75.00 (75.15)
[30/157]	0.0932 (0.0933)	0.0564 (0.0566)	1.102 (0.866)	65.62 (75.10)
[40/157]	0.0943 (0.0935)	0.0579 (0.0570)	1.091 (0.870)	62.50 (74.70)
[50/157]	0.0953 (0.0937)	0.0583 (0.0573)	1.019 (0.881)	75.00 (74.20)
[60/157]	0.0958 (0.0938)	0.0594 (0.0575)	1.053 (0.866)	68.75 (74.44)
[70/157]	0.0939 (0.0938)	0.0572 (0.0576)	0.749 (0.864)	75.00 (74.38)
[80/157]	0.0930 (0.0939)	0.0567 (0.0576)	1.186 (0.864)	68.75 (74.23)
[90/157]	0.0961 (0.0941)	0.0581 (0.0577)	1.062 (0.869)	68.75 (73.80)
[100/157]	0.0939 (0.0941)	0.0578 (0.0577)	0.777 (0.874)	78.12 (73.70)
[110/157]	0.0945 (0.0941)	0.0581 (0.0578)	0.858 (0.876)	71.88 (73.73)
[120/157]	0.0953 (0.0941)	0.0585 (0.0578)	0.947 (0.879)	68.75 (73.66)
[130/157]	0.0947 (0.0941)	0.0577 (0.0578)	0.777 (0.877)	71.88 (73.57)
[140/157]	0.0948 (0.0941)	0.0581 (0.0579)	0.807 (0.877)	84.38 (73.65)
[150/157]	0.0956 (0.0942)	0.0576 (0.0579)	0.675 (0.874)	84.38 (73.86)
[156/157]	0.0776 (0.0941)	0.0538 (0.0579)	1.384 (0.879)	62.50 (73.66)
 * Train Acc 73.660
 * Val Acc 74.800, Total time 0.57
 * Val loss 0.802, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0081 (0.0081)	0.613 (0.613)	87.50 (87.50)
[10/157]	0.0920 (0.0902)	0.0556 (0.0542)	0.909 (0.851)	71.88 (74.43)
[20/157]	0.0940 (0.0924)	0.0574 (0.0562)	0.890 (0.882)	75.00 (74.70)
[30/157]	0.0962 (0.0931)	0.0589 (0.0570)	0.591 (0.840)	84.38 (75.60)
[40/157]	0.0954 (0.0934)	0.0579 (0.0574)	0.814 (0.865)	68.75 (73.93)
[50/157]	0.0957 (0.0936)	0.0575 (0.0575)	0.802 (0.865)	78.12 (74.08)
[60/157]	0.0961 (0.0938)	0.0587 (0.0577)	1.057 (0.883)	59.38 (73.36)
[70/157]	0.0964 (0.0939)	0.0587 (0.0577)	0.697 (0.876)	87.50 (73.55)
[80/157]	0.0939 (0.0940)	0.0571 (0.0579)	0.934 (0.869)	75.00 (73.84)
[90/157]	0.0969 (0.0941)	0.0582 (0.0579)	0.945 (0.869)	75.00 (73.76)
[100/157]	0.0958 (0.0941)	0.0581 (0.0579)	0.906 (0.864)	68.75 (73.89)
[110/157]	0.0938 (0.0942)	0.0567 (0.0579)	0.803 (0.862)	75.00 (73.87)
[120/157]	0.0948 (0.0942)	0.0575 (0.0579)	1.134 (0.866)	59.38 (73.73)
[130/157]	0.0951 (0.0942)	0.0587 (0.0580)	1.064 (0.868)	65.62 (73.81)
[140/157]	0.0985 (0.0942)	0.0589 (0.0580)	1.023 (0.868)	68.75 (73.80)
[150/157]	0.0972 (0.0943)	0.0565 (0.0580)	0.891 (0.872)	75.00 (73.59)
[156/157]	0.0773 (0.0941)	0.0539 (0.0580)	0.523 (0.870)	87.50 (73.60)
 * Train Acc 73.600
 * Val Acc 74.700, Total time 0.57
 * Val loss 0.807, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0082 (0.0082)	0.793 (0.793)	81.25 (81.25)
[10/157]	0.0931 (0.0903)	0.0564 (0.0539)	0.670 (0.847)	81.25 (75.57)
[20/157]	0.0947 (0.0925)	0.0581 (0.0560)	0.705 (0.790)	81.25 (75.30)
[30/157]	0.0954 (0.0932)	0.0580 (0.0567)	0.760 (0.820)	75.00 (74.29)
[40/157]	0.0961 (0.0934)	0.0581 (0.0570)	0.531 (0.843)	84.38 (73.63)
[50/157]	0.0950 (0.0936)	0.0573 (0.0572)	0.680 (0.854)	81.25 (73.59)
[60/157]	0.0956 (0.0938)	0.0579 (0.0573)	0.943 (0.863)	75.00 (73.51)
[70/157]	0.0962 (0.0939)	0.0584 (0.0574)	1.005 (0.874)	65.62 (73.46)
[80/157]	0.0956 (0.0939)	0.0587 (0.0575)	0.850 (0.883)	65.62 (72.88)
[90/157]	0.0948 (0.0940)	0.0579 (0.0576)	0.933 (0.885)	65.62 (72.73)
[100/157]	0.0941 (0.0940)	0.0568 (0.0577)	0.949 (0.884)	78.12 (72.68)
[110/157]	0.0960 (0.0941)	0.0584 (0.0577)	0.890 (0.889)	78.12 (72.92)
[120/157]	0.0955 (0.0941)	0.0578 (0.0577)	0.645 (0.884)	75.00 (72.93)
[130/157]	0.0939 (0.0941)	0.0567 (0.0577)	0.751 (0.876)	78.12 (73.26)
[140/157]	0.0931 (0.0942)	0.0562 (0.0578)	0.880 (0.878)	78.12 (73.12)
[150/157]	0.0941 (0.0942)	0.0574 (0.0578)	0.878 (0.877)	75.00 (73.22)
[156/157]	0.0762 (0.0941)	0.0522 (0.0578)	1.234 (0.876)	50.00 (73.26)
 * Train Acc 73.260
 * Val Acc 74.700, Total time 0.56
 * Val loss 0.804, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0410 (0.0410)	0.0081 (0.0081)	0.710 (0.710)	87.50 (87.50)
[10/157]	0.0933 (0.0908)	0.0563 (0.0539)	0.657 (0.949)	87.50 (71.59)
[20/157]	0.0942 (0.0924)	0.0577 (0.0560)	0.701 (0.885)	78.12 (73.51)
[30/157]	0.0946 (0.0931)	0.0571 (0.0568)	1.366 (0.916)	53.12 (72.18)
[40/157]	0.0939 (0.0934)	0.0567 (0.0571)	1.077 (0.918)	75.00 (72.48)
[50/157]	0.0962 (0.0936)	0.0588 (0.0574)	0.959 (0.913)	71.88 (73.04)
[60/157]	0.0933 (0.0937)	0.0563 (0.0575)	0.825 (0.908)	81.25 (73.57)
[70/157]	0.0927 (0.0938)	0.0566 (0.0577)	0.843 (0.900)	75.00 (73.94)
[80/157]	0.0939 (0.0939)	0.0569 (0.0577)	0.996 (0.901)	59.38 (73.84)
[90/157]	0.0951 (0.0939)	0.0582 (0.0578)	0.759 (0.894)	78.12 (73.94)
[100/157]	0.0946 (0.0939)	0.0578 (0.0578)	0.771 (0.888)	78.12 (74.16)
[110/157]	0.0958 (0.0939)	0.0586 (0.0579)	0.775 (0.884)	75.00 (74.47)
[120/157]	0.0956 (0.0940)	0.0591 (0.0579)	0.905 (0.882)	68.75 (74.28)
[130/157]	0.0941 (0.0940)	0.0570 (0.0579)	0.674 (0.877)	78.12 (74.38)
[140/157]	0.0949 (0.0941)	0.0559 (0.0579)	0.890 (0.878)	75.00 (74.16)
[150/157]	0.0941 (0.0941)	0.0569 (0.0579)	0.761 (0.870)	81.25 (74.52)
[156/157]	0.0780 (0.0939)	0.0542 (0.0579)	0.685 (0.870)	62.50 (74.48)
 * Train Acc 74.480
 * Val Acc 75.000, Total time 0.57
 * Val loss 0.805, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0086 (0.0086)	0.815 (0.815)	81.25 (81.25)
[10/157]	0.0936 (0.0903)	0.0575 (0.0542)	0.778 (0.876)	75.00 (75.57)
[20/157]	0.0931 (0.0921)	0.0566 (0.0559)	0.703 (0.883)	81.25 (74.26)
[30/157]	0.0957 (0.0929)	0.0582 (0.0567)	0.882 (0.872)	71.88 (75.10)
[40/157]	0.0962 (0.0933)	0.0583 (0.0570)	0.968 (0.884)	68.75 (74.09)
[50/157]	0.0952 (0.0934)	0.0578 (0.0572)	0.818 (0.889)	78.12 (73.41)
[60/157]	0.0936 (0.0935)	0.0571 (0.0574)	0.845 (0.878)	62.50 (73.31)
[70/157]	0.0938 (0.0937)	0.0571 (0.0575)	0.845 (0.879)	75.00 (73.11)
[80/157]	0.0938 (0.0937)	0.0566 (0.0576)	0.694 (0.865)	78.12 (73.57)
[90/157]	0.0960 (0.0938)	0.0588 (0.0577)	0.702 (0.862)	81.25 (73.70)
[100/157]	0.0946 (0.0939)	0.0574 (0.0577)	0.859 (0.864)	71.88 (74.01)
[110/157]	0.0961 (0.0940)	0.0580 (0.0577)	0.638 (0.864)	84.38 (73.70)
[120/157]	0.0965 (0.0940)	0.0589 (0.0578)	0.612 (0.865)	84.38 (73.76)
[130/157]	0.0946 (0.0941)	0.0575 (0.0578)	0.695 (0.865)	84.38 (73.81)
[140/157]	0.0935 (0.0941)	0.0568 (0.0578)	1.237 (0.868)	59.38 (73.80)
[150/157]	0.0946 (0.0941)	0.0577 (0.0579)	0.836 (0.863)	71.88 (73.97)
[156/157]	0.0774 (0.0940)	0.0540 (0.0578)	0.956 (0.869)	62.50 (73.72)
 * Train Acc 73.720
 * Val Acc 75.000, Total time 0.57
 * Val loss 0.801, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0086 (0.0086)	0.909 (0.909)	78.12 (78.12)
[10/157]	0.0950 (0.0906)	0.0581 (0.0544)	1.067 (0.886)	65.62 (75.57)
[20/157]	0.0954 (0.0923)	0.0585 (0.0562)	0.791 (0.854)	75.00 (75.60)
[30/157]	0.0954 (0.0930)	0.0584 (0.0569)	0.627 (0.827)	78.12 (75.91)
[40/157]	0.0976 (0.0933)	0.0588 (0.0573)	0.889 (0.831)	68.75 (75.76)
[50/157]	0.0954 (0.0934)	0.0587 (0.0574)	0.934 (0.853)	75.00 (75.06)
[60/157]	0.0957 (0.0936)	0.0592 (0.0576)	0.871 (0.867)	78.12 (74.54)
[70/157]	0.0936 (0.0937)	0.0568 (0.0577)	0.814 (0.873)	81.25 (74.08)
[80/157]	0.0939 (0.0937)	0.0575 (0.0577)	1.000 (0.868)	62.50 (73.73)
[90/157]	0.0946 (0.0938)	0.0579 (0.0578)	0.797 (0.861)	78.12 (74.14)
[100/157]	0.0963 (0.0938)	0.0590 (0.0578)	0.769 (0.855)	78.12 (74.10)
[110/157]	0.0950 (0.0939)	0.0586 (0.0579)	0.792 (0.852)	78.12 (74.21)
[120/157]	0.0937 (0.0939)	0.0573 (0.0579)	1.107 (0.857)	62.50 (73.99)
[130/157]	0.0950 (0.0939)	0.0574 (0.0579)	1.190 (0.863)	62.50 (73.78)
[140/157]	0.0943 (0.0940)	0.0572 (0.0580)	0.730 (0.866)	78.12 (73.76)
[150/157]	0.0953 (0.0940)	0.0584 (0.0580)	0.756 (0.872)	75.00 (73.55)
[156/157]	0.0768 (0.0939)	0.0530 (0.0579)	0.945 (0.870)	75.00 (73.70)
 * Train Acc 73.700
 * Val Acc 74.800, Total time 0.56
 * Val loss 0.805, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0405 (0.0405)	0.0080 (0.0080)	0.936 (0.936)	71.88 (71.88)
[10/157]	0.0942 (0.0903)	0.0578 (0.0543)	1.135 (0.869)	59.38 (74.43)
[20/157]	0.0954 (0.0922)	0.0584 (0.0562)	0.971 (0.864)	68.75 (73.07)
[30/157]	0.0938 (0.0929)	0.0572 (0.0568)	1.095 (0.865)	65.62 (73.08)
[40/157]	0.0954 (0.0932)	0.0579 (0.0570)	0.816 (0.859)	75.00 (73.48)
[50/157]	0.0947 (0.0934)	0.0573 (0.0572)	0.909 (0.862)	68.75 (73.35)
[60/157]	0.0937 (0.0936)	0.0568 (0.0573)	0.788 (0.857)	75.00 (73.72)
[70/157]	0.0955 (0.0937)	0.0576 (0.0574)	0.633 (0.846)	78.12 (74.43)
[80/157]	0.0956 (0.0938)	0.0576 (0.0575)	0.857 (0.845)	81.25 (74.46)
[90/157]	0.0947 (0.0939)	0.0574 (0.0576)	0.762 (0.845)	81.25 (74.52)
[100/157]	0.0941 (0.0939)	0.0574 (0.0577)	1.012 (0.852)	68.75 (74.38)
[110/157]	0.0948 (0.0940)	0.0582 (0.0577)	0.621 (0.855)	81.25 (74.21)
[120/157]	0.0946 (0.0940)	0.0579 (0.0578)	0.979 (0.858)	68.75 (73.86)
[130/157]	0.0958 (0.0940)	0.0589 (0.0578)	0.666 (0.860)	84.38 (73.95)
[140/157]	0.0950 (0.0940)	0.0589 (0.0578)	1.071 (0.862)	65.62 (73.80)
[150/157]	0.0936 (0.0940)	0.0567 (0.0579)	0.784 (0.860)	75.00 (73.80)
[156/157]	0.0769 (0.0939)	0.0524 (0.0578)	0.911 (0.865)	62.50 (73.58)
 * Train Acc 73.580
 * Val Acc 75.100, Total time 0.55
 * Val loss 0.795, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0404 (0.0404)	0.0080 (0.0080)	0.756 (0.756)	71.88 (71.88)
[10/157]	0.0938 (0.0900)	0.0575 (0.0541)	0.997 (0.893)	71.88 (73.30)
[20/157]	0.0954 (0.0921)	0.0584 (0.0561)	0.800 (0.840)	75.00 (75.30)
[30/157]	0.0949 (0.0928)	0.0585 (0.0568)	0.948 (0.847)	71.88 (75.30)
[40/157]	0.0934 (0.0933)	0.0561 (0.0572)	0.822 (0.830)	71.88 (75.69)
[50/157]	0.0950 (0.0935)	0.0576 (0.0574)	0.829 (0.849)	68.75 (74.82)
[60/157]	0.0974 (0.0937)	0.0592 (0.0576)	0.909 (0.847)	78.12 (74.90)
[70/157]	0.0957 (0.0938)	0.0584 (0.0576)	0.868 (0.846)	75.00 (74.91)
[80/157]	0.0938 (0.0938)	0.0562 (0.0576)	0.815 (0.843)	68.75 (75.04)
[90/157]	0.0955 (0.0940)	0.0582 (0.0577)	0.740 (0.843)	84.38 (75.21)
[100/157]	0.0963 (0.0940)	0.0590 (0.0577)	0.868 (0.843)	78.12 (75.09)
[110/157]	0.0953 (0.0940)	0.0587 (0.0577)	0.883 (0.851)	71.88 (74.77)
[120/157]	0.0951 (0.0941)	0.0583 (0.0578)	0.805 (0.852)	78.12 (74.87)
[130/157]	0.0948 (0.0941)	0.0574 (0.0578)	1.157 (0.859)	62.50 (74.52)
[140/157]	0.0952 (0.0941)	0.0584 (0.0578)	0.745 (0.861)	75.00 (74.60)
[150/157]	0.0958 (0.0941)	0.0583 (0.0578)	1.010 (0.859)	62.50 (74.59)
[156/157]	0.0761 (0.0940)	0.0523 (0.0578)	1.894 (0.858)	37.50 (74.54)
 * Train Acc 74.540
 * Val Acc 75.500, Total time 0.55
 * Val loss 0.789, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0081 (0.0081)	1.178 (1.178)	65.62 (65.62)
[10/157]	0.0933 (0.0904)	0.0568 (0.0543)	0.799 (0.804)	75.00 (78.41)
[20/157]	0.0956 (0.0923)	0.0585 (0.0564)	0.621 (0.798)	81.25 (77.68)
[30/157]	0.0981 (0.0930)	0.0599 (0.0569)	0.933 (0.831)	75.00 (75.60)
[40/157]	0.0964 (0.0934)	0.0584 (0.0572)	0.626 (0.847)	81.25 (75.38)
[50/157]	0.0962 (0.0936)	0.0579 (0.0575)	0.917 (0.859)	65.62 (74.45)
[60/157]	0.0960 (0.0938)	0.0586 (0.0576)	0.707 (0.866)	78.12 (73.41)
[70/157]	0.0941 (0.0939)	0.0568 (0.0576)	0.926 (0.865)	68.75 (73.37)
[80/157]	0.0944 (0.0940)	0.0571 (0.0577)	0.836 (0.855)	71.88 (73.80)
[90/157]	0.0967 (0.0941)	0.0589 (0.0577)	0.603 (0.859)	90.62 (73.70)
[100/157]	0.0941 (0.0941)	0.0569 (0.0578)	0.951 (0.864)	71.88 (73.27)
[110/157]	0.0933 (0.0941)	0.0558 (0.0578)	0.630 (0.865)	84.38 (73.34)
[120/157]	0.0952 (0.0942)	0.0579 (0.0579)	0.994 (0.876)	71.88 (73.04)
[130/157]	0.0947 (0.0942)	0.0569 (0.0578)	0.683 (0.867)	78.12 (73.28)
[140/157]	0.0938 (0.0943)	0.0565 (0.0578)	0.952 (0.867)	62.50 (73.16)
[150/157]	0.0954 (0.0943)	0.0576 (0.0579)	0.923 (0.868)	78.12 (73.37)
[156/157]	0.0767 (0.0941)	0.0524 (0.0578)	1.160 (0.869)	25.00 (73.28)
 * Train Acc 73.280
 * Val Acc 74.700, Total time 0.56
 * Val loss 0.795, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0415 (0.0415)	0.0087 (0.0087)	0.576 (0.576)	90.62 (90.62)
[10/157]	0.0958 (0.0910)	0.0581 (0.0543)	0.694 (0.771)	78.12 (78.12)
[20/157]	0.0948 (0.0926)	0.0569 (0.0563)	0.874 (0.808)	71.88 (76.64)
[30/157]	0.0940 (0.0933)	0.0571 (0.0569)	0.655 (0.821)	84.38 (75.50)
[40/157]	0.0959 (0.0936)	0.0572 (0.0571)	1.132 (0.844)	56.25 (74.39)
[50/157]	0.0957 (0.0937)	0.0581 (0.0572)	1.142 (0.866)	65.62 (73.53)
[60/157]	0.0941 (0.0938)	0.0568 (0.0574)	1.127 (0.865)	65.62 (73.77)
[70/157]	0.0944 (0.0939)	0.0568 (0.0575)	0.664 (0.858)	84.38 (74.16)
[80/157]	0.0934 (0.0939)	0.0567 (0.0576)	1.063 (0.861)	59.38 (73.88)
[90/157]	0.0949 (0.0940)	0.0580 (0.0576)	0.599 (0.863)	87.50 (73.80)
[100/157]	0.0947 (0.0940)	0.0579 (0.0577)	0.909 (0.862)	78.12 (73.89)
[110/157]	0.0969 (0.0941)	0.0595 (0.0578)	0.927 (0.855)	71.88 (74.18)
[120/157]	0.0949 (0.0941)	0.0572 (0.0578)	0.612 (0.850)	81.25 (74.51)
[130/157]	0.0959 (0.0941)	0.0585 (0.0579)	0.914 (0.849)	71.88 (74.59)
[140/157]	0.0951 (0.0942)	0.0582 (0.0579)	0.796 (0.849)	75.00 (74.45)
[150/157]	0.0940 (0.0942)	0.0570 (0.0579)	0.848 (0.847)	75.00 (74.63)
[156/157]	0.0787 (0.0941)	0.0548 (0.0579)	1.012 (0.845)	62.50 (74.64)
 * Train Acc 74.640
 * Val Acc 74.900, Total time 0.56
 * Val loss 0.793, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0410 (0.0410)	0.0085 (0.0085)	0.898 (0.898)	65.62 (65.62)
[10/157]	0.0904 (0.0902)	0.0548 (0.0542)	0.917 (0.877)	71.88 (72.16)
[20/157]	0.0943 (0.0924)	0.0577 (0.0562)	0.968 (0.871)	71.88 (73.21)
[30/157]	0.0959 (0.0930)	0.0586 (0.0569)	1.060 (0.844)	71.88 (75.30)
[40/157]	0.0948 (0.0933)	0.0581 (0.0572)	0.582 (0.841)	90.62 (75.23)
[50/157]	0.0956 (0.0936)	0.0585 (0.0575)	0.812 (0.858)	75.00 (74.02)
[60/157]	0.0954 (0.0936)	0.0584 (0.0576)	0.983 (0.858)	71.88 (74.23)
[70/157]	0.0953 (0.0937)	0.0577 (0.0577)	1.164 (0.866)	62.50 (74.47)
[80/157]	0.0950 (0.0938)	0.0580 (0.0577)	0.956 (0.871)	75.00 (74.19)
[90/157]	0.0955 (0.0939)	0.0588 (0.0578)	0.876 (0.875)	65.62 (73.94)
[100/157]	0.0953 (0.0939)	0.0587 (0.0578)	0.781 (0.871)	65.62 (73.95)
[110/157]	0.0929 (0.0939)	0.0571 (0.0579)	0.883 (0.876)	68.75 (73.87)
[120/157]	0.0933 (0.0940)	0.0567 (0.0579)	0.638 (0.868)	81.25 (74.25)
[130/157]	0.0948 (0.0940)	0.0578 (0.0580)	0.845 (0.867)	75.00 (74.14)
[140/157]	0.0948 (0.0941)	0.0579 (0.0580)	0.628 (0.867)	87.50 (74.22)
[150/157]	0.0942 (0.0941)	0.0576 (0.0581)	0.881 (0.867)	71.88 (74.17)
[156/157]	0.0778 (0.0940)	0.0535 (0.0580)	0.921 (0.868)	75.00 (74.08)
 * Train Acc 74.080
 * Val Acc 74.600, Total time 0.57
 * Val loss 0.804, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0089 (0.0089)	0.664 (0.664)	87.50 (87.50)
[10/157]	0.0940 (0.0913)	0.0564 (0.0543)	1.092 (0.858)	62.50 (74.15)
[20/157]	0.0928 (0.0928)	0.0566 (0.0561)	1.117 (0.886)	59.38 (72.77)
[30/157]	0.0941 (0.0933)	0.0575 (0.0568)	1.088 (0.902)	65.62 (72.08)
[40/157]	0.0957 (0.0936)	0.0585 (0.0571)	0.690 (0.894)	87.50 (72.26)
[50/157]	0.0954 (0.0937)	0.0589 (0.0573)	0.935 (0.901)	71.88 (72.12)
[60/157]	0.0943 (0.0938)	0.0565 (0.0574)	1.021 (0.906)	68.75 (71.88)
[70/157]	0.0934 (0.0939)	0.0562 (0.0575)	0.943 (0.886)	65.62 (72.71)
[80/157]	0.0963 (0.0940)	0.0590 (0.0576)	0.663 (0.884)	81.25 (72.65)
[90/157]	0.0949 (0.0940)	0.0582 (0.0577)	0.899 (0.877)	68.75 (72.73)
[100/157]	0.0945 (0.0940)	0.0578 (0.0578)	0.593 (0.869)	90.62 (73.33)
[110/157]	0.0945 (0.0941)	0.0570 (0.0578)	1.204 (0.874)	65.62 (73.48)
[120/157]	0.0934 (0.0941)	0.0568 (0.0579)	0.728 (0.877)	81.25 (73.35)
[130/157]	0.0960 (0.0942)	0.0587 (0.0579)	1.052 (0.874)	65.62 (73.62)
[140/157]	0.0967 (0.0942)	0.0588 (0.0579)	0.587 (0.870)	84.38 (73.54)
[150/157]	0.0942 (0.0942)	0.0571 (0.0580)	0.970 (0.867)	59.38 (73.45)
[156/157]	0.0777 (0.0941)	0.0530 (0.0580)	0.831 (0.867)	50.00 (73.52)
 * Train Acc 73.520
 * Val Acc 74.900, Total time 0.57
 * Val loss 0.797, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0085 (0.0085)	1.041 (1.041)	59.38 (59.38)
[10/157]	0.0951 (0.0911)	0.0580 (0.0539)	0.842 (0.885)	71.88 (72.73)
[20/157]	0.0952 (0.0926)	0.0577 (0.0562)	0.910 (0.879)	65.62 (72.32)
[30/157]	0.0945 (0.0931)	0.0574 (0.0569)	0.673 (0.847)	81.25 (73.29)
[40/157]	0.0946 (0.0934)	0.0578 (0.0571)	1.106 (0.843)	59.38 (73.93)
[50/157]	0.0941 (0.0935)	0.0565 (0.0573)	0.676 (0.851)	78.12 (74.14)
[60/157]	0.0949 (0.0937)	0.0578 (0.0575)	1.131 (0.853)	68.75 (74.23)
[70/157]	0.0966 (0.0938)	0.0578 (0.0576)	0.877 (0.850)	71.88 (74.56)
[80/157]	0.0957 (0.0938)	0.0590 (0.0576)	1.049 (0.865)	68.75 (73.88)
[90/157]	0.0946 (0.0939)	0.0577 (0.0577)	0.556 (0.858)	87.50 (74.14)
[100/157]	0.0943 (0.0940)	0.0575 (0.0577)	0.817 (0.860)	81.25 (74.26)
[110/157]	0.0953 (0.0940)	0.0581 (0.0578)	0.869 (0.860)	65.62 (74.27)
[120/157]	0.0961 (0.0940)	0.0584 (0.0578)	1.174 (0.857)	65.62 (74.41)
[130/157]	0.0952 (0.0940)	0.0580 (0.0578)	0.995 (0.856)	68.75 (74.40)
[140/157]	0.0956 (0.0940)	0.0585 (0.0579)	0.692 (0.858)	78.12 (74.45)
[150/157]	0.0946 (0.0941)	0.0578 (0.0579)	0.735 (0.855)	75.00 (74.67)
[156/157]	0.0774 (0.0940)	0.0531 (0.0579)	0.614 (0.853)	87.50 (74.74)
 * Train Acc 74.740
 * Val Acc 75.000, Total time 0.57
 * Val loss 0.798, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0413 (0.0413)	0.0081 (0.0081)	0.587 (0.587)	81.25 (81.25)
[10/157]	0.0929 (0.0903)	0.0573 (0.0546)	0.756 (0.791)	75.00 (73.58)
[20/157]	0.0954 (0.0925)	0.0583 (0.0565)	0.860 (0.810)	71.88 (74.11)
[30/157]	0.0957 (0.0931)	0.0581 (0.0569)	0.843 (0.808)	75.00 (74.80)
[40/157]	0.0950 (0.0934)	0.0577 (0.0572)	0.803 (0.825)	84.38 (74.31)
[50/157]	0.0935 (0.0936)	0.0567 (0.0574)	1.030 (0.833)	75.00 (74.45)
[60/157]	0.0940 (0.0938)	0.0563 (0.0575)	0.795 (0.848)	78.12 (73.77)
[70/157]	0.0976 (0.0939)	0.0591 (0.0576)	0.695 (0.837)	75.00 (74.38)
[80/157]	0.0949 (0.0940)	0.0574 (0.0577)	0.979 (0.833)	68.75 (74.81)
[90/157]	0.0962 (0.0941)	0.0588 (0.0578)	0.626 (0.834)	81.25 (74.79)
[100/157]	0.0947 (0.0942)	0.0579 (0.0579)	1.310 (0.838)	56.25 (74.81)
[110/157]	0.0956 (0.0942)	0.0584 (0.0580)	0.961 (0.842)	71.88 (74.58)
[120/157]	0.0956 (0.0942)	0.0588 (0.0580)	0.874 (0.843)	71.88 (74.41)
[130/157]	0.0954 (0.0942)	0.0586 (0.0580)	1.025 (0.842)	65.62 (74.48)
[140/157]	0.0939 (0.0942)	0.0575 (0.0581)	0.553 (0.847)	87.50 (74.29)
[150/157]	0.0961 (0.0943)	0.0587 (0.0581)	0.765 (0.849)	78.12 (74.28)
[156/157]	0.0766 (0.0942)	0.0518 (0.0581)	0.766 (0.848)	75.00 (74.36)
 * Train Acc 74.360
 * Val Acc 74.600, Total time 0.56
 * Val loss 0.799, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0084 (0.0084)	0.759 (0.759)	68.75 (68.75)
[10/157]	0.0971 (0.0913)	0.0585 (0.0542)	0.946 (0.908)	78.12 (72.73)
[20/157]	0.0942 (0.0928)	0.0569 (0.0558)	0.737 (0.871)	75.00 (73.21)
[30/157]	0.0929 (0.0933)	0.0563 (0.0566)	0.823 (0.872)	78.12 (73.89)
[40/157]	0.0959 (0.0937)	0.0579 (0.0571)	0.892 (0.859)	71.88 (74.62)
[50/157]	0.0939 (0.0938)	0.0569 (0.0571)	0.795 (0.847)	71.88 (74.82)
[60/157]	0.0937 (0.0939)	0.0564 (0.0572)	0.582 (0.853)	84.38 (74.90)
[70/157]	0.0937 (0.0940)	0.0569 (0.0573)	0.842 (0.868)	71.88 (73.86)
[80/157]	0.0958 (0.0941)	0.0586 (0.0574)	0.665 (0.861)	84.38 (74.04)
[90/157]	0.0945 (0.0941)	0.0575 (0.0575)	1.304 (0.868)	50.00 (73.76)
[100/157]	0.0940 (0.0942)	0.0569 (0.0576)	0.723 (0.864)	81.25 (74.26)
[110/157]	0.0959 (0.0942)	0.0580 (0.0576)	0.827 (0.860)	71.88 (74.07)
[120/157]	0.0947 (0.0943)	0.0570 (0.0577)	0.746 (0.861)	75.00 (74.07)
[130/157]	0.0944 (0.0943)	0.0569 (0.0577)	0.817 (0.866)	71.88 (73.95)
[140/157]	0.0958 (0.0944)	0.0585 (0.0578)	1.169 (0.864)	65.62 (73.96)
[150/157]	0.0941 (0.0944)	0.0569 (0.0578)	0.768 (0.863)	78.12 (74.05)
[156/157]	0.0790 (0.0942)	0.0550 (0.0578)	1.036 (0.861)	62.50 (74.06)
 * Train Acc 74.060
 * Val Acc 75.400, Total time 0.56
 * Val loss 0.793, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0084 (0.0084)	0.699 (0.699)	84.38 (84.38)
[10/157]	0.0926 (0.0903)	0.0563 (0.0543)	0.833 (0.835)	62.50 (73.58)
[20/157]	0.0938 (0.0923)	0.0565 (0.0562)	0.826 (0.860)	71.88 (71.73)
[30/157]	0.0966 (0.0930)	0.0588 (0.0569)	0.986 (0.858)	65.62 (72.78)
[40/157]	0.0959 (0.0933)	0.0586 (0.0572)	0.792 (0.860)	81.25 (73.09)
[50/157]	0.0938 (0.0935)	0.0562 (0.0573)	1.005 (0.880)	75.00 (72.67)
[60/157]	0.0943 (0.0937)	0.0562 (0.0574)	0.781 (0.868)	78.12 (73.31)
[70/157]	0.0958 (0.0938)	0.0580 (0.0574)	0.985 (0.863)	65.62 (73.59)
[80/157]	0.0947 (0.0939)	0.0579 (0.0575)	0.850 (0.864)	84.38 (73.96)
[90/157]	0.0943 (0.0939)	0.0571 (0.0575)	0.928 (0.860)	75.00 (74.21)
[100/157]	0.0947 (0.0940)	0.0565 (0.0576)	0.981 (0.861)	62.50 (73.98)
[110/157]	0.0968 (0.0940)	0.0583 (0.0576)	0.874 (0.863)	71.88 (73.87)
[120/157]	0.0956 (0.0941)	0.0583 (0.0576)	0.701 (0.861)	84.38 (73.89)
[130/157]	0.0936 (0.0941)	0.0567 (0.0576)	0.893 (0.860)	75.00 (74.00)
[140/157]	0.0942 (0.0941)	0.0567 (0.0577)	1.051 (0.857)	71.88 (74.05)
[150/157]	0.0953 (0.0941)	0.0576 (0.0577)	0.610 (0.853)	87.50 (74.28)
[156/157]	0.0783 (0.0940)	0.0540 (0.0577)	1.914 (0.856)	75.00 (74.24)
 * Train Acc 74.240
 * Val Acc 75.400, Total time 0.57
 * Val loss 0.788, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0408 (0.0408)	0.0086 (0.0086)	0.947 (0.947)	65.62 (65.62)
[10/157]	0.0957 (0.0911)	0.0572 (0.0538)	0.646 (0.868)	78.12 (74.43)
[20/157]	0.0922 (0.0927)	0.0556 (0.0559)	0.620 (0.858)	81.25 (73.51)
[30/157]	0.0961 (0.0934)	0.0579 (0.0565)	0.699 (0.837)	68.75 (73.79)
[40/157]	0.0969 (0.0936)	0.0595 (0.0568)	1.107 (0.837)	59.38 (73.40)
[50/157]	0.0934 (0.0937)	0.0561 (0.0571)	0.726 (0.857)	84.38 (72.79)
[60/157]	0.0932 (0.0939)	0.0568 (0.0573)	0.943 (0.861)	78.12 (73.36)
[70/157]	0.0944 (0.0939)	0.0574 (0.0574)	1.056 (0.874)	68.75 (73.06)
[80/157]	0.0950 (0.0939)	0.0581 (0.0575)	0.903 (0.875)	68.75 (73.26)
[90/157]	0.0964 (0.0940)	0.0593 (0.0576)	0.724 (0.881)	84.38 (73.39)
[100/157]	0.0937 (0.0940)	0.0571 (0.0576)	1.086 (0.879)	68.75 (73.64)
[110/157]	0.0946 (0.0940)	0.0583 (0.0577)	0.654 (0.876)	75.00 (73.68)
[120/157]	0.0928 (0.0940)	0.0559 (0.0577)	0.659 (0.870)	87.50 (74.04)
[130/157]	0.0927 (0.0941)	0.0566 (0.0578)	1.058 (0.869)	62.50 (74.07)
[140/157]	0.0938 (0.0941)	0.0571 (0.0578)	0.700 (0.865)	78.12 (74.20)
[150/157]	0.0941 (0.0941)	0.0570 (0.0578)	0.921 (0.863)	75.00 (74.15)
[156/157]	0.0764 (0.0940)	0.0517 (0.0578)	0.589 (0.861)	75.00 (74.16)
 * Train Acc 74.160
 * Val Acc 74.500, Total time 0.57
 * Val loss 0.805, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0084 (0.0084)	0.618 (0.618)	87.50 (87.50)
[10/157]	0.0927 (0.0907)	0.0563 (0.0538)	1.192 (0.824)	68.75 (75.57)
[20/157]	0.0948 (0.0926)	0.0577 (0.0559)	0.633 (0.821)	81.25 (75.89)
[30/157]	0.0955 (0.0932)	0.0585 (0.0566)	0.786 (0.840)	75.00 (75.20)
[40/157]	0.0932 (0.0935)	0.0566 (0.0569)	0.805 (0.838)	84.38 (75.08)
[50/157]	0.0944 (0.0937)	0.0569 (0.0572)	1.168 (0.849)	68.75 (74.39)
[60/157]	0.0962 (0.0939)	0.0587 (0.0574)	0.778 (0.854)	78.12 (74.23)
[70/157]	0.0952 (0.0940)	0.0582 (0.0575)	0.625 (0.854)	75.00 (74.43)
[80/157]	0.0937 (0.0941)	0.0569 (0.0575)	0.693 (0.848)	65.62 (74.31)
[90/157]	0.0943 (0.0941)	0.0572 (0.0576)	0.744 (0.847)	75.00 (74.35)
[100/157]	0.0960 (0.0941)	0.0575 (0.0576)	0.892 (0.857)	62.50 (73.98)
[110/157]	0.0956 (0.0941)	0.0584 (0.0576)	0.609 (0.847)	90.62 (74.38)
[120/157]	0.0935 (0.0942)	0.0565 (0.0576)	0.896 (0.851)	71.88 (74.12)
[130/157]	0.0954 (0.0942)	0.0576 (0.0577)	0.807 (0.855)	81.25 (74.00)
[140/157]	0.0951 (0.0942)	0.0582 (0.0577)	0.921 (0.862)	71.88 (73.89)
[150/157]	0.0946 (0.0943)	0.0576 (0.0577)	0.938 (0.865)	68.75 (73.88)
[156/157]	0.0772 (0.0942)	0.0525 (0.0577)	1.334 (0.866)	75.00 (73.90)
 * Train Acc 73.900
 * Val Acc 75.500, Total time 0.56
 * Val loss 0.782, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0465 (0.0465)	0.0091 (0.0091)	0.696 (0.696)	75.00 (75.00)
[10/157]	0.0933 (0.0908)	0.0576 (0.0540)	0.984 (0.815)	65.62 (74.72)
[20/157]	0.0940 (0.0925)	0.0567 (0.0561)	0.830 (0.862)	75.00 (72.47)
[30/157]	0.0972 (0.0931)	0.0595 (0.0568)	0.764 (0.884)	81.25 (72.88)
[40/157]	0.0959 (0.0934)	0.0585 (0.0571)	1.093 (0.892)	65.62 (72.33)
[50/157]	0.0967 (0.0936)	0.0591 (0.0574)	1.084 (0.883)	68.75 (72.98)
[60/157]	0.0951 (0.0938)	0.0576 (0.0575)	1.023 (0.878)	71.88 (73.41)
[70/157]	0.0952 (0.0938)	0.0578 (0.0576)	0.853 (0.873)	78.12 (73.94)
[80/157]	0.0947 (0.0939)	0.0576 (0.0577)	0.881 (0.877)	78.12 (73.61)
[90/157]	0.0938 (0.0939)	0.0572 (0.0577)	0.751 (0.864)	81.25 (74.00)
[100/157]	0.0939 (0.0940)	0.0561 (0.0578)	1.008 (0.862)	75.00 (74.20)
[110/157]	0.0942 (0.0940)	0.0570 (0.0578)	1.050 (0.863)	68.75 (73.99)
[120/157]	0.0968 (0.0941)	0.0584 (0.0578)	0.735 (0.849)	75.00 (74.28)
[130/157]	0.0949 (0.0941)	0.0580 (0.0578)	1.055 (0.857)	59.38 (74.07)
[140/157]	0.0952 (0.0941)	0.0583 (0.0578)	0.995 (0.858)	62.50 (74.09)
[150/157]	0.0953 (0.0941)	0.0584 (0.0578)	0.824 (0.859)	65.62 (74.01)
[156/157]	0.0766 (0.0940)	0.0528 (0.0578)	1.380 (0.862)	62.50 (73.98)
 * Train Acc 73.980
 * Val Acc 74.800, Total time 0.56
 * Val loss 0.785, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0406 (0.0406)	0.0080 (0.0080)	1.111 (1.111)	62.50 (62.50)
[10/157]	0.0955 (0.0903)	0.0583 (0.0541)	0.721 (0.824)	68.75 (74.15)
[20/157]	0.0947 (0.0924)	0.0575 (0.0560)	1.088 (0.846)	68.75 (75.00)
[30/157]	0.0952 (0.0932)	0.0577 (0.0568)	0.677 (0.851)	84.38 (74.29)
[40/157]	0.0959 (0.0935)	0.0589 (0.0571)	1.179 (0.863)	56.25 (73.70)
[50/157]	0.0962 (0.0937)	0.0584 (0.0573)	1.042 (0.858)	81.25 (74.08)
[60/157]	0.0938 (0.0937)	0.0565 (0.0573)	1.053 (0.860)	68.75 (73.92)
[70/157]	0.0940 (0.0938)	0.0573 (0.0575)	0.746 (0.849)	78.12 (74.56)
[80/157]	0.0954 (0.0939)	0.0580 (0.0575)	0.975 (0.853)	68.75 (74.11)
[90/157]	0.0948 (0.0940)	0.0579 (0.0576)	0.901 (0.850)	71.88 (74.21)
[100/157]	0.0961 (0.0941)	0.0583 (0.0576)	1.041 (0.854)	75.00 (74.04)
[110/157]	0.0960 (0.0941)	0.0582 (0.0577)	0.603 (0.846)	81.25 (74.35)
[120/157]	0.0956 (0.0941)	0.0580 (0.0577)	0.880 (0.848)	68.75 (74.12)
[130/157]	0.0943 (0.0942)	0.0571 (0.0578)	0.714 (0.854)	78.12 (74.07)
[140/157]	0.0951 (0.0942)	0.0579 (0.0578)	0.759 (0.846)	84.38 (74.49)
[150/157]	0.0957 (0.0943)	0.0585 (0.0578)	0.882 (0.843)	65.62 (74.42)
[156/157]	0.0776 (0.0942)	0.0504 (0.0577)	1.305 (0.846)	62.50 (74.32)
 * Train Acc 74.320
 * Val Acc 75.900, Total time 0.57
 * Val loss 0.789, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 5/27; cond: 386333.03125, radio:3.846798790618777e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0042,  0.1073, -0.1581],
          [-0.1382, -0.0746,  0.0502],
          [ 0.0052,  0.1573, -0.0150]],

         [[ 0.0548, -0.0551, -0.0375],
          [-0.1811, -0.1278, -0.0790],
          [ 0.0137,  0.0788,  0.1180]],

         [[-0.1300, -0.0833,  0.0677],
          [ 0.1596, -0.0408,  0.1445],
          [-0.0281,  0.0216,  0.1758]]],


        [[[-0.1679, -0.1051, -0.0260],
          [-0.0667,  0.1737, -0.1108],
          [-0.0791, -0.1351, -0.1714]],

         [[-0.1063,  0.1791,  0.1032],
          [ 0.0971,  0.0174, -0.0889],
          [ 0.0344, -0.1829, -0.1367]],

         [[-0.0979,  0.1279,  0.1212],
          [-0.0830, -0.0013,  0.1284],
          [ 0.1914,  0.0733,  0.0266]]],


        [[[ 0.1331, -0.1136,  0.0365],
          [-0.1474, -0.1349, -0.0991],
          [ 0.0869,  0.0754, -0.1127]],

         [[ 0.0591,  0.1011, -0.0280],
          [ 0.0059,  0.0401,  0.1156],
          [ 0.1802, -0.1534, -0.0733]],

         [[ 0.0776,  0.1566,  0.1639],
          [ 0.1682,  0.0344, -0.1715],
          [ 0.0119, -0.1272, -0.1841]]],


        ...,


        [[[ 0.0681, -0.0763, -0.1206],
          [ 0.0764,  0.0335, -0.1411],
          [-0.1323,  0.0945,  0.1445]],

         [[ 0.0183, -0.0975,  0.1003],
          [ 0.0360, -0.1679, -0.1244],
          [ 0.0342,  0.1230, -0.0629]],

         [[ 0.0808, -0.1307,  0.1821],
          [ 0.1273,  0.0268,  0.0760],
          [ 0.0454, -0.1182, -0.0130]]],


        [[[ 0.0627, -0.0406,  0.1924],
          [-0.0632, -0.1655, -0.0865],
          [-0.1705,  0.1685, -0.0841]],

         [[-0.1291, -0.0211, -0.1255],
          [-0.0925,  0.1033, -0.1425],
          [ 0.0978,  0.1511,  0.0007]],

         [[ 0.0848, -0.1125,  0.1498],
          [ 0.1871,  0.0453,  0.1721],
          [-0.1955, -0.0192,  0.0318]]],


        [[[-0.1668,  0.0943, -0.0724],
          [-0.0707, -0.1544,  0.0819],
          [-0.1407, -0.0457,  0.0853]],

         [[ 0.1453,  0.0467, -0.0712],
          [-0.0837,  0.1103,  0.0391],
          [ 0.1597,  0.0671,  0.1516]],

         [[-0.1560,  0.1530, -0.0940],
          [-0.0552, -0.0043, -0.0431],
          [ 0.0335,  0.1606, -0.1261]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.4591e+06, 1.4790e+05, 1.4553e+05, 6.1885e+04, 5.3264e+04, 4.3538e+04,
        9.8305e+03, 5.7308e+03, 1.8014e+03, 1.7101e+03, 1.7063e+03, 1.5423e+03,
        6.2126e+02, 3.7348e+02, 3.7164e+02, 2.8666e+02, 2.6391e+02, 1.7662e+02,
        7.5625e+01, 6.7817e+01, 5.6164e+01, 3.9182e+01, 3.4907e+01, 1.3704e+01,
        1.2302e+01, 9.7807e+00, 3.7767e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 5]) 

NULL SPACE BASIS :  tensor([[-2.0150e-01, -1.3356e-01, -1.3242e-01,  1.2163e-01,  8.6233e-02],
        [-7.2787e-03,  2.4151e-01,  3.2359e-03, -2.2913e-01, -1.5639e-01],
        [ 2.1070e-01, -1.3122e-01,  1.2941e-01,  1.2527e-01,  8.6399e-02],
        [ 3.8335e-01, -1.3290e-04,  2.4219e-01, -2.2302e-01, -1.5383e-01],
        [ 3.2510e-04,  5.2081e-03, -7.8622e-03,  4.1721e-01,  2.7720e-01],
        [-3.8553e-01, -4.5784e-03, -2.3371e-01, -2.3007e-01, -1.5433e-01],
        [-2.1080e-01,  1.3286e-01, -1.3352e-01,  1.2246e-01,  8.5375e-02],
        [ 8.1200e-03, -2.4590e-01,  5.7020e-03, -2.2990e-01, -1.5505e-01],
        [ 2.0261e-01,  1.3576e-01,  1.2696e-01,  1.2666e-01,  8.5534e-02],
        [ 1.1258e-02,  2.4509e-01,  2.4264e-01,  3.8133e-04, -1.5178e-01],
        [-2.8301e-04, -4.4161e-01,  2.3678e-03,  6.4757e-03,  2.7615e-01],
        [-1.3559e-02,  2.4090e-01, -2.4609e-01, -9.5442e-03, -1.5248e-01],
        [-1.8704e-02, -3.1080e-03, -4.4239e-01, -4.3345e-03,  2.7060e-01],
        [-1.3910e-04, -2.5040e-03,  9.3364e-04, -1.7563e-03, -4.8992e-01],
        [ 2.2253e-02,  4.1546e-03,  4.4175e-01,  1.1107e-02,  2.7258e-01],
        [ 6.8531e-03, -2.4099e-01,  2.4526e-01,  3.2133e-03, -1.4969e-01],
        [-1.2417e-03,  4.4360e-01, -4.3799e-03, -9.7840e-04,  2.7327e-01],
        [-6.4471e-03, -2.4555e-01, -2.4005e-01, -4.1034e-03, -1.5086e-01],
        [ 2.1431e-01, -1.3307e-01, -1.3130e-01, -1.3861e-01,  7.8002e-02],
        [ 9.1262e-03,  2.3892e-01, -6.4056e-03,  2.5254e-01, -1.4255e-01],
        [-2.2278e-01, -1.3092e-01,  1.3871e-01, -1.3156e-01,  7.8615e-02],
        [-4.0953e-01,  3.4265e-03,  2.3855e-01,  2.5733e-01, -1.3909e-01],
        [-1.1473e-03, -2.7630e-03,  7.6739e-03, -4.6935e-01,  2.5346e-01],
        [ 4.0908e-01,  5.4119e-04, -2.4729e-01,  2.4795e-01, -1.4082e-01],
        [ 2.2974e-01,  1.2934e-01, -1.3279e-01, -1.4233e-01,  7.6599e-02],
        [-7.3573e-03, -2.3643e-01, -1.4272e-03,  2.6128e-01, -1.4081e-01],
        [-2.2143e-01,  1.3087e-01,  1.3428e-01, -1.3882e-01,  7.7785e-02]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0439, -0.0325, -0.0087, -0.0669,  0.0335,  0.0304,  0.0289, -0.0049,
         -0.0237, -0.0359,  0.0373, -0.0050,  0.0383, -0.0189, -0.0173, -0.0063,
         -0.0156,  0.0234, -0.0081, -0.0065,  0.0156,  0.0312, -0.0159, -0.0142,
         -0.0254,  0.0237, -0.0004],
        [-0.0325,  0.0605, -0.0336,  0.0327, -0.0616,  0.0348, -0.0037,  0.0078,
         -0.0048,  0.0374, -0.0677,  0.0373, -0.0194,  0.0342, -0.0192, -0.0156,
          0.0289, -0.0159, -0.0065,  0.0099, -0.0052, -0.0146,  0.0301, -0.0172,
          0.0223, -0.0424,  0.0238],
        [-0.0087, -0.0336,  0.0454,  0.0317,  0.0334, -0.0684, -0.0252, -0.0033,
          0.0289, -0.0051,  0.0371, -0.0361, -0.0170, -0.0188,  0.0386,  0.0234,
         -0.0159, -0.0061,  0.0157, -0.0049, -0.0096, -0.0159, -0.0160,  0.0327,
          0.0014,  0.0223, -0.0255],
        [-0.0669,  0.0327,  0.0317,  0.1248, -0.0615, -0.0578, -0.0687,  0.0356,
          0.0300,  0.0386, -0.0194, -0.0176, -0.0693,  0.0340,  0.0318,  0.0377,
         -0.0194, -0.0163,  0.0310, -0.0145, -0.0154, -0.0605,  0.0300,  0.0283,
          0.0339, -0.0178, -0.0149],
        [ 0.0335, -0.0616,  0.0334, -0.0615,  0.1122, -0.0613,  0.0342, -0.0627,
          0.0341, -0.0190,  0.0344, -0.0193,  0.0343, -0.0611,  0.0343, -0.0194,
          0.0347, -0.0192, -0.0160,  0.0300, -0.0156,  0.0299, -0.0562,  0.0297,
         -0.0163,  0.0307, -0.0165],
        [ 0.0304,  0.0348, -0.0684, -0.0578, -0.0613,  0.1252,  0.0315,  0.0329,
         -0.0674, -0.0174, -0.0190,  0.0391,  0.0312,  0.0339, -0.0700, -0.0163,
         -0.0190,  0.0375, -0.0141, -0.0175,  0.0323,  0.0288,  0.0302, -0.0605,
         -0.0166, -0.0153,  0.0328],
        [ 0.0289, -0.0037, -0.0252, -0.0687,  0.0342,  0.0315,  0.0457, -0.0342,
         -0.0084, -0.0068, -0.0155,  0.0239,  0.0381, -0.0190, -0.0172, -0.0351,
          0.0371, -0.0056, -0.0249,  0.0221,  0.0007,  0.0333, -0.0165, -0.0156,
         -0.0109, -0.0043,  0.0160],
        [-0.0049,  0.0078, -0.0033,  0.0356, -0.0627,  0.0329, -0.0342,  0.0615,
         -0.0328, -0.0158,  0.0288, -0.0156, -0.0192,  0.0344, -0.0193,  0.0372,
         -0.0676,  0.0373,  0.0239, -0.0423,  0.0220, -0.0181,  0.0310, -0.0149,
         -0.0044,  0.0089, -0.0060],
        [-0.0237, -0.0048,  0.0289,  0.0300,  0.0341, -0.0674, -0.0084, -0.0328,
          0.0443,  0.0239, -0.0158, -0.0069, -0.0169, -0.0190,  0.0384, -0.0056,
          0.0370, -0.0351, -0.0010,  0.0238, -0.0247, -0.0141, -0.0167,  0.0317,
          0.0160, -0.0057, -0.0094],
        [-0.0359,  0.0374, -0.0051,  0.0386, -0.0190, -0.0174, -0.0068, -0.0158,
          0.0239,  0.0636, -0.0669,  0.0100, -0.0668,  0.0331,  0.0300,  0.0104,
          0.0296, -0.0428, -0.0331,  0.0353, -0.0058,  0.0337, -0.0168, -0.0151,
         -0.0043, -0.0165,  0.0225],
        [ 0.0373, -0.0677,  0.0371, -0.0194,  0.0344, -0.0190, -0.0155,  0.0288,
         -0.0158, -0.0669,  0.1213, -0.0667,  0.0336, -0.0600,  0.0333,  0.0294,
         -0.0539,  0.0296,  0.0353, -0.0641,  0.0354, -0.0168,  0.0305, -0.0171,
         -0.0167,  0.0301, -0.0165],
        [-0.0050,  0.0373, -0.0361, -0.0176, -0.0193,  0.0391,  0.0239, -0.0156,
         -0.0069,  0.0100, -0.0667,  0.0636,  0.0300,  0.0330, -0.0669, -0.0428,
          0.0297,  0.0103, -0.0059,  0.0350, -0.0328, -0.0150, -0.0164,  0.0333,
          0.0225, -0.0168, -0.0040],
        [ 0.0383, -0.0194, -0.0170, -0.0693,  0.0343,  0.0312,  0.0381, -0.0192,
         -0.0169, -0.0668,  0.0336,  0.0300,  0.1204, -0.0595, -0.0546, -0.0664,
          0.0333,  0.0296,  0.0341, -0.0169, -0.0156, -0.0611,  0.0301,  0.0280,
          0.0337, -0.0169, -0.0152],
        [-0.0189,  0.0342, -0.0188,  0.0340, -0.0611,  0.0339, -0.0190,  0.0344,
         -0.0190,  0.0331, -0.0600,  0.0330, -0.0595,  0.1073, -0.0596,  0.0332,
         -0.0604,  0.0332, -0.0169,  0.0308, -0.0169,  0.0304, -0.0552,  0.0305,
         -0.0169,  0.0309, -0.0170],
        [-0.0173, -0.0192,  0.0386,  0.0318,  0.0343, -0.0700, -0.0172, -0.0193,
          0.0384,  0.0300,  0.0333, -0.0669, -0.0546, -0.0596,  0.1208,  0.0298,
          0.0333, -0.0664, -0.0152, -0.0169,  0.0339,  0.0274,  0.0301, -0.0607,
         -0.0151, -0.0167,  0.0334],
        [-0.0063, -0.0156,  0.0234,  0.0377, -0.0194, -0.0163, -0.0351,  0.0372,
         -0.0056,  0.0104,  0.0294, -0.0428, -0.0664,  0.0332,  0.0298,  0.0629,
         -0.0666,  0.0102, -0.0048, -0.0165,  0.0232,  0.0342, -0.0165, -0.0161,
         -0.0331,  0.0351, -0.0055],
        [-0.0156,  0.0289, -0.0159, -0.0194,  0.0347, -0.0190,  0.0371, -0.0676,
          0.0370,  0.0296, -0.0539,  0.0297,  0.0333, -0.0604,  0.0333, -0.0666,
          0.1214, -0.0667, -0.0167,  0.0299, -0.0165, -0.0167,  0.0306, -0.0170,
          0.0352, -0.0642,  0.0354],
        [ 0.0234, -0.0159, -0.0061, -0.0163, -0.0192,  0.0375, -0.0056,  0.0373,
         -0.0351, -0.0428,  0.0296,  0.0103,  0.0296,  0.0332, -0.0664,  0.0102,
         -0.0667,  0.0629,  0.0231, -0.0164, -0.0049, -0.0159, -0.0168,  0.0344,
         -0.0055,  0.0352, -0.0331],
        [-0.0081, -0.0065,  0.0157,  0.0310, -0.0160, -0.0141, -0.0249,  0.0239,
         -0.0010, -0.0331,  0.0353, -0.0059,  0.0341, -0.0169, -0.0152, -0.0048,
         -0.0167,  0.0231,  0.0475, -0.0336, -0.0108, -0.0743,  0.0375,  0.0334,
          0.0336, -0.0077, -0.0256],
        [-0.0065,  0.0099, -0.0049, -0.0145,  0.0300, -0.0175,  0.0221, -0.0423,
          0.0238,  0.0353, -0.0641,  0.0350, -0.0169,  0.0308, -0.0169, -0.0165,
          0.0299, -0.0164, -0.0336,  0.0632, -0.0352,  0.0359, -0.0695,  0.0394,
         -0.0058,  0.0132, -0.0079],
        [ 0.0156, -0.0052, -0.0096, -0.0154, -0.0156,  0.0323,  0.0007,  0.0220,
         -0.0247, -0.0058,  0.0354, -0.0328, -0.0156, -0.0169,  0.0339,  0.0232,
         -0.0165, -0.0049, -0.0108, -0.0352,  0.0490,  0.0354,  0.0373, -0.0757,
         -0.0276, -0.0058,  0.0336],
        [ 0.0312, -0.0146, -0.0159, -0.0605,  0.0299,  0.0288,  0.0333, -0.0181,
         -0.0141,  0.0337, -0.0168, -0.0150, -0.0611,  0.0304,  0.0274,  0.0342,
         -0.0167, -0.0159, -0.0743,  0.0359,  0.0354,  0.1387, -0.0688, -0.0640,
         -0.0772,  0.0397,  0.0343],
        [-0.0159,  0.0301, -0.0160,  0.0300, -0.0562,  0.0302, -0.0165,  0.0310,
         -0.0167, -0.0168,  0.0305, -0.0164,  0.0301, -0.0552,  0.0301, -0.0165,
          0.0306, -0.0168,  0.0375, -0.0695,  0.0373, -0.0688,  0.1273, -0.0691,
          0.0378, -0.0705,  0.0384],
        [-0.0142, -0.0172,  0.0327,  0.0283,  0.0297, -0.0605, -0.0156, -0.0149,
          0.0317, -0.0151, -0.0171,  0.0333,  0.0280,  0.0305, -0.0607, -0.0161,
         -0.0170,  0.0344,  0.0334,  0.0394, -0.0757, -0.0640, -0.0691,  0.1386,
          0.0361,  0.0366, -0.0756],
        [-0.0254,  0.0223,  0.0014,  0.0339, -0.0163, -0.0166, -0.0109, -0.0044,
          0.0160, -0.0043, -0.0167,  0.0225,  0.0337, -0.0169, -0.0151, -0.0331,
          0.0352, -0.0055,  0.0336, -0.0058, -0.0276, -0.0772,  0.0378,  0.0361,
          0.0507, -0.0358, -0.0117],
        [ 0.0237, -0.0424,  0.0223, -0.0178,  0.0307, -0.0153, -0.0043,  0.0089,
         -0.0057, -0.0165,  0.0301, -0.0168, -0.0169,  0.0309, -0.0167,  0.0351,
         -0.0642,  0.0352, -0.0077,  0.0132, -0.0058,  0.0397, -0.0705,  0.0366,
         -0.0358,  0.0644, -0.0343],
        [-0.0004,  0.0238, -0.0255, -0.0149, -0.0165,  0.0328,  0.0160, -0.0060,
         -0.0094,  0.0225, -0.0165, -0.0040, -0.0152, -0.0170,  0.0334, -0.0055,
          0.0354, -0.0331, -0.0256, -0.0079,  0.0336,  0.0343,  0.0384, -0.0756,
         -0.0117, -0.0343,  0.0490]], device='cuda:0') 

reserving basis 71/576; cond: 10360646.0, radio:3.170320633216761e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1342e-02, -3.9853e-02, -7.1437e-03],
          [ 3.8548e-02,  2.6455e-02,  3.1549e-02],
          [ 2.2104e-02, -1.6993e-03,  8.7434e-03]],

         [[-1.4106e-02,  3.7791e-02,  1.8564e-02],
          [-2.9945e-02,  3.6948e-02, -3.2308e-03],
          [-6.9341e-03, -1.7070e-02, -2.0332e-02]],

         [[-3.0406e-03, -2.0810e-02, -3.6796e-02],
          [-2.9224e-02, -1.0346e-02,  3.3326e-02],
          [ 2.0542e-02, -3.1893e-02, -9.4068e-03]],

         ...,

         [[-2.5477e-02,  1.1651e-02, -2.6909e-05],
          [ 1.4844e-02, -6.5521e-03, -9.4185e-03],
          [-2.0423e-03,  8.6654e-03, -3.1123e-02]],

         [[-4.1538e-02,  1.3738e-02, -3.4915e-02],
          [-6.6976e-03,  1.2121e-02,  1.0929e-03],
          [ 2.0780e-02,  7.7822e-05,  4.1378e-03]],

         [[-1.6038e-02,  2.0365e-03,  1.1463e-02],
          [ 2.3208e-02, -3.9586e-02, -1.6212e-02],
          [-4.1615e-02, -1.1466e-02,  2.9655e-02]]],


        [[[-4.2282e-02, -4.7277e-02,  1.4308e-02],
          [-2.3805e-02,  2.0092e-02,  2.7113e-03],
          [ 8.1874e-03,  2.3271e-02, -4.2245e-02]],

         [[ 3.2901e-02,  8.0275e-03, -3.0309e-02],
          [ 1.1827e-02, -1.1411e-02, -1.7352e-02],
          [ 3.4530e-03, -1.1681e-02,  4.0726e-02]],

         [[ 2.6442e-02, -1.6325e-02, -4.4952e-02],
          [-1.7158e-02, -1.1111e-02,  1.0131e-02],
          [-1.2334e-02, -2.3039e-03, -6.4388e-03]],

         ...,

         [[-5.3674e-03, -3.8542e-03,  8.6835e-03],
          [ 1.5645e-02, -3.3927e-02,  1.7247e-02],
          [ 3.3156e-02,  7.4156e-03,  3.0846e-02]],

         [[-2.1748e-02,  6.7501e-03, -4.5419e-04],
          [-1.1125e-02,  2.3791e-02, -4.6486e-02],
          [-1.0251e-03, -1.4389e-02, -2.0129e-02]],

         [[-1.4762e-02, -3.2078e-02,  4.2447e-03],
          [-6.8142e-03, -3.1563e-02, -8.1044e-03],
          [-8.7550e-03,  1.5999e-02, -4.1653e-02]]],


        [[[-3.1405e-03,  1.8431e-02,  2.0636e-02],
          [ 8.0300e-03,  1.3054e-02,  4.1688e-02],
          [-1.3398e-02,  2.5534e-02,  3.2446e-02]],

         [[-3.1279e-02,  1.9980e-02,  2.9019e-02],
          [-2.2827e-03,  3.2305e-02,  6.1396e-03],
          [ 3.5321e-02,  6.7318e-03, -2.0924e-02]],

         [[ 3.8745e-02,  1.5163e-02,  3.4513e-02],
          [ 1.8351e-02,  2.5725e-02,  2.0073e-02],
          [ 1.6725e-02, -5.9719e-03,  9.5010e-03]],

         ...,

         [[ 2.1108e-02, -1.6913e-02,  4.4123e-02],
          [-3.2860e-03,  3.8410e-02, -3.4419e-03],
          [ 2.4472e-02,  3.3271e-02, -3.6192e-02]],

         [[-2.0662e-02, -3.6374e-02,  3.6746e-02],
          [ 4.5252e-02, -1.3525e-02, -1.8853e-02],
          [ 1.1952e-02,  3.1018e-02,  1.3882e-02]],

         [[-1.4924e-02, -2.3205e-02,  3.0095e-02],
          [-2.9727e-02, -4.1728e-02, -2.1364e-02],
          [ 3.6093e-03,  5.8962e-03,  6.1682e-03]]],


        ...,


        [[[-4.4300e-02, -1.8268e-02,  9.4651e-03],
          [ 2.4339e-02, -1.7130e-02, -8.7055e-03],
          [-4.2414e-03,  2.0914e-02, -3.4575e-02]],

         [[-4.1739e-02,  2.7759e-02,  5.7552e-03],
          [-1.3461e-02, -1.1927e-02,  8.7549e-03],
          [-1.9421e-02,  2.8438e-02,  1.2208e-02]],

         [[ 2.4023e-02,  3.2051e-02,  1.0549e-03],
          [ 4.7762e-02, -7.7190e-03, -3.4091e-02],
          [ 3.8614e-02, -1.4899e-02, -3.0953e-03]],

         ...,

         [[-2.5792e-02, -5.3266e-02,  2.3077e-02],
          [-2.0502e-02,  2.2680e-02, -4.7223e-02],
          [ 1.6432e-02,  4.0698e-02,  2.7463e-02]],

         [[-4.6403e-02, -5.5668e-02,  1.6286e-02],
          [-1.3014e-02, -1.4850e-02, -6.4659e-03],
          [-2.0960e-02,  9.8290e-03, -9.3364e-03]],

         [[ 3.6309e-02, -2.2394e-02,  4.1174e-02],
          [ 9.5112e-03, -5.7455e-03,  4.7775e-02],
          [ 3.1262e-02,  2.7892e-02,  4.8141e-02]]],


        [[[ 2.5281e-02,  1.4034e-02, -1.3621e-02],
          [-1.3545e-02, -3.1946e-02, -2.0607e-02],
          [ 1.5989e-02, -5.8682e-02, -4.2033e-02]],

         [[-8.2349e-03,  2.0724e-02,  9.0103e-03],
          [ 1.0002e-02,  1.1936e-03, -4.0714e-02],
          [-9.4751e-03,  3.7872e-02,  1.2346e-02]],

         [[-9.3590e-03,  8.4858e-03,  4.0403e-02],
          [ 4.0079e-04,  5.4086e-03,  3.0287e-02],
          [-3.1431e-02,  2.5801e-02,  3.2523e-02]],

         ...,

         [[ 4.0200e-02,  1.9576e-02,  3.8792e-02],
          [-3.5722e-02,  9.9942e-03,  7.5393e-04],
          [-1.2572e-03, -4.1667e-02, -3.9212e-02]],

         [[ 1.6753e-02,  2.3303e-02, -9.2087e-03],
          [ 2.6050e-02,  8.0846e-03,  1.8223e-02],
          [-4.8320e-02, -2.4966e-02, -5.2811e-03]],

         [[ 2.7813e-02,  2.5215e-02, -3.1753e-02],
          [-3.3127e-02, -2.8782e-02, -1.2091e-02],
          [-8.0293e-03, -3.9900e-02,  2.2198e-02]]],


        [[[ 2.7619e-02,  1.0027e-02, -1.6638e-02],
          [-6.0227e-03,  6.6964e-03, -1.2774e-02],
          [-1.9191e-03, -3.5104e-02, -7.6197e-03]],

         [[-4.3806e-02, -2.8729e-02, -2.9428e-02],
          [ 1.0623e-02, -5.6275e-02, -1.1898e-03],
          [ 2.6710e-02, -3.5920e-02, -6.9526e-03]],

         [[-1.7167e-02,  1.3746e-02,  2.2033e-02],
          [ 1.5460e-02, -1.3619e-02,  1.0671e-02],
          [-2.2144e-02,  3.6071e-02,  3.6477e-03]],

         ...,

         [[-2.1353e-02, -9.0892e-03,  1.0059e-02],
          [ 4.0586e-03,  3.7287e-02, -4.2760e-02],
          [-2.2922e-02,  1.5686e-02,  2.4614e-02]],

         [[-3.1371e-02, -3.8681e-02,  5.5069e-03],
          [-1.9675e-02, -1.0654e-02, -4.8797e-02],
          [-2.8298e-02, -5.1283e-02, -3.9096e-02]],

         [[-4.0193e-03, -1.0373e-02,  3.1093e-02],
          [ 3.3990e-02,  4.4251e-03,  3.5773e-02],
          [-2.8877e-02,  4.5887e-02, -3.2586e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.4350e+07, 1.4924e+06, 1.2009e+06, 1.1313e+06, 8.5781e+05, 6.3473e+05,
        5.8217e+05, 5.3454e+05, 4.5427e+05, 3.5824e+05, 3.0763e+05, 2.8239e+05,
        2.3043e+05, 1.8732e+05, 1.1132e+05, 9.5575e+04, 9.2045e+04, 8.2840e+04,
        6.9471e+04, 6.2902e+04, 6.0580e+04, 5.5435e+04, 4.0937e+04, 3.6916e+04,
        3.5687e+04, 3.4612e+04, 3.1358e+04, 2.9663e+04, 2.8810e+04, 2.5560e+04,
        2.4641e+04, 2.0576e+04, 2.0241e+04, 1.8865e+04, 1.8293e+04, 1.6673e+04,
        1.5397e+04, 1.4114e+04, 1.3657e+04, 1.2327e+04, 1.1730e+04, 1.1460e+04,
        1.0926e+04, 1.0617e+04, 1.0318e+04, 9.4818e+03, 8.9228e+03, 8.7831e+03,
        8.4947e+03, 7.9605e+03, 7.8034e+03, 7.4622e+03, 7.1623e+03, 7.0622e+03,
        6.7801e+03, 6.4227e+03, 6.1888e+03, 5.9091e+03, 5.7675e+03, 5.7016e+03,
        5.6031e+03, 5.2548e+03, 5.2110e+03, 5.0940e+03, 4.8518e+03, 4.7924e+03,
        4.7623e+03, 4.5645e+03, 4.5477e+03, 4.3506e+03, 4.3142e+03, 4.0197e+03,
        4.0029e+03, 3.9240e+03, 3.9038e+03, 3.7138e+03, 3.6291e+03, 3.4138e+03,
        3.3696e+03, 3.3009e+03, 3.2115e+03, 3.1877e+03, 3.1207e+03, 3.0558e+03,
        2.9874e+03, 2.8627e+03, 2.8249e+03, 2.7306e+03, 2.6724e+03, 2.6371e+03,
        2.5931e+03, 2.4831e+03, 2.3946e+03, 2.3833e+03, 2.2954e+03, 2.2094e+03,
        2.1816e+03, 2.0817e+03, 2.0540e+03, 2.0407e+03, 1.9705e+03, 1.9542e+03,
        1.9427e+03, 1.9019e+03, 1.8224e+03, 1.7434e+03, 1.7145e+03, 1.6774e+03,
        1.6573e+03, 1.6276e+03, 1.5872e+03, 1.5580e+03, 1.5122e+03, 1.4915e+03,
        1.4697e+03, 1.4417e+03, 1.4295e+03, 1.4044e+03, 1.3520e+03, 1.3444e+03,
        1.3225e+03, 1.3027e+03, 1.2851e+03, 1.2365e+03, 1.2172e+03, 1.1930e+03,
        1.1681e+03, 1.1415e+03, 1.1379e+03, 1.1279e+03, 1.0990e+03, 1.0834e+03,
        1.0776e+03, 1.0661e+03, 1.0375e+03, 1.0209e+03, 9.9264e+02, 9.8896e+02,
        9.8017e+02, 9.7708e+02, 9.4228e+02, 9.3514e+02, 9.2767e+02, 9.1102e+02,
        8.9420e+02, 8.7062e+02, 8.6744e+02, 8.5025e+02, 8.4727e+02, 8.3353e+02,
        8.2704e+02, 8.1089e+02, 7.9326e+02, 7.8460e+02, 7.7258e+02, 7.6886e+02,
        7.6450e+02, 7.5316e+02, 7.2795e+02, 7.1851e+02, 7.1287e+02, 7.0377e+02,
        6.9901e+02, 6.7986e+02, 6.7247e+02, 6.6939e+02, 6.5193e+02, 6.4759e+02,
        6.3228e+02, 6.2639e+02, 6.1439e+02, 6.1201e+02, 6.0206e+02, 5.9460e+02,
        5.8783e+02, 5.7196e+02, 5.6713e+02, 5.5867e+02, 5.5346e+02, 5.4672e+02,
        5.4099e+02, 5.3675e+02, 5.2728e+02, 5.2107e+02, 5.1868e+02, 5.1606e+02,
        5.0009e+02, 4.9387e+02, 4.9143e+02, 4.8557e+02, 4.8334e+02, 4.7297e+02,
        4.6688e+02, 4.6338e+02, 4.5720e+02, 4.5513e+02, 4.5233e+02, 4.4664e+02,
        4.4354e+02, 4.3067e+02, 4.2725e+02, 4.2188e+02, 4.1492e+02, 4.1310e+02,
        4.0926e+02, 4.0459e+02, 3.9930e+02, 3.9650e+02, 3.8715e+02, 3.8555e+02,
        3.8322e+02, 3.7909e+02, 3.7646e+02, 3.7371e+02, 3.6669e+02, 3.6285e+02,
        3.5763e+02, 3.5428e+02, 3.5134e+02, 3.4592e+02, 3.4215e+02, 3.3800e+02,
        3.3438e+02, 3.3246e+02, 3.2905e+02, 3.2814e+02, 3.2562e+02, 3.2172e+02,
        3.1601e+02, 3.1067e+02, 3.0922e+02, 3.0499e+02, 3.0386e+02, 3.0140e+02,
        2.9738e+02, 2.9585e+02, 2.9263e+02, 2.9038e+02, 2.8538e+02, 2.8317e+02,
        2.8013e+02, 2.7505e+02, 2.7304e+02, 2.7051e+02, 2.7025e+02, 2.6866e+02,
        2.6406e+02, 2.6004e+02, 2.5898e+02, 2.5702e+02, 2.5319e+02, 2.5063e+02,
        2.4987e+02, 2.4715e+02, 2.4617e+02, 2.4233e+02, 2.3953e+02, 2.3784e+02,
        2.3700e+02, 2.3305e+02, 2.3093e+02, 2.3036e+02, 2.2831e+02, 2.2511e+02,
        2.2272e+02, 2.2060e+02, 2.1901e+02, 2.1705e+02, 2.1578e+02, 2.1258e+02,
        2.0971e+02, 2.0771e+02, 2.0690e+02, 2.0551e+02, 2.0205e+02, 2.0068e+02,
        2.0016e+02, 1.9805e+02, 1.9704e+02, 1.9500e+02, 1.9291e+02, 1.9229e+02,
        1.9053e+02, 1.8884e+02, 1.8700e+02, 1.8556e+02, 1.8435e+02, 1.8371e+02,
        1.8051e+02, 1.7912e+02, 1.7752e+02, 1.7635e+02, 1.7449e+02, 1.7364e+02,
        1.7202e+02, 1.6927e+02, 1.6896e+02, 1.6717e+02, 1.6674e+02, 1.6407e+02,
        1.6318e+02, 1.6193e+02, 1.6064e+02, 1.5909e+02, 1.5806e+02, 1.5695e+02,
        1.5543e+02, 1.5382e+02, 1.5320e+02, 1.5271e+02, 1.5046e+02, 1.4913e+02,
        1.4866e+02, 1.4826e+02, 1.4579e+02, 1.4353e+02, 1.4269e+02, 1.4136e+02,
        1.4094e+02, 1.4008e+02, 1.3880e+02, 1.3667e+02, 1.3530e+02, 1.3490e+02,
        1.3395e+02, 1.3276e+02, 1.3200e+02, 1.3101e+02, 1.3021e+02, 1.2976e+02,
        1.2787e+02, 1.2753e+02, 1.2719e+02, 1.2538e+02, 1.2481e+02, 1.2337e+02,
        1.2193e+02, 1.2016e+02, 1.1989e+02, 1.1863e+02, 1.1794e+02, 1.1716e+02,
        1.1599e+02, 1.1546e+02, 1.1403e+02, 1.1350e+02, 1.1332e+02, 1.1256e+02,
        1.1118e+02, 1.1019e+02, 1.0970e+02, 1.0834e+02, 1.0755e+02, 1.0672e+02,
        1.0621e+02, 1.0608e+02, 1.0403e+02, 1.0309e+02, 1.0277e+02, 1.0221e+02,
        1.0081e+02, 9.9931e+01, 9.9633e+01, 9.9186e+01, 9.8433e+01, 9.7027e+01,
        9.6961e+01, 9.6431e+01, 9.5460e+01, 9.4566e+01, 9.3548e+01, 9.2957e+01,
        9.2608e+01, 9.1918e+01, 9.0940e+01, 9.0431e+01, 9.0221e+01, 8.9575e+01,
        8.8680e+01, 8.8142e+01, 8.7753e+01, 8.6703e+01, 8.6418e+01, 8.6121e+01,
        8.5224e+01, 8.4225e+01, 8.4194e+01, 8.3926e+01, 8.3311e+01, 8.2730e+01,
        8.2349e+01, 8.1912e+01, 8.0200e+01, 8.0064e+01, 7.9147e+01, 7.8767e+01,
        7.8265e+01, 7.7444e+01, 7.7086e+01, 7.5955e+01, 7.5612e+01, 7.5489e+01,
        7.4948e+01, 7.4593e+01, 7.4202e+01, 7.3713e+01, 7.3316e+01, 7.2662e+01,
        7.2035e+01, 7.1773e+01, 7.1547e+01, 7.1069e+01, 7.0178e+01, 6.9476e+01,
        6.8814e+01, 6.8207e+01, 6.7860e+01, 6.7498e+01, 6.7032e+01, 6.6503e+01,
        6.6048e+01, 6.5617e+01, 6.5317e+01, 6.4670e+01, 6.4002e+01, 6.3548e+01,
        6.3194e+01, 6.2394e+01, 6.2198e+01, 6.1983e+01, 6.1559e+01, 6.1313e+01,
        6.0591e+01, 6.0172e+01, 5.9484e+01, 5.9023e+01, 5.8410e+01, 5.7803e+01,
        5.7513e+01, 5.7070e+01, 5.6766e+01, 5.6297e+01, 5.5805e+01, 5.5336e+01,
        5.4965e+01, 5.4632e+01, 5.4467e+01, 5.4054e+01, 5.3895e+01, 5.3353e+01,
        5.2465e+01, 5.2282e+01, 5.2060e+01, 5.1453e+01, 5.0970e+01, 5.0605e+01,
        5.0197e+01, 4.9591e+01, 4.9249e+01, 4.9000e+01, 4.8773e+01, 4.8266e+01,
        4.7770e+01, 4.7616e+01, 4.7302e+01, 4.7208e+01, 4.6470e+01, 4.6068e+01,
        4.5796e+01, 4.5367e+01, 4.5156e+01, 4.4964e+01, 4.4185e+01, 4.3777e+01,
        4.3564e+01, 4.2986e+01, 4.2644e+01, 4.2340e+01, 4.1942e+01, 4.1638e+01,
        4.1366e+01, 4.0726e+01, 4.0398e+01, 3.9918e+01, 3.9664e+01, 3.9393e+01,
        3.8960e+01, 3.8694e+01, 3.8476e+01, 3.8218e+01, 3.7784e+01, 3.7191e+01,
        3.7107e+01, 3.6813e+01, 3.6267e+01, 3.5813e+01, 3.5200e+01, 3.5016e+01,
        3.4863e+01, 3.4462e+01, 3.4262e+01, 3.3856e+01, 3.3683e+01, 3.3359e+01,
        3.3294e+01, 3.2776e+01, 3.2313e+01, 3.2082e+01, 3.1677e+01, 3.1659e+01,
        3.1338e+01, 3.0756e+01, 3.0283e+01, 2.9969e+01, 2.9537e+01, 2.9279e+01,
        2.9118e+01, 2.8690e+01, 2.8552e+01, 2.7887e+01, 2.7804e+01, 2.7632e+01,
        2.6845e+01, 2.6697e+01, 2.6434e+01, 2.6046e+01, 2.5667e+01, 2.5185e+01,
        2.4571e+01, 2.4309e+01, 2.4119e+01, 2.3214e+01, 2.2891e+01, 2.2642e+01,
        2.2358e+01, 2.2130e+01, 2.1821e+01, 2.1526e+01, 2.1349e+01, 2.0876e+01,
        2.0816e+01, 2.0097e+01, 1.9800e+01, 1.9096e+01, 1.8873e+01, 1.8671e+01,
        1.8238e+01, 1.8175e+01, 1.7787e+01, 1.7331e+01, 1.6724e+01, 1.6169e+01,
        1.5802e+01, 1.5296e+01, 1.4581e+01, 1.3835e+01, 1.3492e+01, 1.2847e+01,
        1.2300e+01, 1.1699e+01, 1.1423e+01, 1.1215e+01, 1.0759e+01, 1.0348e+01,
        1.0274e+01, 9.9678e+00, 9.2409e+00, 8.8135e+00, 7.7629e+00, 7.4457e+00,
        6.9900e+00, 5.5775e+00, 4.9447e+00, 4.6664e+00, 4.1955e+00, 3.3155e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 71]) 

NULL SPACE BASIS :  tensor([[ 0.0042,  0.0136, -0.0130,  ...,  0.0136, -0.0052,  0.0058],
        [ 0.0199,  0.0365,  0.0576,  ..., -0.0046,  0.0029, -0.0054],
        [ 0.0055, -0.0741, -0.0562,  ..., -0.0038, -0.0014,  0.0022],
        ...,
        [ 0.0392, -0.0189,  0.0061,  ...,  0.0032,  0.0003, -0.0022],
        [ 0.0032,  0.0327, -0.0091,  ..., -0.0026,  0.0002,  0.0023],
        [-0.0049, -0.0066,  0.0029,  ..., -0.0034, -0.0009,  0.0031]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.5457e-03, -6.7963e-03,  1.5967e-03,  ..., -2.4986e-04,
          8.2360e-04, -4.3498e-04],
        [-6.7963e-03,  1.3953e-02, -8.0757e-03,  ...,  1.0026e-03,
         -4.7559e-05, -3.9757e-04],
        [ 1.5967e-03, -8.0757e-03,  8.0293e-03,  ..., -8.6910e-04,
         -1.3867e-04,  7.6341e-04],
        ...,
        [-2.4986e-04,  1.0026e-03, -8.6910e-04,  ...,  1.7855e-03,
         -1.4700e-03,  1.3089e-04],
        [ 8.2360e-04, -4.7559e-05, -1.3867e-04,  ..., -1.4700e-03,
          3.0285e-03, -1.2200e-03],
        [-4.3498e-04, -3.9757e-04,  7.6341e-04,  ...,  1.3089e-04,
         -1.2200e-03,  1.4273e-03]], device='cuda:0') 

reserving basis 186/576; cond: 938259.0, radio:0.0008304673247039318
PARAMETER       :  Parameter containing:
tensor([[[[-0.0138,  0.0039,  0.0141],
          [ 0.0235,  0.0076,  0.0080],
          [-0.0528, -0.0493, -0.0112]],

         [[ 0.0031, -0.0119, -0.0064],
          [ 0.0271, -0.0294,  0.0263],
          [-0.0626, -0.0073, -0.0568]],

         [[-0.0006,  0.0192,  0.0254],
          [-0.0141, -0.0341,  0.0466],
          [-0.0164, -0.0083,  0.0515]],

         ...,

         [[-0.0400,  0.0275,  0.0215],
          [-0.0110,  0.0077, -0.0070],
          [-0.0349,  0.0126,  0.0237]],

         [[-0.0173,  0.0127,  0.0016],
          [-0.0339,  0.0200, -0.0043],
          [ 0.0176, -0.0395,  0.0339]],

         [[ 0.0215, -0.0016,  0.0242],
          [ 0.0104,  0.0108,  0.0409],
          [-0.0295,  0.0201, -0.0058]]],


        [[[ 0.0318,  0.0112, -0.0227],
          [-0.0097,  0.0014,  0.0106],
          [ 0.0038, -0.0462,  0.0082]],

         [[ 0.0341,  0.0250,  0.0226],
          [-0.0202, -0.0089, -0.0241],
          [ 0.0085, -0.0297,  0.0154]],

         [[-0.0309, -0.0145, -0.0276],
          [-0.0037,  0.0131,  0.0100],
          [ 0.0350,  0.0007, -0.0426]],

         ...,

         [[ 0.0166,  0.0329,  0.0059],
          [ 0.0364,  0.0388, -0.0078],
          [-0.0085, -0.0292,  0.0359]],

         [[-0.0056,  0.0129, -0.0182],
          [-0.0052, -0.0200,  0.0152],
          [-0.0409, -0.0229, -0.0348]],

         [[ 0.0353, -0.0173,  0.0099],
          [ 0.0100,  0.0151,  0.0027],
          [ 0.0238, -0.0128,  0.0224]]],


        [[[ 0.0308, -0.0055, -0.0455],
          [ 0.0318,  0.0374,  0.0300],
          [-0.0147, -0.0072,  0.0390]],

         [[ 0.0143,  0.0033,  0.0092],
          [ 0.0189, -0.0287, -0.0029],
          [ 0.0300,  0.0279, -0.0159]],

         [[ 0.0418,  0.0022,  0.0098],
          [ 0.0140, -0.0183,  0.0391],
          [-0.0336, -0.0382,  0.0077]],

         ...,

         [[-0.0002,  0.0308, -0.0125],
          [-0.0160, -0.0451,  0.0063],
          [-0.0103, -0.0253, -0.0253]],

         [[-0.0083,  0.0244, -0.0460],
          [ 0.0332,  0.0239, -0.0224],
          [-0.0274, -0.0249,  0.0117]],

         [[ 0.0376, -0.0033, -0.0089],
          [-0.0257, -0.0258,  0.0091],
          [ 0.0029,  0.0421,  0.0344]]],


        ...,


        [[[ 0.0372, -0.0317, -0.0337],
          [ 0.0359, -0.0350, -0.0472],
          [-0.0158, -0.0215,  0.0143]],

         [[ 0.0383,  0.0163,  0.0285],
          [-0.0416, -0.0429,  0.0031],
          [-0.0480,  0.0246, -0.0388]],

         [[ 0.0131,  0.0451,  0.0055],
          [-0.0449, -0.0424,  0.0212],
          [-0.0213,  0.0177,  0.0273]],

         ...,

         [[-0.0250, -0.0243,  0.0181],
          [ 0.0024,  0.0296,  0.0367],
          [ 0.0341, -0.0489,  0.0122]],

         [[ 0.0008, -0.0230,  0.0332],
          [-0.0381, -0.0053, -0.0034],
          [-0.0083, -0.0441,  0.0060]],

         [[-0.0034, -0.0197, -0.0080],
          [ 0.0313,  0.0485, -0.0031],
          [-0.0030,  0.0313,  0.0076]]],


        [[[-0.0003, -0.0415,  0.0147],
          [-0.0408,  0.0209,  0.0332],
          [ 0.0133,  0.0278, -0.0119]],

         [[ 0.0129, -0.0209, -0.0188],
          [ 0.0071, -0.0444,  0.0016],
          [-0.0218, -0.0201, -0.0053]],

         [[ 0.0269,  0.0250,  0.0154],
          [-0.0049,  0.0060, -0.0200],
          [ 0.0330, -0.0353, -0.0012]],

         ...,

         [[-0.0214,  0.0427,  0.0050],
          [ 0.0249,  0.0237, -0.0309],
          [-0.0043, -0.0387,  0.0146]],

         [[-0.0231, -0.0141, -0.0364],
          [-0.0038,  0.0212,  0.0027],
          [ 0.0094, -0.0307,  0.0330]],

         [[ 0.0100,  0.0433,  0.0127],
          [-0.0027,  0.0222,  0.0184],
          [-0.0128,  0.0048,  0.0226]]],


        [[[ 0.0086, -0.0096, -0.0074],
          [-0.0159,  0.0164, -0.0080],
          [-0.0396, -0.0430,  0.0426]],

         [[-0.0250,  0.0045, -0.0185],
          [-0.0317, -0.0280,  0.0007],
          [-0.0336,  0.0117,  0.0108]],

         [[ 0.0004,  0.0140, -0.0148],
          [ 0.0069, -0.0373, -0.0088],
          [-0.0235,  0.0281, -0.0470]],

         ...,

         [[ 0.0403, -0.0362, -0.0336],
          [-0.0110,  0.0116,  0.0401],
          [-0.0142,  0.0309, -0.0371]],

         [[-0.0125, -0.0311, -0.0101],
          [ 0.0156,  0.0207, -0.0294],
          [ 0.0321, -0.0316,  0.0294]],

         [[-0.0033, -0.0310, -0.0274],
          [-0.0214,  0.0332,  0.0006],
          [-0.0166,  0.0395,  0.0056]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.9362e+07, 1.1410e+06, 9.4589e+05, 8.7049e+05, 8.5979e+05, 6.6035e+05,
        4.5744e+05, 4.1879e+05, 3.7204e+05, 3.3536e+05, 2.4716e+05, 2.4463e+05,
        2.3080e+05, 1.9959e+05, 1.5992e+05, 1.1695e+05, 1.0513e+05, 1.0207e+05,
        8.8772e+04, 7.6685e+04, 7.3635e+04, 5.9467e+04, 5.7245e+04, 5.5679e+04,
        5.1192e+04, 4.6021e+04, 4.4957e+04, 4.1606e+04, 3.7950e+04, 3.3330e+04,
        3.1844e+04, 3.0764e+04, 3.0105e+04, 2.6041e+04, 2.4686e+04, 2.4304e+04,
        2.3210e+04, 2.1968e+04, 2.0255e+04, 1.9103e+04, 1.8507e+04, 1.7310e+04,
        1.6480e+04, 1.5738e+04, 1.5275e+04, 1.4641e+04, 1.4584e+04, 1.3642e+04,
        1.2710e+04, 1.2370e+04, 1.2095e+04, 1.1795e+04, 1.1335e+04, 1.1095e+04,
        1.0927e+04, 1.0040e+04, 9.9448e+03, 9.8514e+03, 9.1623e+03, 8.8371e+03,
        8.6328e+03, 8.5681e+03, 8.3475e+03, 8.0470e+03, 7.8756e+03, 7.5394e+03,
        7.3386e+03, 7.1401e+03, 6.9827e+03, 6.9060e+03, 6.8484e+03, 6.6110e+03,
        6.4813e+03, 6.3545e+03, 6.3170e+03, 6.1452e+03, 6.0414e+03, 5.9114e+03,
        5.8306e+03, 5.7243e+03, 5.5000e+03, 5.3511e+03, 5.2624e+03, 5.1718e+03,
        5.0332e+03, 4.9652e+03, 4.8862e+03, 4.8515e+03, 4.7024e+03, 4.6801e+03,
        4.5835e+03, 4.5351e+03, 4.4450e+03, 4.3158e+03, 4.2878e+03, 4.1973e+03,
        4.1619e+03, 4.0663e+03, 4.0293e+03, 3.9458e+03, 3.9160e+03, 3.8437e+03,
        3.8079e+03, 3.6731e+03, 3.6255e+03, 3.5798e+03, 3.5438e+03, 3.4853e+03,
        3.4371e+03, 3.4019e+03, 3.3229e+03, 3.2791e+03, 3.2651e+03, 3.1759e+03,
        3.1597e+03, 3.0993e+03, 3.0906e+03, 3.0416e+03, 2.9942e+03, 2.9203e+03,
        2.8632e+03, 2.8515e+03, 2.7981e+03, 2.7849e+03, 2.7389e+03, 2.6962e+03,
        2.6680e+03, 2.6278e+03, 2.6141e+03, 2.5662e+03, 2.5374e+03, 2.4780e+03,
        2.4521e+03, 2.4029e+03, 2.3817e+03, 2.3527e+03, 2.3403e+03, 2.3165e+03,
        2.2696e+03, 2.2656e+03, 2.2180e+03, 2.2029e+03, 2.1626e+03, 2.1474e+03,
        2.1197e+03, 2.0632e+03, 2.0252e+03, 2.0157e+03, 2.0018e+03, 1.9923e+03,
        1.9499e+03, 1.9416e+03, 1.9188e+03, 1.8827e+03, 1.8684e+03, 1.8494e+03,
        1.8394e+03, 1.8112e+03, 1.7953e+03, 1.7589e+03, 1.7536e+03, 1.7274e+03,
        1.7071e+03, 1.6958e+03, 1.6708e+03, 1.6378e+03, 1.6327e+03, 1.6219e+03,
        1.6124e+03, 1.5886e+03, 1.5654e+03, 1.5542e+03, 1.5439e+03, 1.5372e+03,
        1.5168e+03, 1.4995e+03, 1.4882e+03, 1.4808e+03, 1.4541e+03, 1.4512e+03,
        1.4362e+03, 1.4204e+03, 1.4107e+03, 1.3964e+03, 1.3751e+03, 1.3684e+03,
        1.3514e+03, 1.3348e+03, 1.3250e+03, 1.3119e+03, 1.3035e+03, 1.2943e+03,
        1.2833e+03, 1.2626e+03, 1.2546e+03, 1.2473e+03, 1.2386e+03, 1.2169e+03,
        1.2080e+03, 1.1962e+03, 1.1941e+03, 1.1832e+03, 1.1720e+03, 1.1660e+03,
        1.1338e+03, 1.1241e+03, 1.1225e+03, 1.1173e+03, 1.0978e+03, 1.0958e+03,
        1.0882e+03, 1.0845e+03, 1.0735e+03, 1.0618e+03, 1.0432e+03, 1.0310e+03,
        1.0225e+03, 1.0122e+03, 1.0064e+03, 1.0032e+03, 9.9824e+02, 9.9016e+02,
        9.8243e+02, 9.7725e+02, 9.5913e+02, 9.5641e+02, 9.5077e+02, 9.4619e+02,
        9.3261e+02, 9.2632e+02, 9.1747e+02, 9.1342e+02, 8.9932e+02, 8.9577e+02,
        8.8768e+02, 8.8688e+02, 8.7606e+02, 8.7314e+02, 8.6186e+02, 8.5760e+02,
        8.5042e+02, 8.4133e+02, 8.3772e+02, 8.3204e+02, 8.2998e+02, 8.1683e+02,
        8.0720e+02, 7.9554e+02, 7.9419e+02, 7.9156e+02, 7.8838e+02, 7.8227e+02,
        7.6733e+02, 7.6624e+02, 7.6136e+02, 7.5536e+02, 7.4861e+02, 7.4609e+02,
        7.4206e+02, 7.3136e+02, 7.2883e+02, 7.2451e+02, 7.2335e+02, 7.1533e+02,
        7.0691e+02, 7.0410e+02, 6.9693e+02, 6.9257e+02, 6.8720e+02, 6.8088e+02,
        6.7489e+02, 6.7315e+02, 6.6843e+02, 6.6384e+02, 6.5757e+02, 6.5017e+02,
        6.4662e+02, 6.4418e+02, 6.3920e+02, 6.3183e+02, 6.2489e+02, 6.2288e+02,
        6.1680e+02, 6.1630e+02, 6.1222e+02, 6.0993e+02, 6.0883e+02, 6.0354e+02,
        6.0041e+02, 5.9071e+02, 5.8462e+02, 5.7981e+02, 5.7832e+02, 5.7265e+02,
        5.7114e+02, 5.6666e+02, 5.6385e+02, 5.5810e+02, 5.5633e+02, 5.5069e+02,
        5.4860e+02, 5.4515e+02, 5.4329e+02, 5.4043e+02, 5.3431e+02, 5.3198e+02,
        5.2857e+02, 5.2666e+02, 5.2141e+02, 5.1667e+02, 5.1122e+02, 5.0823e+02,
        5.0754e+02, 5.0183e+02, 5.0007e+02, 4.9827e+02, 4.9386e+02, 4.8837e+02,
        4.8647e+02, 4.8398e+02, 4.8168e+02, 4.7900e+02, 4.7782e+02, 4.7259e+02,
        4.6938e+02, 4.6851e+02, 4.6487e+02, 4.6349e+02, 4.6131e+02, 4.5574e+02,
        4.5376e+02, 4.5177e+02, 4.4781e+02, 4.4618e+02, 4.4226e+02, 4.3995e+02,
        4.3770e+02, 4.3222e+02, 4.3081e+02, 4.2578e+02, 4.2060e+02, 4.1851e+02,
        4.1456e+02, 4.1342e+02, 4.1234e+02, 4.1154e+02, 4.0951e+02, 4.0605e+02,
        4.0393e+02, 3.9895e+02, 3.9811e+02, 3.9612e+02, 3.9462e+02, 3.9165e+02,
        3.8951e+02, 3.8713e+02, 3.8401e+02, 3.8287e+02, 3.7834e+02, 3.7751e+02,
        3.7479e+02, 3.7196e+02, 3.6751e+02, 3.6688e+02, 3.6582e+02, 3.6314e+02,
        3.6230e+02, 3.5947e+02, 3.5602e+02, 3.5534e+02, 3.5210e+02, 3.5121e+02,
        3.4856e+02, 3.4831e+02, 3.4656e+02, 3.4304e+02, 3.4192e+02, 3.4019e+02,
        3.3866e+02, 3.3542e+02, 3.3327e+02, 3.3255e+02, 3.2907e+02, 3.2736e+02,
        3.2498e+02, 3.2288e+02, 3.2111e+02, 3.1674e+02, 3.1549e+02, 3.1356e+02,
        3.1239e+02, 3.0980e+02, 3.0724e+02, 3.0484e+02, 3.0368e+02, 3.0249e+02,
        2.9863e+02, 2.9817e+02, 2.9495e+02, 2.9362e+02, 2.9127e+02, 2.9029e+02,
        2.8928e+02, 2.8665e+02, 2.8563e+02, 2.8119e+02, 2.7922e+02, 2.7777e+02,
        2.7675e+02, 2.7515e+02, 2.7249e+02, 2.7067e+02, 2.7039e+02, 2.6851e+02,
        2.6700e+02, 2.6444e+02, 2.6218e+02, 2.6098e+02, 2.6016e+02, 2.5895e+02,
        2.5776e+02, 2.5735e+02, 2.5534e+02, 2.5368e+02, 2.5325e+02, 2.4944e+02,
        2.4885e+02, 2.4708e+02, 2.4527e+02, 2.4468e+02, 2.4226e+02, 2.3941e+02,
        2.3874e+02, 2.3697e+02, 2.3505e+02, 2.3389e+02, 2.3219e+02, 2.3091e+02,
        2.3027e+02, 2.2751e+02, 2.2627e+02, 2.2413e+02, 2.2341e+02, 2.2267e+02,
        2.2102e+02, 2.1971e+02, 2.1609e+02, 2.1419e+02, 2.1388e+02, 2.1224e+02,
        2.1130e+02, 2.1042e+02, 2.0951e+02, 2.0729e+02, 2.0619e+02, 2.0578e+02,
        2.0390e+02, 2.0141e+02, 2.0006e+02, 1.9869e+02, 1.9714e+02, 1.9485e+02,
        1.9379e+02, 1.9312e+02, 1.9218e+02, 1.9062e+02, 1.9009e+02, 1.8940e+02,
        1.8759e+02, 1.8643e+02, 1.8522e+02, 1.8314e+02, 1.8148e+02, 1.7989e+02,
        1.7864e+02, 1.7817e+02, 1.7796e+02, 1.7612e+02, 1.7517e+02, 1.7378e+02,
        1.7201e+02, 1.7151e+02, 1.6919e+02, 1.6783e+02, 1.6723e+02, 1.6687e+02,
        1.6539e+02, 1.6465e+02, 1.6164e+02, 1.6039e+02, 1.5969e+02, 1.5893e+02,
        1.5803e+02, 1.5624e+02, 1.5555e+02, 1.5417e+02, 1.5202e+02, 1.5159e+02,
        1.5032e+02, 1.4840e+02, 1.4756e+02, 1.4640e+02, 1.4572e+02, 1.4421e+02,
        1.4326e+02, 1.4243e+02, 1.4075e+02, 1.4043e+02, 1.3904e+02, 1.3882e+02,
        1.3749e+02, 1.3478e+02, 1.3417e+02, 1.3171e+02, 1.3095e+02, 1.2948e+02,
        1.2874e+02, 1.2663e+02, 1.2494e+02, 1.2354e+02, 1.2253e+02, 1.2069e+02,
        1.2009e+02, 1.1880e+02, 1.1789e+02, 1.1764e+02, 1.1558e+02, 1.1374e+02,
        1.1336e+02, 1.1198e+02, 1.1033e+02, 1.0833e+02, 1.0740e+02, 1.0622e+02,
        1.0445e+02, 1.0348e+02, 1.0146e+02, 1.0025e+02, 9.8604e+01, 9.7879e+01,
        9.7387e+01, 9.5855e+01, 9.4202e+01, 9.2602e+01, 9.1760e+01, 9.0108e+01,
        8.9404e+01, 8.8851e+01, 8.7114e+01, 8.6608e+01, 8.4664e+01, 8.3582e+01,
        8.2325e+01, 7.9998e+01, 7.9187e+01, 7.8298e+01, 7.6068e+01, 7.5388e+01,
        7.2736e+01, 7.1608e+01, 7.0463e+01, 6.8474e+01, 6.7267e+01, 6.4916e+01,
        6.2729e+01, 6.0537e+01, 5.8890e+01, 5.5649e+01, 5.5227e+01, 5.2411e+01,
        5.1487e+01, 5.0166e+01, 4.6787e+01, 4.4770e+01, 4.0053e+01, 3.1294e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 186]) 

NULL SPACE BASIS :  tensor([[-0.0513, -0.0255, -0.0104,  ...,  0.0059,  0.0080, -0.0233],
        [-0.0089, -0.0312, -0.0542,  ...,  0.0029,  0.0005,  0.0411],
        [ 0.0454,  0.0092,  0.0531,  ..., -0.0079,  0.0045, -0.0227],
        ...,
        [-0.0268,  0.0889, -0.0617,  ...,  0.0033, -0.1967, -0.0440],
        [-0.0324, -0.0142, -0.0620,  ...,  0.0460,  0.3169,  0.0490],
        [ 0.0160, -0.1061, -0.0309,  ..., -0.0460, -0.1436, -0.0201]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.7141e-02, -1.8230e-02, -6.9881e-03,  ...,  1.1602e-03,
          6.6480e-04,  1.8719e-04],
        [-1.8230e-02,  4.1922e-02, -1.7480e-02,  ..., -5.3217e-04,
         -5.8923e-05,  6.8817e-04],
        [-6.9881e-03, -1.7480e-02,  3.5737e-02,  ..., -1.1435e-03,
         -1.4210e-04,  2.8069e-04],
        ...,
        [ 1.1602e-03, -5.3217e-04, -1.1435e-03,  ...,  4.3831e-02,
         -1.9589e-02, -9.1840e-03],
        [ 6.6480e-04, -5.8923e-05, -1.4210e-04,  ..., -1.9589e-02,
          4.7355e-02, -1.7897e-02],
        [ 1.8719e-04,  6.8817e-04,  2.8069e-04,  ..., -9.1840e-03,
         -1.7897e-02,  4.1388e-02]], device='cuda:0') 

reserving basis 229/576; cond: 1075266.0, radio:0.0008540621493011713
PARAMETER       :  Parameter containing:
tensor([[[[ 1.9418e-02,  3.9952e-02,  3.1569e-02],
          [ 3.2919e-03, -2.7524e-03, -8.9197e-03],
          [-3.5143e-02,  2.8648e-02,  1.6672e-02]],

         [[-4.2719e-02, -1.9655e-02, -2.2073e-02],
          [-4.7701e-02, -4.5864e-02, -2.7721e-02],
          [-2.3376e-02,  5.2071e-03, -5.8709e-03]],

         [[ 1.7617e-02,  2.4152e-02, -5.1119e-02],
          [-1.4819e-02, -2.9364e-02, -1.9072e-02],
          [ 1.0833e-02,  1.0319e-02,  1.9602e-02]],

         ...,

         [[-5.5296e-02,  3.1863e-02,  4.5207e-02],
          [-2.9655e-02, -6.9928e-03, -1.5294e-02],
          [ 2.3799e-02, -2.3309e-02, -3.3663e-02]],

         [[ 1.0897e-02, -2.8663e-02,  3.4783e-02],
          [ 4.0553e-02, -2.5974e-02, -3.3755e-03],
          [-1.8954e-02, -3.2880e-02,  1.9874e-02]],

         [[-2.6714e-02, -2.0953e-02,  6.0952e-03],
          [ 1.7030e-02,  1.4683e-02, -2.4189e-02],
          [-2.5744e-02,  3.6832e-03,  4.1980e-03]]],


        [[[-3.0334e-02, -2.0491e-02, -1.0792e-02],
          [ 3.6914e-02,  2.0455e-02,  2.9215e-02],
          [-1.3644e-02,  4.5823e-02,  1.6148e-03]],

         [[-3.8569e-02, -3.2262e-02, -3.1229e-02],
          [-3.0840e-05, -4.0484e-02, -2.4528e-02],
          [-1.4573e-02,  1.9982e-02,  2.2025e-02]],

         [[-9.0495e-03,  1.4874e-02, -2.3010e-02],
          [-3.5145e-02, -3.0726e-02,  1.4414e-02],
          [ 3.0655e-02,  1.1531e-02, -1.8220e-02]],

         ...,

         [[ 2.4620e-02, -4.6039e-02,  1.7319e-02],
          [-1.2300e-02,  1.2228e-02, -2.0171e-02],
          [ 2.6637e-02,  2.5485e-02, -7.0113e-03]],

         [[ 1.1950e-02, -4.0618e-02, -7.4979e-03],
          [-1.3739e-02, -1.8695e-02, -4.1128e-02],
          [-3.9631e-03,  5.3698e-03, -6.3099e-04]],

         [[-1.2607e-03, -2.7918e-02, -1.0774e-02],
          [-3.0332e-02, -2.7840e-02, -1.5296e-02],
          [ 3.4249e-02,  2.1134e-02, -1.8334e-02]]],


        [[[-4.0262e-02, -8.8827e-03, -2.1405e-03],
          [-5.7852e-02, -4.3560e-02, -3.4628e-02],
          [-1.2950e-03,  3.5142e-02,  3.8541e-02]],

         [[-1.8372e-02, -9.6657e-03, -4.0650e-03],
          [-2.3323e-02, -4.0578e-02, -3.9125e-02],
          [-1.7031e-02, -1.8106e-02, -9.4590e-03]],

         [[ 8.0951e-03, -1.4459e-02,  4.9980e-03],
          [-3.5932e-02,  5.1078e-02, -8.5419e-03],
          [-5.7232e-03,  2.7131e-02,  3.6925e-02]],

         ...,

         [[-4.5235e-02,  2.2929e-02,  3.6791e-02],
          [-1.8603e-02, -4.0090e-02,  3.3558e-02],
          [-1.1327e-02, -2.9609e-02,  1.3045e-02]],

         [[ 2.7044e-02,  1.8370e-02,  4.0003e-02],
          [ 2.3184e-02, -3.0218e-02, -2.2078e-02],
          [-3.4311e-02, -4.9900e-02,  1.9793e-02]],

         [[-2.9347e-02,  5.0757e-02,  3.1902e-02],
          [ 2.0281e-02,  8.5774e-03, -1.1528e-02],
          [-1.5485e-02, -8.1200e-04, -3.7665e-02]]],


        ...,


        [[[ 3.4799e-02,  5.6845e-03, -6.5843e-03],
          [ 3.8745e-02,  4.4934e-02,  1.7957e-03],
          [-1.5032e-02,  1.2182e-02, -3.8104e-02]],

         [[-3.1401e-02,  2.0151e-03, -2.8741e-02],
          [-1.0057e-02,  3.4685e-02,  1.4237e-02],
          [-4.6602e-03, -1.8045e-02,  2.0913e-02]],

         [[-4.2359e-02, -4.1179e-02, -3.6577e-02],
          [-2.7783e-02,  3.6666e-02,  4.2306e-02],
          [-1.1113e-02, -3.3300e-02, -2.7893e-02]],

         ...,

         [[ 3.3970e-02, -6.1207e-03,  3.3741e-02],
          [-1.1629e-02,  4.2887e-02, -1.2297e-02],
          [-3.9380e-02,  2.6155e-02, -2.3751e-02]],

         [[-1.8232e-02, -4.2309e-02, -1.4467e-02],
          [-1.1545e-02,  1.8825e-02, -3.5118e-02],
          [ 2.4479e-02,  2.2382e-02,  1.3373e-02]],

         [[ 2.2023e-02, -1.5633e-02,  1.0198e-02],
          [ 3.2878e-02, -3.9079e-03, -2.6450e-02],
          [-4.0628e-02,  1.4203e-02, -1.1274e-02]]],


        [[[-3.7809e-02,  2.3935e-02, -4.8027e-02],
          [ 3.7530e-03, -4.1826e-02,  2.0343e-02],
          [-1.6644e-02, -6.7915e-03,  3.6782e-02]],

         [[ 1.8039e-02, -3.0507e-02,  7.5182e-03],
          [-8.2445e-03,  2.7096e-02,  5.9269e-03],
          [ 2.4266e-02, -1.2301e-02,  1.6953e-02]],

         [[-3.4094e-02, -4.1046e-02, -2.4350e-02],
          [-2.4329e-02, -1.6829e-02,  1.8215e-02],
          [ 7.0391e-03, -3.3728e-02, -3.7634e-02]],

         ...,

         [[-9.0157e-03, -1.6126e-02, -3.0553e-02],
          [-2.0874e-02,  2.8661e-02, -2.3068e-02],
          [-4.0360e-02,  3.4151e-02,  3.1110e-02]],

         [[-3.0182e-02, -1.0636e-02, -2.9238e-02],
          [ 3.7088e-02,  2.3319e-03, -1.6430e-02],
          [ 2.9665e-02,  3.2304e-02, -2.0463e-02]],

         [[ 1.4604e-02, -2.6723e-02, -7.4444e-03],
          [-4.0504e-02, -3.4599e-02,  3.8554e-02],
          [ 3.9019e-02,  2.2774e-02,  3.8913e-02]]],


        [[[-3.8398e-02,  3.1681e-02, -3.9812e-03],
          [-3.3384e-02,  3.4379e-02, -7.7166e-03],
          [-3.7541e-02, -2.3200e-02,  3.0193e-02]],

         [[-3.3833e-03, -2.5113e-02,  8.5480e-03],
          [ 1.8349e-02,  2.0830e-03, -9.0673e-03],
          [-1.4899e-02, -2.7743e-03,  3.1533e-02]],

         [[-2.2627e-02, -3.7690e-02, -6.3502e-03],
          [-1.5570e-03, -1.8115e-03,  2.9163e-03],
          [ 3.5272e-02, -2.6453e-02,  3.6049e-03]],

         ...,

         [[ 1.2561e-02, -1.6643e-02, -1.9893e-02],
          [-2.0243e-02,  3.0893e-02, -4.4388e-02],
          [-1.9983e-04,  1.5706e-02, -2.9972e-02]],

         [[ 3.5595e-02, -6.5282e-03,  3.1775e-02],
          [ 4.8911e-03, -2.0765e-02, -3.8489e-02],
          [-2.0416e-02,  8.3698e-03, -3.6397e-02]],

         [[ 1.0387e-02,  4.1232e-02, -1.7697e-02],
          [-3.1969e-02,  4.1666e-02,  3.6548e-02],
          [ 2.7099e-02,  1.5240e-02,  7.1446e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.0908e+07, 1.1650e+06, 1.0196e+06, 8.4832e+05, 7.0470e+05, 4.8903e+05,
        4.5057e+05, 3.7601e+05, 3.3868e+05, 2.6898e+05, 2.4771e+05, 2.1914e+05,
        1.8507e+05, 1.4591e+05, 7.7147e+04, 6.9481e+04, 6.4764e+04, 6.1104e+04,
        6.0086e+04, 5.4283e+04, 5.0917e+04, 4.6697e+04, 4.1952e+04, 3.8737e+04,
        3.7891e+04, 3.3172e+04, 3.2313e+04, 3.0410e+04, 2.8945e+04, 2.8032e+04,
        2.5788e+04, 2.3622e+04, 2.3204e+04, 2.1392e+04, 2.0378e+04, 1.8893e+04,
        1.8074e+04, 1.7014e+04, 1.6829e+04, 1.6287e+04, 1.5729e+04, 1.5381e+04,
        1.5088e+04, 1.4541e+04, 1.4033e+04, 1.3168e+04, 1.2886e+04, 1.2375e+04,
        1.2170e+04, 1.1543e+04, 1.0872e+04, 1.0448e+04, 1.0195e+04, 9.5732e+03,
        9.3987e+03, 8.7652e+03, 8.5717e+03, 8.3744e+03, 8.0268e+03, 7.5791e+03,
        7.4614e+03, 7.2798e+03, 7.0785e+03, 7.0593e+03, 6.8016e+03, 6.7055e+03,
        6.5811e+03, 6.3659e+03, 6.2493e+03, 6.2420e+03, 6.0288e+03, 5.8397e+03,
        5.5693e+03, 5.4977e+03, 5.3006e+03, 5.1352e+03, 5.0304e+03, 4.9368e+03,
        4.8582e+03, 4.7419e+03, 4.6728e+03, 4.5888e+03, 4.5774e+03, 4.5137e+03,
        4.2946e+03, 4.2389e+03, 4.1821e+03, 4.0616e+03, 3.9823e+03, 3.9434e+03,
        3.9014e+03, 3.8619e+03, 3.7453e+03, 3.6709e+03, 3.5580e+03, 3.4851e+03,
        3.4658e+03, 3.4045e+03, 3.3182e+03, 3.2827e+03, 3.2574e+03, 3.2197e+03,
        3.0589e+03, 3.0397e+03, 2.9974e+03, 2.9269e+03, 2.9183e+03, 2.8385e+03,
        2.7899e+03, 2.7677e+03, 2.7308e+03, 2.6350e+03, 2.5986e+03, 2.5110e+03,
        2.4892e+03, 2.4799e+03, 2.4466e+03, 2.4157e+03, 2.3541e+03, 2.3214e+03,
        2.3160e+03, 2.2696e+03, 2.2045e+03, 2.1802e+03, 2.1649e+03, 2.1281e+03,
        2.1162e+03, 2.0796e+03, 2.0580e+03, 2.0280e+03, 1.9906e+03, 1.9498e+03,
        1.9257e+03, 1.8954e+03, 1.8603e+03, 1.8356e+03, 1.8065e+03, 1.7970e+03,
        1.7537e+03, 1.7209e+03, 1.6968e+03, 1.6605e+03, 1.6510e+03, 1.6448e+03,
        1.6171e+03, 1.5968e+03, 1.5801e+03, 1.5638e+03, 1.5565e+03, 1.5470e+03,
        1.5164e+03, 1.4885e+03, 1.4611e+03, 1.4494e+03, 1.4322e+03, 1.4190e+03,
        1.4019e+03, 1.3821e+03, 1.3799e+03, 1.3586e+03, 1.3393e+03, 1.3324e+03,
        1.3219e+03, 1.3063e+03, 1.2973e+03, 1.2858e+03, 1.2690e+03, 1.2585e+03,
        1.2223e+03, 1.2183e+03, 1.2068e+03, 1.1975e+03, 1.1846e+03, 1.1672e+03,
        1.1578e+03, 1.1546e+03, 1.1394e+03, 1.1255e+03, 1.1185e+03, 1.1088e+03,
        1.0930e+03, 1.0852e+03, 1.0650e+03, 1.0536e+03, 1.0446e+03, 1.0293e+03,
        1.0246e+03, 1.0096e+03, 1.0058e+03, 1.0049e+03, 9.9770e+02, 9.8330e+02,
        9.6741e+02, 9.5921e+02, 9.4401e+02, 9.3764e+02, 9.2319e+02, 9.1171e+02,
        9.0382e+02, 9.0143e+02, 8.9634e+02, 8.7536e+02, 8.7117e+02, 8.6220e+02,
        8.5836e+02, 8.4721e+02, 8.3961e+02, 8.2686e+02, 8.2354e+02, 8.2075e+02,
        8.0891e+02, 8.0560e+02, 8.0250e+02, 7.9031e+02, 7.8454e+02, 7.7305e+02,
        7.7029e+02, 7.6477e+02, 7.6095e+02, 7.4794e+02, 7.4084e+02, 7.3690e+02,
        7.3171e+02, 7.2576e+02, 7.2228e+02, 7.1154e+02, 7.0843e+02, 6.9828e+02,
        6.9415e+02, 6.9228e+02, 6.8661e+02, 6.7660e+02, 6.6902e+02, 6.6774e+02,
        6.6279e+02, 6.5604e+02, 6.5023e+02, 6.4863e+02, 6.4284e+02, 6.4113e+02,
        6.3839e+02, 6.2923e+02, 6.2153e+02, 6.1890e+02, 6.1009e+02, 6.0244e+02,
        5.9914e+02, 5.8999e+02, 5.8868e+02, 5.8528e+02, 5.8043e+02, 5.7448e+02,
        5.7018e+02, 5.6512e+02, 5.6396e+02, 5.5939e+02, 5.5473e+02, 5.4674e+02,
        5.4401e+02, 5.4035e+02, 5.3811e+02, 5.3344e+02, 5.2878e+02, 5.2439e+02,
        5.2203e+02, 5.1294e+02, 5.0736e+02, 5.0367e+02, 5.0122e+02, 4.9785e+02,
        4.9460e+02, 4.9161e+02, 4.8749e+02, 4.8515e+02, 4.8219e+02, 4.7937e+02,
        4.7711e+02, 4.6979e+02, 4.6382e+02, 4.6338e+02, 4.5957e+02, 4.5719e+02,
        4.5668e+02, 4.5406e+02, 4.5280e+02, 4.4770e+02, 4.4533e+02, 4.4149e+02,
        4.3679e+02, 4.3412e+02, 4.3184e+02, 4.2710e+02, 4.2553e+02, 4.2184e+02,
        4.1835e+02, 4.1753e+02, 4.1529e+02, 4.1324e+02, 4.0874e+02, 4.0473e+02,
        4.0334e+02, 3.9714e+02, 3.9398e+02, 3.9175e+02, 3.8941e+02, 3.8692e+02,
        3.8443e+02, 3.8384e+02, 3.7891e+02, 3.7533e+02, 3.7435e+02, 3.7163e+02,
        3.6792e+02, 3.6679e+02, 3.6658e+02, 3.6255e+02, 3.5886e+02, 3.5602e+02,
        3.5474e+02, 3.5357e+02, 3.4712e+02, 3.4503e+02, 3.4276e+02, 3.3854e+02,
        3.3567e+02, 3.3465e+02, 3.3143e+02, 3.2908e+02, 3.2739e+02, 3.2627e+02,
        3.2359e+02, 3.2159e+02, 3.2017e+02, 3.1893e+02, 3.1388e+02, 3.1142e+02,
        3.0912e+02, 3.0840e+02, 3.0584e+02, 3.0356e+02, 3.0070e+02, 2.9835e+02,
        2.9619e+02, 2.9549e+02, 2.9396e+02, 2.9350e+02, 2.9220e+02, 2.8743e+02,
        2.8551e+02, 2.8387e+02, 2.8280e+02, 2.8058e+02, 2.7959e+02, 2.7644e+02,
        2.7546e+02, 2.7414e+02, 2.7282e+02, 2.7005e+02, 2.6920e+02, 2.6888e+02,
        2.6602e+02, 2.6289e+02, 2.6157e+02, 2.5995e+02, 2.5847e+02, 2.5562e+02,
        2.5451e+02, 2.5295e+02, 2.4982e+02, 2.4788e+02, 2.4658e+02, 2.4483e+02,
        2.4373e+02, 2.4153e+02, 2.3986e+02, 2.3816e+02, 2.3786e+02, 2.3551e+02,
        2.3446e+02, 2.3343e+02, 2.3204e+02, 2.3026e+02, 2.2933e+02, 2.2841e+02,
        2.2541e+02, 2.2435e+02, 2.2258e+02, 2.2035e+02, 2.1939e+02, 2.1859e+02,
        2.1764e+02, 2.1513e+02, 2.1432e+02, 2.1269e+02, 2.1092e+02, 2.1041e+02,
        2.0942e+02, 2.0703e+02, 2.0640e+02, 2.0499e+02, 2.0311e+02, 2.0284e+02,
        2.0181e+02, 2.0088e+02, 1.9835e+02, 1.9811e+02, 1.9661e+02, 1.9564e+02,
        1.9359e+02, 1.9266e+02, 1.9237e+02, 1.9150e+02, 1.8961e+02, 1.8853e+02,
        1.8700e+02, 1.8590e+02, 1.8497e+02, 1.8469e+02, 1.8234e+02, 1.8122e+02,
        1.8024e+02, 1.7699e+02, 1.7679e+02, 1.7554e+02, 1.7431e+02, 1.7292e+02,
        1.7155e+02, 1.7132e+02, 1.7022e+02, 1.6924e+02, 1.6863e+02, 1.6700e+02,
        1.6658e+02, 1.6558e+02, 1.6470e+02, 1.6284e+02, 1.6156e+02, 1.6083e+02,
        1.5913e+02, 1.5867e+02, 1.5745e+02, 1.5693e+02, 1.5620e+02, 1.5553e+02,
        1.5525e+02, 1.5325e+02, 1.5151e+02, 1.5007e+02, 1.4973e+02, 1.4901e+02,
        1.4834e+02, 1.4790e+02, 1.4638e+02, 1.4565e+02, 1.4413e+02, 1.4341e+02,
        1.4252e+02, 1.4176e+02, 1.3960e+02, 1.3862e+02, 1.3792e+02, 1.3708e+02,
        1.3613e+02, 1.3475e+02, 1.3343e+02, 1.3192e+02, 1.3123e+02, 1.3050e+02,
        1.3031e+02, 1.2813e+02, 1.2795e+02, 1.2717e+02, 1.2601e+02, 1.2548e+02,
        1.2461e+02, 1.2315e+02, 1.2307e+02, 1.2189e+02, 1.2028e+02, 1.1967e+02,
        1.1899e+02, 1.1851e+02, 1.1775e+02, 1.1673e+02, 1.1574e+02, 1.1494e+02,
        1.1392e+02, 1.1348e+02, 1.1327e+02, 1.1192e+02, 1.1082e+02, 1.1012e+02,
        1.0879e+02, 1.0847e+02, 1.0823e+02, 1.0666e+02, 1.0613e+02, 1.0460e+02,
        1.0395e+02, 1.0266e+02, 1.0225e+02, 1.0101e+02, 1.0046e+02, 9.9683e+01,
        9.8916e+01, 9.7917e+01, 9.7369e+01, 9.5826e+01, 9.5358e+01, 9.4566e+01,
        9.3257e+01, 9.2783e+01, 9.2024e+01, 9.1026e+01, 9.0356e+01, 8.9422e+01,
        8.9099e+01, 8.8200e+01, 8.7302e+01, 8.7007e+01, 8.6007e+01, 8.4838e+01,
        8.3591e+01, 8.2910e+01, 8.1630e+01, 8.1259e+01, 8.0875e+01, 8.0075e+01,
        7.9420e+01, 7.8583e+01, 7.8443e+01, 7.7590e+01, 7.6948e+01, 7.5525e+01,
        7.4753e+01, 7.3766e+01, 7.2733e+01, 7.1729e+01, 7.1452e+01, 7.0186e+01,
        6.9948e+01, 6.9153e+01, 6.8594e+01, 6.7803e+01, 6.6626e+01, 6.5573e+01,
        6.5218e+01, 6.4863e+01, 6.4529e+01, 6.3923e+01, 6.3483e+01, 6.2122e+01,
        6.1563e+01, 6.1114e+01, 5.9712e+01, 5.9078e+01, 5.8394e+01, 5.7347e+01,
        5.6997e+01, 5.5170e+01, 5.4616e+01, 5.2628e+01, 5.0707e+01, 5.0036e+01,
        4.9123e+01, 4.7788e+01, 4.6128e+01, 4.4780e+01, 4.3740e+01, 4.2918e+01,
        4.1263e+01, 4.0337e+01, 3.7813e+01, 3.7113e+01, 3.2217e+01, 2.8744e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 229]) 

NULL SPACE BASIS :  tensor([[ 0.0006,  0.1097,  0.0144,  ..., -0.0026, -0.0104, -0.0181],
        [-0.0142, -0.0478, -0.0148,  ...,  0.0031,  0.0156,  0.0248],
        [ 0.0294, -0.0094, -0.1312,  ..., -0.0030, -0.0047, -0.0084],
        ...,
        [ 0.0421, -0.0095, -0.0272,  ..., -0.0007,  0.0142, -0.0073],
        [-0.0165, -0.0360, -0.0340,  ...,  0.0174, -0.0118,  0.0098],
        [ 0.0253,  0.0230, -0.0600,  ..., -0.0209,  0.0100, -0.0020]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0231, -0.0170, -0.0011,  ...,  0.0002, -0.0003,  0.0002],
        [-0.0170,  0.0334, -0.0136,  ..., -0.0015,  0.0005, -0.0018],
        [-0.0011, -0.0136,  0.0192,  ...,  0.0004, -0.0011,  0.0008],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ...,  0.0199, -0.0099, -0.0023],
        [-0.0003,  0.0005, -0.0011,  ..., -0.0099,  0.0261, -0.0098],
        [ 0.0002, -0.0018,  0.0008,  ..., -0.0023, -0.0098,  0.0223]],
       device='cuda:0') 

reserving basis 319/576; cond: 260413.265625, radio:0.004436790477484465
PARAMETER       :  Parameter containing:
tensor([[[[-0.0273, -0.0046,  0.0190],
          [-0.0238, -0.0265, -0.0126],
          [ 0.0115,  0.0119,  0.0080]],

         [[ 0.0007,  0.0151,  0.0051],
          [-0.0047,  0.0013,  0.0188],
          [-0.0195, -0.0163, -0.0468]],

         [[ 0.0024,  0.0425,  0.0428],
          [-0.0030, -0.0373, -0.0043],
          [-0.0349, -0.0042, -0.0241]],

         ...,

         [[-0.0192,  0.0443,  0.0028],
          [-0.0264, -0.0085,  0.0416],
          [-0.0143, -0.0383, -0.0169]],

         [[ 0.0295,  0.0162, -0.0442],
          [ 0.0284,  0.0059,  0.0462],
          [-0.0332,  0.0215, -0.0217]],

         [[ 0.0348,  0.0224,  0.0214],
          [-0.0360, -0.0093,  0.0258],
          [ 0.0308, -0.0154, -0.0271]]],


        [[[-0.0273, -0.0238, -0.0471],
          [ 0.0096, -0.0097,  0.0086],
          [-0.0519,  0.0070,  0.0016]],

         [[-0.0325,  0.0181,  0.0513],
          [ 0.0028,  0.0227, -0.0124],
          [-0.0135,  0.0297,  0.0470]],

         [[-0.0015, -0.0171,  0.0300],
          [-0.0005, -0.0319,  0.0365],
          [ 0.0345,  0.0333, -0.0027]],

         ...,

         [[-0.0070,  0.0207, -0.0224],
          [-0.0269,  0.0230,  0.0147],
          [-0.0392,  0.0277, -0.0440]],

         [[-0.0084,  0.0218, -0.0224],
          [-0.0187, -0.0296, -0.0363],
          [-0.0449, -0.0107, -0.0385]],

         [[ 0.0227, -0.0381, -0.0352],
          [-0.0206, -0.0344, -0.0162],
          [-0.0229,  0.0010,  0.0097]]],


        [[[-0.0056, -0.0059, -0.0163],
          [-0.0005, -0.0448, -0.0392],
          [-0.0407, -0.0011, -0.0128]],

         [[-0.0253, -0.0231, -0.0195],
          [ 0.0010, -0.0269, -0.0122],
          [ 0.0162, -0.0262,  0.0008]],

         [[ 0.0479,  0.0409,  0.0021],
          [ 0.0291,  0.0042, -0.0396],
          [ 0.0117,  0.0081, -0.0014]],

         ...,

         [[ 0.0139, -0.0137, -0.0023],
          [ 0.0148,  0.0163, -0.0407],
          [-0.0340,  0.0095, -0.0130]],

         [[ 0.0116,  0.0063, -0.0252],
          [-0.0279,  0.0096,  0.0254],
          [ 0.0049, -0.0226,  0.0007]],

         [[ 0.0175, -0.0120, -0.0318],
          [ 0.0113, -0.0334, -0.0335],
          [ 0.0314, -0.0155,  0.0011]]],


        ...,


        [[[ 0.0130,  0.0323,  0.0145],
          [-0.0016, -0.0157, -0.0188],
          [ 0.0505,  0.0230, -0.0544]],

         [[ 0.0042, -0.0265,  0.0029],
          [-0.0303,  0.0251, -0.0200],
          [ 0.0187, -0.0251,  0.0439]],

         [[ 0.0252, -0.0196,  0.0434],
          [ 0.0323, -0.0387, -0.0394],
          [ 0.0169, -0.0128, -0.0037]],

         ...,

         [[-0.0169, -0.0143,  0.0034],
          [ 0.0134,  0.0087,  0.0144],
          [ 0.0029, -0.0113,  0.0038]],

         [[ 0.0148,  0.0218, -0.0467],
          [-0.0301, -0.0202, -0.0035],
          [ 0.0096,  0.0086,  0.0156]],

         [[ 0.0304,  0.0064,  0.0289],
          [ 0.0160,  0.0436,  0.0023],
          [ 0.0391, -0.0097,  0.0200]]],


        [[[-0.0122,  0.0197,  0.0350],
          [ 0.0163,  0.0243,  0.0073],
          [-0.0030,  0.0285, -0.0510]],

         [[ 0.0350, -0.0143, -0.0268],
          [-0.0375,  0.0055, -0.0356],
          [-0.0394, -0.0036,  0.0409]],

         [[ 0.0206, -0.0418, -0.0241],
          [-0.0360,  0.0277, -0.0173],
          [-0.0355,  0.0148, -0.0040]],

         ...,

         [[-0.0180, -0.0286,  0.0144],
          [ 0.0124,  0.0380,  0.0089],
          [-0.0201, -0.0333, -0.0059]],

         [[-0.0201,  0.0374, -0.0248],
          [ 0.0061, -0.0158,  0.0391],
          [ 0.0459, -0.0019, -0.0096]],

         [[-0.0227,  0.0056,  0.0290],
          [-0.0334, -0.0225,  0.0099],
          [ 0.0104,  0.0107, -0.0027]]],


        [[[ 0.0380,  0.0439,  0.0004],
          [ 0.0200, -0.0113,  0.0251],
          [ 0.0061, -0.0348, -0.0509]],

         [[ 0.0461, -0.0056,  0.0333],
          [ 0.0191,  0.0339,  0.0089],
          [-0.0005, -0.0087,  0.0369]],

         [[-0.0238,  0.0003, -0.0419],
          [-0.0038, -0.0078,  0.0408],
          [ 0.0207, -0.0171,  0.0273]],

         ...,

         [[-0.0080,  0.0378, -0.0333],
          [-0.0096, -0.0384,  0.0272],
          [-0.0377,  0.0066, -0.0002]],

         [[ 0.0027, -0.0364, -0.0158],
          [-0.0369,  0.0260, -0.0211],
          [-0.0234, -0.0400,  0.0073]],

         [[ 0.0422, -0.0057,  0.0356],
          [ 0.0341, -0.0146,  0.0243],
          [ 0.0003,  0.0254,  0.0242]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.5715e+07, 9.8831e+05, 8.5183e+05, 7.3920e+05, 6.5115e+05, 4.4201e+05,
        4.2880e+05, 4.0101e+05, 3.5523e+05, 3.1225e+05, 2.7414e+05, 2.6098e+05,
        2.2233e+05, 1.7244e+05, 1.3407e+05, 1.1245e+05, 9.9867e+04, 8.6471e+04,
        7.9164e+04, 7.3391e+04, 6.8578e+04, 6.6048e+04, 6.3400e+04, 5.7591e+04,
        5.3361e+04, 5.1488e+04, 4.9437e+04, 4.6218e+04, 4.5306e+04, 4.0513e+04,
        3.9192e+04, 3.7308e+04, 3.4528e+04, 3.2961e+04, 3.0128e+04, 2.9185e+04,
        2.7740e+04, 2.6929e+04, 2.5331e+04, 2.4440e+04, 2.3871e+04, 2.2575e+04,
        2.2438e+04, 2.0474e+04, 2.0047e+04, 1.9624e+04, 1.8465e+04, 1.8085e+04,
        1.7208e+04, 1.7111e+04, 1.6268e+04, 1.5998e+04, 1.4925e+04, 1.4751e+04,
        1.4482e+04, 1.3991e+04, 1.3818e+04, 1.3520e+04, 1.3009e+04, 1.2616e+04,
        1.2530e+04, 1.2108e+04, 1.1835e+04, 1.1492e+04, 1.1169e+04, 1.0907e+04,
        1.0442e+04, 1.0300e+04, 1.0275e+04, 1.0022e+04, 9.8421e+03, 9.4922e+03,
        9.4063e+03, 9.2570e+03, 9.0288e+03, 8.9257e+03, 8.7803e+03, 8.6009e+03,
        8.4282e+03, 8.3247e+03, 8.0867e+03, 7.9329e+03, 7.7841e+03, 7.6743e+03,
        7.5921e+03, 7.3899e+03, 7.2737e+03, 7.1261e+03, 6.9743e+03, 6.9359e+03,
        6.7289e+03, 6.6072e+03, 6.5580e+03, 6.5108e+03, 6.2914e+03, 6.2552e+03,
        6.0246e+03, 6.0078e+03, 5.8697e+03, 5.8209e+03, 5.7658e+03, 5.7479e+03,
        5.6438e+03, 5.5504e+03, 5.4786e+03, 5.3568e+03, 5.3074e+03, 5.2426e+03,
        5.1827e+03, 5.0766e+03, 4.9484e+03, 4.9073e+03, 4.8479e+03, 4.7820e+03,
        4.7151e+03, 4.6516e+03, 4.6440e+03, 4.5825e+03, 4.5022e+03, 4.4704e+03,
        4.3986e+03, 4.3629e+03, 4.2928e+03, 4.2822e+03, 4.2025e+03, 4.1663e+03,
        4.1505e+03, 4.1059e+03, 4.0339e+03, 3.9773e+03, 3.9241e+03, 3.8885e+03,
        3.8794e+03, 3.7954e+03, 3.7303e+03, 3.7041e+03, 3.6408e+03, 3.6021e+03,
        3.5890e+03, 3.5275e+03, 3.4842e+03, 3.4627e+03, 3.4383e+03, 3.4082e+03,
        3.3794e+03, 3.3519e+03, 3.2921e+03, 3.2695e+03, 3.2475e+03, 3.2108e+03,
        3.1733e+03, 3.1531e+03, 3.1154e+03, 3.0217e+03, 3.0003e+03, 2.9882e+03,
        2.9747e+03, 2.9353e+03, 2.9228e+03, 2.9051e+03, 2.8701e+03, 2.8422e+03,
        2.8229e+03, 2.7912e+03, 2.7857e+03, 2.7812e+03, 2.7150e+03, 2.7029e+03,
        2.6524e+03, 2.6218e+03, 2.6082e+03, 2.6055e+03, 2.5722e+03, 2.5487e+03,
        2.5413e+03, 2.5175e+03, 2.5140e+03, 2.4940e+03, 2.4671e+03, 2.4484e+03,
        2.4160e+03, 2.4131e+03, 2.3851e+03, 2.3647e+03, 2.3490e+03, 2.3227e+03,
        2.3169e+03, 2.3031e+03, 2.2614e+03, 2.2497e+03, 2.2176e+03, 2.2034e+03,
        2.1964e+03, 2.1774e+03, 2.1503e+03, 2.1332e+03, 2.1302e+03, 2.1013e+03,
        2.0709e+03, 2.0667e+03, 2.0631e+03, 2.0364e+03, 2.0320e+03, 2.0170e+03,
        2.0102e+03, 1.9773e+03, 1.9687e+03, 1.9523e+03, 1.9430e+03, 1.9033e+03,
        1.8976e+03, 1.8773e+03, 1.8594e+03, 1.8503e+03, 1.8262e+03, 1.8181e+03,
        1.8077e+03, 1.7987e+03, 1.7858e+03, 1.7670e+03, 1.7591e+03, 1.7504e+03,
        1.7363e+03, 1.7160e+03, 1.7103e+03, 1.7035e+03, 1.6763e+03, 1.6667e+03,
        1.6572e+03, 1.6475e+03, 1.6348e+03, 1.6211e+03, 1.6139e+03, 1.6029e+03,
        1.5912e+03, 1.5874e+03, 1.5781e+03, 1.5629e+03, 1.5494e+03, 1.5412e+03,
        1.5296e+03, 1.5182e+03, 1.5162e+03, 1.5015e+03, 1.4932e+03, 1.4818e+03,
        1.4601e+03, 1.4526e+03, 1.4465e+03, 1.4362e+03, 1.4325e+03, 1.4217e+03,
        1.4135e+03, 1.4016e+03, 1.3934e+03, 1.3882e+03, 1.3741e+03, 1.3658e+03,
        1.3577e+03, 1.3464e+03, 1.3380e+03, 1.3323e+03, 1.3291e+03, 1.3217e+03,
        1.3103e+03, 1.2959e+03, 1.2866e+03, 1.2830e+03, 1.2789e+03, 1.2666e+03,
        1.2523e+03, 1.2378e+03, 1.2304e+03, 1.2280e+03, 1.2230e+03, 1.2164e+03,
        1.2058e+03, 1.2007e+03, 1.1938e+03, 1.1885e+03, 1.1820e+03, 1.1769e+03,
        1.1674e+03, 1.1648e+03, 1.1464e+03, 1.1451e+03, 1.1349e+03, 1.1268e+03,
        1.1184e+03, 1.1164e+03, 1.1123e+03, 1.1010e+03, 1.0946e+03, 1.0928e+03,
        1.0893e+03, 1.0850e+03, 1.0783e+03, 1.0703e+03, 1.0618e+03, 1.0539e+03,
        1.0414e+03, 1.0348e+03, 1.0324e+03, 1.0246e+03, 1.0173e+03, 1.0113e+03,
        1.0041e+03, 9.9899e+02, 9.9205e+02, 9.8735e+02, 9.8431e+02, 9.8035e+02,
        9.7771e+02, 9.7408e+02, 9.7105e+02, 9.6492e+02, 9.5202e+02, 9.5046e+02,
        9.3916e+02, 9.3773e+02, 9.2830e+02, 9.2408e+02, 9.2112e+02, 9.1555e+02,
        9.1169e+02, 9.0807e+02, 9.0111e+02, 8.9942e+02, 8.9271e+02, 8.9144e+02,
        8.8413e+02, 8.7887e+02, 8.7033e+02, 8.6662e+02, 8.6398e+02, 8.5922e+02,
        8.5464e+02, 8.4803e+02, 8.4456e+02, 8.4184e+02, 8.4100e+02, 8.3218e+02,
        8.2701e+02, 8.1948e+02, 8.1500e+02, 8.1327e+02, 8.1168e+02, 8.0694e+02,
        8.0225e+02, 7.9779e+02, 7.9665e+02, 7.9106e+02, 7.8803e+02, 7.8605e+02,
        7.7983e+02, 7.7698e+02, 7.6288e+02, 7.6068e+02, 7.5760e+02, 7.5118e+02,
        7.4790e+02, 7.4435e+02, 7.3812e+02, 7.3602e+02, 7.3433e+02, 7.3115e+02,
        7.2669e+02, 7.2022e+02, 7.1981e+02, 7.1343e+02, 7.1070e+02, 7.0605e+02,
        7.0066e+02, 6.9787e+02, 6.9390e+02, 6.9118e+02, 6.8930e+02, 6.8461e+02,
        6.8222e+02, 6.8146e+02, 6.7683e+02, 6.7415e+02, 6.6554e+02, 6.6401e+02,
        6.6201e+02, 6.5839e+02, 6.5790e+02, 6.5252e+02, 6.4898e+02, 6.4324e+02,
        6.4021e+02, 6.3682e+02, 6.3551e+02, 6.3130e+02, 6.2822e+02, 6.2346e+02,
        6.1620e+02, 6.1346e+02, 6.1009e+02, 6.0827e+02, 6.0563e+02, 6.0451e+02,
        6.0044e+02, 5.9753e+02, 5.9709e+02, 5.9149e+02, 5.8812e+02, 5.8354e+02,
        5.8058e+02, 5.7895e+02, 5.7534e+02, 5.7324e+02, 5.6946e+02, 5.6595e+02,
        5.6584e+02, 5.6217e+02, 5.5619e+02, 5.5386e+02, 5.5236e+02, 5.5129e+02,
        5.4752e+02, 5.4261e+02, 5.3823e+02, 5.3551e+02, 5.3399e+02, 5.2963e+02,
        5.2831e+02, 5.2482e+02, 5.2321e+02, 5.1972e+02, 5.1730e+02, 5.1356e+02,
        5.1295e+02, 5.0898e+02, 5.0357e+02, 4.9929e+02, 4.9851e+02, 4.9651e+02,
        4.9453e+02, 4.9120e+02, 4.8828e+02, 4.8640e+02, 4.8391e+02, 4.7922e+02,
        4.7632e+02, 4.7421e+02, 4.7378e+02, 4.6988e+02, 4.6656e+02, 4.6485e+02,
        4.6075e+02, 4.5901e+02, 4.5632e+02, 4.5236e+02, 4.5198e+02, 4.5049e+02,
        4.4886e+02, 4.4382e+02, 4.4312e+02, 4.4108e+02, 4.3738e+02, 4.3543e+02,
        4.3389e+02, 4.2939e+02, 4.2719e+02, 4.2501e+02, 4.2061e+02, 4.2042e+02,
        4.1689e+02, 4.1547e+02, 4.1302e+02, 4.0979e+02, 4.0922e+02, 4.0454e+02,
        3.9916e+02, 3.9893e+02, 3.9693e+02, 3.9494e+02, 3.9051e+02, 3.8937e+02,
        3.8772e+02, 3.8615e+02, 3.8302e+02, 3.8142e+02, 3.7513e+02, 3.7396e+02,
        3.7009e+02, 3.6977e+02, 3.6707e+02, 3.6656e+02, 3.6374e+02, 3.5930e+02,
        3.5736e+02, 3.5340e+02, 3.5194e+02, 3.5103e+02, 3.4884e+02, 3.4650e+02,
        3.4441e+02, 3.3792e+02, 3.3705e+02, 3.3618e+02, 3.3451e+02, 3.3320e+02,
        3.2975e+02, 3.2763e+02, 3.2730e+02, 3.2413e+02, 3.2005e+02, 3.1739e+02,
        3.1519e+02, 3.1343e+02, 3.1154e+02, 3.1050e+02, 3.0628e+02, 3.0501e+02,
        3.0290e+02, 2.9988e+02, 2.9886e+02, 2.9527e+02, 2.9177e+02, 2.8819e+02,
        2.8652e+02, 2.8500e+02, 2.8115e+02, 2.7697e+02, 2.7591e+02, 2.7409e+02,
        2.7112e+02, 2.6861e+02, 2.6770e+02, 2.6598e+02, 2.6418e+02, 2.6159e+02,
        2.6060e+02, 2.5688e+02, 2.5572e+02, 2.5351e+02, 2.5175e+02, 2.4849e+02,
        2.4510e+02, 2.4301e+02, 2.4000e+02, 2.3868e+02, 2.3609e+02, 2.3410e+02,
        2.3315e+02, 2.3132e+02, 2.2976e+02, 2.2723e+02, 2.2260e+02, 2.2191e+02,
        2.1782e+02, 2.1508e+02, 2.1102e+02, 2.0924e+02, 2.0431e+02, 2.0403e+02,
        2.0065e+02, 1.9926e+02, 1.9501e+02, 1.9272e+02, 1.8927e+02, 1.8797e+02,
        1.8178e+02, 1.8102e+02, 1.7663e+02, 1.6928e+02, 1.6857e+02, 1.6384e+02,
        1.5960e+02, 1.5795e+02, 1.5353e+02, 1.4960e+02, 1.4291e+02, 1.3715e+02],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 319]) 

NULL SPACE BASIS :  tensor([[-0.0201,  0.0514,  0.1512,  ...,  0.0134,  0.0037, -0.0010],
        [-0.0007,  0.0474,  0.0056,  ..., -0.0245, -0.0136, -0.0153],
        [-0.0004, -0.0512, -0.0144,  ...,  0.0123,  0.0048,  0.0100],
        ...,
        [-0.0362,  0.0258,  0.0029,  ...,  0.0136,  0.0005, -0.0307],
        [ 0.0063, -0.0106, -0.0075,  ..., -0.0257, -0.0017,  0.0552],
        [-0.0251,  0.0457,  0.0015,  ...,  0.0100,  0.0045, -0.0201]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.8170e-02, -8.9328e-03, -2.3374e-03,  ...,  1.2392e-03,
         -2.8110e-04,  1.5345e-04],
        [-8.9328e-03,  3.2014e-02, -9.4023e-03,  ..., -7.5861e-04,
         -4.1522e-04, -1.3206e-05],
        [-2.3374e-03, -9.4023e-03,  2.4279e-02,  ...,  1.4619e-04,
         -7.9823e-04, -1.4396e-03],
        ...,
        [ 1.2392e-03, -7.5861e-04,  1.4619e-04,  ...,  3.2709e-02,
         -8.1838e-03, -3.7024e-03],
        [-2.8110e-04, -4.1522e-04, -7.9823e-04,  ..., -8.1838e-03,
          3.6713e-02, -7.8613e-03],
        [ 1.5345e-04, -1.3206e-05, -1.4396e-03,  ..., -3.7024e-03,
         -7.8613e-03,  3.0140e-02]], device='cuda:0') 

reserving basis 281/576; cond: 494776.09375, radio:0.0021933827083557844
PARAMETER       :  Parameter containing:
tensor([[[[-2.7701e-02, -1.2277e-02, -2.4561e-02],
          [ 3.3324e-02, -1.6115e-02, -2.3971e-02],
          [ 1.6158e-02, -6.9832e-03,  4.5025e-02]],

         [[ 3.5091e-02, -7.4721e-03, -2.6447e-02],
          [ 3.2742e-02,  6.0248e-03,  3.6078e-02],
          [ 8.6110e-03, -2.1123e-02,  2.4951e-03]],

         [[-6.9250e-03, -2.2365e-02, -1.7320e-02],
          [ 2.5622e-02, -4.6497e-02,  1.9960e-02],
          [-3.2473e-02,  3.2895e-03, -1.0790e-02]],

         ...,

         [[-1.7252e-02,  1.5311e-02,  2.5404e-02],
          [ 1.4677e-02, -1.3323e-02, -1.0259e-02],
          [ 1.5293e-02, -1.4663e-02, -3.2880e-02]],

         [[ 2.8672e-03,  3.1882e-02,  4.3095e-02],
          [ 4.3905e-03,  2.4185e-02,  2.3815e-02],
          [ 5.6064e-03,  9.8382e-03,  3.4015e-02]],

         [[-4.0481e-02, -2.2989e-02, -1.2785e-02],
          [-2.1288e-02, -2.2648e-02,  2.2761e-02],
          [-3.5676e-02, -2.1582e-03, -3.7589e-02]]],


        [[[ 5.4118e-03, -1.7531e-02, -5.0408e-02],
          [-1.1498e-02, -2.3255e-02, -4.4904e-02],
          [-5.5268e-03, -1.8812e-02,  1.2170e-02]],

         [[-1.4049e-02, -5.3148e-03,  9.6219e-03],
          [ 2.0111e-02,  3.9975e-02,  2.5741e-02],
          [-1.2941e-02, -4.1691e-03,  7.6609e-03]],

         [[-2.1376e-02,  1.0257e-02, -1.6817e-02],
          [ 2.7900e-02, -3.3884e-02,  3.7795e-02],
          [-1.1983e-02,  7.3050e-03, -9.1308e-04]],

         ...,

         [[ 3.9290e-03,  4.2451e-02, -1.3903e-02],
          [-1.5502e-03, -8.3286e-04, -2.0566e-02],
          [-2.5870e-02, -1.0431e-02, -1.2151e-02]],

         [[-5.1358e-02,  3.1722e-03,  1.5016e-02],
          [ 1.3955e-02, -1.5625e-03, -1.1285e-02],
          [ 4.8286e-02, -9.3485e-03,  7.0008e-05]],

         [[-4.1952e-02,  7.8369e-03,  1.5115e-02],
          [-1.3961e-02,  2.7075e-02, -9.5183e-03],
          [ 2.3101e-02,  3.6130e-02,  3.7247e-02]]],


        [[[-3.9282e-02,  2.2609e-02,  1.3554e-02],
          [ 3.0876e-02,  1.7864e-02,  3.0233e-02],
          [ 3.3063e-02,  1.6070e-02,  2.2606e-02]],

         [[-5.5707e-03,  3.8702e-03, -8.2757e-03],
          [ 2.4146e-02, -2.1205e-02,  2.0418e-02],
          [-8.9524e-03, -3.6756e-02, -2.5524e-02]],

         [[ 4.3132e-02, -1.5043e-02, -9.2043e-03],
          [ 1.0449e-02,  3.6041e-02, -5.2320e-03],
          [-2.2510e-02,  1.2525e-02,  3.8396e-02]],

         ...,

         [[ 1.9436e-02, -2.4392e-02,  2.3627e-02],
          [ 4.3095e-02,  1.2428e-02, -2.0517e-02],
          [ 3.4167e-02,  2.5165e-02, -1.3497e-02]],

         [[ 2.0346e-02, -4.0353e-02, -1.7395e-02],
          [-2.9557e-02,  2.7415e-02,  5.2353e-03],
          [ 3.5021e-03,  2.9490e-02,  2.1171e-02]],

         [[-2.3807e-02,  8.2522e-03,  7.7784e-03],
          [-2.4630e-02,  3.8600e-03, -3.4625e-02],
          [ 3.8813e-03,  1.6659e-02,  5.1792e-03]]],


        ...,


        [[[-1.5197e-02, -2.8609e-02, -3.5107e-02],
          [-4.0193e-04,  1.1900e-02, -4.9206e-02],
          [-9.9246e-03, -9.8462e-03, -3.8292e-02]],

         [[-2.1044e-02,  2.6626e-02,  3.8941e-02],
          [-1.5600e-02, -3.0151e-02, -3.1991e-02],
          [-1.2291e-02,  3.8159e-02,  4.3013e-02]],

         [[-2.8059e-03, -1.8262e-02,  4.0677e-02],
          [-5.5019e-03,  2.9526e-02,  3.9294e-02],
          [ 2.4416e-03,  4.1826e-02,  1.8517e-02]],

         ...,

         [[ 3.0383e-02, -4.3286e-02, -3.5712e-02],
          [ 1.6442e-02,  2.1443e-02, -9.9393e-03],
          [ 1.4689e-03, -4.6861e-03,  8.8201e-03]],

         [[ 2.2740e-02, -4.5020e-04,  9.3692e-03],
          [ 2.6526e-02,  8.7547e-03, -3.9999e-02],
          [ 4.5392e-02,  3.7293e-02, -3.8833e-02]],

         [[ 2.8742e-03,  4.2639e-02, -1.7962e-04],
          [ 3.0547e-02, -3.4363e-02, -7.1092e-03],
          [ 3.3236e-03, -3.3160e-02,  1.8254e-02]]],


        [[[ 3.6579e-02, -1.2126e-02,  4.5585e-02],
          [-4.6886e-02,  1.9966e-02, -1.4285e-02],
          [ 3.0423e-02, -1.3861e-02, -2.0414e-02]],

         [[ 2.6610e-02, -2.2327e-02, -1.2955e-02],
          [ 2.4201e-02,  8.0940e-03, -2.2923e-02],
          [ 5.2422e-02, -2.4080e-02,  4.6096e-02]],

         [[-3.0977e-02, -3.5761e-02,  2.7509e-02],
          [ 1.7197e-02, -3.6761e-02, -5.3466e-02],
          [-3.8254e-02, -4.0998e-04, -1.2297e-02]],

         ...,

         [[-2.0007e-02,  1.9295e-02,  3.2111e-02],
          [-3.5178e-02,  9.5575e-03, -9.2642e-03],
          [ 3.1512e-02, -3.3861e-02,  3.6013e-02]],

         [[-4.9011e-03, -1.8070e-02, -3.2397e-02],
          [ 2.3976e-02, -1.0582e-02,  1.3223e-02],
          [-2.8720e-04,  3.4623e-02,  1.2962e-02]],

         [[-3.0383e-02, -1.4312e-02, -4.1715e-02],
          [ 2.6405e-02, -4.2411e-02, -2.9623e-02],
          [-3.6024e-02,  4.3807e-02,  1.6455e-02]]],


        [[[-1.8428e-03, -4.3681e-03,  1.5337e-02],
          [-3.9112e-02, -5.6789e-03, -7.8523e-03],
          [ 2.5311e-02,  4.5366e-03,  3.7153e-02]],

         [[-2.3203e-02,  2.8739e-02,  1.5281e-02],
          [ 3.8631e-02,  2.6027e-02,  1.8860e-02],
          [-4.9931e-03,  2.3830e-02,  1.1658e-02]],

         [[-1.4827e-02, -2.7002e-02,  3.5081e-02],
          [-3.7567e-02,  1.9871e-02,  1.7042e-02],
          [ 3.5805e-03, -2.6806e-02, -3.2764e-02]],

         ...,

         [[ 5.1446e-02, -2.0047e-02,  2.7579e-02],
          [ 3.3403e-02, -3.8976e-02,  2.7500e-02],
          [-7.9184e-03, -4.2019e-02, -1.9613e-02]],

         [[-1.6150e-02, -5.8901e-04, -1.8861e-02],
          [-3.0908e-02, -4.8306e-02, -3.8862e-02],
          [ 2.1877e-02,  1.9097e-02, -1.4461e-02]],

         [[-8.6257e-03, -1.3338e-02,  4.7471e-02],
          [ 2.3041e-02,  1.8056e-02,  2.3407e-03],
          [ 1.1964e-02,  3.0482e-02, -6.1326e-04]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([7.1892e+06, 3.5228e+05, 3.1940e+05, 1.9782e+05, 1.4546e+05, 8.7957e+04,
        8.1591e+04, 5.2034e+04, 4.5478e+04, 3.8464e+04, 2.8567e+04, 2.3550e+04,
        2.0503e+04, 1.8269e+04, 1.6509e+04, 1.5160e+04, 1.4429e+04, 1.4188e+04,
        1.3112e+04, 1.2106e+04, 1.1752e+04, 1.0373e+04, 9.7898e+03, 9.1530e+03,
        8.8752e+03, 8.2512e+03, 7.6569e+03, 7.4876e+03, 6.9003e+03, 6.6065e+03,
        6.2567e+03, 5.9599e+03, 5.7184e+03, 5.2340e+03, 5.0759e+03, 4.9905e+03,
        4.4996e+03, 4.2842e+03, 4.0718e+03, 3.8283e+03, 3.5607e+03, 3.4154e+03,
        3.3621e+03, 3.2369e+03, 3.2042e+03, 3.0686e+03, 2.9377e+03, 2.7929e+03,
        2.6945e+03, 2.6527e+03, 2.5913e+03, 2.4395e+03, 2.3053e+03, 2.2283e+03,
        2.1792e+03, 2.1194e+03, 2.0825e+03, 2.0639e+03, 2.0292e+03, 1.9017e+03,
        1.8900e+03, 1.8475e+03, 1.8118e+03, 1.7806e+03, 1.7212e+03, 1.6835e+03,
        1.6519e+03, 1.6401e+03, 1.6058e+03, 1.5597e+03, 1.5164e+03, 1.5007e+03,
        1.4637e+03, 1.4129e+03, 1.3681e+03, 1.3428e+03, 1.3285e+03, 1.3048e+03,
        1.2602e+03, 1.2394e+03, 1.2190e+03, 1.1842e+03, 1.1707e+03, 1.1237e+03,
        1.1209e+03, 1.0993e+03, 1.0776e+03, 1.0555e+03, 1.0414e+03, 1.0175e+03,
        1.0044e+03, 9.7344e+02, 9.6377e+02, 9.5943e+02, 9.4897e+02, 9.2332e+02,
        9.1498e+02, 8.9116e+02, 8.7894e+02, 8.6006e+02, 8.5500e+02, 8.4928e+02,
        8.1497e+02, 8.0786e+02, 7.9365e+02, 7.9207e+02, 7.6974e+02, 7.5220e+02,
        7.4204e+02, 7.3914e+02, 7.3778e+02, 7.2720e+02, 7.2548e+02, 6.9807e+02,
        6.9034e+02, 6.8334e+02, 6.7253e+02, 6.5222e+02, 6.4724e+02, 6.3989e+02,
        6.3374e+02, 6.3004e+02, 6.1749e+02, 6.0392e+02, 5.9951e+02, 5.9244e+02,
        5.8726e+02, 5.7483e+02, 5.7057e+02, 5.6394e+02, 5.5872e+02, 5.5626e+02,
        5.4659e+02, 5.4412e+02, 5.3960e+02, 5.3075e+02, 5.2909e+02, 5.2332e+02,
        5.1322e+02, 5.1080e+02, 5.0623e+02, 4.9557e+02, 4.8861e+02, 4.7963e+02,
        4.7583e+02, 4.6924e+02, 4.6690e+02, 4.6415e+02, 4.5842e+02, 4.5373e+02,
        4.5111e+02, 4.4019e+02, 4.3617e+02, 4.3155e+02, 4.3027e+02, 4.2664e+02,
        4.1991e+02, 4.1572e+02, 4.1185e+02, 4.0402e+02, 3.9818e+02, 3.9666e+02,
        3.9354e+02, 3.9176e+02, 3.9022e+02, 3.8597e+02, 3.7966e+02, 3.7727e+02,
        3.7514e+02, 3.7220e+02, 3.6879e+02, 3.6561e+02, 3.6324e+02, 3.6023e+02,
        3.5696e+02, 3.5400e+02, 3.4722e+02, 3.4391e+02, 3.3843e+02, 3.3768e+02,
        3.3690e+02, 3.3190e+02, 3.3101e+02, 3.2664e+02, 3.2332e+02, 3.1934e+02,
        3.1735e+02, 3.1345e+02, 3.0997e+02, 3.0837e+02, 3.0702e+02, 3.0413e+02,
        3.0229e+02, 2.9689e+02, 2.9577e+02, 2.9153e+02, 2.9046e+02, 2.8662e+02,
        2.8527e+02, 2.8228e+02, 2.8223e+02, 2.7845e+02, 2.7591e+02, 2.7563e+02,
        2.7281e+02, 2.7063e+02, 2.6929e+02, 2.6695e+02, 2.6531e+02, 2.6271e+02,
        2.5871e+02, 2.5612e+02, 2.5385e+02, 2.5277e+02, 2.5170e+02, 2.5052e+02,
        2.4801e+02, 2.4669e+02, 2.4598e+02, 2.4477e+02, 2.4180e+02, 2.4063e+02,
        2.3813e+02, 2.3796e+02, 2.3635e+02, 2.3439e+02, 2.3027e+02, 2.2942e+02,
        2.2797e+02, 2.2602e+02, 2.2269e+02, 2.2239e+02, 2.2007e+02, 2.1859e+02,
        2.1774e+02, 2.1664e+02, 2.1436e+02, 2.1297e+02, 2.1253e+02, 2.0982e+02,
        2.0931e+02, 2.0591e+02, 2.0574e+02, 2.0306e+02, 2.0123e+02, 2.0039e+02,
        1.9865e+02, 1.9798e+02, 1.9722e+02, 1.9502e+02, 1.9233e+02, 1.8982e+02,
        1.8943e+02, 1.8912e+02, 1.8851e+02, 1.8723e+02, 1.8580e+02, 1.8402e+02,
        1.8185e+02, 1.8101e+02, 1.7933e+02, 1.7830e+02, 1.7711e+02, 1.7651e+02,
        1.7539e+02, 1.7334e+02, 1.7259e+02, 1.7156e+02, 1.7008e+02, 1.6990e+02,
        1.6912e+02, 1.6780e+02, 1.6637e+02, 1.6476e+02, 1.6452e+02, 1.6261e+02,
        1.6224e+02, 1.6159e+02, 1.6034e+02, 1.5940e+02, 1.5872e+02, 1.5796e+02,
        1.5759e+02, 1.5580e+02, 1.5501e+02, 1.5428e+02, 1.5275e+02, 1.5114e+02,
        1.5058e+02, 1.4931e+02, 1.4883e+02, 1.4829e+02, 1.4765e+02, 1.4710e+02,
        1.4587e+02, 1.4458e+02, 1.4349e+02, 1.4214e+02, 1.4102e+02, 1.4077e+02,
        1.4025e+02, 1.3928e+02, 1.3877e+02, 1.3798e+02, 1.3718e+02, 1.3650e+02,
        1.3443e+02, 1.3403e+02, 1.3334e+02, 1.3269e+02, 1.3254e+02, 1.3151e+02,
        1.3017e+02, 1.3005e+02, 1.2889e+02, 1.2883e+02, 1.2783e+02, 1.2752e+02,
        1.2629e+02, 1.2551e+02, 1.2385e+02, 1.2327e+02, 1.2288e+02, 1.2245e+02,
        1.2204e+02, 1.2140e+02, 1.2004e+02, 1.1963e+02, 1.1872e+02, 1.1776e+02,
        1.1715e+02, 1.1689e+02, 1.1611e+02, 1.1440e+02, 1.1403e+02, 1.1365e+02,
        1.1309e+02, 1.1290e+02, 1.1220e+02, 1.1113e+02, 1.1045e+02, 1.1010e+02,
        1.0943e+02, 1.0902e+02, 1.0820e+02, 1.0712e+02, 1.0664e+02, 1.0590e+02,
        1.0546e+02, 1.0449e+02, 1.0386e+02, 1.0296e+02, 1.0266e+02, 1.0228e+02,
        1.0139e+02, 1.0102e+02, 1.0082e+02, 9.9494e+01, 9.9071e+01, 9.8843e+01,
        9.8065e+01, 9.7769e+01, 9.7018e+01, 9.6930e+01, 9.5919e+01, 9.5699e+01,
        9.5076e+01, 9.4372e+01, 9.4122e+01, 9.3316e+01, 9.3044e+01, 9.2514e+01,
        9.2101e+01, 9.1488e+01, 9.1046e+01, 9.0732e+01, 9.0399e+01, 8.9744e+01,
        8.9024e+01, 8.8486e+01, 8.8053e+01, 8.7347e+01, 8.7091e+01, 8.6670e+01,
        8.6518e+01, 8.6183e+01, 8.5626e+01, 8.4870e+01, 8.4819e+01, 8.3936e+01,
        8.3617e+01, 8.2947e+01, 8.2810e+01, 8.2220e+01, 8.1631e+01, 8.0826e+01,
        8.0591e+01, 8.0353e+01, 7.9976e+01, 7.9232e+01, 7.8361e+01, 7.8248e+01,
        7.8034e+01, 7.7293e+01, 7.6904e+01, 7.6612e+01, 7.5918e+01, 7.5711e+01,
        7.5609e+01, 7.4965e+01, 7.4067e+01, 7.3594e+01, 7.3280e+01, 7.3127e+01,
        7.2857e+01, 7.2633e+01, 7.2272e+01, 7.2109e+01, 7.1740e+01, 7.1397e+01,
        7.0945e+01, 7.0547e+01, 7.0052e+01, 6.9963e+01, 6.9252e+01, 6.8806e+01,
        6.8494e+01, 6.7818e+01, 6.7629e+01, 6.7260e+01, 6.6976e+01, 6.6454e+01,
        6.5880e+01, 6.5514e+01, 6.5188e+01, 6.4523e+01, 6.4294e+01, 6.4205e+01,
        6.3824e+01, 6.3516e+01, 6.3402e+01, 6.2829e+01, 6.2550e+01, 6.2247e+01,
        6.1796e+01, 6.1480e+01, 6.1341e+01, 6.1081e+01, 6.0625e+01, 6.0547e+01,
        6.0198e+01, 5.9661e+01, 5.9339e+01, 5.9044e+01, 5.8630e+01, 5.8083e+01,
        5.7588e+01, 5.7300e+01, 5.7020e+01, 5.6757e+01, 5.6563e+01, 5.6111e+01,
        5.5820e+01, 5.5355e+01, 5.5002e+01, 5.4463e+01, 5.4230e+01, 5.3912e+01,
        5.3837e+01, 5.3218e+01, 5.2923e+01, 5.2810e+01, 5.2416e+01, 5.2126e+01,
        5.1906e+01, 5.1343e+01, 5.1156e+01, 5.1050e+01, 5.0343e+01, 5.0018e+01,
        4.9800e+01, 4.9513e+01, 4.8763e+01, 4.8563e+01, 4.8205e+01, 4.8175e+01,
        4.7776e+01, 4.7104e+01, 4.6883e+01, 4.6691e+01, 4.6038e+01, 4.5676e+01,
        4.5457e+01, 4.5263e+01, 4.5223e+01, 4.4758e+01, 4.4253e+01, 4.3746e+01,
        4.3661e+01, 4.3223e+01, 4.2937e+01, 4.2747e+01, 4.2657e+01, 4.2242e+01,
        4.2122e+01, 4.1500e+01, 4.1358e+01, 4.1118e+01, 4.0975e+01, 4.0266e+01,
        4.0113e+01, 3.9713e+01, 3.9200e+01, 3.9131e+01, 3.8748e+01, 3.8665e+01,
        3.8119e+01, 3.8000e+01, 3.7871e+01, 3.7273e+01, 3.7169e+01, 3.6991e+01,
        3.6616e+01, 3.6358e+01, 3.6347e+01, 3.5673e+01, 3.5284e+01, 3.5263e+01,
        3.4993e+01, 3.4873e+01, 3.4355e+01, 3.4038e+01, 3.3791e+01, 3.3530e+01,
        3.3277e+01, 3.3219e+01, 3.2738e+01, 3.2449e+01, 3.2177e+01, 3.1768e+01,
        3.1278e+01, 3.0929e+01, 3.0782e+01, 3.0612e+01, 2.9918e+01, 2.9803e+01,
        2.9708e+01, 2.9290e+01, 2.9237e+01, 2.8632e+01, 2.8486e+01, 2.8253e+01,
        2.7249e+01, 2.7113e+01, 2.6707e+01, 2.6119e+01, 2.5777e+01, 2.5478e+01,
        2.5272e+01, 2.4985e+01, 2.4417e+01, 2.4105e+01, 2.3406e+01, 2.3078e+01,
        2.2313e+01, 2.1758e+01, 2.1467e+01, 2.1049e+01, 2.0487e+01, 2.0172e+01,
        1.9909e+01, 1.9481e+01, 1.8556e+01, 1.8074e+01, 1.7520e+01, 1.4530e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 281]) 

NULL SPACE BASIS :  tensor([[ 1.1169e-01,  9.4298e-03,  1.8571e-02,  ...,  6.7146e-04,
         -1.1590e-03,  1.2987e-02],
        [-1.5386e-02,  5.2195e-02, -2.7182e-02,  ...,  4.7473e-03,
          7.2812e-03, -1.4487e-02],
        [-8.6395e-02,  2.8242e-02, -4.8169e-02,  ..., -2.9815e-03,
         -1.0644e-02,  2.2705e-03],
        ...,
        [ 6.4886e-02, -7.0386e-03,  1.1696e-02,  ..., -1.2367e-03,
         -3.8031e-04,  1.5599e-03],
        [-1.1239e-01,  3.0929e-02, -7.8436e-02,  ...,  7.9042e-03,
          5.3886e-06,  1.4905e-02],
        [-1.0719e-02, -2.2829e-02,  4.9752e-02,  ..., -2.0169e-03,
          5.8910e-04, -1.2077e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0220, -0.0171,  0.0010,  ..., -0.0010,  0.0002,  0.0002],
        [-0.0171,  0.0320, -0.0145,  ..., -0.0002,  0.0003, -0.0006],
        [ 0.0010, -0.0145,  0.0182,  ...,  0.0003,  0.0004,  0.0002],
        ...,
        [-0.0010, -0.0002,  0.0003,  ...,  0.0137, -0.0080, -0.0010],
        [ 0.0002,  0.0003,  0.0004,  ..., -0.0080,  0.0186, -0.0068],
        [ 0.0002, -0.0006,  0.0002,  ..., -0.0010, -0.0068,  0.0130]],
       device='cuda:0') 

reserving basis 818/1152; cond: 271940.8125, radio:0.00984941329807043
PARAMETER       :  Parameter containing:
tensor([[[[ 1.5583e-02,  3.0783e-02, -1.8162e-02],
          [-3.4797e-04, -1.6108e-02,  9.4694e-03],
          [ 1.8596e-02,  1.8856e-02, -3.7206e-03]],

         [[-1.8493e-02, -2.3423e-02, -2.6577e-02],
          [-2.2054e-02, -2.2398e-02,  6.3766e-03],
          [-1.4816e-02,  9.4381e-03, -2.8782e-02]],

         [[ 8.0723e-04, -1.4525e-02,  4.2138e-03],
          [ 8.1531e-03,  1.4939e-02, -1.5391e-02],
          [-1.4362e-02, -2.5168e-02,  2.6820e-02]],

         ...,

         [[-1.2439e-02,  1.9089e-03,  5.4714e-03],
          [-3.0287e-02, -2.1193e-02, -1.3607e-03],
          [ 6.3603e-03,  1.5155e-02,  2.8824e-02]],

         [[ 9.3430e-03, -3.4048e-02, -1.8797e-02],
          [ 2.2836e-02,  1.5082e-02,  1.0070e-04],
          [ 1.5291e-02, -1.5422e-02,  3.4450e-02]],

         [[ 1.8569e-02,  5.7217e-03,  1.5265e-02],
          [-2.2771e-02, -3.4031e-02, -2.5794e-02],
          [-2.9084e-02,  1.8980e-02, -1.4865e-02]]],


        [[[ 2.5060e-02,  4.0612e-04,  7.5050e-03],
          [-1.5474e-02, -1.3600e-02,  2.1419e-02],
          [-2.7832e-02, -3.0334e-02, -1.6212e-02]],

         [[ 7.5967e-03, -2.4190e-02, -2.2974e-02],
          [ 9.5185e-03, -1.2822e-02,  1.6736e-03],
          [-2.5992e-02,  6.6692e-03, -1.2581e-02]],

         [[ 2.1592e-04, -1.7486e-02, -1.8918e-02],
          [-4.9679e-03,  1.3545e-02, -2.9583e-02],
          [ 2.5635e-02,  1.8486e-02,  2.5391e-03]],

         ...,

         [[ 4.4957e-03,  1.3579e-02, -3.9939e-03],
          [-2.1220e-02,  2.5813e-02,  2.7022e-02],
          [ 1.5010e-02,  2.0617e-02,  2.1534e-02]],

         [[ 2.5442e-02,  1.4649e-03,  6.4599e-03],
          [-7.8971e-03, -2.1399e-02, -2.7458e-02],
          [ 4.1666e-02, -1.8969e-02, -2.9559e-02]],

         [[-1.5522e-02, -1.8636e-02, -3.2952e-02],
          [-2.3219e-02, -1.3028e-02, -8.1195e-03],
          [ 7.0574e-03, -1.0893e-02, -1.4914e-03]]],


        [[[-1.7051e-02,  1.8607e-02, -2.8282e-02],
          [-4.8214e-03,  1.7882e-02, -2.6324e-03],
          [ 3.5625e-04,  2.7808e-02,  1.6152e-02]],

         [[ 4.9280e-03, -1.2376e-02,  2.3271e-02],
          [-4.0458e-03, -9.0498e-03,  5.4364e-03],
          [-2.1550e-02,  4.8025e-03,  8.9641e-03]],

         [[-1.8483e-02, -1.5637e-02, -1.3326e-02],
          [-2.5793e-02, -2.7151e-02, -3.1469e-02],
          [-1.3175e-02, -1.8458e-02,  1.6413e-02]],

         ...,

         [[ 2.6774e-02,  8.4628e-03, -1.7738e-02],
          [-3.0806e-02,  1.4543e-02,  3.5755e-03],
          [ 2.0388e-02, -1.6087e-02, -3.2623e-02]],

         [[-4.0287e-02,  2.5390e-03, -1.0829e-02],
          [-1.6350e-02,  1.8012e-02,  8.9886e-03],
          [-1.3769e-02,  8.6219e-03, -1.4121e-02]],

         [[-1.3452e-02, -1.9032e-02,  1.1080e-02],
          [-3.8360e-03, -9.9576e-03,  5.1788e-03],
          [ 1.0664e-02, -1.7636e-02,  3.0814e-02]]],


        ...,


        [[[ 2.6978e-02,  2.5158e-04,  3.0714e-02],
          [ 2.3864e-02,  1.7577e-02, -3.0368e-02],
          [ 4.0959e-03,  9.6971e-05,  2.7442e-02]],

         [[ 9.9201e-03, -7.3732e-03,  1.2918e-02],
          [ 5.1020e-03, -2.5560e-02,  4.6199e-03],
          [-1.6171e-02, -7.8429e-03,  1.5988e-02]],

         [[ 1.5845e-02,  1.1961e-02, -1.0644e-02],
          [-1.3641e-02,  2.7658e-02,  6.0254e-03],
          [ 6.7306e-03, -6.4258e-03, -1.3862e-02]],

         ...,

         [[-2.8771e-02,  1.4657e-02, -1.3799e-02],
          [ 6.9419e-03, -2.8933e-02, -1.5449e-02],
          [-1.9025e-02,  2.6567e-03,  8.4554e-03]],

         [[ 1.9180e-02,  1.7329e-02,  2.8604e-02],
          [-2.2333e-02,  6.0824e-04,  1.6781e-02],
          [-5.8169e-03,  1.0660e-02,  5.4786e-03]],

         [[-1.4929e-02, -3.2105e-03,  2.3062e-02],
          [ 1.5696e-02,  9.2260e-03,  3.1051e-02],
          [ 5.3912e-03,  8.8552e-03,  1.9010e-02]]],


        [[[-4.3645e-03, -1.1979e-02,  4.8422e-03],
          [-2.9025e-02, -8.6556e-03,  2.0093e-02],
          [ 1.0469e-02, -2.7049e-02, -2.1947e-02]],

         [[-1.2164e-02, -1.5572e-02,  2.3954e-02],
          [-1.8785e-02, -3.5470e-02, -5.1617e-03],
          [-2.3760e-02, -8.5837e-04, -2.2929e-02]],

         [[-1.5796e-02, -2.4001e-03, -2.0742e-02],
          [-1.1924e-02,  1.4289e-02,  2.2055e-02],
          [ 8.4225e-03, -1.9913e-02,  1.9606e-02]],

         ...,

         [[-1.4818e-02,  1.1018e-02,  4.6594e-03],
          [-2.9692e-02, -2.1077e-02, -1.4025e-02],
          [-2.3595e-02, -4.9699e-02, -1.7026e-02]],

         [[-2.9655e-03, -6.8469e-03, -1.3160e-02],
          [-3.0552e-02,  3.0629e-02,  2.9593e-02],
          [-3.6270e-02,  2.2416e-02, -8.4870e-03]],

         [[ 2.1509e-02, -2.2408e-02, -8.6722e-03],
          [ 3.0476e-02, -1.5823e-02,  3.2259e-02],
          [-4.6955e-03,  1.8338e-02,  1.6294e-03]]],


        [[[ 3.8765e-03, -2.6804e-02,  2.9789e-02],
          [-2.4550e-02,  4.8164e-03, -3.5876e-02],
          [-1.3798e-03, -2.7914e-02, -1.7873e-02]],

         [[ 1.5037e-02,  1.4620e-02, -1.6024e-02],
          [ 3.6221e-03,  1.3109e-02,  8.8705e-03],
          [-1.7050e-02, -2.7985e-02,  2.2886e-02]],

         [[-8.3019e-03, -7.8607e-03,  7.9279e-03],
          [-1.8061e-02,  2.0153e-03,  2.6714e-02],
          [-1.7048e-02,  2.3979e-02, -4.6918e-03]],

         ...,

         [[ 2.7143e-02,  1.1512e-02, -7.0024e-03],
          [-1.7271e-02,  3.1406e-02,  1.5341e-02],
          [ 2.8306e-02, -9.9011e-03, -1.8263e-04]],

         [[-1.5514e-02,  9.7687e-03, -4.4191e-03],
          [-2.2790e-02,  3.2011e-02,  2.8763e-02],
          [ 1.6687e-02,  1.3391e-02,  8.3907e-03]],

         [[ 1.1428e-02,  3.8582e-02, -1.7517e-02],
          [-1.4006e-02,  1.2321e-02,  3.2436e-02],
          [ 5.7115e-03, -2.5391e-02, -1.7861e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.3728e+07, 5.2439e+05, 5.0638e+05,  ..., 5.9403e+01, 5.8297e+01,
        5.0480e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 818]) 

NULL SPACE BASIS :  tensor([[-0.0034,  0.0195,  0.0294,  ..., -0.0053,  0.0598, -0.0117],
        [-0.0055,  0.0093, -0.0010,  ...,  0.0018, -0.1075,  0.0102],
        [ 0.0145,  0.0335,  0.0506,  ...,  0.0010,  0.0472,  0.0003],
        ...,
        [ 0.0098,  0.0227,  0.0145,  ...,  0.0042,  0.0216,  0.0033],
        [-0.0337,  0.0044, -0.0035,  ..., -0.0151, -0.0209,  0.0119],
        [-0.0192, -0.0068, -0.0357,  ...,  0.0104,  0.0042, -0.0133]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.9308e-02, -9.7331e-04, -6.1470e-04,  ..., -3.1597e-04,
          1.6891e-04, -2.5052e-05],
        [-9.7331e-04,  2.9961e-02, -7.3942e-04,  ...,  1.3296e-04,
          2.6104e-04,  2.5292e-04],
        [-6.1470e-04, -7.3942e-04,  2.9613e-02,  ..., -1.0537e-04,
          1.1884e-04,  1.6257e-04],
        ...,
        [-3.1597e-04,  1.3296e-04, -1.0537e-04,  ...,  2.3116e-02,
         -2.8304e-03, -7.2470e-04],
        [ 1.6891e-04,  2.6104e-04,  1.1884e-04,  ..., -2.8304e-03,
          2.4635e-02, -2.8388e-03],
        [-2.5052e-05,  2.5292e-04,  1.6257e-04,  ..., -7.2470e-04,
         -2.8388e-03,  2.5130e-02]], device='cuda:0') 

reserving basis 44/64; cond: 7022.501953125, radio:0.017710713669657707
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0127]],

         [[ 0.0903]],

         [[ 0.0572]],

         ...,

         [[-0.0817]],

         [[-0.0892]],

         [[-0.0510]]],


        [[[ 0.0269]],

         [[-0.0341]],

         [[-0.0923]],

         ...,

         [[-0.0441]],

         [[ 0.0195]],

         [[ 0.0231]]],


        [[[ 0.0206]],

         [[-0.0640]],

         [[ 0.0015]],

         ...,

         [[-0.1448]],

         [[ 0.0936]],

         [[ 0.0155]]],


        ...,


        [[[ 0.0291]],

         [[-0.0866]],

         [[-0.1019]],

         ...,

         [[-0.0215]],

         [[ 0.0629]],

         [[-0.0207]]],


        [[[ 0.0183]],

         [[-0.0701]],

         [[ 0.1196]],

         ...,

         [[-0.0050]],

         [[ 0.0380]],

         [[ 0.0669]]],


        [[[ 0.0040]],

         [[ 0.0033]],

         [[ 0.0812]],

         ...,

         [[-0.0083]],

         [[ 0.0657]],

         [[ 0.0923]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([9.0496e+05, 4.7563e+04, 4.0813e+04, 1.7987e+04, 1.3578e+04, 1.0958e+04,
        7.5687e+03, 5.7441e+03, 4.3390e+03, 3.8997e+03, 3.4761e+03, 3.0057e+03,
        2.5234e+03, 2.3297e+03, 2.0901e+03, 2.0652e+03, 1.7981e+03, 1.7327e+03,
        1.5196e+03, 1.4657e+03, 1.2200e+03, 1.0892e+03, 9.7355e+02, 9.3385e+02,
        8.9794e+02, 8.6262e+02, 8.2780e+02, 7.1918e+02, 6.9262e+02, 6.4286e+02,
        5.9533e+02, 5.6688e+02, 5.4447e+02, 5.2353e+02, 5.0360e+02, 4.6352e+02,
        4.5475e+02, 4.4190e+02, 4.1733e+02, 4.0228e+02, 3.8739e+02, 3.6726e+02,
        3.3792e+02, 3.2351e+02, 3.0568e+02, 3.0085e+02, 2.7800e+02, 2.5967e+02,
        2.5653e+02, 2.4895e+02, 2.4673e+02, 2.4294e+02, 2.2644e+02, 2.1290e+02,
        2.0480e+02, 1.9530e+02, 1.9001e+02, 1.8052e+02, 1.7402e+02, 1.6867e+02,
        1.5676e+02, 1.4944e+02, 1.4546e+02, 1.2887e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 44]) 

NULL SPACE BASIS :  tensor([[-0.2195, -0.0356,  0.1566,  ...,  0.1062, -0.0760, -0.1240],
        [-0.0552, -0.0796, -0.1309,  ...,  0.0027,  0.0462, -0.0483],
        [-0.0653, -0.1486, -0.0200,  ..., -0.2230, -0.1118,  0.0402],
        ...,
        [-0.0876,  0.1185, -0.0186,  ..., -0.0122, -0.0075,  0.0243],
        [ 0.0855,  0.0940,  0.0395,  ...,  0.0473, -0.0167, -0.0298],
        [ 0.2459, -0.0144,  0.0028,  ..., -0.0110, -0.0231,  0.0153]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.1013,  0.0022,  0.0165,  ...,  0.0012, -0.0050, -0.0128],
        [ 0.0022,  0.1206, -0.0116,  ...,  0.0119, -0.0105, -0.0038],
        [ 0.0165, -0.0116,  0.0923,  ..., -0.0043,  0.0028,  0.0039],
        ...,
        [ 0.0012,  0.0119, -0.0043,  ...,  0.0351, -0.0329, -0.0025],
        [-0.0050, -0.0105,  0.0028,  ..., -0.0329,  0.1117,  0.0042],
        [-0.0128, -0.0038,  0.0039,  ..., -0.0025,  0.0042,  0.0844]],
       device='cuda:0') 

reserving basis 776/1152; cond: 307890.375, radio:0.00902059581130743
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1837e-02, -3.3335e-03,  9.9354e-03],
          [-3.1050e-03,  1.6246e-02,  1.1530e-02],
          [-1.0017e-02, -1.2413e-02, -1.3246e-02]],

         [[ 3.7005e-03, -1.8246e-02, -2.2632e-02],
          [ 2.5591e-02, -2.4064e-03,  3.3689e-04],
          [ 6.2574e-03,  1.8940e-02, -1.6843e-02]],

         [[ 6.9016e-03, -5.1133e-04,  6.2495e-03],
          [ 1.7004e-02,  1.7120e-02,  1.6185e-02],
          [-2.4271e-02, -1.4536e-02,  1.2552e-02]],

         ...,

         [[ 1.6719e-02,  6.1339e-03,  1.1281e-04],
          [-1.1751e-02,  2.6640e-02,  6.7335e-03],
          [ 1.5050e-02, -1.5118e-03, -6.3433e-03]],

         [[ 9.0418e-03,  1.1781e-02,  6.3484e-03],
          [ 2.4287e-03,  2.2643e-02,  2.0091e-02],
          [ 2.4085e-02,  2.8513e-02, -5.8907e-03]],

         [[ 9.5552e-04,  2.6793e-02,  4.6773e-02],
          [-1.9124e-02,  1.4729e-02,  2.0532e-02],
          [ 5.5463e-03, -1.9994e-02,  1.7285e-02]]],


        [[[-1.1065e-02,  7.4946e-03, -2.1289e-02],
          [ 1.3987e-02, -6.6338e-03,  5.8378e-03],
          [ 2.5843e-03, -2.3942e-02,  1.9701e-02]],

         [[-1.5780e-03,  1.8764e-02,  2.6539e-02],
          [ 2.1312e-02,  1.6462e-02,  3.8842e-04],
          [-9.3090e-03,  1.6567e-02,  1.9003e-02]],

         [[ 1.8578e-02,  1.7927e-02,  1.0053e-02],
          [ 2.6385e-02, -4.7200e-03,  2.2302e-02],
          [-1.6716e-02,  1.3456e-03,  9.5911e-03]],

         ...,

         [[-3.7834e-03,  1.5822e-02, -1.7955e-02],
          [-2.7174e-02, -2.2526e-02, -3.0895e-02],
          [-2.1404e-02, -2.1951e-02, -2.3696e-02]],

         [[-2.2814e-02,  1.8304e-03,  3.1754e-02],
          [-1.4949e-02, -1.7718e-02, -1.8694e-03],
          [ 1.5333e-02, -1.0219e-02, -1.7453e-02]],

         [[ 2.3011e-02, -2.3665e-02,  1.3209e-02],
          [-7.3298e-03, -1.3668e-02, -2.8403e-02],
          [ 2.5606e-03,  1.6398e-02, -3.1689e-02]]],


        [[[ 1.9056e-02,  2.0753e-02,  1.4646e-02],
          [-5.9991e-03, -2.9202e-02,  2.7891e-02],
          [-1.2083e-02, -6.3372e-05, -2.4280e-02]],

         [[-1.0188e-02,  5.8901e-03, -1.5613e-02],
          [ 3.2292e-02,  6.7424e-03, -4.7337e-03],
          [-7.2803e-03, -1.8341e-02,  1.8958e-02]],

         [[-3.0383e-02, -1.6051e-02, -2.2803e-02],
          [-1.5407e-02, -1.5259e-02,  2.0465e-03],
          [ 2.4532e-02, -1.3266e-02, -7.6921e-03]],

         ...,

         [[-3.5516e-02, -7.0431e-03, -2.1478e-02],
          [-7.3589e-03, -3.4928e-02, -2.7229e-02],
          [-3.7933e-03, -2.7672e-02, -1.1481e-03]],

         [[ 2.5102e-02, -1.3221e-02,  5.4547e-03],
          [ 1.9657e-02, -1.6945e-02, -2.0435e-02],
          [-3.3959e-03, -3.0069e-02,  1.0242e-02]],

         [[ 1.0953e-02,  1.3962e-02, -1.8754e-05],
          [ 5.7709e-03,  1.0860e-02, -1.0693e-02],
          [ 4.2448e-03,  2.3747e-02, -1.1798e-02]]],


        ...,


        [[[-1.4388e-02, -2.3520e-02, -3.4053e-02],
          [-1.0367e-02, -1.5115e-02,  1.9567e-03],
          [ 1.5475e-02,  7.8121e-03,  2.4065e-02]],

         [[-3.1511e-02, -7.3541e-03, -2.5593e-02],
          [-1.2617e-02,  1.5223e-02,  1.0663e-02],
          [-1.2317e-02,  3.1954e-02,  2.8328e-02]],

         [[ 2.9958e-02,  5.7691e-03,  3.6388e-04],
          [-1.6141e-02,  2.3778e-03, -2.7477e-02],
          [ 3.1315e-02,  2.3459e-02,  8.9855e-03]],

         ...,

         [[-2.1833e-02, -2.7586e-02,  7.1373e-03],
          [ 1.6135e-02, -1.4677e-02,  1.6143e-02],
          [-1.1562e-02,  3.3682e-02, -7.0139e-03]],

         [[ 1.1648e-02,  3.5087e-02,  1.1480e-02],
          [-1.1291e-02, -1.9742e-02,  2.5145e-02],
          [ 1.6459e-02, -2.7290e-03, -4.3250e-03]],

         [[ 3.7746e-02,  7.6127e-03,  1.7348e-02],
          [ 2.0665e-02, -1.5173e-02,  1.6421e-02],
          [-1.1455e-02, -3.4147e-02, -3.7877e-02]]],


        [[[-2.6613e-02, -8.7344e-04,  1.4033e-02],
          [ 2.1554e-02,  1.4699e-02, -2.0447e-02],
          [ 2.8672e-02,  2.4876e-02, -1.9461e-03]],

         [[ 7.9016e-03, -3.9079e-03,  1.5182e-03],
          [ 2.0094e-02,  3.8528e-02,  1.8142e-02],
          [ 2.2045e-02,  3.7277e-02, -1.2005e-02]],

         [[-1.9962e-02, -1.6320e-02, -2.7649e-02],
          [-1.7380e-02, -1.0974e-03, -9.4067e-03],
          [-2.2179e-03,  5.8770e-03, -2.7720e-02]],

         ...,

         [[-2.6597e-02,  1.6332e-02, -2.5253e-02],
          [ 1.6257e-02, -1.1633e-02, -6.6509e-03],
          [-1.0515e-02,  2.7399e-02,  1.2319e-02]],

         [[-2.7993e-02,  2.4816e-03,  1.4736e-02],
          [-2.2407e-03, -4.7683e-03,  3.7540e-03],
          [ 2.1785e-02,  2.2791e-02,  1.9492e-03]],

         [[-2.8541e-02, -7.1143e-03,  9.2201e-03],
          [-2.6899e-02,  1.2257e-02,  2.8123e-02],
          [-2.9311e-02,  8.6799e-04,  1.4909e-02]]],


        [[[-1.5504e-02,  1.5179e-02, -2.8661e-03],
          [-3.7924e-03,  1.1700e-02,  5.2106e-03],
          [-4.7743e-03, -1.8160e-03,  1.9546e-02]],

         [[-1.1102e-03, -2.1723e-02, -1.8837e-02],
          [-2.4420e-02, -1.2788e-02, -3.8447e-02],
          [ 3.8336e-02,  1.6080e-02,  3.7791e-03]],

         [[ 2.8301e-02,  6.3175e-04, -9.1134e-03],
          [-1.7408e-02, -1.3171e-02, -1.5431e-02],
          [ 2.6173e-02, -7.0382e-03, -2.1066e-02]],

         ...,

         [[ 9.8900e-03, -4.0770e-03,  9.9160e-03],
          [-2.0025e-02, -1.0047e-02,  5.1054e-03],
          [ 1.3926e-02,  7.0946e-04, -1.3206e-02]],

         [[-1.6244e-02, -9.7791e-03,  2.5906e-03],
          [ 1.7056e-02,  1.0492e-02, -3.2284e-02],
          [-1.6517e-02, -6.3304e-03, -8.6841e-03]],

         [[-2.5802e-03, -2.9778e-02, -1.6343e-02],
          [ 2.9199e-03,  2.3614e-02, -2.7316e-02],
          [ 2.5752e-02,  1.2127e-02,  2.1151e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.3430e+07, 5.1833e+05, 4.9495e+05,  ..., 5.5178e+01, 5.3606e+01,
        4.3618e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 776]) 

NULL SPACE BASIS :  tensor([[-0.0155, -0.0154, -0.0059,  ...,  0.0172,  0.0029,  0.0170],
        [ 0.0417,  0.0165,  0.0184,  ..., -0.0325, -0.0060, -0.0147],
        [ 0.0456, -0.0077, -0.0522,  ...,  0.0090,  0.0087,  0.0053],
        ...,
        [-0.0526, -0.0267,  0.0189,  ..., -0.0074, -0.0021, -0.0002],
        [-0.0095, -0.0007, -0.0154,  ...,  0.0008, -0.0017,  0.0024],
        [-0.0257, -0.0020,  0.0014,  ...,  0.0049,  0.0019,  0.0015]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2009e-02, -5.8329e-03, -2.7379e-03,  ..., -4.2502e-04,
         -5.3272e-04,  1.0798e-04],
        [-5.8329e-03,  2.2961e-02, -5.4349e-03,  ..., -1.1050e-04,
         -6.4735e-06, -2.9886e-04],
        [-2.7379e-03, -5.4349e-03,  2.1652e-02,  ..., -5.0115e-05,
          4.3078e-05,  5.7649e-05],
        ...,
        [-4.2502e-04, -1.1050e-04, -5.0115e-05,  ...,  1.9583e-02,
         -6.3100e-03, -2.6675e-03],
        [-5.3272e-04, -6.4735e-06,  4.3078e-05,  ..., -6.3100e-03,
          2.0268e-02, -6.6030e-03],
        [ 1.0798e-04, -2.9886e-04,  5.7649e-05,  ..., -2.6675e-03,
         -6.6030e-03,  1.9718e-02]], device='cuda:0') 

reserving basis 613/1152; cond: 424852.40625, radio:0.006033742800354958
PARAMETER       :  Parameter containing:
tensor([[[[ 1.8761e-03, -1.3253e-02, -7.0824e-03],
          [ 1.8185e-02,  1.7523e-02,  3.7778e-02],
          [-1.1550e-02, -6.0285e-03,  1.5567e-02]],

         [[-1.6018e-02,  1.9432e-02, -9.9696e-03],
          [-1.1741e-02, -2.3736e-02, -2.5823e-02],
          [-3.2766e-04, -1.6238e-02, -3.5129e-03]],

         [[-2.0143e-02, -1.4314e-02,  3.0143e-02],
          [ 3.7928e-02,  2.4880e-02, -3.3681e-04],
          [ 5.5000e-03,  7.1004e-03, -6.6209e-03]],

         ...,

         [[ 3.7822e-02,  1.2878e-02, -6.0830e-03],
          [ 3.9873e-02,  1.5170e-02,  1.6047e-02],
          [-1.5886e-02,  1.6135e-03, -1.5683e-02]],

         [[ 4.2574e-02,  4.7238e-03,  2.8674e-02],
          [ 3.0364e-02,  1.3058e-02,  1.9350e-02],
          [-2.2694e-02, -3.7179e-03,  1.8592e-02]],

         [[ 1.1919e-02, -1.3507e-02, -1.4402e-02],
          [ 2.0916e-02,  1.8509e-02,  3.7515e-02],
          [-1.4505e-02,  1.4923e-02,  9.2480e-04]]],


        [[[ 1.9850e-02,  3.6534e-02, -2.5904e-02],
          [ 2.4366e-02, -3.7604e-03, -2.7428e-02],
          [ 9.1891e-03, -2.5085e-02, -1.8361e-02]],

         [[-3.1694e-02, -3.4417e-02, -4.2408e-02],
          [-1.9419e-02, -2.3110e-02, -7.1912e-03],
          [ 1.8318e-02, -1.3554e-02,  5.8220e-03]],

         [[ 1.1296e-02,  1.8623e-02,  2.8294e-02],
          [-4.3426e-03,  1.3755e-02, -5.7763e-03],
          [ 1.2482e-02, -9.1551e-03, -2.9372e-02]],

         ...,

         [[-1.0801e-02,  3.0392e-02, -1.6768e-02],
          [ 3.1346e-02, -7.9774e-03,  1.2074e-02],
          [ 9.1100e-03,  1.8882e-02, -2.0702e-02]],

         [[-2.5921e-02,  1.8660e-02, -1.8397e-02],
          [-1.6271e-02, -3.1988e-03, -2.0241e-02],
          [ 2.8504e-02, -1.0082e-02,  2.0566e-02]],

         [[ 1.3284e-02, -2.5408e-02,  3.4143e-02],
          [-1.9542e-02,  2.2894e-02,  7.1747e-04],
          [ 1.4896e-02, -6.7802e-04, -8.0938e-03]]],


        [[[-1.5626e-03, -1.4923e-03,  9.1170e-03],
          [-2.0332e-02,  6.1629e-03, -1.7468e-02],
          [-1.6369e-02,  2.1097e-03, -5.6513e-03]],

         [[ 1.3950e-02, -1.7235e-02, -1.5140e-02],
          [ 2.6217e-02,  5.4675e-06,  3.0759e-02],
          [ 3.3740e-02,  1.9938e-02, -1.4825e-02]],

         [[-1.4862e-02,  1.0517e-02, -2.4529e-02],
          [ 6.3909e-03,  1.7469e-02,  2.3854e-03],
          [-2.9030e-04, -6.4411e-03,  3.1163e-02]],

         ...,

         [[ 1.8695e-02,  2.3330e-02,  1.6362e-02],
          [-1.6535e-02, -2.5348e-02,  3.9053e-03],
          [ 2.8405e-03,  1.7526e-02, -1.0306e-02]],

         [[ 1.5793e-02,  1.6826e-02,  8.7793e-03],
          [ 8.6817e-03, -9.3006e-03, -2.0403e-02],
          [ 3.9056e-03,  1.0203e-02,  3.1754e-02]],

         [[-1.2649e-02, -1.4730e-02, -4.5374e-03],
          [-2.0530e-02,  2.1599e-03,  7.6111e-04],
          [-2.2831e-03,  6.8683e-03,  4.1689e-02]]],


        ...,


        [[[-9.9218e-03, -2.4314e-02,  1.1794e-02],
          [ 1.9529e-02, -7.4341e-03,  1.6929e-02],
          [ 2.9634e-03, -1.5404e-02,  2.4582e-02]],

         [[-2.2279e-02, -2.3758e-02, -8.9700e-03],
          [-1.8825e-02, -1.7669e-02,  1.9745e-03],
          [-3.2550e-02,  2.3476e-02, -5.9818e-03]],

         [[-6.2876e-03,  3.5192e-03, -6.3630e-03],
          [-7.5056e-03,  1.9070e-02,  2.9149e-03],
          [ 3.0461e-02,  9.9379e-03,  1.8738e-02]],

         ...,

         [[ 2.5836e-04, -1.2237e-02,  3.9477e-02],
          [-1.9948e-03,  2.0495e-02,  2.1188e-02],
          [ 1.4846e-02,  9.9641e-03,  8.9553e-03]],

         [[-2.1359e-02,  2.0334e-03, -5.3337e-03],
          [-1.7451e-02,  1.2552e-02,  5.1261e-03],
          [ 1.1989e-02, -1.5494e-03,  1.9602e-02]],

         [[-3.9204e-03, -5.1921e-03, -9.2199e-03],
          [ 5.6919e-03,  2.3579e-02, -1.7643e-02],
          [-2.0521e-03, -1.3019e-02, -5.9262e-03]]],


        [[[-8.5856e-03,  3.8436e-02,  1.0809e-02],
          [-2.3419e-03, -5.6061e-04, -7.2915e-04],
          [-2.3541e-02,  2.3399e-02,  2.2300e-02]],

         [[ 1.6429e-02, -8.3077e-03,  3.5349e-04],
          [-3.5462e-03,  3.2498e-02,  1.1831e-02],
          [ 5.4069e-03, -8.0274e-03, -2.5014e-02]],

         [[-8.9830e-03, -1.4017e-02,  6.6298e-03],
          [-1.3222e-02, -3.1474e-02, -3.1828e-03],
          [ 1.4782e-02, -1.4692e-02,  2.7510e-02]],

         ...,

         [[-2.0899e-03,  1.1824e-02, -2.0134e-02],
          [ 1.5915e-02,  7.5263e-04,  1.6776e-02],
          [ 3.0297e-02,  1.2657e-02,  2.3975e-02]],

         [[ 9.5651e-03, -2.8364e-02,  2.8040e-03],
          [-1.6784e-02,  9.8676e-04, -1.9661e-02],
          [ 1.7752e-02,  1.9386e-02,  2.4648e-02]],

         [[-4.3504e-03, -3.0860e-02, -1.9063e-02],
          [ 1.2446e-02,  1.0171e-02,  2.9906e-03],
          [ 1.5810e-02,  5.1192e-03, -2.7831e-02]]],


        [[[ 2.2181e-02, -2.6602e-02,  1.7349e-04],
          [ 1.1673e-02,  7.7368e-03, -2.8013e-02],
          [ 2.0544e-02,  2.4957e-02,  1.1253e-02]],

         [[-4.1495e-02, -1.7489e-02, -3.0630e-02],
          [ 4.5260e-04,  1.2935e-02, -1.2397e-03],
          [ 4.1699e-03, -2.7154e-02,  1.0186e-02]],

         [[-9.9356e-03,  8.1067e-04, -8.2200e-03],
          [-2.5718e-02, -1.6762e-02, -2.1876e-02],
          [ 2.1409e-03, -1.1101e-02,  1.5788e-02]],

         ...,

         [[-1.4891e-02, -2.8460e-02, -2.8412e-02],
          [ 1.0849e-02,  1.5151e-03,  9.9351e-04],
          [ 1.8912e-02, -1.5755e-02, -1.5087e-02]],

         [[-3.1761e-03, -1.7972e-02, -6.8378e-03],
          [ 2.8062e-03,  4.1489e-03,  1.9201e-02],
          [ 1.1104e-03,  1.8696e-03,  2.9264e-03]],

         [[ 2.0630e-02,  1.2984e-02,  9.8402e-04],
          [ 2.0898e-02, -7.1771e-03,  2.0835e-02],
          [-1.5115e-02, -1.5469e-02,  1.4585e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.4258e+07, 5.3317e+05, 4.9152e+05,  ..., 3.8418e+01, 3.5460e+01,
        3.3560e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 613]) 

NULL SPACE BASIS :  tensor([[ 3.7921e-02, -1.2463e-02,  6.7598e-03,  ...,  1.3496e-07,
         -5.3533e-03, -1.0058e-03],
        [-2.1209e-02,  8.3293e-02,  2.9435e-02,  ...,  4.4472e-03,
         -9.1475e-04,  3.1703e-03],
        [-4.0967e-03,  3.3517e-03,  1.6738e-02,  ..., -2.4672e-04,
          3.4072e-03, -3.1843e-03],
        ...,
        [ 3.3961e-03,  2.6326e-02,  6.8228e-03,  ..., -2.2231e-03,
          1.0984e-02,  3.3169e-03],
        [-2.9328e-02, -2.6199e-02,  2.9756e-02,  ...,  4.1394e-03,
         -7.6389e-03, -2.7773e-03],
        [-2.3706e-02, -8.0480e-03,  2.3698e-02,  ...,  4.9357e-03,
          1.5002e-03,  2.5683e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.0767e-02, -2.9430e-03, -8.6558e-04,  ...,  2.4307e-04,
          1.6245e-04,  6.9854e-04],
        [-2.9430e-03,  1.2426e-02, -2.2933e-03,  ..., -8.7715e-05,
          2.8690e-04,  3.6701e-04],
        [-8.6558e-04, -2.2933e-03,  1.1080e-02,  ...,  3.8069e-04,
         -2.9017e-04, -1.0302e-04],
        ...,
        [ 2.4307e-04, -8.7715e-05,  3.8069e-04,  ...,  2.9114e-02,
         -5.4084e-03, -2.0306e-03],
        [ 1.6245e-04,  2.8690e-04, -2.9017e-04,  ..., -5.4084e-03,
          2.8534e-02, -4.8497e-03],
        [ 6.9854e-04,  3.6701e-04, -1.0302e-04,  ..., -2.0306e-03,
         -4.8497e-03,  2.7959e-02]], device='cuda:0') 

reserving basis 793/1152; cond: 317792.71875, radio:0.008656459860503674
PARAMETER       :  Parameter containing:
tensor([[[[-1.9987e-02, -1.4917e-02,  4.1907e-02],
          [ 2.2099e-02,  8.2410e-03, -1.9126e-03],
          [ 1.6711e-02,  2.1312e-02, -2.7463e-02]],

         [[ 6.7191e-03,  7.6698e-04,  3.3189e-02],
          [-2.6151e-03,  1.6771e-02,  2.9972e-02],
          [ 2.6613e-02, -2.4443e-02, -7.0147e-03]],

         [[ 1.7102e-02,  5.6793e-03,  8.3213e-03],
          [-5.5592e-03, -8.0210e-03,  2.6110e-02],
          [ 1.1248e-02,  1.9999e-02,  2.5967e-02]],

         ...,

         [[ 7.2390e-03,  1.6345e-02,  2.6365e-02],
          [ 3.3588e-02, -2.0446e-02,  2.3340e-02],
          [ 6.2041e-04, -1.4023e-02, -1.3548e-02]],

         [[-7.6111e-03, -7.4175e-03, -1.9181e-02],
          [-2.4089e-03,  3.4046e-03,  4.8914e-03],
          [ 1.8091e-03, -6.5618e-04, -7.2423e-03]],

         [[-3.7812e-02,  9.0471e-03, -1.7543e-02],
          [ 9.6470e-03,  2.4282e-03,  1.7608e-02],
          [ 2.6494e-02, -4.4027e-03,  2.3917e-02]]],


        [[[ 5.3815e-03, -3.1534e-03,  1.5930e-02],
          [ 2.1149e-02,  1.2822e-02, -1.6209e-03],
          [-3.2130e-03, -1.0295e-02, -3.1583e-03]],

         [[ 3.2077e-03,  1.1947e-02,  1.6946e-02],
          [-2.5522e-02, -2.3619e-03, -2.4870e-02],
          [-1.3479e-02,  1.8852e-02, -2.3117e-03]],

         [[-1.8634e-02, -6.2949e-03,  2.7841e-02],
          [ 3.5484e-03, -1.1187e-02, -1.1091e-02],
          [ 2.9980e-04,  1.2751e-02, -1.2838e-02]],

         ...,

         [[-2.0469e-02, -1.5890e-02, -2.1803e-02],
          [-2.6066e-02,  7.6757e-03, -2.1122e-02],
          [ 3.2407e-02,  1.8731e-03, -3.5152e-02]],

         [[ 1.7895e-02,  2.6415e-02,  3.4579e-02],
          [-1.4077e-02,  1.9557e-02, -9.7697e-03],
          [ 2.2656e-02,  1.9047e-02,  4.0482e-02]],

         [[ 9.1920e-04,  3.6058e-03,  7.1644e-04],
          [-4.9076e-03, -9.4888e-03,  2.0742e-02],
          [ 6.5668e-03,  4.7269e-02,  4.2657e-02]]],


        [[[-2.0923e-02, -2.1100e-02,  4.1672e-03],
          [-2.2747e-02,  4.1121e-03,  1.2950e-03],
          [ 5.9144e-03,  3.1322e-03,  5.1506e-03]],

         [[-1.2757e-03,  8.6137e-03,  1.0160e-02],
          [-5.0951e-05,  2.2594e-02,  7.2077e-03],
          [ 2.1184e-03,  3.3768e-02, -1.8996e-02]],

         [[-4.7271e-03, -1.8026e-02, -2.2401e-02],
          [-2.2640e-02, -2.9838e-02, -5.8069e-03],
          [-7.6959e-03, -2.8529e-02, -1.1057e-02]],

         ...,

         [[ 1.2715e-02,  1.0425e-02,  4.2161e-02],
          [-2.1748e-02,  8.0760e-03, -1.6361e-02],
          [-1.8750e-02,  2.8681e-02, -2.2663e-02]],

         [[-3.6777e-02, -1.1160e-02, -2.3181e-03],
          [ 6.4307e-03, -1.1013e-02,  8.2220e-03],
          [-8.9062e-03,  1.7517e-02, -2.6524e-02]],

         [[ 2.2180e-03, -8.9405e-03, -1.5868e-02],
          [-2.9993e-02,  3.8592e-03,  2.4056e-02],
          [-2.1337e-02,  1.0778e-02,  1.9142e-02]]],


        ...,


        [[[-2.1125e-03,  1.0536e-02, -2.0910e-02],
          [-1.9630e-02, -1.3808e-02, -2.3738e-02],
          [-4.1674e-03, -1.5785e-02, -2.4091e-02]],

         [[ 3.0910e-02,  3.1020e-03,  6.9530e-03],
          [-8.1066e-03,  5.9055e-03, -2.3356e-02],
          [-3.5230e-02, -3.9409e-02, -1.6924e-02]],

         [[ 9.5921e-03,  1.1973e-02, -2.2636e-03],
          [-5.2360e-03, -1.6174e-02,  3.4180e-03],
          [-4.4937e-03,  3.6540e-02,  2.3029e-02]],

         ...,

         [[ 2.0162e-02, -6.8949e-03,  8.8717e-03],
          [-1.2844e-02, -2.3184e-02,  2.0530e-02],
          [ 1.2122e-02, -2.5937e-02, -1.7809e-02]],

         [[-8.0881e-03, -1.2016e-02,  2.9687e-02],
          [-3.5660e-02,  2.5684e-02,  1.5574e-02],
          [-1.9055e-02,  4.3043e-03,  1.5719e-02]],

         [[-2.0544e-02, -1.9736e-02,  4.3271e-03],
          [-2.3490e-03, -1.8814e-02, -2.3959e-02],
          [-5.3778e-03,  1.3633e-02, -1.8951e-02]]],


        [[[-3.0160e-02,  8.6513e-03,  9.8820e-03],
          [-8.2530e-03, -1.5188e-02, -2.5118e-02],
          [-2.5016e-02, -9.8427e-03,  5.1304e-05]],

         [[ 1.0532e-02,  1.7765e-02,  1.2767e-02],
          [-1.7878e-02, -2.1134e-02, -2.2703e-02],
          [-8.5506e-03, -8.2286e-03, -1.7567e-02]],

         [[-2.7597e-02, -1.9849e-02, -1.7803e-02],
          [ 1.8947e-02,  2.2010e-02,  3.0911e-02],
          [ 1.7663e-02, -1.6614e-02, -1.6458e-02]],

         ...,

         [[-3.6851e-02,  1.5146e-02,  1.0376e-02],
          [-3.2125e-02,  1.1673e-03, -5.5949e-03],
          [-2.2863e-03,  1.3979e-03,  1.1535e-02]],

         [[ 2.8439e-03, -1.2743e-02, -3.0483e-04],
          [ 4.6664e-03, -2.9793e-02, -8.7049e-04],
          [ 6.3162e-03, -4.9168e-03,  9.8265e-03]],

         [[-3.3116e-03,  6.8035e-03,  3.8749e-03],
          [-1.1471e-02,  6.1003e-04, -2.2067e-02],
          [ 5.3794e-03,  9.8462e-03, -1.7791e-02]]],


        [[[ 7.5712e-03, -7.5947e-03,  1.8503e-02],
          [ 1.2635e-02, -2.0456e-02,  9.5129e-03],
          [ 7.9579e-03,  2.1054e-02,  4.9839e-03]],

         [[ 3.1097e-02,  5.9869e-03,  9.5920e-03],
          [-9.7950e-03,  3.1541e-02,  1.7918e-02],
          [ 9.3959e-03,  1.5998e-03,  1.4446e-02]],

         [[ 6.7113e-03, -1.2825e-02,  4.7801e-03],
          [-6.5949e-03, -3.5547e-02, -1.8305e-02],
          [-3.5749e-02, -1.1765e-02,  4.7553e-03]],

         ...,

         [[ 9.1158e-03, -3.1748e-02,  1.3147e-02],
          [-3.6307e-02,  1.7778e-02,  1.2512e-03],
          [-1.5833e-02,  5.6512e-03, -1.3032e-02]],

         [[ 1.5770e-02, -2.1309e-03,  8.9362e-04],
          [ 2.5986e-03,  2.7781e-02,  2.7840e-02],
          [ 8.9089e-03,  1.4871e-02,  2.4208e-02]],

         [[ 2.0948e-02,  3.1182e-02,  2.2728e-02],
          [-6.0139e-03,  7.3869e-03, -5.4741e-03],
          [-2.1597e-02,  7.9342e-03, -1.5575e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.5622e+06, 2.0898e+05, 1.9628e+05,  ..., 1.3124e+01, 1.2627e+01,
        1.1209e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 793]) 

NULL SPACE BASIS :  tensor([[-2.9558e-02,  2.2398e-02,  2.1440e-02,  ...,  9.4160e-03,
         -1.9769e-02, -1.0051e-02],
        [-5.7080e-02,  3.3591e-02,  4.1681e-02,  ..., -1.5664e-02,
          1.9054e-02,  3.9527e-03],
        [-2.0558e-02, -7.2425e-03,  4.3245e-02,  ...,  4.5700e-05,
         -8.0115e-03,  4.6526e-03],
        ...,
        [ 9.5956e-03, -6.3921e-02,  2.1648e-02,  ..., -3.1595e-03,
         -5.5499e-03,  3.9898e-03],
        [ 1.4592e-02, -5.3819e-02,  2.0414e-02,  ...,  3.4961e-03,
          1.8535e-02, -1.4260e-03],
        [ 6.1454e-02,  2.9822e-02,  5.1609e-02,  ...,  1.7800e-03,
         -3.2321e-03, -9.2611e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.4924e-02, -4.1793e-03, -1.7936e-03,  ..., -2.5618e-04,
         -2.5495e-04,  1.0275e-04],
        [-4.1793e-03,  2.5026e-02, -4.3167e-03,  ..., -6.9179e-05,
         -1.4736e-04,  2.6021e-04],
        [-1.7936e-03, -4.3167e-03,  2.4781e-02,  ..., -1.9003e-04,
          2.8613e-04, -9.0681e-05],
        ...,
        [-2.5618e-04, -6.9179e-05, -1.9003e-04,  ...,  2.3592e-02,
         -6.3284e-03, -2.7229e-03],
        [-2.5495e-04, -1.4736e-04,  2.8613e-04,  ..., -6.3284e-03,
          2.2514e-02, -6.8206e-03],
        [ 1.0275e-04,  2.6021e-04, -9.0681e-05,  ..., -2.7229e-03,
         -6.8206e-03,  2.1315e-02]], device='cuda:0') 

reserving basis 1639/2304; cond: 516184.46875, radio:0.009835217148065567
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0041, -0.0046,  0.0004],
          [-0.0234, -0.0069,  0.0143],
          [-0.0245, -0.0062, -0.0173]],

         [[ 0.0067,  0.0214,  0.0026],
          [-0.0062, -0.0184,  0.0206],
          [-0.0120, -0.0022,  0.0131]],

         [[-0.0181, -0.0171, -0.0118],
          [-0.0019, -0.0156, -0.0206],
          [ 0.0063,  0.0082, -0.0113]],

         ...,

         [[ 0.0143,  0.0112,  0.0149],
          [-0.0171, -0.0145,  0.0097],
          [ 0.0023, -0.0007,  0.0012]],

         [[ 0.0215,  0.0006,  0.0148],
          [-0.0138,  0.0006,  0.0235],
          [ 0.0210, -0.0018,  0.0081]],

         [[ 0.0076,  0.0085, -0.0209],
          [ 0.0064, -0.0033,  0.0033],
          [ 0.0246,  0.0062,  0.0125]]],


        [[[ 0.0091, -0.0052, -0.0020],
          [ 0.0054,  0.0075, -0.0097],
          [ 0.0234, -0.0101, -0.0140]],

         [[-0.0106,  0.0116, -0.0018],
          [-0.0189,  0.0221,  0.0116],
          [ 0.0013, -0.0052, -0.0002]],

         [[-0.0155, -0.0007,  0.0124],
          [ 0.0250,  0.0059,  0.0081],
          [-0.0067,  0.0238, -0.0117]],

         ...,

         [[-0.0123,  0.0228,  0.0026],
          [ 0.0208, -0.0139,  0.0160],
          [ 0.0029, -0.0194, -0.0123]],

         [[-0.0152, -0.0261, -0.0054],
          [-0.0090,  0.0034, -0.0057],
          [-0.0156,  0.0165,  0.0180]],

         [[ 0.0076, -0.0051, -0.0200],
          [-0.0087, -0.0142,  0.0065],
          [-0.0196,  0.0095, -0.0066]]],


        [[[ 0.0166, -0.0220, -0.0119],
          [ 0.0167,  0.0189, -0.0021],
          [ 0.0179,  0.0221, -0.0242]],

         [[-0.0011,  0.0119,  0.0015],
          [-0.0105, -0.0138, -0.0199],
          [-0.0080,  0.0230,  0.0016]],

         [[-0.0035, -0.0101,  0.0018],
          [-0.0048, -0.0046,  0.0112],
          [-0.0001, -0.0140,  0.0097]],

         ...,

         [[-0.0164,  0.0223, -0.0076],
          [ 0.0180, -0.0150, -0.0032],
          [-0.0164, -0.0130, -0.0212]],

         [[-0.0037,  0.0182, -0.0120],
          [-0.0175, -0.0216, -0.0209],
          [ 0.0183, -0.0190, -0.0106]],

         [[ 0.0084,  0.0030, -0.0198],
          [ 0.0028, -0.0044,  0.0113],
          [-0.0095,  0.0003, -0.0088]]],


        ...,


        [[[ 0.0075,  0.0178,  0.0076],
          [ 0.0201, -0.0156, -0.0159],
          [-0.0043, -0.0137, -0.0121]],

         [[-0.0150, -0.0098,  0.0304],
          [ 0.0003,  0.0097, -0.0027],
          [ 0.0243, -0.0196,  0.0111]],

         [[-0.0170, -0.0007, -0.0146],
          [ 0.0047,  0.0253, -0.0256],
          [ 0.0095,  0.0313,  0.0128]],

         ...,

         [[-0.0065,  0.0114,  0.0018],
          [-0.0129, -0.0111,  0.0070],
          [ 0.0010,  0.0013, -0.0290]],

         [[-0.0120,  0.0022, -0.0115],
          [-0.0162, -0.0105, -0.0019],
          [-0.0084,  0.0044,  0.0203]],

         [[-0.0249, -0.0409, -0.0271],
          [ 0.0110, -0.0274, -0.0191],
          [ 0.0124, -0.0111,  0.0102]]],


        [[[-0.0092,  0.0160,  0.0010],
          [-0.0127, -0.0202,  0.0035],
          [ 0.0103, -0.0041, -0.0144]],

         [[-0.0034,  0.0062, -0.0036],
          [ 0.0053, -0.0147,  0.0150],
          [ 0.0086, -0.0137, -0.0162]],

         [[-0.0209, -0.0275, -0.0198],
          [ 0.0258,  0.0181,  0.0002],
          [-0.0032,  0.0091, -0.0048]],

         ...,

         [[ 0.0077, -0.0143, -0.0309],
          [ 0.0153,  0.0145, -0.0242],
          [-0.0154, -0.0230,  0.0125]],

         [[ 0.0047,  0.0007,  0.0105],
          [-0.0142, -0.0085, -0.0107],
          [-0.0082,  0.0078,  0.0043]],

         [[-0.0234,  0.0174,  0.0177],
          [-0.0263, -0.0091,  0.0264],
          [-0.0151,  0.0372,  0.0407]]],


        [[[-0.0020, -0.0306,  0.0089],
          [-0.0154, -0.0079,  0.0274],
          [-0.0199, -0.0087,  0.0094]],

         [[-0.0196, -0.0166,  0.0137],
          [-0.0109,  0.0072, -0.0066],
          [ 0.0055,  0.0122,  0.0015]],

         [[ 0.0095, -0.0068, -0.0056],
          [-0.0118,  0.0229, -0.0033],
          [-0.0219,  0.0014, -0.0048]],

         ...,

         [[-0.0023, -0.0286, -0.0009],
          [-0.0228, -0.0259,  0.0084],
          [ 0.0126, -0.0138,  0.0191]],

         [[ 0.0095,  0.0028,  0.0176],
          [ 0.0068, -0.0122, -0.0008],
          [-0.0227,  0.0145, -0.0261]],

         [[-0.0248,  0.0084, -0.0342],
          [ 0.0106, -0.0142, -0.0058],
          [ 0.0128, -0.0253, -0.0194]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([5.1573e+06, 3.1376e+05, 2.9265e+05,  ..., 1.0611e+01, 1.0276e+01,
        9.9913e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1639]) 

NULL SPACE BASIS :  tensor([[-0.0177, -0.0060,  0.0053,  ..., -0.0058,  0.0006,  0.0022],
        [ 0.0122, -0.0017, -0.0020,  ..., -0.0013,  0.0032,  0.0024],
        [ 0.0396,  0.0136,  0.0261,  ...,  0.0032,  0.0014, -0.0008],
        ...,
        [-0.0038,  0.0051,  0.0229,  ...,  0.0224,  0.0047, -0.0206],
        [-0.0357, -0.0114,  0.0219,  ..., -0.0188, -0.0150,  0.0107],
        [-0.0103, -0.0120, -0.0030,  ..., -0.0003,  0.0075, -0.0034]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.7850e-02, -4.4261e-04, -1.0585e-04,  ..., -1.0395e-04,
         -1.0989e-04,  1.6499e-05],
        [-4.4261e-04,  1.7562e-02, -6.6269e-04,  ..., -8.4544e-05,
         -1.8476e-04, -1.1482e-04],
        [-1.0585e-04, -6.6269e-04,  1.8045e-02,  ...,  1.4315e-04,
         -1.5418e-05, -1.7034e-04],
        ...,
        [-1.0395e-04, -8.4544e-05,  1.4315e-04,  ...,  2.0073e-02,
         -1.0243e-04, -1.3595e-04],
        [-1.0989e-04, -1.8476e-04, -1.5418e-05,  ..., -1.0243e-04,
          1.9676e-02, -7.4280e-05],
        [ 1.6499e-05, -1.1482e-04, -1.7034e-04,  ..., -1.3595e-04,
         -7.4280e-05,  2.0213e-02]], device='cuda:0') 

reserving basis 100/128; cond: 11785.58984375, radio:0.02277369424700737
PARAMETER       :  Parameter containing:
tensor([[[[ 2.4098e-02]],

         [[-3.3779e-02]],

         [[-2.8167e-02]],

         ...,

         [[-3.6130e-02]],

         [[-7.0955e-02]],

         [[ 7.5704e-02]]],


        [[[-4.7198e-02]],

         [[ 6.0031e-02]],

         [[ 6.9617e-02]],

         ...,

         [[-5.2172e-02]],

         [[-3.2583e-04]],

         [[-1.6175e-02]]],


        [[[ 5.7069e-02]],

         [[-2.3051e-02]],

         [[-3.2708e-02]],

         ...,

         [[ 5.3807e-02]],

         [[ 1.5886e-02]],

         [[-2.1764e-02]]],


        ...,


        [[[-6.3917e-02]],

         [[ 2.6340e-02]],

         [[ 7.8287e-02]],

         ...,

         [[-8.0435e-02]],

         [[-6.1869e-02]],

         [[ 2.6847e-02]]],


        [[[-2.4598e-02]],

         [[-6.9888e-02]],

         [[ 8.3435e-05]],

         ...,

         [[ 3.0912e-02]],

         [[ 4.9989e-02]],

         [[ 2.3290e-02]]],


        [[[ 3.6884e-02]],

         [[-3.3744e-02]],

         [[-6.6433e-02]],

         ...,

         [[-5.2041e-02]],

         [[ 6.4075e-02]],

         [[-3.3388e-02]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([4.9962e+05, 2.6494e+04, 2.1974e+04, 8.7296e+03, 5.4891e+03, 4.9122e+03,
        4.5806e+03, 3.4393e+03, 2.5431e+03, 2.4108e+03, 2.2768e+03, 1.6924e+03,
        1.4842e+03, 1.2928e+03, 1.1247e+03, 1.0626e+03, 8.8998e+02, 8.6688e+02,
        7.6504e+02, 7.0525e+02, 6.7482e+02, 6.5844e+02, 6.1355e+02, 5.6757e+02,
        5.5972e+02, 5.2699e+02, 4.8431e+02, 4.6670e+02, 4.2133e+02, 4.0814e+02,
        3.7625e+02, 3.6647e+02, 3.5121e+02, 3.3664e+02, 3.1862e+02, 3.1166e+02,
        2.9309e+02, 2.7634e+02, 2.6210e+02, 2.5892e+02, 2.5213e+02, 2.4505e+02,
        2.3961e+02, 2.3298e+02, 2.2283e+02, 2.1522e+02, 2.1106e+02, 2.0951e+02,
        1.9900e+02, 1.9658e+02, 1.8774e+02, 1.8216e+02, 1.7666e+02, 1.7426e+02,
        1.7101e+02, 1.6619e+02, 1.6109e+02, 1.5806e+02, 1.5501e+02, 1.5169e+02,
        1.4745e+02, 1.4655e+02, 1.4411e+02, 1.4021e+02, 1.3741e+02, 1.3323e+02,
        1.3115e+02, 1.3012e+02, 1.2743e+02, 1.2636e+02, 1.2401e+02, 1.2107e+02,
        1.1878e+02, 1.1702e+02, 1.1460e+02, 1.1176e+02, 1.0922e+02, 1.0755e+02,
        1.0514e+02, 1.0459e+02, 1.0244e+02, 1.0053e+02, 1.0016e+02, 9.9012e+01,
        9.6655e+01, 9.4307e+01, 9.2855e+01, 9.1257e+01, 9.0471e+01, 8.9217e+01,
        8.8248e+01, 8.6943e+01, 8.6010e+01, 8.5055e+01, 8.4257e+01, 8.3054e+01,
        8.0932e+01, 7.9401e+01, 7.8656e+01, 7.8241e+01, 7.6145e+01, 7.5474e+01,
        7.4889e+01, 7.3749e+01, 7.2802e+01, 7.0878e+01, 7.0386e+01, 6.9289e+01,
        6.8738e+01, 6.7393e+01, 6.6362e+01, 6.5651e+01, 6.3727e+01, 6.1735e+01,
        6.1124e+01, 5.9266e+01, 5.8986e+01, 5.8412e+01, 5.7924e+01, 5.6825e+01,
        5.6068e+01, 5.4336e+01, 5.4145e+01, 5.2736e+01, 5.0733e+01, 4.9501e+01,
        4.6924e+01, 4.2393e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 100]) 

NULL SPACE BASIS :  tensor([[ 0.0146,  0.1235,  0.0664,  ..., -0.0584,  0.0007, -0.0566],
        [ 0.1923,  0.0581, -0.0575,  ...,  0.1479, -0.0267,  0.0374],
        [-0.0027,  0.0385,  0.0602,  ...,  0.0052, -0.2118,  0.0889],
        ...,
        [-0.0324,  0.0368,  0.0127,  ...,  0.0590,  0.0939,  0.0260],
        [ 0.0509,  0.0574, -0.0028,  ..., -0.0766,  0.0175, -0.0586],
        [-0.0206, -0.0295, -0.0069,  ..., -0.0734, -0.0028, -0.0757]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0787,  0.0019,  0.0005,  ..., -0.0047,  0.0038, -0.0095],
        [ 0.0019,  0.0698,  0.0009,  ..., -0.0021, -0.0037,  0.0070],
        [ 0.0005,  0.0009,  0.0845,  ...,  0.0008, -0.0013, -0.0035],
        ...,
        [-0.0047, -0.0021,  0.0008,  ...,  0.0753,  0.0009, -0.0064],
        [ 0.0038, -0.0037, -0.0013,  ...,  0.0009,  0.0787, -0.0018],
        [-0.0095,  0.0070, -0.0035,  ..., -0.0064, -0.0018,  0.0687]],
       device='cuda:0') 

reserving basis 1594/2304; cond: 517497.90625, radio:0.009612226858735085
PARAMETER       :  Parameter containing:
tensor([[[[ 5.3493e-03,  1.8617e-03, -1.0238e-02],
          [ 4.5231e-03, -1.9978e-02,  8.6648e-04],
          [-6.6930e-03,  2.0962e-02, -5.9478e-03]],

         [[-2.2094e-03,  1.2118e-02,  1.6660e-03],
          [ 2.6536e-02, -6.3772e-03, -2.1396e-02],
          [-3.5171e-03,  5.7651e-03,  1.0063e-02]],

         [[ 1.4764e-02,  2.6893e-02, -1.3110e-02],
          [ 1.3472e-02,  1.0086e-02,  1.1816e-02],
          [-4.4010e-03,  9.3957e-03,  5.7236e-03]],

         ...,

         [[-6.7367e-04,  2.0497e-02,  3.0082e-03],
          [-2.8186e-02, -6.1711e-03, -3.1240e-02],
          [-5.3788e-04, -1.6000e-02, -2.4744e-03]],

         [[-2.3408e-02,  1.4122e-02,  9.6420e-03],
          [ 4.5695e-03, -2.5564e-02, -1.8772e-02],
          [-1.9429e-02, -1.5235e-02, -1.5896e-02]],

         [[ 1.2816e-03, -1.3303e-03, -2.1430e-02],
          [ 7.8607e-03,  1.1832e-02,  5.2980e-03],
          [ 2.3394e-02,  2.0577e-02, -1.3412e-02]]],


        [[[-5.6869e-03,  2.2769e-02,  3.0789e-02],
          [ 2.0384e-03, -1.9048e-02,  1.5981e-02],
          [ 6.7018e-03,  1.7943e-02, -2.4898e-02]],

         [[-1.3923e-02, -2.4555e-03, -3.9547e-02],
          [-4.6806e-05, -5.7345e-03,  8.2365e-03],
          [ 1.8463e-03,  1.8670e-02,  2.5651e-02]],

         [[-7.4482e-03, -2.3064e-02,  1.5293e-02],
          [-1.0280e-02,  4.6742e-04, -1.4457e-02],
          [ 7.0447e-03,  7.5497e-03,  1.9774e-02]],

         ...,

         [[-1.1899e-02, -2.6251e-02, -1.1096e-02],
          [-8.7878e-03,  6.0866e-03,  1.7045e-02],
          [ 3.8595e-03,  1.3854e-02, -8.4048e-03]],

         [[ 1.7140e-03, -1.7078e-02, -7.3000e-04],
          [ 1.0319e-02,  1.7404e-02,  2.2604e-02],
          [-1.6480e-02, -1.9300e-02,  1.9709e-02]],

         [[ 6.3734e-03, -1.2106e-02, -2.9080e-02],
          [-7.3648e-03,  5.4245e-03,  3.7196e-03],
          [-1.3194e-02, -2.5544e-02, -1.3386e-02]]],


        [[[-1.3886e-02,  2.6051e-04,  3.1426e-03],
          [ 1.2299e-02, -2.0525e-02, -1.7289e-02],
          [-1.0417e-02,  2.1408e-02,  4.0903e-03]],

         [[-1.6904e-02,  7.2735e-03,  2.9390e-03],
          [-1.1675e-02, -4.7673e-03,  1.8314e-02],
          [-1.8624e-02,  1.5390e-02,  1.2390e-03]],

         [[ 2.1002e-02, -9.6905e-03,  1.8744e-02],
          [-1.5261e-02, -3.7390e-03,  2.5432e-02],
          [-1.6832e-02, -1.1837e-02,  2.2163e-02]],

         ...,

         [[ 1.6891e-02, -5.9330e-03, -1.7039e-02],
          [ 1.0966e-02, -2.9514e-02, -3.7838e-03],
          [ 7.6771e-03, -2.9780e-02, -3.3655e-02]],

         [[-8.2473e-03,  9.8567e-04, -1.4480e-02],
          [ 1.3443e-02,  5.9313e-03,  4.2250e-03],
          [ 2.4748e-02, -3.1774e-02, -8.7258e-03]],

         [[-2.1381e-03, -1.1594e-02, -1.1611e-02],
          [-1.0317e-02, -1.4961e-02,  3.1439e-03],
          [ 1.7062e-02,  2.9934e-02,  1.7802e-02]]],


        ...,


        [[[-2.0022e-02,  1.1484e-02, -2.7088e-02],
          [ 4.1088e-03, -1.1651e-02, -2.9860e-02],
          [ 1.0478e-02,  3.1950e-04, -5.8487e-03]],

         [[-9.7164e-03, -2.4413e-02, -2.1405e-03],
          [ 1.4119e-02,  1.6048e-02,  7.3693e-03],
          [-1.6963e-02, -2.5116e-02, -1.1975e-02]],

         [[ 3.5971e-03, -1.7860e-02,  1.2206e-02],
          [-2.0094e-02,  1.0339e-04,  2.1130e-02],
          [ 1.6595e-02,  5.2797e-03, -1.6561e-02]],

         ...,

         [[ 1.5538e-02, -3.6687e-02,  1.2578e-02],
          [-1.2714e-02, -1.5134e-03,  6.2509e-03],
          [-1.3698e-02, -2.3592e-02,  3.5956e-03]],

         [[ 1.4800e-02, -1.7131e-03,  2.6550e-03],
          [-1.5061e-02,  1.1653e-02,  9.3657e-04],
          [-2.1115e-02,  2.4370e-03,  3.2760e-03]],

         [[-2.0087e-02, -1.1852e-02, -2.4961e-02],
          [ 9.0850e-03,  1.3102e-02, -5.1638e-03],
          [-2.2456e-02,  2.5110e-02,  3.3006e-03]]],


        [[[ 7.9397e-03, -4.8133e-03,  3.5826e-02],
          [ 6.2464e-03,  1.9247e-02,  1.1390e-02],
          [ 2.6419e-02, -5.3221e-03,  3.4437e-02]],

         [[ 4.5601e-03,  2.0423e-02, -1.7578e-02],
          [-1.3386e-02,  3.9726e-03,  2.1985e-02],
          [ 3.7208e-03,  9.9413e-03,  2.7363e-02]],

         [[-1.2467e-02,  2.8316e-03,  1.4603e-02],
          [-1.4015e-02,  8.1212e-03, -2.2826e-02],
          [ 3.7735e-03,  1.9181e-02,  1.0350e-02]],

         ...,

         [[ 9.9120e-03,  1.2171e-02, -1.4235e-02],
          [ 1.4006e-02,  1.7648e-02, -1.7966e-02],
          [ 2.8734e-04,  1.8112e-02,  3.0905e-02]],

         [[ 4.4755e-03, -9.7415e-03,  1.9177e-02],
          [ 1.0087e-02,  1.5988e-02,  2.0881e-02],
          [-1.3602e-02,  8.0024e-03,  1.4429e-02]],

         [[-1.2864e-02, -9.6163e-03,  4.5642e-03],
          [ 6.8710e-03, -4.6092e-03,  2.1450e-03],
          [-1.6166e-02, -2.8074e-03,  1.8608e-02]]],


        [[[ 2.8542e-02,  4.2592e-03, -6.7251e-03],
          [ 8.7204e-03, -4.9921e-03,  7.9399e-03],
          [-2.4493e-03, -1.2583e-02, -8.0148e-03]],

         [[-1.0155e-02, -6.7771e-03,  1.4007e-02],
          [ 1.7595e-02, -1.9765e-03,  2.4161e-02],
          [-2.0719e-03,  2.5884e-02,  1.0580e-02]],

         [[-1.2309e-02, -2.1605e-02, -3.8046e-03],
          [-1.3613e-02, -2.7325e-02, -1.3922e-02],
          [ 1.7219e-03,  7.9213e-04,  4.8018e-03]],

         ...,

         [[ 1.0316e-02,  8.3467e-03,  1.8041e-02],
          [ 1.6046e-02,  1.4159e-02, -1.2085e-02],
          [-5.6514e-04, -1.3496e-02, -1.4999e-02]],

         [[ 1.3519e-02, -1.0893e-02, -1.3562e-02],
          [-5.6137e-04, -2.3358e-02, -1.4409e-02],
          [ 4.3929e-04, -2.1112e-02, -2.0490e-02]],

         [[ 6.1549e-03, -2.6317e-03, -1.1526e-02],
          [-3.7453e-04, -8.3460e-03, -1.5957e-02],
          [ 2.1502e-02,  3.0724e-03,  1.4348e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.4475e+06, 2.9880e+05, 2.8636e+05,  ..., 9.7632e+00, 9.2769e+00,
        8.5943e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1594]) 

NULL SPACE BASIS :  tensor([[ 0.0168,  0.0144,  0.0296,  ...,  0.0084,  0.0040, -0.0081],
        [ 0.0544, -0.0432, -0.0152,  ..., -0.0037,  0.0066,  0.0088],
        [ 0.0141, -0.0337, -0.0116,  ...,  0.0056, -0.0025, -0.0003],
        ...,
        [-0.0006,  0.0013,  0.0035,  ..., -0.0068, -0.0246,  0.0018],
        [ 0.0223, -0.0060,  0.0214,  ..., -0.0165,  0.0350, -0.0068],
        [-0.0312,  0.0287, -0.0205,  ...,  0.0281, -0.0156, -0.0003]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.4395e-02, -4.0874e-03, -8.3482e-04,  ...,  1.3419e-04,
         -4.0178e-05,  6.4137e-05],
        [-4.0874e-03,  1.5026e-02, -4.3169e-03,  ..., -1.1593e-05,
          8.8686e-05, -4.4357e-05],
        [-8.3482e-04, -4.3169e-03,  1.4563e-02,  ..., -3.2909e-05,
          2.5727e-06,  4.4056e-04],
        ...,
        [ 1.3419e-04, -1.1593e-05, -3.2909e-05,  ...,  2.0453e-02,
         -5.6963e-04, -4.1250e-04],
        [-4.0178e-05,  8.8686e-05,  2.5727e-06,  ..., -5.6963e-04,
          2.0205e-02, -4.5882e-04],
        [ 6.4137e-05, -4.4357e-05,  4.4056e-04,  ..., -4.1250e-04,
         -4.5882e-04,  2.0021e-02]], device='cuda:0') 

reserving basis 1283/2304; cond: 779535.5, radio:0.00536175724118948
PARAMETER       :  Parameter containing:
tensor([[[[-1.3557e-02, -6.2041e-03, -1.5019e-02],
          [-4.1996e-03, -2.9312e-02,  6.8527e-03],
          [-1.4821e-02, -1.2746e-02, -4.5840e-03]],

         [[ 2.2944e-02,  2.6612e-02,  2.8764e-02],
          [-1.1260e-02,  1.1966e-02, -1.9112e-02],
          [-8.3752e-03,  2.4830e-02,  2.6599e-02]],

         [[-4.6044e-03, -1.2018e-02, -2.8061e-02],
          [-1.4326e-02, -1.7527e-02, -3.3118e-03],
          [ 1.3640e-02, -2.2040e-02, -1.5894e-02]],

         ...,

         [[-1.7882e-02,  2.2471e-02, -1.8697e-02],
          [ 5.0020e-03,  2.5080e-03, -2.2904e-02],
          [ 6.0984e-04,  2.2354e-03,  8.9460e-03]],

         [[ 2.0453e-02,  2.1645e-03, -2.0236e-02],
          [-1.2319e-02,  2.7391e-03, -8.8481e-03],
          [-2.7089e-02,  1.5537e-02,  1.0088e-02]],

         [[ 3.8375e-03,  2.5432e-02,  7.4008e-03],
          [-1.0318e-02,  1.4355e-02, -1.7894e-02],
          [ 9.3088e-03, -4.7648e-03,  1.5759e-02]]],


        [[[ 1.2043e-03,  2.0799e-02,  3.3066e-02],
          [-2.4035e-02,  1.9246e-02,  8.1860e-03],
          [ 3.9524e-03, -4.4781e-03,  7.4053e-03]],

         [[ 2.0823e-02,  1.6496e-02, -1.9833e-02],
          [ 1.2398e-02,  2.8890e-03, -3.8135e-03],
          [-2.9257e-02, -1.1835e-02, -1.7674e-02]],

         [[ 2.3187e-03, -3.0733e-03,  9.6416e-03],
          [ 4.5700e-03, -2.6214e-02, -1.6841e-02],
          [ 1.5219e-02, -8.6733e-03,  1.6387e-02]],

         ...,

         [[-1.8693e-02, -3.6378e-03, -1.4306e-02],
          [ 8.1276e-03,  1.2517e-02, -3.6304e-04],
          [-1.7507e-03,  1.4122e-02, -9.5940e-03]],

         [[ 2.6982e-03, -7.1893e-03,  2.0749e-02],
          [-1.1416e-03, -4.2511e-03,  2.0794e-02],
          [ 6.1736e-03,  9.8880e-03,  1.4653e-02]],

         [[-2.2870e-02, -1.5767e-02, -2.1936e-02],
          [-1.8635e-02, -1.5360e-02, -1.7591e-02],
          [-1.9400e-02, -1.7068e-02,  2.9379e-03]]],


        [[[-2.8303e-02, -1.9946e-02, -1.5984e-02],
          [ 1.4743e-03,  1.7097e-02,  7.0577e-03],
          [ 1.1024e-02, -8.2816e-03, -1.6085e-02]],

         [[-1.4681e-02,  1.6258e-02,  5.1088e-03],
          [-8.7932e-03,  3.3159e-03,  3.4633e-03],
          [-3.0496e-03, -1.4339e-02,  1.8897e-02]],

         [[-8.3447e-03,  7.4250e-03,  5.7447e-04],
          [-8.9912e-04, -1.6707e-02, -2.4508e-02],
          [ 9.1295e-03, -5.3218e-03,  2.3436e-03]],

         ...,

         [[-5.4251e-03,  2.7344e-02, -8.9612e-03],
          [-1.8156e-02,  1.6294e-03, -1.5941e-02],
          [-2.0647e-02, -2.3877e-02,  7.6056e-03]],

         [[-9.9715e-05,  2.4804e-03, -7.0871e-04],
          [-3.1772e-03,  1.3165e-02,  8.9018e-03],
          [-1.8447e-02, -1.6928e-02,  1.3044e-03]],

         [[-2.0026e-02, -1.0487e-02, -1.0396e-04],
          [ 1.0755e-02, -8.1263e-03,  1.9467e-02],
          [-6.5149e-03, -1.6152e-03, -1.2998e-02]]],


        ...,


        [[[-1.3609e-02, -1.6537e-02,  1.7202e-03],
          [-1.4547e-03,  1.0918e-02,  1.1597e-02],
          [-1.2698e-02, -1.2322e-02,  4.3182e-03]],

         [[ 4.9978e-04,  2.1159e-02, -2.6712e-03],
          [ 1.4553e-02, -2.4359e-03,  2.8975e-02],
          [-2.4016e-03, -1.3668e-02, -1.0547e-02]],

         [[-2.2698e-03, -1.9664e-02,  6.8304e-03],
          [-5.9409e-04,  1.9989e-02, -4.1529e-03],
          [-3.6413e-02, -6.5231e-03,  2.3093e-02]],

         ...,

         [[ 2.4113e-02,  4.4099e-03,  1.4052e-02],
          [ 9.1581e-03,  1.7680e-02,  1.6485e-02],
          [-1.8709e-03, -1.0713e-02,  6.5115e-03]],

         [[ 3.3821e-03, -1.4358e-02, -2.3271e-02],
          [-4.6293e-03,  1.8879e-02, -5.9935e-03],
          [ 1.1100e-02, -1.3096e-02,  1.6616e-02]],

         [[ 1.2221e-02,  1.3894e-02, -6.7205e-03],
          [-1.1639e-02, -2.2040e-03,  1.7759e-02],
          [ 1.2686e-02, -1.9449e-03,  6.9997e-03]]],


        [[[ 9.2506e-03, -1.7795e-02,  7.0388e-03],
          [ 2.7085e-02,  4.6666e-03, -9.6728e-03],
          [-2.5120e-03,  2.2434e-03, -1.6920e-02]],

         [[ 2.5054e-03, -9.2165e-03,  1.4224e-02],
          [ 1.7836e-02,  1.9299e-02,  3.5131e-02],
          [ 2.9792e-02,  2.0239e-02,  3.3505e-02]],

         [[-1.3405e-02,  1.6159e-02, -7.7704e-03],
          [ 2.7239e-02,  3.0244e-02,  1.1429e-02],
          [-1.7414e-02,  6.0602e-04,  2.4147e-02]],

         ...,

         [[-1.3743e-02,  6.7582e-03,  2.0679e-02],
          [ 1.6922e-02,  9.5204e-03,  1.1788e-02],
          [-1.3921e-02,  9.5595e-03, -1.4967e-02]],

         [[-2.1625e-02, -2.0209e-02,  8.4065e-03],
          [-1.0859e-02,  9.2779e-03, -8.4003e-04],
          [ 1.6985e-02, -1.4808e-02, -8.4282e-03]],

         [[-1.8898e-02,  2.0189e-03,  2.2221e-02],
          [ 1.3603e-02,  2.0324e-02, -1.1736e-02],
          [ 1.3240e-02,  1.1900e-02, -7.1669e-03]]],


        [[[ 1.5459e-02, -1.1322e-02, -2.0888e-03],
          [-6.3831e-03, -1.0818e-02, -2.2939e-03],
          [-2.1175e-02, -1.8827e-02, -1.0137e-03]],

         [[ 5.0918e-03, -6.0264e-03, -1.3334e-02],
          [-6.4646e-03, -2.2201e-03, -1.7901e-02],
          [ 3.5252e-04, -2.1063e-02, -9.8084e-03]],

         [[ 3.0666e-02,  1.6034e-03,  1.9136e-03],
          [ 2.5051e-02,  4.5647e-03,  9.5815e-03],
          [-2.5197e-03, -5.9811e-03, -1.3635e-02]],

         ...,

         [[ 1.4732e-02,  1.2568e-02,  1.8995e-03],
          [ 1.9970e-02, -7.5188e-03,  5.8795e-03],
          [-6.1697e-03, -3.5573e-03,  1.8827e-05]],

         [[-6.7826e-03,  3.7672e-03, -2.0132e-02],
          [ 2.0772e-02,  1.8554e-02,  1.2831e-02],
          [ 6.2259e-04,  3.8323e-02,  3.3128e-02]],

         [[ 2.0535e-02, -6.5121e-04, -1.1119e-02],
          [ 7.5906e-03,  7.7677e-03, -1.2516e-02],
          [-3.0845e-02, -3.6748e-03, -1.7400e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.0159e+06, 2.9371e+05, 2.6804e+05,  ..., 6.0279e+00, 5.9179e+00,
        5.1517e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1283]) 

NULL SPACE BASIS :  tensor([[-0.0141, -0.0156, -0.0025,  ..., -0.0007,  0.0060,  0.0089],
        [ 0.0263,  0.0139,  0.0042,  ..., -0.0019, -0.0035, -0.0049],
        [-0.0104,  0.0031,  0.0314,  ...,  0.0035, -0.0079, -0.0044],
        ...,
        [ 0.0066,  0.0249,  0.0179,  ..., -0.0067,  0.0040, -0.0020],
        [-0.0008, -0.0003, -0.0038,  ...,  0.0033, -0.0047,  0.0013],
        [ 0.0085, -0.0075, -0.0229,  ...,  0.0002, -0.0003,  0.0015]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.8319e-02, -3.1147e-03, -6.5214e-04,  ...,  2.7158e-04,
         -3.0999e-04,  1.9346e-04],
        [-3.1147e-03,  1.7965e-02, -3.3340e-03,  ...,  8.2382e-05,
          1.6565e-04, -4.3165e-07],
        [-6.5214e-04, -3.3340e-03,  2.0080e-02,  ...,  1.0922e-04,
          2.7432e-05,  3.2094e-04],
        ...,
        [ 2.7158e-04,  8.2382e-05,  1.0922e-04,  ...,  1.0025e-02,
         -2.4271e-03, -3.7978e-05],
        [-3.0999e-04,  1.6565e-04,  2.7432e-05,  ..., -2.4271e-03,
          9.6179e-03, -2.5311e-03],
        [ 1.9346e-04, -4.3165e-07,  3.2094e-04,  ..., -3.7978e-05,
         -2.5311e-03,  9.2202e-03]], device='cuda:0') 

reserving basis 1150/2304; cond: 1220155.75, radio:0.003168757539242506
PARAMETER       :  Parameter containing:
tensor([[[[-5.7746e-03, -8.9813e-03, -6.6836e-03],
          [-8.0342e-03,  1.5844e-02,  1.3082e-02],
          [-2.3848e-02, -4.1879e-03, -2.7784e-03]],

         [[ 4.1711e-04,  2.0039e-03, -1.2488e-02],
          [-8.0047e-03,  1.1867e-02, -5.1465e-03],
          [ 1.7029e-02, -2.4837e-02,  4.8009e-03]],

         [[ 1.4800e-02, -5.5273e-03,  8.6443e-03],
          [ 1.2248e-02, -1.0553e-02,  1.6179e-03],
          [-1.8447e-02, -1.1544e-02, -7.1936e-03]],

         ...,

         [[ 1.4950e-02, -1.4805e-02,  1.7275e-03],
          [ 1.0337e-02,  1.0572e-02,  1.4115e-02],
          [-1.5920e-03, -2.5792e-02,  1.2822e-02]],

         [[ 2.0138e-02,  1.1133e-02, -2.3090e-03],
          [-1.0371e-02,  2.6859e-03, -1.0941e-02],
          [-2.6113e-03,  1.8002e-02, -1.6784e-02]],

         [[-6.1968e-03, -4.5056e-03,  7.7749e-03],
          [-3.1808e-02,  2.6636e-03, -8.7783e-04],
          [ 5.1625e-03, -1.0559e-02,  2.0208e-02]]],


        [[[-1.1871e-02,  8.4966e-03,  7.5961e-03],
          [ 1.0677e-02, -7.1947e-03,  1.5343e-02],
          [ 1.5834e-02, -5.9691e-03, -1.0394e-03]],

         [[ 1.7930e-02, -1.5295e-03, -1.9250e-02],
          [ 1.0692e-03, -4.7033e-03,  8.2743e-03],
          [ 1.3650e-02, -2.1240e-02, -1.5580e-02]],

         [[ 1.4753e-02, -2.6500e-02,  2.1856e-03],
          [-2.2971e-02,  6.8123e-04,  8.3360e-03],
          [-1.1340e-02, -1.8048e-02,  4.0434e-03]],

         ...,

         [[-2.1164e-02, -1.8037e-02, -7.8655e-03],
          [ 6.6688e-04, -1.1740e-02, -2.5534e-02],
          [ 4.0816e-03, -4.4935e-03, -9.8969e-05]],

         [[-8.9175e-03,  5.8170e-03, -6.5436e-03],
          [ 8.5555e-03,  2.4129e-02, -9.9350e-03],
          [-1.4969e-02,  1.1755e-02, -8.7235e-03]],

         [[ 1.0884e-02,  2.0990e-02,  4.1327e-03],
          [ 9.7396e-04,  6.1537e-03, -9.3118e-03],
          [-8.4240e-03,  2.6038e-02, -2.7065e-03]]],


        [[[ 3.3484e-03, -2.4344e-02, -4.3605e-03],
          [-3.3012e-03,  1.0914e-02, -5.0002e-03],
          [ 7.0764e-03,  1.0291e-02,  3.4017e-03]],

         [[ 2.1946e-02,  1.9405e-02,  2.1138e-02],
          [ 2.0494e-02,  9.3769e-03,  5.7532e-03],
          [ 9.2189e-03, -3.5953e-03,  9.4382e-03]],

         [[-1.7971e-02, -1.4137e-02,  2.5317e-03],
          [ 5.9595e-03, -1.9852e-02, -5.9526e-03],
          [-1.3317e-02, -8.6109e-03, -3.1069e-02]],

         ...,

         [[ 1.3328e-03, -4.2590e-03, -3.2243e-02],
          [-5.2671e-03,  3.2695e-03, -8.3138e-03],
          [ 1.9067e-02,  1.1093e-03, -3.1035e-03]],

         [[ 1.2019e-02,  4.2768e-03, -4.7473e-03],
          [-5.7032e-03, -1.6363e-03,  6.2986e-03],
          [-2.2739e-02, -2.2547e-02, -1.4884e-02]],

         [[ 3.4313e-04,  4.7981e-03, -6.5299e-03],
          [ 5.1268e-04,  1.6423e-02, -3.3531e-04],
          [ 1.0914e-02,  1.1092e-02, -9.7455e-03]]],


        ...,


        [[[-3.8664e-04,  1.0601e-03, -1.4067e-02],
          [-1.4518e-02,  2.2181e-03,  3.9524e-04],
          [ 7.5427e-03, -1.0683e-02, -8.0721e-03]],

         [[ 1.6724e-02, -3.7043e-03, -1.1224e-02],
          [ 2.1468e-02,  1.5110e-02, -1.5008e-02],
          [-6.4234e-03,  3.1449e-03, -1.4589e-02]],

         [[-2.0910e-02, -2.1293e-03,  7.8282e-03],
          [-6.7010e-03, -1.5247e-02,  6.2890e-03],
          [ 4.7476e-03,  3.4112e-03, -2.5037e-02]],

         ...,

         [[ 8.6409e-03, -1.3011e-02,  7.5478e-03],
          [ 1.2930e-03,  5.0227e-03, -6.0557e-03],
          [-5.0151e-03, -7.3240e-03, -2.9633e-02]],

         [[-1.5664e-02,  6.8023e-03, -2.8742e-03],
          [ 7.8805e-03, -2.3659e-02, -2.1161e-02],
          [-7.5016e-03, -1.3590e-02, -1.1639e-02]],

         [[ 1.0274e-02,  1.8558e-02,  6.7027e-03],
          [-1.0187e-02,  1.2060e-02, -1.0543e-02],
          [-2.2782e-03,  9.6853e-03, -6.2166e-03]]],


        [[[-1.8972e-02, -3.9505e-03, -1.3028e-03],
          [-1.0397e-02,  2.3687e-03,  7.5591e-03],
          [ 2.0068e-03,  1.1363e-02,  7.8268e-03]],

         [[-1.2241e-02,  2.1537e-02,  1.0992e-02],
          [-3.2608e-03,  2.7491e-03,  1.1467e-02],
          [ 8.5386e-03, -1.0301e-02,  1.1888e-02]],

         [[-7.3821e-03,  4.4405e-03,  1.8253e-02],
          [ 1.3837e-02, -5.7074e-03,  1.4294e-02],
          [-4.7986e-03, -1.0582e-02, -1.2196e-02]],

         ...,

         [[ 1.1773e-02, -1.8165e-02, -1.7685e-03],
          [-2.7116e-03, -1.5352e-02,  1.0587e-02],
          [-1.5481e-02, -1.2514e-02,  3.8772e-03]],

         [[-2.7298e-03,  7.0029e-03, -2.6435e-02],
          [-1.5798e-02, -5.8468e-03, -3.3689e-02],
          [-1.4891e-02, -2.0148e-02, -5.8590e-03]],

         [[ 1.1769e-02,  1.7959e-02,  2.7394e-02],
          [ 1.4260e-02,  2.6896e-03,  2.9704e-03],
          [ 7.9968e-03, -1.0870e-03,  1.2270e-02]]],


        [[[ 2.1746e-03,  8.1254e-03,  2.3110e-02],
          [-3.8529e-03, -3.1119e-03, -1.6679e-02],
          [ 6.2813e-03,  5.0916e-03, -8.1525e-03]],

         [[-1.9085e-04, -4.2847e-03, -1.0644e-02],
          [-1.0208e-02, -1.0907e-02, -1.6701e-02],
          [ 6.4974e-03, -2.6493e-02,  3.2332e-03]],

         [[ 1.0981e-03, -1.8229e-02,  7.7897e-03],
          [-1.3090e-02,  9.8790e-03,  4.5516e-03],
          [-2.0888e-02,  5.2682e-04, -9.2603e-03]],

         ...,

         [[-1.4597e-02, -1.6311e-02,  2.0044e-02],
          [-1.2042e-02, -1.2098e-02, -2.2938e-02],
          [ 9.1316e-03, -1.3835e-02,  1.8853e-02]],

         [[ 2.1391e-02,  8.8396e-03, -8.5373e-03],
          [ 1.3740e-02,  1.1046e-02,  1.6463e-02],
          [-9.7655e-03, -2.1411e-02, -9.5107e-04]],

         [[-9.4903e-03, -9.8672e-03,  1.6955e-03],
          [-1.7224e-02,  2.8037e-02,  1.5734e-02],
          [-2.0755e-02, -4.5813e-03, -1.7990e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.2218e+06, 1.3674e+05, 1.3117e+05,  ..., 1.0764e+00, 1.0457e+00,
        1.0013e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1150]) 

NULL SPACE BASIS :  tensor([[ 0.0189,  0.0234,  0.0018,  ...,  0.0141,  0.0003, -0.0093],
        [-0.0051,  0.0067, -0.0195,  ..., -0.0095,  0.0071,  0.0048],
        [ 0.0134, -0.0047,  0.0125,  ...,  0.0087, -0.0029, -0.0021],
        ...,
        [-0.0234,  0.0175,  0.0316,  ..., -0.0048, -0.0277,  0.0231],
        [-0.0103,  0.0152,  0.0019,  ...,  0.0262,  0.0145, -0.0073],
        [-0.0012,  0.0124, -0.0347,  ..., -0.0079,  0.0039, -0.0047]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.7314e-03, -3.2100e-03,  4.2054e-04,  ...,  2.0349e-04,
         -1.0452e-04, -8.0720e-05],
        [-3.2100e-03,  7.9994e-03, -3.5066e-03,  ...,  2.9547e-05,
         -1.9113e-05,  1.1937e-04],
        [ 4.2054e-04, -3.5066e-03,  6.1172e-03,  ...,  3.2182e-04,
          4.1174e-04, -3.6409e-04],
        ...,
        [ 2.0349e-04,  2.9547e-05,  3.2182e-04,  ...,  1.8804e-02,
         -3.2033e-03, -9.7463e-04],
        [-1.0452e-04, -1.9113e-05,  4.1174e-04,  ..., -3.2033e-03,
          1.8064e-02, -3.5800e-03],
        [-8.0720e-05,  1.1937e-04, -3.6409e-04,  ..., -9.7463e-04,
         -3.5800e-03,  1.6555e-02]], device='cuda:0') 

reserving basis 1051/4608; cond: 10862754.0, radio:0.00028564591775648296
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0046,  0.0042,  0.0127],
          [-0.0074, -0.0086,  0.0125],
          [-0.0042, -0.0050,  0.0096]],

         [[ 0.0042, -0.0059,  0.0020],
          [ 0.0056,  0.0098,  0.0021],
          [ 0.0175, -0.0071,  0.0087]],

         [[-0.0025, -0.0143, -0.0002],
          [-0.0052, -0.0065, -0.0230],
          [-0.0113,  0.0097, -0.0210]],

         ...,

         [[-0.0048,  0.0021,  0.0040],
          [-0.0024, -0.0021, -0.0180],
          [-0.0106, -0.0015, -0.0256]],

         [[-0.0050,  0.0102,  0.0135],
          [ 0.0029,  0.0166,  0.0039],
          [ 0.0005,  0.0051,  0.0052]],

         [[-0.0142, -0.0314, -0.0288],
          [-0.0066, -0.0004, -0.0201],
          [ 0.0072,  0.0012, -0.0007]]],


        [[[ 0.0082, -0.0161,  0.0086],
          [-0.0039,  0.0055,  0.0066],
          [ 0.0050, -0.0130, -0.0117]],

         [[-0.0148,  0.0079, -0.0093],
          [-0.0035, -0.0025, -0.0034],
          [ 0.0073,  0.0048, -0.0104]],

         [[-0.0009,  0.0002,  0.0113],
          [ 0.0009,  0.0130,  0.0015],
          [ 0.0110,  0.0230,  0.0182]],

         ...,

         [[-0.0025,  0.0033, -0.0092],
          [ 0.0006, -0.0065, -0.0022],
          [-0.0017,  0.0069,  0.0138]],

         [[ 0.0052, -0.0040, -0.0125],
          [-0.0042, -0.0034,  0.0032],
          [-0.0049,  0.0006, -0.0119]],

         [[ 0.0001,  0.0008, -0.0054],
          [-0.0157, -0.0020, -0.0003],
          [ 0.0108, -0.0065, -0.0046]]],


        [[[ 0.0031,  0.0106,  0.0032],
          [ 0.0081,  0.0151,  0.0195],
          [-0.0070, -0.0021,  0.0044]],

         [[ 0.0010,  0.0130,  0.0084],
          [-0.0122, -0.0071,  0.0035],
          [-0.0103, -0.0076, -0.0057]],

         [[-0.0042, -0.0071,  0.0063],
          [-0.0064, -0.0115,  0.0094],
          [-0.0014, -0.0051, -0.0013]],

         ...,

         [[ 0.0081,  0.0029, -0.0092],
          [ 0.0072, -0.0015, -0.0117],
          [ 0.0087,  0.0040,  0.0017]],

         [[ 0.0089,  0.0058,  0.0207],
          [-0.0069, -0.0052,  0.0191],
          [-0.0043,  0.0034, -0.0031]],

         [[-0.0046,  0.0049, -0.0128],
          [-0.0118,  0.0033, -0.0068],
          [-0.0073,  0.0107,  0.0086]]],


        ...,


        [[[ 0.0060, -0.0040, -0.0163],
          [-0.0103,  0.0051, -0.0036],
          [ 0.0018,  0.0108, -0.0141]],

         [[-0.0065,  0.0007,  0.0044],
          [-0.0079,  0.0154, -0.0003],
          [-0.0065,  0.0091, -0.0083]],

         [[ 0.0011, -0.0098,  0.0012],
          [-0.0033, -0.0262, -0.0112],
          [ 0.0127, -0.0200, -0.0178]],

         ...,

         [[-0.0183,  0.0057, -0.0071],
          [-0.0076, -0.0055, -0.0124],
          [-0.0068,  0.0106,  0.0062]],

         [[-0.0007, -0.0040, -0.0039],
          [-0.0063,  0.0053,  0.0130],
          [ 0.0093, -0.0057, -0.0098]],

         [[ 0.0141,  0.0137,  0.0056],
          [-0.0011,  0.0047,  0.0022],
          [-0.0006,  0.0003, -0.0015]]],


        [[[-0.0136, -0.0037,  0.0035],
          [-0.0128,  0.0087, -0.0184],
          [-0.0061, -0.0024,  0.0030]],

         [[ 0.0018, -0.0141, -0.0228],
          [ 0.0099,  0.0043, -0.0164],
          [ 0.0092,  0.0046,  0.0050]],

         [[-0.0069,  0.0009,  0.0108],
          [ 0.0017, -0.0014, -0.0060],
          [-0.0068, -0.0048, -0.0072]],

         ...,

         [[ 0.0025,  0.0011, -0.0074],
          [-0.0241, -0.0176, -0.0171],
          [-0.0005, -0.0028,  0.0138]],

         [[ 0.0041, -0.0036,  0.0032],
          [-0.0038,  0.0001, -0.0009],
          [ 0.0100,  0.0098,  0.0045]],

         [[ 0.0073, -0.0142, -0.0134],
          [ 0.0020, -0.0033,  0.0140],
          [-0.0134,  0.0033, -0.0050]]],


        [[[ 0.0088, -0.0066, -0.0034],
          [-0.0147, -0.0009,  0.0048],
          [ 0.0013, -0.0095, -0.0120]],

         [[-0.0049,  0.0138,  0.0024],
          [ 0.0055, -0.0103,  0.0004],
          [ 0.0041,  0.0071, -0.0057]],

         [[-0.0122, -0.0036,  0.0165],
          [-0.0028,  0.0047,  0.0181],
          [-0.0084, -0.0011,  0.0025]],

         ...,

         [[ 0.0137,  0.0053, -0.0002],
          [ 0.0116, -0.0141,  0.0058],
          [ 0.0018, -0.0149,  0.0106]],

         [[ 0.0202,  0.0243,  0.0027],
          [ 0.0168,  0.0020, -0.0047],
          [-0.0095,  0.0130, -0.0121]],

         [[-0.0042, -0.0134,  0.0083],
          [-0.0116, -0.0055, -0.0052],
          [ 0.0106, -0.0029,  0.0020]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.4519e+06, 2.3664e+05, 2.2744e+05,  ..., 1.5104e-01, 1.4978e-01,
        1.3366e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1051]) 

NULL SPACE BASIS :  tensor([[-0.0027,  0.0085,  0.0087,  ..., -0.0078,  0.0049,  0.0112],
        [ 0.0110,  0.0010,  0.0068,  ..., -0.0131,  0.0098,  0.0042],
        [-0.0242,  0.0216,  0.0211,  ..., -0.0053,  0.0107,  0.0041],
        ...,
        [ 0.0273,  0.0098, -0.0048,  ...,  0.0036,  0.0150, -0.0095],
        [-0.0014,  0.0042, -0.0289,  ...,  0.0087, -0.0092, -0.0041],
        [-0.0060,  0.0278,  0.0149,  ...,  0.0108,  0.0054, -0.0094]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.2979e-03,  6.3560e-06, -1.6598e-04,  ..., -3.7443e-04,
         -1.1718e-04,  1.0937e-04],
        [ 6.3560e-06,  5.0724e-03, -1.0791e-04,  ..., -1.5179e-04,
         -8.4096e-06, -2.4503e-04],
        [-1.6598e-04, -1.0791e-04,  6.0167e-03,  ..., -2.1175e-04,
         -1.0567e-04,  9.4559e-05],
        ...,
        [-3.7443e-04, -1.5179e-04, -2.1175e-04,  ...,  8.7076e-03,
         -5.5698e-04, -1.7503e-04],
        [-1.1718e-04, -8.4096e-06, -1.0567e-04,  ..., -5.5698e-04,
          6.1780e-03, -1.6086e-04],
        [ 1.0937e-04, -2.4503e-04,  9.4559e-05,  ..., -1.7503e-04,
         -1.6086e-04,  6.6321e-03]], device='cuda:0') 

reserving basis 197/256; cond: 43795.375, radio:0.013166823424398899
PARAMETER       :  Parameter containing:
tensor([[[[-0.0179]],

         [[-0.0354]],

         [[-0.0424]],

         ...,

         [[ 0.0077]],

         [[-0.0454]],

         [[-0.0537]]],


        [[[ 0.0655]],

         [[-0.0604]],

         [[ 0.0161]],

         ...,

         [[ 0.0056]],

         [[ 0.0067]],

         [[ 0.0177]]],


        [[[ 0.0419]],

         [[ 0.0278]],

         [[-0.0064]],

         ...,

         [[ 0.0205]],

         [[ 0.0362]],

         [[-0.0335]]],


        ...,


        [[[-0.0266]],

         [[ 0.0081]],

         [[ 0.0456]],

         ...,

         [[ 0.0169]],

         [[-0.0175]],

         [[-0.0105]]],


        [[[ 0.0163]],

         [[ 0.0212]],

         [[-0.0264]],

         ...,

         [[ 0.0409]],

         [[ 0.0107]],

         [[ 0.0466]]],


        [[[-0.0358]],

         [[ 0.0260]],

         [[ 0.0378]],

         ...,

         [[ 0.0537]],

         [[-0.0626]],

         [[ 0.0352]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.4379e+05, 1.4321e+04, 1.1754e+04, 5.3138e+03, 4.2534e+03, 3.2935e+03,
        2.4034e+03, 1.2780e+03, 9.6592e+02, 7.7399e+02, 6.8845e+02, 5.9706e+02,
        4.9751e+02, 4.6873e+02, 4.3347e+02, 3.8462e+02, 3.8028e+02, 3.6094e+02,
        3.4689e+02, 3.1383e+02, 2.9365e+02, 2.7232e+02, 2.5683e+02, 2.4532e+02,
        2.0974e+02, 2.0105e+02, 1.8536e+02, 1.7862e+02, 1.7084e+02, 1.5702e+02,
        1.4916e+02, 1.4224e+02, 1.3849e+02, 1.3037e+02, 1.2697e+02, 1.2468e+02,
        1.1557e+02, 1.0525e+02, 1.0453e+02, 1.0089e+02, 9.8113e+01, 9.5002e+01,
        8.8257e+01, 8.6395e+01, 8.3928e+01, 8.2037e+01, 7.9969e+01, 7.6638e+01,
        7.5976e+01, 7.2807e+01, 6.9667e+01, 6.7978e+01, 6.6737e+01, 6.3036e+01,
        6.1516e+01, 6.1383e+01, 5.9761e+01, 5.7516e+01, 5.5744e+01, 5.4608e+01,
        5.4045e+01, 5.0552e+01, 5.0397e+01, 4.9506e+01, 4.8764e+01, 4.8534e+01,
        4.7035e+01, 4.5963e+01, 4.5106e+01, 4.4907e+01, 4.4260e+01, 4.3684e+01,
        4.2408e+01, 4.1240e+01, 4.0397e+01, 4.0131e+01, 3.8903e+01, 3.8481e+01,
        3.7900e+01, 3.7737e+01, 3.7078e+01, 3.6265e+01, 3.5780e+01, 3.5474e+01,
        3.5106e+01, 3.4403e+01, 3.4275e+01, 3.3754e+01, 3.2915e+01, 3.2672e+01,
        3.2568e+01, 3.2020e+01, 3.1643e+01, 3.1285e+01, 3.1040e+01, 3.0777e+01,
        3.0196e+01, 2.9703e+01, 2.9519e+01, 2.9303e+01, 2.8751e+01, 2.8148e+01,
        2.8093e+01, 2.7911e+01, 2.7403e+01, 2.6933e+01, 2.6747e+01, 2.6458e+01,
        2.6213e+01, 2.5602e+01, 2.5525e+01, 2.5151e+01, 2.4840e+01, 2.4790e+01,
        2.4563e+01, 2.4024e+01, 2.4015e+01, 2.3952e+01, 2.3683e+01, 2.3408e+01,
        2.3154e+01, 2.3072e+01, 2.2802e+01, 2.2597e+01, 2.2176e+01, 2.2053e+01,
        2.1866e+01, 2.1626e+01, 2.1205e+01, 2.0915e+01, 2.0803e+01, 2.0701e+01,
        2.0409e+01, 2.0198e+01, 2.0040e+01, 1.9933e+01, 1.9799e+01, 1.9615e+01,
        1.9327e+01, 1.9209e+01, 1.9116e+01, 1.8869e+01, 1.8830e+01, 1.8720e+01,
        1.8531e+01, 1.8394e+01, 1.8275e+01, 1.7939e+01, 1.7876e+01, 1.7661e+01,
        1.7593e+01, 1.7560e+01, 1.7540e+01, 1.7327e+01, 1.7220e+01, 1.7073e+01,
        1.6726e+01, 1.6699e+01, 1.6558e+01, 1.6408e+01, 1.6321e+01, 1.6128e+01,
        1.5989e+01, 1.5870e+01, 1.5760e+01, 1.5697e+01, 1.5613e+01, 1.5454e+01,
        1.5374e+01, 1.5248e+01, 1.5124e+01, 1.5065e+01, 1.4847e+01, 1.4777e+01,
        1.4694e+01, 1.4578e+01, 1.4425e+01, 1.4284e+01, 1.4196e+01, 1.4047e+01,
        1.3941e+01, 1.3824e+01, 1.3810e+01, 1.3556e+01, 1.3392e+01, 1.3343e+01,
        1.3204e+01, 1.3191e+01, 1.3052e+01, 1.3009e+01, 1.2846e+01, 1.2710e+01,
        1.2652e+01, 1.2586e+01, 1.2495e+01, 1.2363e+01, 1.2315e+01, 1.2139e+01,
        1.1949e+01, 1.1876e+01, 1.1750e+01, 1.1618e+01, 1.1499e+01, 1.1448e+01,
        1.1359e+01, 1.1337e+01, 1.1242e+01, 1.1141e+01, 1.1073e+01, 1.0960e+01,
        1.0836e+01, 1.0773e+01, 1.0627e+01, 1.0543e+01, 1.0502e+01, 1.0404e+01,
        1.0339e+01, 1.0165e+01, 1.0129e+01, 1.0075e+01, 1.0047e+01, 9.8016e+00,
        9.7595e+00, 9.6635e+00, 9.5602e+00, 9.4465e+00, 9.3509e+00, 9.2316e+00,
        9.1945e+00, 9.0602e+00, 8.9760e+00, 8.9044e+00, 8.8227e+00, 8.7256e+00,
        8.5929e+00, 8.5605e+00, 8.4363e+00, 8.3877e+00, 8.2780e+00, 8.2683e+00,
        8.0913e+00, 8.0055e+00, 7.8765e+00, 7.8384e+00, 7.7181e+00, 7.6592e+00,
        7.4435e+00, 7.2937e+00, 7.1946e+00, 7.0334e+00, 6.9977e+00, 6.8559e+00,
        6.5551e+00, 6.4962e+00, 6.3053e+00, 5.5667e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 197]) 

NULL SPACE BASIS :  tensor([[ 0.1268,  0.0009, -0.0896,  ..., -0.0180,  0.0138,  0.0066],
        [-0.0146,  0.0741, -0.0613,  ..., -0.0186, -0.0227,  0.0409],
        [ 0.0209, -0.0359, -0.0805,  ..., -0.0016,  0.1025,  0.0463],
        ...,
        [ 0.0282, -0.0547, -0.0595,  ..., -0.0121, -0.0570, -0.0031],
        [ 0.0659, -0.0132, -0.0171,  ...,  0.0461,  0.0093, -0.0025],
        [-0.0160,  0.0477,  0.0408,  ...,  0.0329, -0.0386,  0.0484]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0349,  0.0034, -0.0013,  ...,  0.0018, -0.0019,  0.0024],
        [ 0.0034,  0.0575, -0.0025,  ..., -0.0057,  0.0036,  0.0017],
        [-0.0013, -0.0025,  0.0614,  ..., -0.0020,  0.0035, -0.0011],
        ...,
        [ 0.0018, -0.0057, -0.0020,  ...,  0.0557, -0.0018,  0.0013],
        [-0.0019,  0.0036,  0.0035,  ..., -0.0018,  0.0462, -0.0010],
        [ 0.0024,  0.0017, -0.0011,  ...,  0.0013, -0.0010,  0.0633]],
       device='cuda:0') 

reserving basis 1205/4608; cond: 12511926.0, radio:0.0002772566513158381
PARAMETER       :  Parameter containing:
tensor([[[[-0.0038,  0.0052, -0.0121],
          [-0.0076,  0.0058, -0.0163],
          [ 0.0098, -0.0030,  0.0069]],

         [[-0.0078, -0.0031,  0.0035],
          [ 0.0086,  0.0047, -0.0035],
          [ 0.0046, -0.0051,  0.0057]],

         [[ 0.0009, -0.0141, -0.0148],
          [-0.0041, -0.0017, -0.0125],
          [ 0.0020,  0.0144, -0.0046]],

         ...,

         [[-0.0038,  0.0093, -0.0012],
          [ 0.0104,  0.0004,  0.0057],
          [ 0.0048,  0.0110, -0.0048]],

         [[-0.0130,  0.0009, -0.0116],
          [ 0.0021, -0.0076, -0.0029],
          [-0.0006, -0.0107,  0.0098]],

         [[-0.0007,  0.0103,  0.0078],
          [ 0.0071,  0.0099,  0.0129],
          [-0.0127,  0.0036, -0.0099]]],


        [[[ 0.0060, -0.0040,  0.0077],
          [-0.0136, -0.0089,  0.0055],
          [-0.0231,  0.0011,  0.0068]],

         [[-0.0074,  0.0015,  0.0028],
          [-0.0066,  0.0164, -0.0021],
          [ 0.0118,  0.0082,  0.0163]],

         [[ 0.0053, -0.0070,  0.0057],
          [ 0.0010, -0.0109,  0.0052],
          [ 0.0117,  0.0002,  0.0074]],

         ...,

         [[ 0.0093,  0.0037,  0.0082],
          [ 0.0120, -0.0034, -0.0041],
          [-0.0064, -0.0011,  0.0027]],

         [[ 0.0021, -0.0020, -0.0100],
          [ 0.0008, -0.0053, -0.0024],
          [-0.0033,  0.0127,  0.0030]],

         [[ 0.0121, -0.0073, -0.0096],
          [ 0.0128, -0.0055,  0.0016],
          [-0.0019,  0.0059, -0.0105]]],


        [[[-0.0058, -0.0182, -0.0082],
          [-0.0026,  0.0077, -0.0151],
          [ 0.0153,  0.0179,  0.0057]],

         [[ 0.0069, -0.0106,  0.0011],
          [-0.0057, -0.0032, -0.0212],
          [-0.0041, -0.0041, -0.0209]],

         [[-0.0016, -0.0022,  0.0051],
          [ 0.0015,  0.0024, -0.0003],
          [-0.0059,  0.0104, -0.0020]],

         ...,

         [[ 0.0073,  0.0139,  0.0085],
          [-0.0054,  0.0095, -0.0082],
          [ 0.0007, -0.0126, -0.0020]],

         [[ 0.0055, -0.0114, -0.0057],
          [ 0.0042, -0.0068, -0.0193],
          [-0.0033,  0.0018, -0.0132]],

         [[-0.0125, -0.0191, -0.0039],
          [ 0.0075,  0.0061, -0.0114],
          [-0.0051, -0.0113,  0.0063]]],


        ...,


        [[[-0.0113,  0.0089, -0.0084],
          [ 0.0090,  0.0070,  0.0104],
          [-0.0126, -0.0176,  0.0011]],

         [[-0.0160, -0.0015, -0.0020],
          [ 0.0042,  0.0030, -0.0019],
          [-0.0096,  0.0047, -0.0033]],

         [[-0.0153,  0.0015,  0.0018],
          [-0.0133, -0.0057, -0.0160],
          [-0.0082, -0.0144, -0.0030]],

         ...,

         [[ 0.0101, -0.0149,  0.0056],
          [-0.0034,  0.0038, -0.0108],
          [-0.0085, -0.0015,  0.0053]],

         [[-0.0015, -0.0023, -0.0015],
          [-0.0126, -0.0081, -0.0192],
          [-0.0148, -0.0046, -0.0038]],

         [[-0.0033, -0.0038, -0.0104],
          [ 0.0090,  0.0099, -0.0014],
          [ 0.0106,  0.0060,  0.0101]]],


        [[[-0.0148, -0.0028,  0.0096],
          [ 0.0068, -0.0010,  0.0227],
          [-0.0062, -0.0077,  0.0208]],

         [[ 0.0106, -0.0077, -0.0021],
          [-0.0041, -0.0048,  0.0085],
          [-0.0194, -0.0071, -0.0191]],

         [[-0.0158, -0.0146, -0.0137],
          [ 0.0138,  0.0015, -0.0154],
          [ 0.0086, -0.0121, -0.0030]],

         ...,

         [[ 0.0128,  0.0075, -0.0026],
          [ 0.0132,  0.0100,  0.0157],
          [-0.0059,  0.0098, -0.0151]],

         [[-0.0085,  0.0018, -0.0048],
          [ 0.0053, -0.0045,  0.0016],
          [ 0.0160,  0.0055,  0.0066]],

         [[ 0.0036,  0.0118,  0.0106],
          [-0.0009,  0.0044,  0.0034],
          [-0.0018,  0.0168,  0.0023]]],


        [[[ 0.0194,  0.0046, -0.0018],
          [ 0.0028, -0.0055, -0.0072],
          [-0.0104,  0.0025,  0.0034]],

         [[ 0.0158,  0.0006, -0.0029],
          [-0.0013,  0.0171, -0.0026],
          [ 0.0067,  0.0026,  0.0145]],

         [[ 0.0183,  0.0025,  0.0061],
          [-0.0054, -0.0096, -0.0093],
          [-0.0107, -0.0004, -0.0091]],

         ...,

         [[ 0.0106, -0.0144,  0.0020],
          [ 0.0038,  0.0093,  0.0046],
          [ 0.0195,  0.0235,  0.0266]],

         [[-0.0016,  0.0116, -0.0080],
          [-0.0137,  0.0010, -0.0177],
          [ 0.0072,  0.0008, -0.0014]],

         [[ 0.0001,  0.0040, -0.0138],
          [-0.0104,  0.0063, -0.0110],
          [-0.0050, -0.0118,  0.0011]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.8520e+06, 3.7363e+05, 3.6372e+05,  ..., 1.5900e-01, 1.5463e-01,
        1.4802e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1205]) 

NULL SPACE BASIS :  tensor([[ 0.0020, -0.0023,  0.0083,  ...,  0.0040, -0.0046, -0.0146],
        [ 0.0131, -0.0121,  0.0060,  ..., -0.0108, -0.0082,  0.0017],
        [-0.0051,  0.0112, -0.0088,  ..., -0.0041, -0.0129,  0.0060],
        ...,
        [ 0.0103,  0.0124,  0.0010,  ...,  0.0074, -0.0086,  0.0141],
        [-0.0153,  0.0027,  0.0010,  ...,  0.0076, -0.0075,  0.0087],
        [-0.0021,  0.0021,  0.0027,  ...,  0.0007,  0.0046,  0.0144]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.7831e-03, -1.3313e-03,  1.8641e-04,  ...,  3.7780e-05,
          2.3185e-05,  6.4674e-05],
        [-1.3313e-03,  5.5729e-03, -1.2124e-03,  ...,  7.2671e-05,
          1.9688e-04,  4.9180e-05],
        [ 1.8641e-04, -1.2124e-03,  6.0240e-03,  ..., -7.0028e-05,
         -5.8332e-05,  1.6184e-05],
        ...,
        [ 3.7780e-05,  7.2671e-05, -7.0028e-05,  ...,  4.2569e-03,
         -1.0800e-04, -4.0097e-05],
        [ 2.3185e-05,  1.9688e-04, -5.8332e-05,  ..., -1.0800e-04,
          3.6229e-03, -1.1988e-04],
        [ 6.4674e-05,  4.9180e-05,  1.6184e-05,  ..., -4.0097e-05,
         -1.1988e-04,  3.7350e-03]], device='cuda:0') 

reserving basis 536/4608; cond: 94138800.0, radio:1.440054074919317e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 9.0633e-03,  4.2945e-03, -5.5319e-03],
          [-5.0724e-04, -8.8563e-03, -2.6493e-03],
          [ 1.7241e-03, -8.0931e-03,  5.1010e-03]],

         [[-5.1552e-03, -1.0721e-02,  1.0993e-03],
          [-4.9166e-03, -1.0116e-03, -2.7715e-03],
          [ 2.9810e-03,  9.5557e-03, -2.1118e-03]],

         [[-1.0905e-02,  6.3188e-03,  1.1889e-02],
          [ 1.5354e-02,  1.2998e-02,  3.4983e-03],
          [-1.2899e-03,  1.7473e-02,  5.5766e-03]],

         ...,

         [[ 1.1847e-02,  3.8843e-03, -4.3255e-04],
          [ 2.5591e-03, -8.8530e-03,  1.1669e-03],
          [ 6.8196e-03, -5.7621e-03,  7.6964e-03]],

         [[ 7.2520e-03, -8.8161e-04,  3.0295e-03],
          [ 3.7760e-03,  1.4264e-03,  7.1371e-03],
          [ 8.2247e-03,  8.2569e-03, -8.7501e-05]],

         [[-7.7341e-03,  1.2416e-02, -1.6614e-04],
          [-9.8362e-03, -1.2968e-02,  1.3282e-03],
          [-1.1722e-02,  4.5247e-03, -1.4022e-02]]],


        [[[-1.5175e-04,  8.4874e-03, -2.3505e-03],
          [ 1.3696e-03, -3.5894e-03, -8.8424e-03],
          [-6.0298e-03, -1.5222e-02,  6.9141e-03]],

         [[ 3.9406e-03,  6.8214e-03,  2.3525e-03],
          [ 1.0154e-02,  1.4033e-02, -1.2746e-02],
          [ 1.7531e-02,  7.3045e-03, -9.5706e-03]],

         [[ 6.7667e-03,  6.1466e-03,  2.6242e-03],
          [ 3.1884e-03, -4.3443e-03,  6.5742e-03],
          [-2.6342e-03, -7.5218e-03,  2.5312e-04]],

         ...,

         [[-1.8094e-03,  9.2509e-03,  4.5210e-03],
          [-8.4593e-03, -7.7189e-03,  7.0018e-03],
          [-6.6346e-03,  6.3755e-03, -2.5520e-03]],

         [[ 4.8525e-03, -2.9989e-03,  9.2323e-03],
          [-2.8718e-03, -1.4264e-02, -4.8208e-03],
          [ 4.5027e-03, -1.5384e-03, -1.3960e-02]],

         [[ 1.4822e-02,  1.9728e-02, -2.1894e-03],
          [ 3.9318e-04,  1.3720e-04,  5.3494e-03],
          [ 4.2624e-03, -2.3509e-03, -8.9676e-04]]],


        [[[-4.5819e-03, -4.8445e-03, -6.9087e-03],
          [-2.7508e-03, -1.2885e-02, -6.8333e-03],
          [-8.2422e-03, -1.2371e-03, -1.4969e-03]],

         [[-4.0297e-03,  5.4656e-03, -1.1391e-02],
          [ 8.6464e-04, -9.7277e-03, -1.3023e-02],
          [-5.0338e-03, -3.3343e-03, -8.8508e-03]],

         [[-1.4247e-03, -1.2759e-02,  4.7911e-03],
          [-5.5715e-03, -7.5250e-03, -1.3552e-02],
          [-5.2655e-03,  1.3436e-02, -6.6283e-03]],

         ...,

         [[ 2.9372e-03, -1.8443e-02, -1.0295e-03],
          [ 1.5717e-02,  6.5072e-04, -6.1429e-03],
          [-4.9092e-03,  8.0310e-03, -2.2937e-03]],

         [[ 3.4693e-03,  1.8552e-02,  4.7050e-03],
          [-2.8705e-03, -5.9100e-03,  9.2686e-03],
          [ 4.0433e-04, -3.8341e-03,  1.5583e-04]],

         [[-1.3293e-02, -1.5285e-02, -1.6370e-02],
          [-1.2555e-02, -2.6684e-04,  9.2739e-04],
          [-1.2345e-02, -1.1458e-02, -1.6863e-02]]],


        ...,


        [[[-1.7323e-03,  3.0618e-04,  9.5436e-03],
          [ 1.9526e-04,  8.7946e-03, -9.1609e-03],
          [ 8.6791e-03,  2.1827e-04,  4.0957e-03]],

         [[-7.9467e-03, -4.4529e-03,  9.1589e-03],
          [ 8.1356e-03,  7.4194e-03, -7.6530e-03],
          [ 1.1960e-02, -4.0710e-03, -2.8692e-03]],

         [[-1.4450e-02, -1.3848e-03, -1.1632e-02],
          [-1.2383e-02, -1.1472e-02, -3.5305e-03],
          [ 1.1507e-02,  1.6240e-02, -7.0968e-03]],

         ...,

         [[-4.1549e-03,  7.0307e-03,  5.7622e-03],
          [ 9.5299e-03,  1.9686e-03,  5.8164e-03],
          [-4.9499e-03, -3.4730e-03, -2.2948e-03]],

         [[-8.6194e-03,  1.1606e-02, -8.9860e-03],
          [ 9.8790e-03, -1.2424e-02, -8.3108e-03],
          [ 2.0666e-04,  1.1301e-03, -1.6813e-02]],

         [[ 1.7091e-02,  2.0367e-02,  2.4186e-02],
          [ 1.1564e-02, -1.4744e-03,  3.6049e-03],
          [ 1.1178e-02,  1.5848e-02,  6.7266e-03]]],


        [[[ 1.3705e-03, -2.1386e-03, -3.6523e-04],
          [-3.9580e-03, -1.1223e-02, -8.9053e-03],
          [-1.1635e-02,  1.6733e-03, -2.3523e-03]],

         [[-4.2787e-03,  9.9508e-03,  2.4330e-03],
          [-5.5636e-04,  6.0397e-03,  1.2928e-02],
          [-6.8077e-03, -1.0770e-02,  1.3571e-03]],

         [[ 4.1395e-03,  4.9691e-04,  4.6411e-03],
          [-1.2564e-02, -8.1680e-03,  9.6363e-03],
          [ 3.5074e-03,  9.7488e-03, -6.5144e-03]],

         ...,

         [[ 4.5947e-03,  4.8494e-03, -4.0851e-03],
          [ 9.2971e-03,  4.6331e-03,  3.1007e-03],
          [ 2.0634e-03,  1.9367e-03, -1.4518e-02]],

         [[-8.3192e-03, -3.5480e-03, -5.1009e-03],
          [ 2.3041e-03,  1.5209e-03, -1.4528e-02],
          [-9.4624e-04, -8.2861e-03,  2.9996e-03]],

         [[-1.3445e-02,  2.7174e-04,  5.3612e-05],
          [ 3.1880e-03, -4.0529e-03,  4.0419e-03],
          [-1.1133e-02, -1.1940e-02,  1.5115e-03]]],


        [[[-6.4353e-03, -9.2885e-03, -8.3209e-03],
          [ 3.5863e-03, -6.3860e-03,  1.9455e-03],
          [ 4.8398e-03,  3.8287e-03, -4.9108e-03]],

         [[-6.3293e-03,  8.9176e-03,  7.2510e-04],
          [ 1.8887e-03,  7.5773e-03,  1.8162e-03],
          [ 1.9084e-03,  1.1259e-02,  2.3618e-03]],

         [[-2.1652e-03,  6.0483e-03,  6.1100e-03],
          [-7.0379e-03,  1.0350e-02, -1.3213e-02],
          [ 2.3178e-04,  2.1149e-03,  2.9837e-03]],

         ...,

         [[ 3.7014e-03, -3.2378e-04,  1.3953e-02],
          [ 1.0479e-02,  3.0148e-03,  1.4650e-02],
          [ 7.1600e-03,  1.1207e-02,  3.2941e-03]],

         [[-8.0380e-03, -1.9703e-02, -9.4620e-04],
          [-1.1354e-02, -1.1654e-02,  3.7609e-03],
          [ 5.6542e-03,  2.3319e-04,  4.9867e-03]],

         [[-7.0563e-03,  4.6348e-03, -1.5179e-02],
          [-4.8487e-03,  1.0088e-02, -1.2261e-02],
          [ 7.2224e-03, -3.9601e-03, -5.4034e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.2578e+05, 1.1772e+05, 1.0503e+05,  ..., 6.9070e-03, 6.7432e-03,
        5.5852e-03], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 536]) 

NULL SPACE BASIS :  tensor([[ 0.0078,  0.0336,  0.0095,  ..., -0.0045, -0.0065, -0.0182],
        [-0.0252,  0.0116,  0.0233,  ...,  0.0089,  0.0051,  0.0329],
        [-0.0142, -0.0280, -0.0111,  ..., -0.0012,  0.0220,  0.0212],
        ...,
        [-0.0006,  0.0072, -0.0065,  ...,  0.0033,  0.0049, -0.0056],
        [ 0.0055,  0.0054,  0.0262,  ..., -0.0008, -0.0016, -0.0003],
        [ 0.0051,  0.0105, -0.0088,  ...,  0.0001, -0.0020,  0.0054]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.3627e-02, -1.7619e-03, -1.7219e-04,  ..., -2.5908e-04,
          2.6170e-04,  1.0490e-04],
        [-1.7619e-03,  1.1122e-02, -1.6530e-03,  ...,  2.0091e-04,
         -6.4345e-05,  7.2613e-05],
        [-1.7219e-04, -1.6530e-03,  1.3025e-02,  ..., -8.5646e-05,
          5.6534e-05, -1.6793e-04],
        ...,
        [-2.5908e-04,  2.0091e-04, -8.5646e-05,  ...,  1.5070e-03,
         -6.2730e-04,  1.7874e-04],
        [ 2.6170e-04, -6.4345e-05,  5.6534e-05,  ..., -6.2730e-04,
          1.5882e-03, -5.7982e-04],
        [ 1.0490e-04,  7.2613e-05, -1.6793e-04,  ...,  1.7874e-04,
         -5.7982e-04,  1.3454e-03]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 84.100, Total time 0.55
 * Val loss 0.890, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 98.060, Total time 3.08
 * Val loss 0.050, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 69.900, Total time 0.55
 * Val loss 0.878, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 72.220, Total time 3.07
 * Val loss 0.806, Total time 0.00
**************************************************
validation split name: 3
 * Val Acc 75.900, Total time 0.55
 * Val loss 0.789, Total time 0.00
**************************************************
training split name: 3
 * Val Acc 76.120, Total time 3.09
 * Val loss 0.727, Total time 0.00
**************************************************
====================== 4 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0406 (0.0406)	0.0078 (0.0078)	2.462 (2.462)	3.12 (3.12)
[10/157]	0.0945 (0.0887)	0.0578 (0.0534)	2.221 (2.273)	21.88 (17.33)
[20/157]	0.0938 (0.0908)	0.0580 (0.0555)	1.861 (2.163)	43.75 (24.70)
[30/157]	0.0913 (0.0916)	0.0545 (0.0562)	1.847 (2.080)	43.75 (29.84)
[40/157]	0.0934 (0.0921)	0.0567 (0.0566)	1.737 (2.003)	43.75 (33.31)
[50/157]	0.0938 (0.0924)	0.0578 (0.0568)	1.798 (1.948)	40.62 (35.48)
[60/157]	0.0958 (0.0926)	0.0582 (0.0570)	1.755 (1.888)	50.00 (38.11)
[70/157]	0.0943 (0.0927)	0.0578 (0.0571)	1.715 (1.844)	46.88 (40.14)
[80/157]	0.0921 (0.0928)	0.0559 (0.0571)	1.672 (1.800)	43.75 (42.09)
[90/157]	0.0920 (0.0930)	0.0557 (0.0572)	1.857 (1.767)	28.12 (43.23)
[100/157]	0.0927 (0.0930)	0.0568 (0.0573)	1.483 (1.738)	56.25 (44.28)
[110/157]	0.0936 (0.0931)	0.0571 (0.0573)	1.534 (1.716)	50.00 (44.93)
[120/157]	0.0951 (0.0931)	0.0581 (0.0574)	1.632 (1.698)	56.25 (45.71)
[130/157]	0.0956 (0.0932)	0.0588 (0.0574)	1.497 (1.677)	46.88 (46.33)
[140/157]	0.0939 (0.0932)	0.0572 (0.0575)	1.275 (1.657)	56.25 (46.90)
[150/157]	0.0944 (0.0932)	0.0585 (0.0575)	1.111 (1.644)	68.75 (47.31)
[156/157]	0.0752 (0.0931)	0.0520 (0.0575)	0.998 (1.631)	87.50 (47.82)
 * Train Acc 47.820
 * Val Acc 57.300, Total time 0.55
 * Val loss 1.246, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0401 (0.0401)	0.0080 (0.0080)	1.146 (1.146)	68.75 (68.75)
[10/157]	0.0927 (0.0894)	0.0575 (0.0538)	1.194 (1.393)	68.75 (53.41)
[20/157]	0.0937 (0.0914)	0.0571 (0.0559)	1.555 (1.404)	56.25 (54.32)
[30/157]	0.0950 (0.0920)	0.0582 (0.0566)	1.484 (1.409)	37.50 (54.03)
[40/157]	0.0969 (0.0925)	0.0583 (0.0569)	1.078 (1.389)	65.62 (55.49)
[50/157]	0.0947 (0.0927)	0.0572 (0.0571)	1.350 (1.377)	53.12 (55.51)
[60/157]	0.0947 (0.0929)	0.0588 (0.0574)	1.883 (1.380)	34.38 (55.58)
[70/157]	0.0923 (0.0930)	0.0566 (0.0574)	1.250 (1.389)	59.38 (55.50)
[80/157]	0.0935 (0.0931)	0.0564 (0.0575)	1.352 (1.387)	59.38 (55.17)
[90/157]	0.0940 (0.0931)	0.0575 (0.0576)	1.653 (1.387)	56.25 (55.08)
[100/157]	0.0952 (0.0932)	0.0581 (0.0576)	1.347 (1.379)	53.12 (55.38)
[110/157]	0.0963 (0.0932)	0.0585 (0.0576)	1.425 (1.378)	50.00 (55.24)
[120/157]	0.0954 (0.0933)	0.0585 (0.0576)	1.207 (1.372)	50.00 (55.42)
[130/157]	0.0951 (0.0934)	0.0577 (0.0576)	1.296 (1.375)	46.88 (55.10)
[140/157]	0.0940 (0.0934)	0.0571 (0.0576)	1.216 (1.368)	65.62 (55.16)
[150/157]	0.0956 (0.0935)	0.0577 (0.0576)	1.092 (1.362)	68.75 (55.36)
[156/157]	0.0752 (0.0933)	0.0501 (0.0576)	1.317 (1.362)	37.50 (55.30)
 * Train Acc 55.300
 * Val Acc 59.700, Total time 0.55
 * Val loss 1.172, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0405 (0.0405)	0.0078 (0.0078)	1.637 (1.637)	40.62 (40.62)
[10/157]	0.0919 (0.0895)	0.0558 (0.0537)	1.320 (1.302)	56.25 (57.67)
[20/157]	0.0941 (0.0918)	0.0568 (0.0557)	1.233 (1.324)	68.75 (55.95)
[30/157]	0.0932 (0.0924)	0.0557 (0.0563)	1.101 (1.312)	68.75 (57.56)
[40/157]	0.0933 (0.0928)	0.0569 (0.0565)	1.078 (1.331)	59.38 (57.62)
[50/157]	0.0956 (0.0930)	0.0589 (0.0569)	1.322 (1.341)	50.00 (57.17)
[60/157]	0.0956 (0.0931)	0.0587 (0.0571)	1.791 (1.349)	40.62 (56.92)
[70/157]	0.0963 (0.0932)	0.0585 (0.0572)	1.093 (1.351)	65.62 (56.43)
[80/157]	0.0965 (0.0933)	0.0578 (0.0572)	1.294 (1.350)	62.50 (56.48)
[90/157]	0.0952 (0.0934)	0.0574 (0.0572)	1.386 (1.332)	62.50 (57.28)
[100/157]	0.1003 (0.0935)	0.0595 (0.0573)	1.267 (1.324)	46.88 (57.27)
[110/157]	0.0950 (0.0935)	0.0584 (0.0573)	1.125 (1.317)	62.50 (57.55)
[120/157]	0.0947 (0.0935)	0.0579 (0.0574)	1.269 (1.312)	68.75 (57.90)
[130/157]	0.0955 (0.0936)	0.0585 (0.0574)	1.019 (1.311)	71.88 (57.92)
[140/157]	0.0937 (0.0936)	0.0577 (0.0575)	1.576 (1.317)	40.62 (57.42)
[150/157]	0.0936 (0.0936)	0.0581 (0.0575)	1.455 (1.313)	56.25 (57.53)
[156/157]	0.0773 (0.0935)	0.0540 (0.0575)	2.587 (1.313)	37.50 (57.54)
 * Train Acc 57.540
 * Val Acc 60.200, Total time 0.55
 * Val loss 1.177, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0405 (0.0405)	0.0081 (0.0081)	1.255 (1.255)	50.00 (50.00)
[10/157]	0.0916 (0.0898)	0.0560 (0.0538)	1.719 (1.309)	40.62 (56.82)
[20/157]	0.0926 (0.0916)	0.0561 (0.0559)	0.983 (1.285)	71.88 (58.18)
[30/157]	0.0942 (0.0922)	0.0580 (0.0566)	1.246 (1.314)	53.12 (55.44)
[40/157]	0.0952 (0.0926)	0.0585 (0.0570)	1.346 (1.323)	53.12 (55.49)
[50/157]	0.0946 (0.0928)	0.0579 (0.0572)	1.119 (1.310)	50.00 (56.25)
[60/157]	0.0933 (0.0929)	0.0569 (0.0573)	1.194 (1.301)	59.38 (56.66)
[70/157]	0.0939 (0.0931)	0.0581 (0.0574)	1.226 (1.291)	62.50 (57.09)
[80/157]	0.0922 (0.0932)	0.0543 (0.0575)	1.368 (1.294)	53.12 (56.71)
[90/157]	0.0932 (0.0933)	0.0569 (0.0575)	1.356 (1.293)	53.12 (56.94)
[100/157]	0.0933 (0.0933)	0.0559 (0.0575)	1.170 (1.286)	65.62 (57.43)
[110/157]	0.0932 (0.0934)	0.0563 (0.0576)	1.333 (1.285)	62.50 (57.57)
[120/157]	0.0931 (0.0934)	0.0569 (0.0576)	1.104 (1.283)	56.25 (57.54)
[130/157]	0.0948 (0.0935)	0.0572 (0.0576)	0.921 (1.272)	71.88 (57.97)
[140/157]	0.0910 (0.0935)	0.0543 (0.0577)	1.483 (1.273)	40.62 (57.96)
[150/157]	0.0939 (0.0935)	0.0568 (0.0577)	1.015 (1.263)	68.75 (58.40)
[156/157]	0.0763 (0.0934)	0.0518 (0.0576)	0.927 (1.262)	75.00 (58.42)
 * Train Acc 58.420
 * Val Acc 61.200, Total time 0.56
 * Val loss 1.124, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0419 (0.0419)	0.0084 (0.0084)	1.223 (1.223)	59.38 (59.38)
[10/157]	0.0951 (0.0899)	0.0578 (0.0540)	1.093 (1.265)	68.75 (58.52)
[20/157]	0.0950 (0.0919)	0.0585 (0.0556)	1.295 (1.226)	62.50 (60.57)
[30/157]	0.0952 (0.0926)	0.0578 (0.0564)	1.432 (1.234)	46.88 (59.07)
[40/157]	0.0951 (0.0930)	0.0580 (0.0567)	1.441 (1.249)	59.38 (59.68)
[50/157]	0.0965 (0.0932)	0.0591 (0.0570)	1.303 (1.226)	59.38 (60.60)
[60/157]	0.0940 (0.0933)	0.0573 (0.0571)	1.317 (1.223)	53.12 (60.50)
[70/157]	0.0937 (0.0934)	0.0558 (0.0572)	1.800 (1.221)	40.62 (60.78)
[80/157]	0.0945 (0.0935)	0.0573 (0.0572)	1.076 (1.224)	53.12 (60.49)
[90/157]	0.0961 (0.0936)	0.0586 (0.0573)	1.276 (1.226)	59.38 (60.51)
[100/157]	0.0945 (0.0936)	0.0572 (0.0574)	1.312 (1.232)	56.25 (60.09)
[110/157]	0.0934 (0.0936)	0.0576 (0.0574)	1.344 (1.244)	62.50 (59.83)
[120/157]	0.0956 (0.0936)	0.0592 (0.0575)	1.200 (1.236)	53.12 (60.02)
[130/157]	0.0950 (0.0937)	0.0581 (0.0576)	1.400 (1.233)	53.12 (60.23)
[140/157]	0.0940 (0.0937)	0.0576 (0.0576)	1.387 (1.229)	53.12 (60.26)
[150/157]	0.0937 (0.0937)	0.0567 (0.0576)	1.048 (1.232)	53.12 (60.00)
[156/157]	0.0758 (0.0936)	0.0513 (0.0576)	0.967 (1.230)	75.00 (60.02)
 * Train Acc 60.020
 * Val Acc 60.500, Total time 0.56
 * Val loss 1.119, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0408 (0.0408)	0.0083 (0.0083)	1.407 (1.407)	43.75 (43.75)
[10/157]	0.0955 (0.0905)	0.0579 (0.0536)	1.242 (1.261)	53.12 (58.52)
[20/157]	0.0956 (0.0922)	0.0580 (0.0554)	1.124 (1.265)	56.25 (57.74)
[30/157]	0.0929 (0.0928)	0.0560 (0.0561)	1.313 (1.275)	62.50 (58.97)
[40/157]	0.0932 (0.0932)	0.0565 (0.0565)	1.293 (1.278)	62.50 (58.38)
[50/157]	0.0937 (0.0934)	0.0568 (0.0568)	1.030 (1.267)	68.75 (59.31)
[60/157]	0.0927 (0.0935)	0.0556 (0.0569)	0.938 (1.251)	62.50 (59.78)
[70/157]	0.0943 (0.0936)	0.0572 (0.0570)	1.387 (1.256)	56.25 (59.64)
[80/157]	0.0961 (0.0937)	0.0585 (0.0572)	1.174 (1.253)	56.25 (59.80)
[90/157]	0.0959 (0.0938)	0.0590 (0.0573)	1.263 (1.244)	65.62 (60.06)
[100/157]	0.0968 (0.0939)	0.0583 (0.0574)	1.271 (1.244)	56.25 (60.12)
[110/157]	0.0939 (0.0938)	0.0565 (0.0574)	1.511 (1.243)	53.12 (59.85)
[120/157]	0.0950 (0.0939)	0.0577 (0.0574)	1.198 (1.238)	65.62 (60.23)
[130/157]	0.0927 (0.0939)	0.0562 (0.0575)	1.156 (1.233)	62.50 (60.42)
[140/157]	0.0929 (0.0939)	0.0572 (0.0575)	1.467 (1.227)	43.75 (60.57)
[150/157]	0.0941 (0.0939)	0.0580 (0.0576)	1.316 (1.225)	50.00 (60.68)
[156/157]	0.0768 (0.0938)	0.0532 (0.0576)	2.087 (1.224)	25.00 (60.68)
 * Train Acc 60.680
 * Val Acc 63.000, Total time 0.55
 * Val loss 1.059, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0407 (0.0407)	0.0082 (0.0082)	1.153 (1.153)	59.38 (59.38)
[10/157]	0.0926 (0.0898)	0.0563 (0.0539)	1.269 (1.157)	62.50 (64.20)
[20/157]	0.0929 (0.0919)	0.0577 (0.0559)	1.155 (1.179)	68.75 (63.84)
[30/157]	0.0964 (0.0927)	0.0583 (0.0568)	1.256 (1.191)	68.75 (63.21)
[40/157]	0.0958 (0.0930)	0.0581 (0.0571)	1.349 (1.193)	59.38 (62.96)
[50/157]	0.0956 (0.0933)	0.0586 (0.0573)	1.110 (1.185)	59.38 (62.93)
[60/157]	0.0944 (0.0934)	0.0576 (0.0573)	1.162 (1.193)	68.75 (62.60)
[70/157]	0.0933 (0.0935)	0.0560 (0.0574)	1.084 (1.184)	71.88 (63.03)
[80/157]	0.0936 (0.0936)	0.0568 (0.0575)	1.434 (1.179)	59.38 (63.16)
[90/157]	0.0931 (0.0937)	0.0575 (0.0576)	1.175 (1.192)	53.12 (62.84)
[100/157]	0.0938 (0.0937)	0.0573 (0.0576)	1.324 (1.189)	68.75 (62.96)
[110/157]	0.0929 (0.0937)	0.0558 (0.0577)	0.799 (1.188)	75.00 (63.09)
[120/157]	0.0939 (0.0938)	0.0572 (0.0577)	0.974 (1.183)	71.88 (63.25)
[130/157]	0.0943 (0.0938)	0.0578 (0.0577)	1.112 (1.182)	62.50 (63.10)
[140/157]	0.0959 (0.0938)	0.0580 (0.0577)	1.206 (1.179)	56.25 (63.08)
[150/157]	0.0964 (0.0938)	0.0590 (0.0578)	1.004 (1.176)	71.88 (63.06)
[156/157]	0.0769 (0.0937)	0.0538 (0.0578)	1.617 (1.184)	37.50 (62.86)
 * Train Acc 62.860
 * Val Acc 63.700, Total time 0.56
 * Val loss 1.044, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0402 (0.0402)	0.0085 (0.0085)	1.009 (1.009)	71.88 (71.88)
[10/157]	0.0941 (0.0901)	0.0575 (0.0542)	1.551 (1.113)	53.12 (66.19)
[20/157]	0.0947 (0.0921)	0.0582 (0.0563)	1.058 (1.131)	71.88 (65.77)
[30/157]	0.0958 (0.0928)	0.0584 (0.0570)	1.089 (1.156)	62.50 (63.81)
[40/157]	0.0949 (0.0932)	0.0581 (0.0572)	1.140 (1.148)	62.50 (63.57)
[50/157]	0.0930 (0.0934)	0.0564 (0.0573)	1.182 (1.161)	59.38 (63.17)
[60/157]	0.0940 (0.0936)	0.0569 (0.0575)	1.208 (1.181)	50.00 (62.14)
[70/157]	0.0918 (0.0936)	0.0557 (0.0576)	1.366 (1.177)	65.62 (62.41)
[80/157]	0.0949 (0.0938)	0.0587 (0.0577)	1.294 (1.172)	62.50 (62.62)
[90/157]	0.0956 (0.0938)	0.0591 (0.0578)	1.224 (1.172)	62.50 (62.40)
[100/157]	0.0950 (0.0939)	0.0577 (0.0578)	1.535 (1.172)	56.25 (62.44)
[110/157]	0.0930 (0.0939)	0.0562 (0.0578)	1.336 (1.175)	50.00 (62.19)
[120/157]	0.0959 (0.0940)	0.0581 (0.0578)	1.395 (1.180)	53.12 (61.73)
[130/157]	0.0968 (0.0940)	0.0593 (0.0578)	1.065 (1.176)	65.62 (61.86)
[140/157]	0.0962 (0.0940)	0.0589 (0.0579)	1.078 (1.171)	68.75 (62.06)
[150/157]	0.0945 (0.0940)	0.0583 (0.0579)	0.968 (1.164)	71.88 (62.42)
[156/157]	0.0765 (0.0939)	0.0517 (0.0579)	1.394 (1.167)	62.50 (62.32)
 * Train Acc 62.320
 * Val Acc 65.400, Total time 0.56
 * Val loss 1.022, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0085 (0.0085)	1.146 (1.146)	56.25 (56.25)
[10/157]	0.0927 (0.0908)	0.0562 (0.0540)	1.197 (1.107)	53.12 (62.78)
[20/157]	0.0929 (0.0924)	0.0564 (0.0560)	1.165 (1.135)	68.75 (62.05)
[30/157]	0.0955 (0.0930)	0.0583 (0.0569)	1.302 (1.145)	62.50 (63.10)
[40/157]	0.0954 (0.0933)	0.0585 (0.0573)	1.190 (1.145)	62.50 (62.42)
[50/157]	0.0967 (0.0935)	0.0598 (0.0576)	1.088 (1.145)	62.50 (62.25)
[60/157]	0.0948 (0.0937)	0.0584 (0.0577)	1.024 (1.159)	71.88 (61.73)
[70/157]	0.0935 (0.0938)	0.0571 (0.0578)	1.096 (1.152)	71.88 (62.24)
[80/157]	0.0944 (0.0938)	0.0574 (0.0579)	1.138 (1.159)	65.62 (61.65)
[90/157]	0.0929 (0.0938)	0.0564 (0.0579)	0.932 (1.152)	65.62 (61.98)
[100/157]	0.0934 (0.0939)	0.0571 (0.0580)	1.083 (1.153)	62.50 (62.13)
[110/157]	0.0948 (0.0939)	0.0581 (0.0580)	0.983 (1.157)	62.50 (61.94)
[120/157]	0.0955 (0.0939)	0.0585 (0.0580)	1.266 (1.160)	65.62 (62.06)
[130/157]	0.0972 (0.0940)	0.0597 (0.0581)	1.002 (1.160)	68.75 (62.12)
[140/157]	0.0930 (0.0940)	0.0565 (0.0581)	1.734 (1.156)	40.62 (62.32)
[150/157]	0.0916 (0.0940)	0.0566 (0.0581)	1.149 (1.156)	62.50 (62.25)
[156/157]	0.0778 (0.0939)	0.0540 (0.0581)	1.321 (1.157)	62.50 (62.36)
 * Train Acc 62.360
 * Val Acc 65.100, Total time 0.57
 * Val loss 1.012, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0085 (0.0085)	1.092 (1.092)	68.75 (68.75)
[10/157]	0.0934 (0.0902)	0.0567 (0.0538)	1.007 (1.169)	62.50 (62.78)
[20/157]	0.0933 (0.0920)	0.0566 (0.0557)	1.021 (1.147)	68.75 (61.90)
[30/157]	0.0970 (0.0929)	0.0581 (0.0564)	1.353 (1.135)	53.12 (62.70)
[40/157]	0.0962 (0.0933)	0.0587 (0.0566)	1.473 (1.125)	62.50 (63.26)
[50/157]	0.0948 (0.0935)	0.0575 (0.0569)	1.238 (1.121)	59.38 (63.48)
[60/157]	0.0943 (0.0936)	0.0569 (0.0571)	0.945 (1.133)	59.38 (62.91)
[70/157]	0.0938 (0.0937)	0.0567 (0.0572)	1.075 (1.137)	59.38 (62.59)
[80/157]	0.0947 (0.0938)	0.0575 (0.0573)	0.978 (1.136)	68.75 (62.62)
[90/157]	0.0946 (0.0938)	0.0576 (0.0573)	1.237 (1.133)	53.12 (62.77)
[100/157]	0.0944 (0.0939)	0.0571 (0.0574)	1.300 (1.127)	56.25 (63.09)
[110/157]	0.0941 (0.0940)	0.0569 (0.0574)	0.895 (1.136)	65.62 (62.87)
[120/157]	0.0952 (0.0940)	0.0578 (0.0575)	1.010 (1.139)	62.50 (62.73)
[130/157]	0.0953 (0.0940)	0.0583 (0.0575)	0.988 (1.143)	62.50 (62.81)
[140/157]	0.0947 (0.0941)	0.0579 (0.0576)	1.068 (1.139)	65.62 (63.12)
[150/157]	0.0932 (0.0941)	0.0564 (0.0576)	1.073 (1.143)	59.38 (63.22)
[156/157]	0.0789 (0.0940)	0.0541 (0.0576)	1.086 (1.146)	75.00 (63.12)
 * Train Acc 63.120
 * Val Acc 66.400, Total time 0.57
 * Val loss 1.016, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0393 (0.0393)	0.0078 (0.0078)	1.304 (1.304)	46.88 (46.88)
[10/157]	0.0952 (0.0907)	0.0577 (0.0542)	0.936 (1.139)	71.88 (61.08)
[20/157]	0.0962 (0.0924)	0.0585 (0.0560)	1.325 (1.136)	50.00 (62.80)
[30/157]	0.0955 (0.0931)	0.0587 (0.0565)	1.003 (1.174)	62.50 (60.58)
[40/157]	0.0971 (0.0934)	0.0592 (0.0569)	0.964 (1.155)	62.50 (61.28)
[50/157]	0.0943 (0.0936)	0.0567 (0.0571)	1.170 (1.154)	59.38 (61.58)
[60/157]	0.0958 (0.0938)	0.0577 (0.0572)	1.076 (1.136)	75.00 (63.01)
[70/157]	0.0948 (0.0939)	0.0583 (0.0573)	1.160 (1.144)	62.50 (63.16)
[80/157]	0.0967 (0.0940)	0.0577 (0.0574)	1.251 (1.138)	62.50 (63.35)
[90/157]	0.0953 (0.0941)	0.0577 (0.0575)	1.224 (1.138)	62.50 (63.12)
[100/157]	0.0955 (0.0941)	0.0587 (0.0575)	1.201 (1.139)	68.75 (63.24)
[110/157]	0.0916 (0.0942)	0.0562 (0.0576)	1.179 (1.131)	53.12 (63.15)
[120/157]	0.0932 (0.0941)	0.0573 (0.0576)	1.198 (1.125)	53.12 (63.22)
[130/157]	0.0961 (0.0942)	0.0587 (0.0577)	1.525 (1.129)	59.38 (63.29)
[140/157]	0.0954 (0.0942)	0.0574 (0.0577)	1.185 (1.128)	65.62 (63.39)
[150/157]	0.0942 (0.0942)	0.0568 (0.0577)	1.053 (1.124)	65.62 (63.53)
[156/157]	0.0783 (0.0941)	0.0546 (0.0577)	1.081 (1.126)	62.50 (63.46)
 * Train Acc 63.460
 * Val Acc 66.000, Total time 0.57
 * Val loss 1.000, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0412 (0.0412)	0.0083 (0.0083)	0.896 (0.896)	71.88 (71.88)
[10/157]	0.0926 (0.0902)	0.0563 (0.0542)	1.272 (1.090)	56.25 (65.06)
[20/157]	0.0950 (0.0924)	0.0575 (0.0560)	0.897 (1.090)	81.25 (66.37)
[30/157]	0.0956 (0.0930)	0.0588 (0.0567)	1.494 (1.111)	53.12 (64.82)
[40/157]	0.0928 (0.0933)	0.0566 (0.0570)	1.139 (1.109)	65.62 (64.86)
[50/157]	0.0944 (0.0936)	0.0569 (0.0573)	1.039 (1.105)	65.62 (64.77)
[60/157]	0.0949 (0.0937)	0.0578 (0.0574)	0.741 (1.087)	81.25 (65.27)
[70/157]	0.0951 (0.0938)	0.0581 (0.0575)	1.000 (1.081)	68.75 (65.62)
[80/157]	0.0957 (0.0939)	0.0587 (0.0576)	1.058 (1.090)	71.88 (65.28)
[90/157]	0.0945 (0.0939)	0.0574 (0.0577)	1.577 (1.104)	53.12 (64.94)
[100/157]	0.0937 (0.0940)	0.0565 (0.0577)	0.995 (1.097)	71.88 (65.22)
[110/157]	0.0963 (0.0941)	0.0589 (0.0578)	1.220 (1.110)	59.38 (64.72)
[120/157]	0.0952 (0.0941)	0.0581 (0.0578)	1.012 (1.107)	71.88 (64.95)
[130/157]	0.0952 (0.0942)	0.0575 (0.0578)	0.933 (1.106)	75.00 (65.03)
[140/157]	0.0968 (0.0942)	0.0591 (0.0579)	1.058 (1.106)	62.50 (65.14)
[150/157]	0.0953 (0.0942)	0.0582 (0.0579)	0.982 (1.102)	68.75 (65.36)
[156/157]	0.0785 (0.0941)	0.0534 (0.0579)	1.315 (1.104)	75.00 (65.26)
 * Train Acc 65.260
 * Val Acc 67.600, Total time 0.56
 * Val loss 0.990, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0084 (0.0084)	0.900 (0.900)	68.75 (68.75)
[10/157]	0.0933 (0.0901)	0.0566 (0.0535)	1.104 (1.161)	65.62 (63.64)
[20/157]	0.0944 (0.0923)	0.0573 (0.0558)	0.978 (1.125)	65.62 (64.29)
[30/157]	0.0962 (0.0930)	0.0590 (0.0566)	0.936 (1.099)	75.00 (65.52)
[40/157]	0.0962 (0.0934)	0.0588 (0.0570)	1.363 (1.116)	59.38 (64.71)
[50/157]	0.0954 (0.0936)	0.0569 (0.0571)	1.345 (1.110)	53.12 (65.13)
[60/157]	0.0955 (0.0938)	0.0578 (0.0572)	0.955 (1.108)	78.12 (65.01)
[70/157]	0.0956 (0.0939)	0.0586 (0.0573)	1.345 (1.114)	62.50 (64.92)
[80/157]	0.0948 (0.0939)	0.0576 (0.0574)	1.147 (1.117)	62.50 (64.66)
[90/157]	0.0944 (0.0940)	0.0570 (0.0574)	0.937 (1.107)	68.75 (64.63)
[100/157]	0.0955 (0.0941)	0.0583 (0.0574)	1.111 (1.109)	65.62 (64.54)
[110/157]	0.0943 (0.0941)	0.0572 (0.0575)	0.993 (1.111)	62.50 (64.39)
[120/157]	0.0933 (0.0942)	0.0569 (0.0576)	1.202 (1.107)	68.75 (64.67)
[130/157]	0.0964 (0.0942)	0.0580 (0.0576)	1.277 (1.104)	53.12 (64.69)
[140/157]	0.0957 (0.0942)	0.0587 (0.0576)	1.022 (1.105)	65.62 (64.80)
[150/157]	0.0955 (0.0942)	0.0586 (0.0576)	1.090 (1.106)	56.25 (64.69)
[156/157]	0.0764 (0.0941)	0.0525 (0.0576)	1.974 (1.108)	50.00 (64.60)
 * Train Acc 64.600
 * Val Acc 67.300, Total time 0.57
 * Val loss 0.999, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0080 (0.0080)	1.008 (1.008)	68.75 (68.75)
[10/157]	0.0937 (0.0906)	0.0564 (0.0539)	1.182 (1.170)	59.38 (64.20)
[20/157]	0.0936 (0.0925)	0.0570 (0.0559)	0.981 (1.159)	71.88 (64.58)
[30/157]	0.0957 (0.0931)	0.0580 (0.0566)	1.301 (1.173)	50.00 (63.21)
[40/157]	0.0966 (0.0934)	0.0593 (0.0570)	1.055 (1.180)	65.62 (61.89)
[50/157]	0.0955 (0.0936)	0.0579 (0.0573)	1.071 (1.167)	56.25 (62.44)
[60/157]	0.0942 (0.0938)	0.0572 (0.0575)	0.980 (1.153)	71.88 (63.32)
[70/157]	0.0945 (0.0938)	0.0574 (0.0576)	0.894 (1.144)	68.75 (63.64)
[80/157]	0.0959 (0.0939)	0.0584 (0.0577)	0.765 (1.134)	75.00 (64.00)
[90/157]	0.0944 (0.0940)	0.0576 (0.0577)	1.222 (1.138)	62.50 (63.74)
[100/157]	0.0955 (0.0941)	0.0583 (0.0578)	1.278 (1.134)	59.38 (63.64)
[110/157]	0.0960 (0.0941)	0.0586 (0.0579)	0.946 (1.127)	75.00 (63.85)
[120/157]	0.0959 (0.0941)	0.0593 (0.0579)	1.127 (1.117)	68.75 (64.18)
[130/157]	0.0940 (0.0942)	0.0565 (0.0579)	0.837 (1.110)	68.75 (64.58)
[140/157]	0.0939 (0.0942)	0.0569 (0.0580)	1.204 (1.114)	56.25 (64.27)
[150/157]	0.0975 (0.0942)	0.0583 (0.0580)	1.231 (1.114)	56.25 (64.24)
[156/157]	0.0787 (0.0941)	0.0538 (0.0579)	1.208 (1.112)	50.00 (64.36)
 * Train Acc 64.360
 * Val Acc 67.200, Total time 0.56
 * Val loss 0.978, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0079 (0.0079)	1.352 (1.352)	53.12 (53.12)
[10/157]	0.0948 (0.0909)	0.0578 (0.0537)	1.049 (1.076)	65.62 (64.20)
[20/157]	0.0952 (0.0927)	0.0584 (0.0560)	1.078 (1.083)	62.50 (64.88)
[30/157]	0.0954 (0.0933)	0.0579 (0.0566)	0.827 (1.081)	75.00 (65.12)
[40/157]	0.0961 (0.0936)	0.0584 (0.0569)	0.948 (1.099)	78.12 (64.18)
[50/157]	0.0944 (0.0938)	0.0573 (0.0571)	1.122 (1.098)	68.75 (64.34)
[60/157]	0.0958 (0.0940)	0.0583 (0.0573)	1.407 (1.108)	50.00 (64.45)
[70/157]	0.0952 (0.0941)	0.0580 (0.0574)	0.817 (1.116)	78.12 (64.39)
[80/157]	0.0943 (0.0941)	0.0568 (0.0574)	1.203 (1.120)	65.62 (64.51)
[90/157]	0.0955 (0.0942)	0.0578 (0.0575)	1.352 (1.118)	59.38 (64.63)
[100/157]	0.0969 (0.0943)	0.0585 (0.0576)	1.158 (1.108)	68.75 (65.01)
[110/157]	0.0948 (0.0943)	0.0571 (0.0576)	1.042 (1.103)	65.62 (65.03)
[120/157]	0.0969 (0.0944)	0.0581 (0.0576)	1.329 (1.107)	56.25 (64.90)
[130/157]	0.0942 (0.0944)	0.0574 (0.0577)	0.942 (1.099)	75.00 (65.05)
[140/157]	0.0933 (0.0944)	0.0566 (0.0577)	1.045 (1.103)	71.88 (65.00)
[150/157]	0.0946 (0.0944)	0.0577 (0.0578)	1.168 (1.106)	65.62 (64.92)
[156/157]	0.0780 (0.0942)	0.0544 (0.0577)	1.117 (1.104)	50.00 (64.88)
 * Train Acc 64.880
 * Val Acc 66.800, Total time 0.57
 * Val loss 0.983, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0082 (0.0082)	0.828 (0.828)	81.25 (81.25)
[10/157]	0.0934 (0.0911)	0.0568 (0.0544)	0.906 (1.076)	71.88 (67.05)
[20/157]	0.0932 (0.0928)	0.0568 (0.0562)	1.137 (1.089)	68.75 (65.18)
[30/157]	0.0960 (0.0934)	0.0586 (0.0570)	1.138 (1.084)	68.75 (64.92)
[40/157]	0.0952 (0.0937)	0.0581 (0.0573)	0.871 (1.080)	75.00 (65.09)
[50/157]	0.0950 (0.0938)	0.0584 (0.0575)	1.019 (1.091)	59.38 (63.91)
[60/157]	0.0951 (0.0940)	0.0577 (0.0577)	1.207 (1.087)	65.62 (64.50)
[70/157]	0.0946 (0.0941)	0.0569 (0.0577)	1.218 (1.085)	62.50 (64.44)
[80/157]	0.0958 (0.0942)	0.0571 (0.0578)	1.158 (1.085)	65.62 (64.54)
[90/157]	0.0969 (0.0943)	0.0595 (0.0578)	1.197 (1.094)	59.38 (64.11)
[100/157]	0.0921 (0.0943)	0.0553 (0.0579)	0.901 (1.083)	75.00 (64.76)
[110/157]	0.0967 (0.0944)	0.0585 (0.0579)	1.208 (1.097)	65.62 (64.53)
[120/157]	0.0942 (0.0944)	0.0571 (0.0579)	1.038 (1.089)	75.00 (64.93)
[130/157]	0.0971 (0.0945)	0.0590 (0.0579)	1.130 (1.079)	68.75 (65.22)
[140/157]	0.0950 (0.0945)	0.0574 (0.0579)	1.060 (1.083)	71.88 (65.14)
[150/157]	0.0968 (0.0945)	0.0588 (0.0579)	1.081 (1.081)	68.75 (65.31)
[156/157]	0.0772 (0.0944)	0.0523 (0.0579)	0.858 (1.079)	75.00 (65.40)
 * Train Acc 65.400
 * Val Acc 67.200, Total time 0.57
 * Val loss 0.961, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0085 (0.0085)	0.745 (0.745)	78.12 (78.12)
[10/157]	0.0935 (0.0911)	0.0561 (0.0540)	1.028 (1.061)	71.88 (66.48)
[20/157]	0.0957 (0.0929)	0.0583 (0.0558)	1.179 (1.058)	62.50 (66.52)
[30/157]	0.0942 (0.0934)	0.0576 (0.0565)	0.691 (1.055)	81.25 (66.33)
[40/157]	0.0945 (0.0937)	0.0573 (0.0568)	0.986 (1.058)	68.75 (66.23)
[50/157]	0.0959 (0.0940)	0.0587 (0.0571)	1.062 (1.075)	71.88 (65.75)
[60/157]	0.0948 (0.0941)	0.0576 (0.0572)	1.207 (1.080)	59.38 (65.88)
[70/157]	0.0964 (0.0942)	0.0586 (0.0574)	0.965 (1.080)	65.62 (65.80)
[80/157]	0.0951 (0.0942)	0.0581 (0.0575)	1.126 (1.077)	75.00 (66.55)
[90/157]	0.0923 (0.0942)	0.0558 (0.0576)	1.111 (1.082)	56.25 (66.04)
[100/157]	0.0964 (0.0943)	0.0588 (0.0577)	1.031 (1.088)	68.75 (65.72)
[110/157]	0.0973 (0.0943)	0.0588 (0.0577)	1.278 (1.091)	50.00 (65.43)
[120/157]	0.0937 (0.0944)	0.0566 (0.0577)	1.136 (1.085)	62.50 (65.42)
[130/157]	0.0957 (0.0944)	0.0580 (0.0577)	1.013 (1.088)	65.62 (65.24)
[140/157]	0.0954 (0.0944)	0.0580 (0.0577)	1.284 (1.084)	53.12 (65.36)
[150/157]	0.0950 (0.0944)	0.0574 (0.0578)	0.694 (1.078)	81.25 (65.42)
[156/157]	0.0777 (0.0943)	0.0530 (0.0578)	1.840 (1.078)	50.00 (65.44)
 * Train Acc 65.440
 * Val Acc 67.100, Total time 0.57
 * Val loss 0.967, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0406 (0.0406)	0.0083 (0.0083)	1.068 (1.068)	62.50 (62.50)
[10/157]	0.0942 (0.0912)	0.0574 (0.0546)	0.802 (1.061)	75.00 (67.05)
[20/157]	0.0955 (0.0930)	0.0583 (0.0565)	1.066 (1.074)	71.88 (66.37)
[30/157]	0.0956 (0.0936)	0.0582 (0.0570)	0.832 (1.056)	71.88 (66.43)
[40/157]	0.0953 (0.0939)	0.0575 (0.0574)	0.702 (1.053)	81.25 (67.00)
[50/157]	0.0976 (0.0941)	0.0592 (0.0575)	0.927 (1.055)	71.88 (66.91)
[60/157]	0.0946 (0.0942)	0.0573 (0.0576)	1.188 (1.062)	56.25 (66.70)
[70/157]	0.0948 (0.0943)	0.0576 (0.0577)	0.957 (1.051)	81.25 (67.12)
[80/157]	0.0942 (0.0944)	0.0559 (0.0577)	1.432 (1.058)	56.25 (67.25)
[90/157]	0.0968 (0.0945)	0.0588 (0.0578)	0.986 (1.065)	71.88 (66.93)
[100/157]	0.0943 (0.0945)	0.0568 (0.0578)	0.993 (1.074)	71.88 (66.46)
[110/157]	0.0968 (0.0946)	0.0583 (0.0578)	1.107 (1.075)	62.50 (66.41)
[120/157]	0.0967 (0.0946)	0.0571 (0.0578)	1.394 (1.075)	53.12 (66.30)
[130/157]	0.0970 (0.0946)	0.0597 (0.0578)	0.877 (1.066)	68.75 (66.51)
[140/157]	0.0945 (0.0946)	0.0574 (0.0578)	1.056 (1.064)	68.75 (66.58)
[150/157]	0.0961 (0.0946)	0.0585 (0.0578)	0.882 (1.061)	65.62 (66.60)
[156/157]	0.0762 (0.0945)	0.0520 (0.0578)	1.106 (1.061)	62.50 (66.62)
 * Train Acc 66.620
 * Val Acc 68.000, Total time 0.58
 * Val loss 0.951, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0412 (0.0412)	0.0082 (0.0082)	0.983 (0.983)	68.75 (68.75)
[10/157]	0.0944 (0.0905)	0.0570 (0.0542)	1.057 (1.027)	62.50 (67.61)
[20/157]	0.0961 (0.0926)	0.0582 (0.0563)	1.098 (1.032)	59.38 (68.01)
[30/157]	0.0961 (0.0932)	0.0580 (0.0569)	0.825 (1.020)	71.88 (68.15)
[40/157]	0.0952 (0.0937)	0.0577 (0.0573)	1.224 (1.040)	65.62 (67.38)
[50/157]	0.0940 (0.0940)	0.0568 (0.0575)	0.985 (1.050)	65.62 (66.67)
[60/157]	0.0937 (0.0941)	0.0566 (0.0576)	1.127 (1.052)	71.88 (66.85)
[70/157]	0.0965 (0.0942)	0.0588 (0.0577)	1.135 (1.053)	65.62 (67.03)
[80/157]	0.0969 (0.0944)	0.0583 (0.0578)	1.131 (1.059)	59.38 (66.51)
[90/157]	0.0936 (0.0945)	0.0566 (0.0578)	1.034 (1.064)	71.88 (66.62)
[100/157]	0.0964 (0.0945)	0.0585 (0.0578)	1.008 (1.064)	65.62 (66.43)
[110/157]	0.0965 (0.0946)	0.0579 (0.0579)	1.015 (1.057)	59.38 (66.53)
[120/157]	0.0951 (0.0946)	0.0576 (0.0579)	1.132 (1.054)	71.88 (66.97)
[130/157]	0.0967 (0.0946)	0.0578 (0.0579)	1.114 (1.061)	65.62 (66.70)
[140/157]	0.0956 (0.0946)	0.0585 (0.0579)	0.904 (1.065)	78.12 (66.42)
[150/157]	0.0935 (0.0947)	0.0565 (0.0579)	1.054 (1.063)	68.75 (66.60)
[156/157]	0.0790 (0.0946)	0.0546 (0.0579)	1.281 (1.059)	87.50 (66.74)
 * Train Acc 66.740
 * Val Acc 67.900, Total time 0.57
 * Val loss 0.950, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0410 (0.0410)	0.0083 (0.0083)	1.111 (1.111)	62.50 (62.50)
[10/157]	0.0972 (0.0923)	0.0583 (0.0548)	1.041 (1.104)	59.38 (63.07)
[20/157]	0.0988 (0.0950)	0.0602 (0.0570)	1.365 (1.054)	50.00 (66.07)
[30/157]	0.0970 (0.0959)	0.0584 (0.0579)	0.764 (1.041)	75.00 (67.34)
[40/157]	0.0982 (0.0963)	0.0598 (0.0584)	1.237 (1.062)	56.25 (66.39)
[50/157]	0.0967 (0.0965)	0.0583 (0.0586)	0.684 (1.037)	78.12 (66.97)
[60/157]	0.0976 (0.0966)	0.0593 (0.0588)	0.826 (1.041)	75.00 (67.21)
[70/157]	0.0990 (0.0968)	0.0594 (0.0589)	0.990 (1.047)	62.50 (66.73)
[80/157]	0.0952 (0.0968)	0.0574 (0.0590)	1.435 (1.050)	53.12 (66.67)
[90/157]	0.0973 (0.0966)	0.0590 (0.0589)	1.327 (1.060)	53.12 (66.14)
[100/157]	0.0960 (0.0965)	0.0578 (0.0588)	1.012 (1.063)	68.75 (66.00)
[110/157]	0.0972 (0.0963)	0.0594 (0.0587)	1.442 (1.074)	53.12 (65.40)
[120/157]	0.0958 (0.0963)	0.0575 (0.0587)	0.985 (1.068)	71.88 (65.65)
[130/157]	0.0954 (0.0962)	0.0580 (0.0586)	1.007 (1.072)	78.12 (65.55)
[140/157]	0.0969 (0.0961)	0.0579 (0.0586)	1.121 (1.070)	65.62 (65.67)
[150/157]	0.0961 (0.0960)	0.0576 (0.0586)	0.925 (1.069)	62.50 (65.77)
[156/157]	0.0796 (0.0959)	0.0549 (0.0585)	1.333 (1.065)	50.00 (65.86)
 * Train Acc 65.860
 * Val Acc 68.100, Total time 0.58
 * Val loss 0.949, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0086 (0.0086)	1.030 (1.030)	65.62 (65.62)
[10/157]	0.0940 (0.0911)	0.0571 (0.0543)	1.104 (1.082)	65.62 (67.90)
[20/157]	0.0982 (0.0930)	0.0594 (0.0564)	1.207 (1.068)	75.00 (67.71)
[30/157]	0.0948 (0.0935)	0.0574 (0.0570)	0.978 (1.054)	75.00 (67.24)
[40/157]	0.0979 (0.0940)	0.0595 (0.0574)	1.243 (1.040)	62.50 (66.84)
[50/157]	0.0946 (0.0943)	0.0565 (0.0575)	1.145 (1.025)	65.62 (67.65)
[60/157]	0.0945 (0.0944)	0.0572 (0.0575)	1.104 (1.023)	65.62 (67.47)
[70/157]	0.0969 (0.0945)	0.0585 (0.0576)	1.170 (1.033)	68.75 (67.30)
[80/157]	0.0954 (0.0946)	0.0577 (0.0577)	1.133 (1.029)	62.50 (67.48)
[90/157]	0.0973 (0.0946)	0.0589 (0.0578)	1.207 (1.042)	65.62 (67.10)
[100/157]	0.0962 (0.0946)	0.0579 (0.0579)	0.810 (1.036)	75.00 (67.26)
[110/157]	0.0966 (0.0947)	0.0585 (0.0579)	1.006 (1.039)	71.88 (67.20)
[120/157]	0.0969 (0.0948)	0.0581 (0.0579)	1.163 (1.039)	65.62 (67.23)
[130/157]	0.0937 (0.0948)	0.0566 (0.0579)	1.019 (1.035)	68.75 (67.13)
[140/157]	0.0967 (0.0948)	0.0585 (0.0580)	0.990 (1.043)	71.88 (66.87)
[150/157]	0.0947 (0.0948)	0.0569 (0.0580)	0.849 (1.044)	71.88 (66.83)
[156/157]	0.0780 (0.0947)	0.0534 (0.0580)	1.321 (1.047)	50.00 (66.62)
 * Train Acc 66.620
 * Val Acc 69.600, Total time 0.57
 * Val loss 0.949, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0084 (0.0084)	1.227 (1.227)	53.12 (53.12)
[10/157]	0.0935 (0.0920)	0.0566 (0.0549)	1.065 (1.096)	62.50 (61.93)
[20/157]	0.1009 (0.0954)	0.0622 (0.0579)	1.036 (1.061)	59.38 (64.29)
[30/157]	0.1007 (0.0971)	0.0620 (0.0594)	0.805 (1.062)	75.00 (64.82)
[40/157]	0.0952 (0.0978)	0.0578 (0.0600)	0.989 (1.055)	78.12 (65.70)
[50/157]	0.0939 (0.0972)	0.0565 (0.0597)	0.789 (1.036)	75.00 (66.54)
[60/157]	0.0953 (0.0969)	0.0579 (0.0594)	1.111 (1.044)	62.50 (65.98)
[70/157]	0.0962 (0.0966)	0.0577 (0.0593)	1.156 (1.044)	62.50 (66.11)
[80/157]	0.0945 (0.0964)	0.0574 (0.0591)	0.994 (1.041)	68.75 (66.01)
[90/157]	0.0968 (0.0963)	0.0582 (0.0591)	1.110 (1.042)	65.62 (66.14)
[100/157]	0.0942 (0.0962)	0.0572 (0.0590)	0.888 (1.038)	75.00 (66.27)
[110/157]	0.0985 (0.0974)	0.0617 (0.0601)	1.346 (1.045)	43.75 (65.99)
[120/157]	0.0976 (0.0972)	0.0592 (0.0600)	1.015 (1.038)	62.50 (66.32)
[130/157]	0.0953 (0.0970)	0.0576 (0.0599)	1.149 (1.039)	65.62 (66.58)
[140/157]	0.0970 (0.0969)	0.0588 (0.0598)	0.945 (1.042)	68.75 (66.67)
[150/157]	0.0948 (0.0967)	0.0574 (0.0597)	1.171 (1.045)	59.38 (66.58)
[156/157]	0.0779 (0.0965)	0.0534 (0.0596)	0.428 (1.044)	87.50 (66.64)
 * Train Acc 66.640
 * Val Acc 68.000, Total time 0.57
 * Val loss 0.945, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0406 (0.0406)	0.0083 (0.0083)	0.976 (0.976)	71.88 (71.88)
[10/157]	0.0954 (0.0915)	0.0575 (0.0545)	1.222 (1.066)	65.62 (66.19)
[20/157]	0.1007 (0.0952)	0.0614 (0.0574)	0.933 (1.012)	71.88 (67.41)
[30/157]	0.0983 (0.0967)	0.0590 (0.0586)	0.796 (1.031)	78.12 (67.04)
[40/157]	0.0964 (0.0963)	0.0588 (0.0584)	1.018 (1.021)	71.88 (67.15)
[50/157]	0.0980 (0.0961)	0.0599 (0.0583)	1.251 (1.045)	56.25 (66.05)
[60/157]	0.0951 (0.0959)	0.0575 (0.0584)	0.961 (1.051)	68.75 (66.44)
[70/157]	0.0953 (0.0958)	0.0577 (0.0584)	1.087 (1.044)	68.75 (66.46)
[80/157]	0.0960 (0.0957)	0.0577 (0.0583)	0.911 (1.033)	81.25 (66.86)
[90/157]	0.0965 (0.0956)	0.0587 (0.0583)	1.063 (1.039)	65.62 (66.72)
[100/157]	0.0965 (0.0956)	0.0582 (0.0583)	1.137 (1.045)	65.62 (66.43)
[110/157]	0.0942 (0.0956)	0.0566 (0.0583)	1.232 (1.055)	56.25 (66.24)
[120/157]	0.0943 (0.0955)	0.0563 (0.0583)	1.126 (1.056)	59.38 (66.19)
[130/157]	0.0996 (0.0963)	0.0606 (0.0589)	0.870 (1.051)	78.12 (66.51)
[140/157]	0.0970 (0.0962)	0.0589 (0.0588)	0.731 (1.050)	81.25 (66.53)
[150/157]	0.0977 (0.0961)	0.0585 (0.0587)	1.153 (1.051)	68.75 (66.37)
[156/157]	0.0793 (0.0960)	0.0534 (0.0587)	1.009 (1.052)	75.00 (66.42)
 * Train Acc 66.420
 * Val Acc 68.000, Total time 0.58
 * Val loss 0.947, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0088 (0.0088)	1.374 (1.374)	65.62 (65.62)
[10/157]	0.0958 (0.0954)	0.0579 (0.0573)	1.453 (1.134)	62.50 (64.49)
[20/157]	0.0980 (0.0961)	0.0586 (0.0581)	1.001 (1.064)	68.75 (65.18)
[30/157]	0.0966 (0.0964)	0.0589 (0.0584)	0.939 (1.052)	78.12 (66.43)
[40/157]	0.0974 (0.0965)	0.0581 (0.0586)	0.947 (1.055)	71.88 (66.54)
[50/157]	0.0974 (0.0965)	0.0594 (0.0587)	1.096 (1.060)	65.62 (66.67)
[60/157]	0.0962 (0.0965)	0.0584 (0.0587)	0.905 (1.051)	71.88 (66.91)
[70/157]	0.0962 (0.0965)	0.0580 (0.0587)	1.111 (1.055)	65.62 (66.90)
[80/157]	0.0982 (0.0965)	0.0588 (0.0587)	0.717 (1.052)	78.12 (66.90)
[90/157]	0.0963 (0.0966)	0.0585 (0.0587)	1.607 (1.063)	50.00 (66.38)
[100/157]	0.0947 (0.0966)	0.0569 (0.0587)	0.812 (1.060)	71.88 (66.46)
[110/157]	0.0979 (0.0966)	0.0592 (0.0587)	0.837 (1.049)	78.12 (66.55)
[120/157]	0.0968 (0.0966)	0.0585 (0.0587)	1.027 (1.043)	68.75 (67.10)
[130/157]	0.0973 (0.0966)	0.0587 (0.0588)	1.265 (1.039)	46.88 (67.01)
[140/157]	0.0976 (0.0966)	0.0593 (0.0588)	1.258 (1.039)	59.38 (67.07)
[150/157]	0.0978 (0.0965)	0.0593 (0.0588)	0.797 (1.039)	75.00 (67.26)
[156/157]	0.0798 (0.0964)	0.0554 (0.0588)	1.272 (1.035)	62.50 (67.36)
 * Train Acc 67.360
 * Val Acc 69.100, Total time 0.58
 * Val loss 0.934, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0084 (0.0084)	0.932 (0.932)	75.00 (75.00)
[10/157]	0.0985 (0.0914)	0.0595 (0.0544)	1.068 (1.078)	71.88 (64.49)
[20/157]	0.0971 (0.0938)	0.0596 (0.0566)	0.991 (1.070)	68.75 (65.77)
[30/157]	0.0951 (0.0945)	0.0580 (0.0574)	0.835 (1.040)	78.12 (67.54)
[40/157]	0.0954 (0.0949)	0.0582 (0.0578)	1.061 (1.047)	56.25 (67.23)
[50/157]	0.0957 (0.0953)	0.0587 (0.0581)	1.018 (1.025)	68.75 (67.83)
[60/157]	0.0970 (0.0954)	0.0585 (0.0583)	1.161 (1.033)	62.50 (67.47)
[70/157]	0.0986 (0.0956)	0.0604 (0.0585)	1.482 (1.040)	53.12 (67.03)
[80/157]	0.0967 (0.0957)	0.0589 (0.0586)	1.020 (1.023)	59.38 (67.48)
[90/157]	0.0972 (0.0958)	0.0583 (0.0586)	0.929 (1.020)	71.88 (67.96)
[100/157]	0.0982 (0.0959)	0.0598 (0.0587)	0.924 (1.025)	68.75 (67.98)
[110/157]	0.0972 (0.0959)	0.0593 (0.0587)	1.150 (1.020)	65.62 (67.91)
[120/157]	0.0976 (0.0960)	0.0595 (0.0588)	0.838 (1.019)	62.50 (67.82)
[130/157]	0.0964 (0.0960)	0.0579 (0.0588)	1.256 (1.026)	62.50 (67.56)
[140/157]	0.0992 (0.0961)	0.0595 (0.0588)	1.144 (1.028)	65.62 (67.60)
[150/157]	0.0974 (0.0961)	0.0591 (0.0589)	0.922 (1.026)	68.75 (67.40)
[156/157]	0.0763 (0.0960)	0.0524 (0.0588)	1.212 (1.025)	50.00 (67.52)
 * Train Acc 67.520
 * Val Acc 67.600, Total time 0.58
 * Val loss 0.953, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0090 (0.0090)	0.741 (0.741)	75.00 (75.00)
[10/157]	0.0963 (0.0914)	0.0584 (0.0546)	1.442 (1.027)	50.00 (70.45)
[20/157]	0.0983 (0.0942)	0.0588 (0.0566)	1.253 (1.044)	62.50 (69.35)
[30/157]	0.0966 (0.0950)	0.0584 (0.0573)	0.905 (1.052)	65.62 (68.55)
[40/157]	0.0988 (0.0956)	0.0594 (0.0579)	1.216 (1.062)	59.38 (67.99)
[50/157]	0.0971 (0.0958)	0.0596 (0.0582)	1.291 (1.060)	65.62 (67.83)
[60/157]	0.0982 (0.0960)	0.0593 (0.0583)	0.828 (1.030)	84.38 (68.90)
[70/157]	0.0976 (0.0961)	0.0599 (0.0585)	0.961 (1.017)	68.75 (69.01)
[80/157]	0.0975 (0.0962)	0.0592 (0.0586)	1.009 (1.019)	68.75 (68.56)
[90/157]	0.0967 (0.0962)	0.0589 (0.0587)	0.948 (1.021)	68.75 (68.37)
[100/157]	0.0960 (0.0963)	0.0581 (0.0587)	0.986 (1.025)	68.75 (68.01)
[110/157]	0.0980 (0.0963)	0.0598 (0.0588)	0.886 (1.021)	65.62 (68.02)
[120/157]	0.0965 (0.0963)	0.0588 (0.0589)	0.927 (1.020)	68.75 (68.16)
[130/157]	0.0977 (0.0964)	0.0596 (0.0589)	1.108 (1.019)	68.75 (68.03)
[140/157]	0.0983 (0.0965)	0.0607 (0.0590)	1.087 (1.025)	65.62 (67.86)
[150/157]	0.0972 (0.0966)	0.0590 (0.0590)	1.064 (1.024)	62.50 (67.78)
[156/157]	0.0804 (0.0965)	0.0543 (0.0590)	1.307 (1.022)	50.00 (67.78)
 * Train Acc 67.780
 * Val Acc 69.500, Total time 0.58
 * Val loss 0.938, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0416 (0.0416)	0.0082 (0.0082)	1.075 (1.075)	68.75 (68.75)
[10/157]	0.0970 (0.0910)	0.0593 (0.0543)	1.057 (1.020)	71.88 (69.60)
[20/157]	0.0984 (0.0940)	0.0597 (0.0568)	0.809 (1.008)	81.25 (68.60)
[30/157]	0.0966 (0.0949)	0.0587 (0.0576)	1.157 (1.027)	59.38 (67.04)
[40/157]	0.0990 (0.0955)	0.0594 (0.0581)	0.892 (1.011)	68.75 (67.84)
[50/157]	0.0955 (0.0958)	0.0572 (0.0582)	0.913 (1.019)	68.75 (67.16)
[60/157]	0.0985 (0.0960)	0.0595 (0.0584)	1.062 (1.017)	68.75 (67.37)
[70/157]	0.0955 (0.0961)	0.0586 (0.0585)	1.023 (1.030)	71.88 (67.39)
[80/157]	0.0977 (0.0962)	0.0589 (0.0587)	1.114 (1.036)	68.75 (67.59)
[90/157]	0.0967 (0.0963)	0.0586 (0.0588)	1.257 (1.037)	56.25 (67.65)
[100/157]	0.0974 (0.0963)	0.0592 (0.0589)	0.906 (1.034)	65.62 (67.57)
[110/157]	0.0982 (0.0965)	0.0589 (0.0589)	1.052 (1.029)	71.88 (67.76)
[120/157]	0.0966 (0.0965)	0.0584 (0.0589)	1.029 (1.023)	68.75 (67.85)
[130/157]	0.0990 (0.0966)	0.0601 (0.0590)	1.152 (1.027)	65.62 (67.72)
[140/157]	0.0974 (0.0967)	0.0589 (0.0590)	1.029 (1.032)	71.88 (67.51)
[150/157]	0.0965 (0.0967)	0.0585 (0.0590)	0.957 (1.036)	78.12 (67.47)
[156/157]	0.0795 (0.0966)	0.0540 (0.0590)	0.707 (1.033)	87.50 (67.56)
 * Train Acc 67.560
 * Val Acc 69.800, Total time 0.59
 * Val loss 0.907, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0087 (0.0087)	1.085 (1.085)	68.75 (68.75)
[10/157]	0.0974 (0.1015)	0.0585 (0.0633)	1.151 (1.048)	65.62 (67.90)
[20/157]	0.0970 (0.0987)	0.0593 (0.0610)	1.172 (1.066)	59.38 (65.48)
[30/157]	0.0969 (0.0978)	0.0585 (0.0602)	1.188 (1.069)	56.25 (64.82)
[40/157]	0.0959 (0.0972)	0.0581 (0.0598)	0.838 (1.055)	78.12 (66.23)
[50/157]	0.0960 (0.0969)	0.0585 (0.0596)	0.761 (1.071)	78.12 (65.75)
[60/157]	0.0968 (0.0968)	0.0585 (0.0595)	1.051 (1.057)	75.00 (66.80)
[70/157]	0.0965 (0.0967)	0.0586 (0.0595)	1.113 (1.048)	62.50 (66.81)
[80/157]	0.0962 (0.0965)	0.0584 (0.0593)	1.307 (1.047)	53.12 (67.21)
[90/157]	0.0954 (0.0965)	0.0577 (0.0592)	0.984 (1.043)	65.62 (67.24)
[100/157]	0.0962 (0.0965)	0.0567 (0.0592)	0.753 (1.035)	81.25 (67.70)
[110/157]	0.0949 (0.0964)	0.0569 (0.0591)	0.998 (1.028)	68.75 (67.91)
[120/157]	0.0962 (0.0973)	0.0578 (0.0598)	0.733 (1.023)	87.50 (68.18)
[130/157]	0.0980 (0.0972)	0.0589 (0.0597)	1.080 (1.017)	71.88 (68.27)
[140/157]	0.0979 (0.0971)	0.0592 (0.0596)	0.963 (1.023)	71.88 (68.17)
[150/157]	0.0960 (0.0970)	0.0576 (0.0596)	0.927 (1.017)	62.50 (68.36)
[156/157]	0.0778 (0.0968)	0.0526 (0.0595)	1.789 (1.019)	25.00 (68.20)
 * Train Acc 68.200
 * Val Acc 70.400, Total time 0.57
 * Val loss 0.907, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0083 (0.0083)	0.928 (0.928)	62.50 (62.50)
[10/157]	0.0954 (0.0909)	0.0576 (0.0541)	1.127 (0.950)	59.38 (69.03)
[20/157]	0.0955 (0.0929)	0.0580 (0.0562)	1.286 (0.981)	62.50 (68.01)
[30/157]	0.1005 (0.0987)	0.0624 (0.0614)	0.986 (0.991)	68.75 (68.15)
[40/157]	0.0948 (0.0979)	0.0576 (0.0607)	1.297 (0.985)	56.25 (68.60)
[50/157]	0.0956 (0.0975)	0.0573 (0.0601)	0.946 (0.984)	65.62 (68.81)
[60/157]	0.0954 (0.0972)	0.0574 (0.0598)	0.693 (0.984)	75.00 (68.80)
[70/157]	0.0967 (0.0970)	0.0589 (0.0596)	0.608 (0.988)	78.12 (68.84)
[80/157]	0.0986 (0.0978)	0.0595 (0.0604)	1.053 (0.988)	65.62 (68.52)
[90/157]	0.1003 (0.0980)	0.0609 (0.0604)	1.328 (0.994)	53.12 (68.78)
[100/157]	0.1016 (0.0982)	0.0615 (0.0605)	0.780 (0.992)	68.75 (68.81)
[110/157]	0.1011 (0.0984)	0.0609 (0.0605)	0.879 (0.993)	68.75 (68.78)
[120/157]	0.0991 (0.0984)	0.0609 (0.0605)	0.695 (0.994)	81.25 (68.75)
[130/157]	0.0990 (0.0985)	0.0605 (0.0605)	0.895 (0.996)	75.00 (68.80)
[140/157]	0.1012 (0.0986)	0.0609 (0.0606)	0.847 (0.997)	65.62 (68.59)
[150/157]	0.0998 (0.0986)	0.0607 (0.0606)	1.058 (1.001)	62.50 (68.27)
[156/157]	0.0839 (0.0986)	0.0551 (0.0606)	0.714 (1.003)	87.50 (68.30)
 * Train Acc 68.300
 * Val Acc 69.800, Total time 0.59
 * Val loss 0.909, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0086 (0.0086)	0.889 (0.889)	71.88 (71.88)
[10/157]	0.1011 (0.0936)	0.0599 (0.0555)	1.182 (1.092)	59.38 (66.19)
[20/157]	0.0979 (0.0962)	0.0602 (0.0578)	0.828 (1.077)	84.38 (66.96)
[30/157]	0.1012 (0.0973)	0.0626 (0.0589)	0.855 (1.022)	65.62 (68.65)
[40/157]	0.0983 (0.0978)	0.0600 (0.0593)	1.202 (1.031)	68.75 (68.52)
[50/157]	0.0996 (0.0981)	0.0621 (0.0597)	1.325 (1.029)	65.62 (69.00)
[60/157]	0.1001 (0.0983)	0.0607 (0.0600)	1.095 (1.028)	62.50 (68.60)
[70/157]	0.1004 (0.0984)	0.0609 (0.0601)	1.021 (1.023)	68.75 (68.75)
[80/157]	0.1002 (0.0986)	0.0608 (0.0601)	1.211 (1.026)	62.50 (68.25)
[90/157]	0.0993 (0.0987)	0.0600 (0.0602)	0.673 (1.022)	87.50 (68.41)
[100/157]	0.1000 (0.0987)	0.0603 (0.0603)	0.894 (1.014)	68.75 (68.69)
[110/157]	0.0979 (0.0987)	0.0600 (0.0603)	1.268 (1.021)	65.62 (68.55)
[120/157]	0.0996 (0.0988)	0.0608 (0.0604)	0.691 (1.020)	81.25 (68.36)
[130/157]	0.0995 (0.0988)	0.0615 (0.0604)	0.881 (1.022)	71.88 (68.32)
[140/157]	0.1001 (0.0988)	0.0611 (0.0605)	1.167 (1.020)	65.62 (68.62)
[150/157]	0.0989 (0.0989)	0.0593 (0.0605)	0.868 (1.019)	68.75 (68.67)
[156/157]	0.0853 (0.0988)	0.0553 (0.0605)	0.974 (1.015)	62.50 (68.66)
 * Train Acc 68.660
 * Val Acc 69.700, Total time 0.60
 * Val loss 0.910, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0087 (0.0087)	1.183 (1.183)	65.62 (65.62)
[10/157]	0.0993 (0.0932)	0.0617 (0.0559)	0.792 (1.012)	81.25 (67.05)
[20/157]	0.0992 (0.0960)	0.0596 (0.0584)	1.121 (0.988)	68.75 (68.01)
[30/157]	0.1001 (0.0969)	0.0612 (0.0591)	1.019 (1.006)	59.38 (67.24)
[40/157]	0.0993 (0.0975)	0.0601 (0.0596)	1.198 (0.993)	53.12 (67.76)
[50/157]	0.0973 (0.0978)	0.0585 (0.0598)	0.890 (1.010)	68.75 (66.91)
[60/157]	0.0988 (0.0980)	0.0599 (0.0600)	0.838 (1.013)	81.25 (67.52)
[70/157]	0.0993 (0.0982)	0.0599 (0.0601)	0.885 (1.010)	75.00 (67.83)
[80/157]	0.0997 (0.0984)	0.0607 (0.0602)	0.877 (1.004)	71.88 (68.13)
[90/157]	0.0991 (0.0985)	0.0599 (0.0602)	0.773 (0.985)	78.12 (68.78)
[100/157]	0.1016 (0.0986)	0.0624 (0.0604)	0.818 (0.985)	78.12 (68.60)
[110/157]	0.1004 (0.0987)	0.0601 (0.0604)	0.810 (0.981)	71.88 (68.69)
[120/157]	0.1013 (0.0987)	0.0611 (0.0604)	1.297 (0.987)	56.25 (68.34)
[130/157]	0.0994 (0.0988)	0.0610 (0.0605)	1.201 (0.988)	68.75 (68.54)
[140/157]	0.0997 (0.0988)	0.0596 (0.0605)	0.803 (0.987)	78.12 (68.66)
[150/157]	0.0994 (0.0988)	0.0607 (0.0605)	1.044 (0.989)	71.88 (68.77)
[156/157]	0.0828 (0.0987)	0.0561 (0.0605)	1.248 (0.990)	50.00 (68.66)
 * Train Acc 68.660
 * Val Acc 69.100, Total time 0.59
 * Val loss 0.894, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0082 (0.0082)	1.054 (1.054)	71.88 (71.88)
[10/157]	0.1002 (0.0941)	0.0602 (0.0555)	0.675 (0.979)	84.38 (71.02)
[20/157]	0.0993 (0.0968)	0.0588 (0.0577)	0.754 (0.961)	81.25 (71.43)
[30/157]	0.1000 (0.0977)	0.0605 (0.0587)	0.938 (0.991)	65.62 (69.56)
[40/157]	0.0991 (0.0980)	0.0605 (0.0592)	0.745 (1.023)	68.75 (68.52)
[50/157]	0.0998 (0.0983)	0.0603 (0.0595)	0.679 (0.985)	84.38 (69.98)
[60/157]	0.0987 (0.0985)	0.0593 (0.0597)	0.875 (0.973)	71.88 (70.18)
[70/157]	0.0995 (0.0986)	0.0610 (0.0600)	0.967 (0.975)	68.75 (70.16)
[80/157]	0.0999 (0.0987)	0.0606 (0.0601)	0.848 (0.974)	65.62 (70.18)
[90/157]	0.0994 (0.0988)	0.0599 (0.0601)	0.893 (0.964)	71.88 (70.60)
[100/157]	0.0986 (0.0988)	0.0602 (0.0602)	1.255 (0.973)	65.62 (70.08)
[110/157]	0.0997 (0.0988)	0.0595 (0.0603)	0.920 (0.975)	65.62 (69.93)
[120/157]	0.0984 (0.0988)	0.0601 (0.0603)	0.896 (0.972)	75.00 (70.02)
[130/157]	0.0991 (0.0989)	0.0608 (0.0604)	1.265 (0.975)	56.25 (69.70)
[140/157]	0.0995 (0.0989)	0.0602 (0.0605)	1.052 (0.974)	68.75 (69.86)
[150/157]	0.0990 (0.0990)	0.0605 (0.0605)	1.073 (0.979)	59.38 (69.52)
[156/157]	0.0825 (0.0988)	0.0565 (0.0605)	1.188 (0.976)	50.00 (69.56)
 * Train Acc 69.560
 * Val Acc 70.700, Total time 0.59
 * Val loss 0.897, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0084 (0.0084)	0.808 (0.808)	75.00 (75.00)
[10/157]	0.1009 (0.0937)	0.0612 (0.0555)	1.196 (0.957)	56.25 (69.60)
[20/157]	0.0993 (0.0962)	0.0608 (0.0580)	0.939 (0.970)	78.12 (70.98)
[30/157]	0.1000 (0.0972)	0.0610 (0.0590)	1.003 (1.007)	68.75 (68.55)
[40/157]	0.1019 (0.0978)	0.0621 (0.0594)	0.958 (0.993)	78.12 (69.82)
[50/157]	0.0981 (0.0980)	0.0609 (0.0597)	1.038 (0.986)	71.88 (70.10)
[60/157]	0.1020 (0.0983)	0.0612 (0.0600)	1.101 (0.988)	65.62 (69.67)
[70/157]	0.0993 (0.0984)	0.0596 (0.0600)	0.924 (0.984)	68.75 (69.94)
[80/157]	0.0997 (0.0985)	0.0611 (0.0601)	1.077 (0.987)	65.62 (69.56)
[90/157]	0.0984 (0.0986)	0.0600 (0.0602)	0.845 (0.980)	81.25 (69.92)
[100/157]	0.0985 (0.0987)	0.0589 (0.0602)	1.115 (0.979)	65.62 (69.83)
[110/157]	0.1001 (0.0988)	0.0611 (0.0603)	1.320 (0.982)	62.50 (69.62)
[120/157]	0.0999 (0.0988)	0.0603 (0.0603)	0.970 (0.975)	65.62 (69.89)
[130/157]	0.0997 (0.0988)	0.0606 (0.0603)	1.613 (0.974)	50.00 (69.97)
[140/157]	0.1001 (0.0989)	0.0606 (0.0604)	1.260 (0.978)	56.25 (69.75)
[150/157]	0.1001 (0.0989)	0.0610 (0.0604)	0.813 (0.975)	81.25 (69.74)
[156/157]	0.0830 (0.0988)	0.0566 (0.0604)	1.003 (0.970)	75.00 (69.86)
 * Train Acc 69.860
 * Val Acc 70.500, Total time 0.59
 * Val loss 0.902, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0087 (0.0087)	0.992 (0.992)	68.75 (68.75)
[10/157]	0.0986 (0.0934)	0.0600 (0.0557)	0.895 (0.964)	71.88 (69.03)
[20/157]	0.0995 (0.0959)	0.0616 (0.0581)	1.296 (0.963)	62.50 (70.09)
[30/157]	0.0993 (0.0970)	0.0606 (0.0592)	1.100 (0.983)	68.75 (70.06)
[40/157]	0.0998 (0.0975)	0.0613 (0.0596)	0.782 (0.958)	81.25 (70.73)
[50/157]	0.0977 (0.0977)	0.0588 (0.0598)	1.059 (0.967)	65.62 (70.53)
[60/157]	0.0986 (0.0979)	0.0602 (0.0600)	1.085 (0.974)	65.62 (70.03)
[70/157]	0.0981 (0.0981)	0.0589 (0.0601)	0.709 (0.966)	75.00 (70.03)
[80/157]	0.0996 (0.0982)	0.0616 (0.0603)	1.047 (0.972)	71.88 (69.68)
[90/157]	0.0992 (0.0983)	0.0605 (0.0604)	0.996 (0.974)	68.75 (69.57)
[100/157]	0.0981 (0.0984)	0.0607 (0.0604)	1.173 (0.984)	65.62 (69.12)
[110/157]	0.0997 (0.0984)	0.0609 (0.0605)	0.914 (0.982)	78.12 (69.23)
[120/157]	0.1002 (0.0985)	0.0608 (0.0605)	0.946 (0.977)	71.88 (69.45)
[130/157]	0.0984 (0.0985)	0.0597 (0.0606)	0.995 (0.972)	65.62 (69.47)
[140/157]	0.0960 (0.0986)	0.0562 (0.0605)	0.673 (0.971)	81.25 (69.59)
[150/157]	0.1003 (0.0986)	0.0615 (0.0605)	1.351 (0.975)	43.75 (69.41)
[156/157]	0.0821 (0.0985)	0.0560 (0.0605)	0.997 (0.974)	75.00 (69.36)
 * Train Acc 69.360
 * Val Acc 70.500, Total time 0.59
 * Val loss 0.885, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0089 (0.0089)	0.876 (0.876)	62.50 (62.50)
[10/157]	0.0993 (0.0932)	0.0608 (0.0556)	0.722 (1.017)	78.12 (66.19)
[20/157]	0.0993 (0.0959)	0.0606 (0.0583)	0.678 (0.974)	75.00 (68.45)
[30/157]	0.0993 (0.0968)	0.0611 (0.0590)	1.342 (0.964)	59.38 (69.66)
[40/157]	0.1007 (0.0974)	0.0615 (0.0595)	1.120 (0.946)	65.62 (70.66)
[50/157]	0.0983 (0.0978)	0.0589 (0.0597)	1.110 (0.962)	62.50 (69.79)
[60/157]	0.0980 (0.0980)	0.0588 (0.0599)	1.037 (0.961)	65.62 (69.52)
[70/157]	0.0997 (0.0982)	0.0599 (0.0600)	0.789 (0.963)	71.88 (69.63)
[80/157]	0.0993 (0.0983)	0.0612 (0.0601)	0.966 (0.969)	68.75 (69.02)
[90/157]	0.0989 (0.0984)	0.0604 (0.0603)	1.121 (0.967)	62.50 (68.89)
[100/157]	0.0985 (0.0984)	0.0606 (0.0603)	0.796 (0.965)	71.88 (68.87)
[110/157]	0.1000 (0.0985)	0.0612 (0.0604)	1.099 (0.969)	62.50 (68.67)
[120/157]	0.0985 (0.0986)	0.0592 (0.0605)	0.856 (0.964)	71.88 (68.96)
[130/157]	0.1006 (0.0986)	0.0617 (0.0605)	1.200 (0.959)	65.62 (69.30)
[140/157]	0.0994 (0.0987)	0.0612 (0.0606)	1.094 (0.962)	62.50 (69.35)
[150/157]	0.0986 (0.0987)	0.0592 (0.0606)	1.132 (0.964)	56.25 (69.08)
[156/157]	0.0817 (0.0986)	0.0554 (0.0606)	1.339 (0.970)	62.50 (68.96)
 * Train Acc 68.960
 * Val Acc 69.900, Total time 0.58
 * Val loss 0.891, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0083 (0.0083)	1.132 (1.132)	68.75 (68.75)
[10/157]	0.0995 (0.0933)	0.0613 (0.0561)	0.919 (0.972)	78.12 (72.44)
[20/157]	0.0977 (0.0959)	0.0599 (0.0583)	0.983 (0.982)	68.75 (69.64)
[30/157]	0.1005 (0.0970)	0.0623 (0.0593)	0.940 (0.972)	71.88 (70.16)
[40/157]	0.0996 (0.0975)	0.0601 (0.0597)	0.696 (0.977)	81.25 (70.05)
[50/157]	0.0989 (0.0978)	0.0606 (0.0599)	0.711 (0.956)	78.12 (70.04)
[60/157]	0.0984 (0.0979)	0.0604 (0.0600)	0.969 (0.954)	75.00 (69.98)
[70/157]	0.0994 (0.0980)	0.0603 (0.0602)	1.010 (0.964)	65.62 (69.54)
[80/157]	0.1006 (0.0982)	0.0609 (0.0602)	1.061 (0.964)	65.62 (69.48)
[90/157]	0.0993 (0.0983)	0.0610 (0.0603)	0.945 (0.968)	68.75 (69.27)
[100/157]	0.0995 (0.0984)	0.0598 (0.0603)	1.351 (0.974)	65.62 (69.15)
[110/157]	0.1016 (0.0985)	0.0613 (0.0603)	1.468 (0.975)	46.88 (69.20)
[120/157]	0.1004 (0.0986)	0.0608 (0.0604)	1.012 (0.971)	71.88 (69.32)
[130/157]	0.0976 (0.0986)	0.0582 (0.0604)	0.748 (0.972)	68.75 (69.01)
[140/157]	0.0994 (0.0986)	0.0602 (0.0604)	0.926 (0.972)	65.62 (69.08)
[150/157]	0.0995 (0.0987)	0.0612 (0.0605)	0.802 (0.970)	78.12 (69.31)
[156/157]	0.0815 (0.0985)	0.0557 (0.0605)	1.232 (0.968)	50.00 (69.32)
 * Train Acc 69.320
 * Val Acc 69.800, Total time 0.59
 * Val loss 0.900, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0094 (0.0094)	0.657 (0.657)	84.38 (84.38)
[10/157]	0.1004 (0.0937)	0.0617 (0.0560)	0.905 (0.937)	62.50 (70.17)
[20/157]	0.0995 (0.0962)	0.0605 (0.0585)	0.868 (0.937)	75.00 (70.98)
[30/157]	0.0993 (0.0973)	0.0596 (0.0592)	0.789 (0.936)	65.62 (70.16)
[40/157]	0.0975 (0.0978)	0.0589 (0.0594)	0.884 (0.937)	68.75 (70.05)
[50/157]	0.0986 (0.0982)	0.0601 (0.0598)	0.771 (0.943)	75.00 (69.73)
[60/157]	0.0991 (0.0985)	0.0607 (0.0600)	1.194 (0.954)	62.50 (69.57)
[70/157]	0.0995 (0.0986)	0.0608 (0.0601)	0.992 (0.958)	65.62 (69.15)
[80/157]	0.0999 (0.0988)	0.0608 (0.0602)	0.993 (0.955)	68.75 (69.06)
[90/157]	0.1005 (0.0988)	0.0601 (0.0602)	0.890 (0.961)	75.00 (69.23)
[100/157]	0.0998 (0.0989)	0.0599 (0.0603)	0.862 (0.960)	71.88 (69.12)
[110/157]	0.1017 (0.0990)	0.0621 (0.0604)	0.767 (0.961)	87.50 (69.20)
[120/157]	0.0993 (0.0990)	0.0600 (0.0604)	1.061 (0.963)	62.50 (69.09)
[130/157]	0.0995 (0.0990)	0.0612 (0.0604)	1.352 (0.963)	62.50 (69.30)
[140/157]	0.0989 (0.0990)	0.0605 (0.0604)	0.837 (0.960)	75.00 (69.53)
[150/157]	0.0995 (0.0990)	0.0603 (0.0605)	1.313 (0.961)	59.38 (69.62)
[156/157]	0.0832 (0.0989)	0.0557 (0.0605)	1.957 (0.966)	37.50 (69.40)
 * Train Acc 69.400
 * Val Acc 70.100, Total time 0.59
 * Val loss 0.902, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0085 (0.0085)	0.853 (0.853)	71.88 (71.88)
[10/157]	0.0994 (0.0934)	0.0614 (0.0556)	1.011 (0.932)	65.62 (70.17)
[20/157]	0.0980 (0.0961)	0.0594 (0.0583)	0.938 (0.940)	68.75 (70.09)
[30/157]	0.1003 (0.0971)	0.0606 (0.0590)	1.028 (0.968)	78.12 (70.26)
[40/157]	0.1011 (0.0977)	0.0620 (0.0595)	1.185 (0.961)	59.38 (71.04)
[50/157]	0.0993 (0.0980)	0.0600 (0.0598)	1.052 (0.941)	62.50 (71.32)
[60/157]	0.0999 (0.0982)	0.0605 (0.0599)	1.178 (0.953)	59.38 (71.21)
[70/157]	0.1011 (0.0984)	0.0608 (0.0601)	0.944 (0.964)	65.62 (70.16)
[80/157]	0.1017 (0.0986)	0.0621 (0.0602)	0.969 (0.973)	62.50 (69.71)
[90/157]	0.0987 (0.0987)	0.0599 (0.0603)	1.500 (0.978)	50.00 (69.44)
[100/157]	0.0981 (0.0987)	0.0593 (0.0603)	1.249 (0.981)	56.25 (69.12)
[110/157]	0.1010 (0.0988)	0.0620 (0.0604)	0.970 (0.979)	81.25 (69.17)
[120/157]	0.0993 (0.0988)	0.0611 (0.0605)	0.806 (0.975)	75.00 (69.50)
[130/157]	0.1002 (0.0989)	0.0608 (0.0605)	0.857 (0.974)	68.75 (69.23)
[140/157]	0.0994 (0.0989)	0.0601 (0.0605)	1.040 (0.974)	59.38 (69.33)
[150/157]	0.0984 (0.0990)	0.0591 (0.0605)	0.821 (0.969)	78.12 (69.62)
[156/157]	0.0826 (0.0989)	0.0562 (0.0605)	1.098 (0.974)	62.50 (69.42)
 * Train Acc 69.420
 * Val Acc 70.900, Total time 0.59
 * Val loss 0.898, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0089 (0.0089)	0.726 (0.726)	84.38 (84.38)
[10/157]	0.1013 (0.0937)	0.0606 (0.0558)	1.548 (1.010)	53.12 (69.03)
[20/157]	0.0995 (0.0963)	0.0592 (0.0581)	0.948 (0.990)	59.38 (68.90)
[30/157]	0.1006 (0.0974)	0.0597 (0.0588)	1.288 (1.001)	59.38 (68.85)
[40/157]	0.0991 (0.0979)	0.0601 (0.0592)	1.160 (1.011)	68.75 (68.14)
[50/157]	0.1000 (0.0982)	0.0611 (0.0596)	0.857 (1.010)	75.00 (67.89)
[60/157]	0.1000 (0.0984)	0.0607 (0.0598)	0.927 (1.005)	65.62 (68.24)
[70/157]	0.0992 (0.0986)	0.0596 (0.0599)	0.837 (1.014)	81.25 (68.09)
[80/157]	0.0991 (0.0987)	0.0605 (0.0600)	0.844 (1.013)	71.88 (68.02)
[90/157]	0.1003 (0.0987)	0.0611 (0.0601)	1.003 (0.997)	71.88 (68.75)
[100/157]	0.0991 (0.0988)	0.0598 (0.0602)	0.969 (0.988)	65.62 (68.90)
[110/157]	0.0992 (0.0988)	0.0601 (0.0602)	1.245 (0.991)	59.38 (68.92)
[120/157]	0.0993 (0.0989)	0.0608 (0.0603)	0.817 (0.991)	71.88 (68.67)
[130/157]	0.0991 (0.0989)	0.0594 (0.0603)	0.720 (0.987)	84.38 (68.89)
[140/157]	0.0989 (0.0989)	0.0610 (0.0604)	0.898 (0.983)	75.00 (69.08)
[150/157]	0.0989 (0.0989)	0.0597 (0.0603)	0.793 (0.980)	75.00 (69.16)
[156/157]	0.0809 (0.0988)	0.0551 (0.0603)	0.794 (0.980)	75.00 (69.14)
 * Train Acc 69.140
 * Val Acc 71.100, Total time 0.59
 * Val loss 0.883, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0084 (0.0084)	1.113 (1.113)	59.38 (59.38)
[10/157]	0.0992 (0.0935)	0.0614 (0.0555)	1.090 (1.032)	62.50 (65.91)
[20/157]	0.1001 (0.0961)	0.0606 (0.0580)	0.836 (1.019)	75.00 (67.11)
[30/157]	0.0985 (0.0969)	0.0607 (0.0589)	1.153 (1.009)	62.50 (68.04)
[40/157]	0.1000 (0.0975)	0.0607 (0.0594)	1.128 (1.009)	65.62 (68.37)
[50/157]	0.0988 (0.0978)	0.0601 (0.0597)	1.023 (0.988)	65.62 (68.75)
[60/157]	0.0998 (0.0981)	0.0608 (0.0599)	1.220 (0.984)	56.25 (68.75)
[70/157]	0.0994 (0.0983)	0.0591 (0.0600)	0.891 (0.976)	71.88 (68.79)
[80/157]	0.0981 (0.0984)	0.0600 (0.0600)	0.731 (0.974)	75.00 (68.67)
[90/157]	0.0997 (0.0984)	0.0608 (0.0602)	0.708 (0.971)	71.88 (68.54)
[100/157]	0.1003 (0.0985)	0.0620 (0.0602)	0.997 (0.982)	71.88 (68.19)
[110/157]	0.0989 (0.0986)	0.0606 (0.0603)	0.848 (0.986)	75.00 (68.05)
[120/157]	0.0994 (0.0986)	0.0597 (0.0604)	0.876 (0.980)	71.88 (68.39)
[130/157]	0.0988 (0.0987)	0.0606 (0.0604)	0.867 (0.969)	84.38 (69.04)
[140/157]	0.1003 (0.0988)	0.0617 (0.0605)	0.790 (0.967)	84.38 (69.17)
[150/157]	0.0996 (0.0988)	0.0598 (0.0605)	0.873 (0.961)	71.88 (69.52)
[156/157]	0.0836 (0.0987)	0.0550 (0.0605)	1.156 (0.965)	62.50 (69.32)
 * Train Acc 69.320
 * Val Acc 70.100, Total time 0.59
 * Val loss 0.888, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0452 (0.0452)	0.0084 (0.0084)	1.272 (1.272)	53.12 (53.12)
[10/157]	0.0985 (0.0931)	0.0605 (0.0554)	1.604 (1.118)	50.00 (64.77)
[20/157]	0.0999 (0.0959)	0.0609 (0.0580)	1.119 (1.046)	62.50 (67.71)
[30/157]	0.0999 (0.0969)	0.0618 (0.0590)	1.109 (1.026)	59.38 (68.75)
[40/157]	0.0990 (0.0975)	0.0595 (0.0594)	1.008 (0.998)	59.38 (68.98)
[50/157]	0.1004 (0.0979)	0.0598 (0.0597)	1.443 (1.000)	50.00 (68.75)
[60/157]	0.0978 (0.0982)	0.0598 (0.0598)	1.107 (1.005)	56.25 (69.06)
[70/157]	0.1003 (0.0983)	0.0619 (0.0600)	0.837 (1.003)	78.12 (68.88)
[80/157]	0.0996 (0.0985)	0.0604 (0.0602)	0.855 (0.989)	71.88 (69.37)
[90/157]	0.1007 (0.0986)	0.0601 (0.0602)	0.952 (0.981)	68.75 (69.47)
[100/157]	0.0992 (0.0987)	0.0596 (0.0603)	0.865 (0.985)	65.62 (68.90)
[110/157]	0.1000 (0.0988)	0.0611 (0.0604)	0.881 (0.982)	65.62 (69.00)
[120/157]	0.0998 (0.0988)	0.0610 (0.0605)	1.030 (0.975)	62.50 (69.21)
[130/157]	0.1003 (0.0989)	0.0605 (0.0605)	0.855 (0.969)	75.00 (69.47)
[140/157]	0.1013 (0.0989)	0.0616 (0.0606)	0.939 (0.968)	62.50 (69.46)
[150/157]	0.0998 (0.0989)	0.0609 (0.0606)	1.007 (0.969)	62.50 (69.45)
[156/157]	0.0831 (0.0988)	0.0565 (0.0606)	0.901 (0.970)	75.00 (69.42)
 * Train Acc 69.420
 * Val Acc 71.200, Total time 0.59
 * Val loss 0.884, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0085 (0.0085)	0.769 (0.769)	75.00 (75.00)
[10/157]	0.1202 (0.1054)	0.0803 (0.0673)	1.049 (0.921)	62.50 (70.74)
[20/157]	0.1041 (0.1064)	0.0651 (0.0681)	0.952 (0.975)	71.88 (70.39)
[30/157]	0.0954 (0.1031)	0.0579 (0.0652)	0.864 (0.972)	75.00 (70.77)
[40/157]	0.0970 (0.1012)	0.0587 (0.0635)	0.748 (0.962)	71.88 (70.58)
[50/157]	0.1012 (0.1009)	0.0620 (0.0631)	1.154 (0.978)	62.50 (70.40)
[60/157]	0.1037 (0.1010)	0.0636 (0.0631)	1.131 (0.980)	71.88 (70.08)
[70/157]	0.1012 (0.1010)	0.0621 (0.0631)	1.069 (0.971)	62.50 (70.25)
[80/157]	0.1008 (0.1011)	0.0622 (0.0631)	0.825 (0.972)	75.00 (69.87)
[90/157]	0.1007 (0.1011)	0.0618 (0.0631)	1.075 (0.966)	65.62 (70.26)
[100/157]	0.1009 (0.1012)	0.0621 (0.0631)	1.201 (0.968)	71.88 (69.83)
[110/157]	0.1020 (0.1013)	0.0626 (0.0631)	1.115 (0.970)	68.75 (69.68)
[120/157]	0.1002 (0.1013)	0.0610 (0.0630)	0.943 (0.963)	65.62 (70.02)
[130/157]	0.1009 (0.1013)	0.0618 (0.0630)	0.855 (0.960)	71.88 (70.28)
[140/157]	0.1006 (0.1013)	0.0618 (0.0631)	0.970 (0.968)	68.75 (70.08)
[150/157]	0.1008 (0.1014)	0.0616 (0.0630)	1.214 (0.977)	65.62 (69.78)
[156/157]	0.0858 (0.1013)	0.0580 (0.0630)	0.498 (0.973)	87.50 (69.92)
 * Train Acc 69.920
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.875, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0092 (0.0092)	0.804 (0.804)	81.25 (81.25)
[10/157]	0.1004 (0.0952)	0.0627 (0.0572)	1.018 (0.959)	62.50 (71.31)
[20/157]	0.1010 (0.0984)	0.0616 (0.0600)	1.257 (0.930)	59.38 (72.62)
[30/157]	0.1013 (0.0994)	0.0629 (0.0610)	1.024 (0.949)	68.75 (71.47)
[40/157]	0.0994 (0.0997)	0.0603 (0.0613)	0.994 (0.950)	68.75 (70.88)
[50/157]	0.1012 (0.1001)	0.0619 (0.0616)	1.008 (0.947)	68.75 (70.71)
[60/157]	0.1009 (0.1004)	0.0615 (0.0618)	1.336 (0.958)	65.62 (70.39)
[70/157]	0.1016 (0.1006)	0.0624 (0.0620)	0.669 (0.966)	84.38 (70.29)
[80/157]	0.1011 (0.1006)	0.0607 (0.0620)	0.908 (0.965)	75.00 (70.22)
[90/157]	0.1015 (0.1007)	0.0620 (0.0621)	0.840 (0.967)	65.62 (70.19)
[100/157]	0.1033 (0.1007)	0.0635 (0.0622)	1.035 (0.967)	65.62 (70.08)
[110/157]	0.1014 (0.1008)	0.0625 (0.0622)	0.805 (0.960)	75.00 (70.19)
[120/157]	0.0971 (0.1005)	0.0587 (0.0620)	0.915 (0.967)	81.25 (70.12)
[130/157]	0.0952 (0.1009)	0.0571 (0.0624)	0.960 (0.968)	78.12 (70.16)
[140/157]	0.1157 (0.1006)	0.0752 (0.0622)	1.165 (0.967)	62.50 (70.06)
[150/157]	0.0960 (0.1011)	0.0586 (0.0627)	0.954 (0.968)	65.62 (69.99)
[156/157]	0.0786 (0.1007)	0.0540 (0.0625)	1.553 (0.968)	50.00 (69.88)
 * Train Acc 69.880
 * Val Acc 70.800, Total time 0.58
 * Val loss 0.893, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0087 (0.0087)	0.840 (0.840)	78.12 (78.12)
[10/157]	0.0967 (0.0915)	0.0585 (0.0546)	0.951 (0.921)	71.88 (70.74)
[20/157]	0.1019 (0.1006)	0.0624 (0.0628)	1.071 (0.938)	68.75 (71.73)
[30/157]	0.1019 (0.1011)	0.0626 (0.0631)	0.912 (0.940)	62.50 (70.87)
[40/157]	0.0977 (0.1002)	0.0590 (0.0622)	1.165 (0.945)	68.75 (71.11)
[50/157]	0.0991 (0.0997)	0.0597 (0.0617)	0.822 (0.956)	75.00 (70.89)
[60/157]	0.0999 (0.0995)	0.0606 (0.0614)	1.229 (0.962)	59.38 (70.65)
[70/157]	0.0997 (0.0995)	0.0601 (0.0613)	0.991 (0.948)	62.50 (70.86)
[80/157]	0.0995 (0.0994)	0.0606 (0.0612)	0.923 (0.950)	71.88 (70.83)
[90/157]	0.0989 (0.0993)	0.0597 (0.0611)	1.191 (0.959)	65.62 (70.60)
[100/157]	0.0997 (0.0992)	0.0601 (0.0610)	0.942 (0.959)	71.88 (70.76)
[110/157]	0.0992 (0.0992)	0.0609 (0.0609)	0.779 (0.951)	75.00 (70.92)
[120/157]	0.0991 (0.0991)	0.0603 (0.0608)	0.980 (0.946)	62.50 (70.69)
[130/157]	0.0993 (0.0991)	0.0598 (0.0608)	1.210 (0.947)	53.12 (70.42)
[140/157]	0.0995 (0.0990)	0.0606 (0.0607)	1.326 (0.950)	56.25 (70.41)
[150/157]	0.0982 (0.0990)	0.0579 (0.0607)	1.007 (0.952)	68.75 (70.45)
[156/157]	0.0789 (0.0987)	0.0540 (0.0605)	0.866 (0.953)	75.00 (70.36)
 * Train Acc 70.360
 * Val Acc 70.700, Total time 0.61
 * Val loss 0.877, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0090 (0.0090)	0.894 (0.894)	81.25 (81.25)
[10/157]	0.0976 (0.0965)	0.0582 (0.0580)	0.902 (0.989)	71.88 (70.17)
[20/157]	0.1223 (0.1021)	0.0797 (0.0632)	0.931 (0.968)	78.12 (69.79)
[30/157]	0.1053 (0.1032)	0.0653 (0.0641)	0.944 (0.953)	71.88 (70.67)
[40/157]	0.1031 (0.1037)	0.0651 (0.0646)	0.865 (0.960)	75.00 (69.97)
[50/157]	0.1210 (0.1028)	0.0793 (0.0640)	1.014 (0.978)	65.62 (69.30)
[60/157]	0.1011 (0.1036)	0.0616 (0.0647)	0.799 (0.965)	75.00 (69.72)
[70/157]	0.1009 (0.1031)	0.0619 (0.0643)	0.761 (0.961)	81.25 (70.03)
[80/157]	0.1019 (0.1028)	0.0631 (0.0641)	1.092 (0.965)	68.75 (69.87)
[90/157]	0.1004 (0.1026)	0.0613 (0.0639)	0.765 (0.962)	68.75 (70.02)
[100/157]	0.1002 (0.1024)	0.0607 (0.0637)	0.792 (0.959)	68.75 (70.42)
[110/157]	0.0996 (0.1022)	0.0602 (0.0634)	0.871 (0.958)	78.12 (70.33)
[120/157]	0.1023 (0.1021)	0.0628 (0.0634)	1.004 (0.956)	65.62 (70.30)
[130/157]	0.1004 (0.1020)	0.0617 (0.0632)	0.629 (0.952)	78.12 (70.35)
[140/157]	0.0998 (0.1018)	0.0612 (0.0631)	0.955 (0.958)	81.25 (70.04)
[150/157]	0.0998 (0.1017)	0.0605 (0.0630)	1.231 (0.959)	59.38 (70.05)
[156/157]	0.0838 (0.1015)	0.0558 (0.0629)	1.005 (0.958)	75.00 (70.12)
 * Train Acc 70.120
 * Val Acc 71.600, Total time 0.59
 * Val loss 0.871, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0089 (0.0089)	1.011 (1.011)	78.12 (78.12)
[10/157]	0.1012 (0.0950)	0.0618 (0.0560)	0.662 (0.895)	84.38 (74.15)
[20/157]	0.1006 (0.0974)	0.0615 (0.0586)	0.869 (0.896)	81.25 (74.11)
[30/157]	0.0988 (0.0983)	0.0594 (0.0595)	0.789 (0.914)	78.12 (72.58)
[40/157]	0.1012 (0.0987)	0.0608 (0.0599)	0.879 (0.928)	68.75 (71.80)
[50/157]	0.1007 (0.0991)	0.0621 (0.0604)	0.947 (0.923)	65.62 (71.94)
[60/157]	0.1010 (0.0994)	0.0617 (0.0607)	1.045 (0.930)	68.75 (71.41)
[70/157]	0.1019 (0.0996)	0.0622 (0.0609)	0.712 (0.932)	81.25 (71.48)
[80/157]	0.1010 (0.0998)	0.0611 (0.0610)	1.102 (0.935)	62.50 (71.33)
[90/157]	0.1003 (0.0999)	0.0612 (0.0610)	1.217 (0.941)	62.50 (70.84)
[100/157]	0.1001 (0.1000)	0.0605 (0.0611)	0.969 (0.936)	65.62 (70.92)
[110/157]	0.1040 (0.1001)	0.0614 (0.0612)	0.845 (0.940)	75.00 (70.58)
[120/157]	0.1025 (0.1001)	0.0635 (0.0613)	1.053 (0.946)	65.62 (70.25)
[130/157]	0.0999 (0.1002)	0.0609 (0.0613)	0.862 (0.946)	75.00 (70.35)
[140/157]	0.0988 (0.1002)	0.0602 (0.0613)	0.968 (0.942)	65.62 (70.59)
[150/157]	0.0994 (0.1001)	0.0602 (0.0613)	0.762 (0.945)	75.00 (70.30)
[156/157]	0.0859 (0.1001)	0.0569 (0.0613)	1.464 (0.948)	37.50 (70.02)
 * Train Acc 70.020
 * Val Acc 70.200, Total time 0.59
 * Val loss 0.894, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0089 (0.0089)	0.804 (0.804)	84.38 (84.38)
[10/157]	0.0966 (0.0904)	0.0593 (0.0532)	1.055 (0.949)	71.88 (72.73)
[20/157]	0.1026 (0.0951)	0.0633 (0.0574)	0.876 (0.916)	71.88 (70.98)
[30/157]	0.1004 (0.0978)	0.0626 (0.0596)	1.209 (0.924)	59.38 (71.37)
[40/157]	0.0985 (0.0974)	0.0593 (0.0592)	1.035 (0.917)	59.38 (71.42)
[50/157]	0.1009 (0.0975)	0.0605 (0.0593)	0.938 (0.927)	75.00 (71.02)
[60/157]	0.1015 (0.0983)	0.0615 (0.0599)	1.204 (0.931)	50.00 (70.59)
[70/157]	0.1026 (0.0989)	0.0632 (0.0604)	0.990 (0.944)	65.62 (70.20)
[80/157]	0.1051 (0.0993)	0.0649 (0.0608)	0.922 (0.943)	71.88 (70.45)
[90/157]	0.1036 (0.0997)	0.0641 (0.0611)	1.086 (0.945)	62.50 (70.36)
[100/157]	0.1010 (0.0999)	0.0616 (0.0613)	1.004 (0.953)	68.75 (70.20)
[110/157]	0.1030 (0.1001)	0.0640 (0.0615)	1.332 (0.948)	53.12 (70.16)
[120/157]	0.1017 (0.1002)	0.0623 (0.0616)	0.796 (0.950)	81.25 (70.02)
[130/157]	0.1025 (0.1004)	0.0636 (0.0618)	1.016 (0.956)	75.00 (69.70)
[140/157]	0.1024 (0.1006)	0.0632 (0.0619)	1.042 (0.956)	68.75 (69.77)
[150/157]	0.0959 (0.1005)	0.0574 (0.0619)	1.002 (0.958)	71.88 (69.74)
[156/157]	0.0771 (0.1002)	0.0531 (0.0617)	1.104 (0.957)	62.50 (69.70)
 * Train Acc 69.700
 * Val Acc 69.700, Total time 0.58
 * Val loss 0.901, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0419 (0.0419)	0.0082 (0.0082)	1.303 (1.303)	56.25 (56.25)
[10/157]	0.1003 (0.0945)	0.0609 (0.0567)	0.702 (0.959)	81.25 (71.59)
[20/157]	0.0995 (0.0971)	0.0603 (0.0590)	0.842 (0.948)	71.88 (70.98)
[30/157]	0.1048 (0.0982)	0.0611 (0.0598)	0.797 (0.987)	78.12 (69.15)
[40/157]	0.0992 (0.0985)	0.0600 (0.0601)	1.196 (0.984)	56.25 (69.05)
[50/157]	0.1000 (0.0988)	0.0608 (0.0603)	0.798 (0.974)	78.12 (69.61)
[60/157]	0.0998 (0.0990)	0.0606 (0.0605)	0.923 (0.966)	84.38 (70.03)
[70/157]	0.1003 (0.0991)	0.0611 (0.0606)	0.736 (0.982)	81.25 (69.54)
[80/157]	0.1003 (0.0992)	0.0610 (0.0606)	1.051 (0.983)	65.62 (69.56)
[90/157]	0.0998 (0.0993)	0.0608 (0.0608)	0.595 (0.971)	87.50 (70.05)
[100/157]	0.1006 (0.0994)	0.0614 (0.0608)	0.886 (0.969)	71.88 (70.14)
[110/157]	0.1018 (0.0996)	0.0610 (0.0610)	0.914 (0.960)	68.75 (70.41)
[120/157]	0.0994 (0.0996)	0.0600 (0.0610)	1.119 (0.954)	68.75 (70.51)
[130/157]	0.0994 (0.0997)	0.0601 (0.0610)	1.070 (0.958)	68.75 (70.44)
[140/157]	0.1007 (0.0997)	0.0616 (0.0610)	0.986 (0.957)	59.38 (70.35)
[150/157]	0.1005 (0.0997)	0.0617 (0.0611)	1.004 (0.960)	65.62 (70.24)
[156/157]	0.0825 (0.0996)	0.0566 (0.0610)	0.759 (0.960)	75.00 (70.22)
 * Train Acc 70.220
 * Val Acc 71.700, Total time 0.59
 * Val loss 0.873, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0087 (0.0087)	1.262 (1.262)	62.50 (62.50)
[10/157]	0.1019 (0.0949)	0.0623 (0.0568)	0.868 (0.981)	71.88 (67.61)
[20/157]	0.1004 (0.0977)	0.0611 (0.0594)	0.907 (0.957)	78.12 (68.90)
[30/157]	0.1001 (0.0984)	0.0608 (0.0601)	0.881 (0.926)	75.00 (70.56)
[40/157]	0.1013 (0.0989)	0.0612 (0.0604)	1.142 (0.941)	75.00 (70.73)
[50/157]	0.0981 (0.0991)	0.0589 (0.0605)	0.849 (0.936)	71.88 (70.59)
[60/157]	0.1033 (0.0994)	0.0627 (0.0607)	0.722 (0.931)	81.25 (70.80)
[70/157]	0.1020 (0.0995)	0.0619 (0.0609)	0.933 (0.924)	78.12 (70.91)
[80/157]	0.0999 (0.0996)	0.0602 (0.0610)	1.087 (0.929)	68.75 (71.10)
[90/157]	0.0995 (0.0997)	0.0602 (0.0610)	0.966 (0.938)	68.75 (70.78)
[100/157]	0.1015 (0.0997)	0.0613 (0.0610)	0.998 (0.941)	62.50 (70.70)
[110/157]	0.1000 (0.0997)	0.0612 (0.0610)	0.897 (0.945)	78.12 (70.61)
[120/157]	0.1024 (0.0998)	0.0621 (0.0611)	0.853 (0.949)	75.00 (70.45)
[130/157]	0.1003 (0.0998)	0.0606 (0.0611)	1.175 (0.953)	65.62 (70.28)
[140/157]	0.0999 (0.0998)	0.0604 (0.0611)	0.829 (0.956)	71.88 (69.99)
[150/157]	0.1012 (0.0998)	0.0614 (0.0611)	1.275 (0.955)	75.00 (70.01)
[156/157]	0.0842 (0.0997)	0.0564 (0.0611)	0.976 (0.957)	50.00 (70.00)
 * Train Acc 70.000
 * Val Acc 70.900, Total time 0.60
 * Val loss 0.868, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0095 (0.0095)	1.251 (1.251)	56.25 (56.25)
[10/157]	0.1002 (0.0948)	0.0610 (0.0561)	0.775 (0.941)	75.00 (71.02)
[20/157]	0.1003 (0.0972)	0.0609 (0.0586)	0.803 (0.906)	78.12 (73.07)
[30/157]	0.0999 (0.0981)	0.0600 (0.0594)	0.767 (0.905)	71.88 (72.58)
[40/157]	0.1001 (0.0986)	0.0608 (0.0598)	1.048 (0.912)	62.50 (71.72)
[50/157]	0.1005 (0.0989)	0.0611 (0.0601)	1.143 (0.912)	65.62 (71.94)
[60/157]	0.1020 (0.0991)	0.0623 (0.0604)	0.872 (0.932)	81.25 (71.16)
[70/157]	0.1011 (0.0994)	0.0611 (0.0607)	0.669 (0.934)	84.38 (71.30)
[80/157]	0.1000 (0.0995)	0.0604 (0.0607)	0.850 (0.934)	65.62 (71.22)
[90/157]	0.1005 (0.0996)	0.0603 (0.0608)	0.858 (0.932)	71.88 (71.26)
[100/157]	0.1000 (0.0996)	0.0608 (0.0608)	0.717 (0.936)	78.12 (70.95)
[110/157]	0.1000 (0.0996)	0.0603 (0.0608)	1.005 (0.929)	65.62 (71.20)
[120/157]	0.1009 (0.0997)	0.0612 (0.0608)	1.232 (0.931)	59.38 (71.18)
[130/157]	0.0999 (0.0997)	0.0606 (0.0609)	1.224 (0.940)	65.62 (70.71)
[140/157]	0.1005 (0.0997)	0.0613 (0.0609)	1.048 (0.942)	62.50 (70.50)
[150/157]	0.0999 (0.0997)	0.0598 (0.0609)	0.976 (0.942)	75.00 (70.49)
[156/157]	0.0848 (0.0997)	0.0557 (0.0609)	1.015 (0.942)	62.50 (70.50)
 * Train Acc 70.500
 * Val Acc 71.000, Total time 0.60
 * Val loss 0.887, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0088 (0.0088)	0.932 (0.932)	68.75 (68.75)
[10/157]	0.1007 (0.0948)	0.0604 (0.0557)	0.995 (0.911)	75.00 (69.60)
[20/157]	0.1002 (0.0972)	0.0607 (0.0582)	1.286 (0.971)	53.12 (68.90)
[30/157]	0.0995 (0.0983)	0.0603 (0.0595)	1.043 (0.962)	68.75 (69.05)
[40/157]	0.1028 (0.0989)	0.0622 (0.0602)	1.264 (0.979)	68.75 (68.60)
[50/157]	0.1015 (0.0992)	0.0613 (0.0605)	0.927 (0.978)	75.00 (69.18)
[60/157]	0.1004 (0.0994)	0.0610 (0.0607)	0.983 (0.963)	62.50 (69.52)
[70/157]	0.0992 (0.0994)	0.0600 (0.0608)	1.161 (0.967)	59.38 (69.59)
[80/157]	0.1006 (0.0995)	0.0603 (0.0607)	0.624 (0.952)	90.62 (70.22)
[90/157]	0.1011 (0.0996)	0.0623 (0.0608)	1.291 (0.953)	56.25 (70.02)
[100/157]	0.1003 (0.0996)	0.0612 (0.0609)	0.935 (0.955)	75.00 (69.89)
[110/157]	0.1005 (0.0998)	0.0610 (0.0610)	0.803 (0.955)	78.12 (69.93)
[120/157]	0.1006 (0.0998)	0.0610 (0.0610)	1.222 (0.961)	53.12 (69.58)
[130/157]	0.1007 (0.0998)	0.0608 (0.0610)	1.022 (0.955)	68.75 (69.87)
[140/157]	0.0997 (0.0998)	0.0607 (0.0610)	0.752 (0.948)	81.25 (70.39)
[150/157]	0.1005 (0.0998)	0.0613 (0.0610)	0.786 (0.946)	78.12 (70.30)
[156/157]	0.0834 (0.0997)	0.0554 (0.0610)	1.164 (0.949)	50.00 (70.16)
 * Train Acc 70.160
 * Val Acc 70.500, Total time 0.61
 * Val loss 0.873, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0089 (0.0089)	0.896 (0.896)	75.00 (75.00)
[10/157]	0.1000 (0.0947)	0.0607 (0.0562)	1.076 (0.901)	62.50 (73.30)
[20/157]	0.1009 (0.0973)	0.0613 (0.0585)	0.922 (0.926)	68.75 (72.32)
[30/157]	0.0987 (0.0981)	0.0596 (0.0595)	0.972 (0.929)	68.75 (71.77)
[40/157]	0.0975 (0.0986)	0.0584 (0.0600)	1.027 (0.929)	65.62 (71.65)
[50/157]	0.1004 (0.0989)	0.0611 (0.0603)	0.761 (0.935)	75.00 (71.75)
[60/157]	0.1049 (0.0993)	0.0608 (0.0606)	1.162 (0.944)	50.00 (71.11)
[70/157]	0.1005 (0.0994)	0.0610 (0.0607)	0.709 (0.941)	81.25 (70.86)
[80/157]	0.0999 (0.0995)	0.0600 (0.0608)	1.015 (0.946)	68.75 (70.60)
[90/157]	0.1015 (0.0996)	0.0625 (0.0609)	0.968 (0.959)	65.62 (69.92)
[100/157]	0.1009 (0.0997)	0.0614 (0.0610)	0.934 (0.965)	71.88 (69.46)
[110/157]	0.1011 (0.0998)	0.0617 (0.0611)	0.763 (0.961)	78.12 (69.62)
[120/157]	0.1006 (0.0998)	0.0614 (0.0611)	0.976 (0.961)	71.88 (69.60)
[130/157]	0.1010 (0.0998)	0.0617 (0.0611)	0.987 (0.963)	62.50 (69.58)
[140/157]	0.1003 (0.0998)	0.0610 (0.0612)	0.973 (0.959)	78.12 (69.84)
[150/157]	0.1000 (0.0998)	0.0606 (0.0612)	1.106 (0.956)	75.00 (69.97)
[156/157]	0.0825 (0.0997)	0.0562 (0.0611)	1.132 (0.954)	87.50 (69.92)
 * Train Acc 69.920
 * Val Acc 71.400, Total time 0.60
 * Val loss 0.863, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0088 (0.0088)	0.824 (0.824)	75.00 (75.00)
[10/157]	0.1013 (0.0945)	0.0617 (0.0569)	0.906 (0.941)	78.12 (69.32)
[20/157]	0.1007 (0.0973)	0.0617 (0.0597)	0.727 (0.938)	84.38 (69.79)
[30/157]	0.0993 (0.0983)	0.0603 (0.0604)	0.997 (0.944)	68.75 (70.16)
[40/157]	0.0999 (0.0988)	0.0595 (0.0606)	1.000 (0.935)	68.75 (70.35)
[50/157]	0.1004 (0.0990)	0.0609 (0.0608)	0.951 (0.921)	75.00 (71.51)
[60/157]	0.0998 (0.0992)	0.0604 (0.0608)	0.842 (0.934)	68.75 (71.16)
[70/157]	0.1003 (0.0993)	0.0614 (0.0609)	0.840 (0.919)	81.25 (72.10)
[80/157]	0.1006 (0.0994)	0.0608 (0.0610)	1.249 (0.927)	59.38 (71.72)
[90/157]	0.1004 (0.0994)	0.0617 (0.0610)	1.141 (0.932)	65.62 (71.33)
[100/157]	0.1001 (0.0995)	0.0606 (0.0611)	1.266 (0.937)	56.25 (71.07)
[110/157]	0.1003 (0.0995)	0.0612 (0.0611)	0.860 (0.932)	75.00 (71.51)
[120/157]	0.1004 (0.0996)	0.0613 (0.0612)	0.793 (0.932)	75.00 (71.59)
[130/157]	0.1002 (0.0996)	0.0608 (0.0612)	0.680 (0.931)	81.25 (71.56)
[140/157]	0.1003 (0.0996)	0.0611 (0.0612)	0.903 (0.936)	75.00 (71.37)
[150/157]	0.0998 (0.0996)	0.0602 (0.0612)	0.834 (0.934)	84.38 (71.32)
[156/157]	0.0810 (0.0995)	0.0548 (0.0612)	2.350 (0.944)	37.50 (71.02)
 * Train Acc 71.020
 * Val Acc 71.400, Total time 0.59
 * Val loss 0.876, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0090 (0.0090)	1.240 (1.240)	62.50 (62.50)
[10/157]	0.1006 (0.0942)	0.0616 (0.0564)	0.844 (1.026)	71.88 (66.19)
[20/157]	0.1002 (0.0968)	0.0611 (0.0588)	1.012 (1.001)	68.75 (67.26)
[30/157]	0.0999 (0.0979)	0.0612 (0.0596)	0.971 (0.974)	71.88 (69.05)
[40/157]	0.0998 (0.0983)	0.0608 (0.0601)	0.914 (0.951)	78.12 (70.50)
[50/157]	0.1013 (0.0987)	0.0618 (0.0603)	1.272 (0.975)	53.12 (69.36)
[60/157]	0.0999 (0.0989)	0.0601 (0.0605)	0.895 (0.961)	75.00 (69.88)
[70/157]	0.0990 (0.0991)	0.0598 (0.0606)	0.740 (0.952)	68.75 (70.07)
[80/157]	0.1003 (0.0992)	0.0606 (0.0607)	0.818 (0.944)	78.12 (70.76)
[90/157]	0.1014 (0.0993)	0.0612 (0.0608)	0.949 (0.947)	75.00 (70.74)
[100/157]	0.0975 (0.0993)	0.0591 (0.0608)	0.699 (0.951)	78.12 (70.70)
[110/157]	0.1004 (0.0994)	0.0618 (0.0608)	0.896 (0.951)	65.62 (70.58)
[120/157]	0.1005 (0.0994)	0.0603 (0.0608)	0.776 (0.953)	81.25 (70.45)
[130/157]	0.1002 (0.0995)	0.0603 (0.0608)	0.825 (0.953)	68.75 (70.21)
[140/157]	0.1003 (0.0995)	0.0612 (0.0609)	0.700 (0.949)	84.38 (70.19)
[150/157]	0.1017 (0.0996)	0.0623 (0.0610)	0.985 (0.945)	65.62 (70.22)
[156/157]	0.0854 (0.0995)	0.0566 (0.0610)	1.582 (0.945)	75.00 (70.32)
 * Train Acc 70.320
 * Val Acc 72.300, Total time 0.60
 * Val loss 0.871, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0468 (0.0468)	0.0093 (0.0093)	1.162 (1.162)	75.00 (75.00)
[10/157]	0.1012 (0.0947)	0.0620 (0.0570)	0.876 (0.979)	65.62 (70.45)
[20/157]	0.0971 (0.0976)	0.0581 (0.0592)	1.091 (0.970)	56.25 (70.09)
[30/157]	0.0998 (0.0984)	0.0598 (0.0598)	0.866 (0.967)	75.00 (70.16)
[40/157]	0.1009 (0.0988)	0.0614 (0.0603)	0.518 (0.945)	96.88 (70.73)
[50/157]	0.1003 (0.0990)	0.0613 (0.0606)	0.665 (0.942)	78.12 (70.77)
[60/157]	0.1000 (0.0992)	0.0609 (0.0607)	0.788 (0.939)	75.00 (70.65)
[70/157]	0.1001 (0.0993)	0.0612 (0.0609)	0.799 (0.939)	71.88 (70.64)
[80/157]	0.1013 (0.0996)	0.0616 (0.0611)	0.565 (0.945)	84.38 (70.56)
[90/157]	0.1007 (0.0996)	0.0615 (0.0612)	0.738 (0.938)	81.25 (70.88)
[100/157]	0.1008 (0.0997)	0.0613 (0.0612)	1.064 (0.933)	68.75 (71.04)
[110/157]	0.0998 (0.0997)	0.0605 (0.0612)	0.958 (0.929)	71.88 (71.34)
[120/157]	0.1006 (0.0997)	0.0611 (0.0613)	1.193 (0.931)	59.38 (71.23)
[130/157]	0.1010 (0.0997)	0.0620 (0.0612)	1.209 (0.932)	53.12 (71.21)
[140/157]	0.1005 (0.0998)	0.0613 (0.0613)	0.994 (0.937)	71.88 (70.92)
[150/157]	0.0996 (0.0998)	0.0606 (0.0613)	0.890 (0.935)	65.62 (70.90)
[156/157]	0.0842 (0.0997)	0.0571 (0.0613)	0.669 (0.938)	75.00 (70.76)
 * Train Acc 70.760
 * Val Acc 71.700, Total time 0.59
 * Val loss 0.864, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0088 (0.0088)	0.910 (0.910)	68.75 (68.75)
[10/157]	0.1061 (0.1029)	0.0667 (0.0646)	0.739 (0.895)	78.12 (72.16)
[20/157]	0.1043 (0.1036)	0.0656 (0.0652)	0.617 (0.877)	84.38 (73.21)
[30/157]	0.1044 (0.1038)	0.0652 (0.0653)	1.134 (0.895)	59.38 (71.67)
[40/157]	0.1042 (0.1039)	0.0650 (0.0653)	0.690 (0.876)	78.12 (72.41)
[50/157]	0.1050 (0.1040)	0.0650 (0.0654)	1.080 (0.892)	62.50 (72.00)
[60/157]	0.1064 (0.1042)	0.0667 (0.0655)	0.947 (0.896)	75.00 (72.23)
[70/157]	0.1163 (0.1032)	0.0765 (0.0648)	1.268 (0.914)	65.62 (71.70)
[80/157]	0.1099 (0.1045)	0.0700 (0.0661)	0.664 (0.909)	87.50 (71.84)
[90/157]	0.0967 (0.1038)	0.0584 (0.0654)	0.502 (0.909)	87.50 (72.12)
[100/157]	0.0936 (0.1029)	0.0564 (0.0647)	1.204 (0.914)	71.88 (72.03)
[110/157]	0.1048 (0.1037)	0.0651 (0.0653)	0.721 (0.922)	75.00 (71.90)
[120/157]	0.1077 (0.1040)	0.0674 (0.0656)	1.598 (0.925)	43.75 (71.69)
[130/157]	0.1072 (0.1042)	0.0673 (0.0658)	0.759 (0.926)	81.25 (71.61)
[140/157]	0.0960 (0.1043)	0.0583 (0.0659)	1.061 (0.938)	65.62 (71.19)
[150/157]	0.0961 (0.1038)	0.0577 (0.0654)	0.691 (0.941)	84.38 (71.05)
[156/157]	0.0785 (0.1033)	0.0541 (0.0651)	0.793 (0.940)	87.50 (71.10)
 * Train Acc 71.100
 * Val Acc 70.700, Total time 0.58
 * Val loss 0.873, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0086 (0.0086)	0.998 (0.998)	59.38 (59.38)
[10/157]	0.1047 (0.0973)	0.0636 (0.0587)	0.798 (0.904)	68.75 (70.45)
[20/157]	0.1033 (0.1000)	0.0635 (0.0611)	0.776 (0.935)	75.00 (70.68)
[30/157]	0.1054 (0.1011)	0.0655 (0.0622)	0.523 (0.923)	87.50 (71.17)
[40/157]	0.1026 (0.1015)	0.0630 (0.0626)	1.103 (0.920)	59.38 (71.27)
[50/157]	0.1028 (0.1019)	0.0632 (0.0629)	1.004 (0.912)	65.62 (71.81)
[60/157]	0.1039 (0.1022)	0.0639 (0.0632)	0.822 (0.906)	71.88 (72.34)
[70/157]	0.1039 (0.1024)	0.0643 (0.0633)	0.906 (0.912)	81.25 (72.01)
[80/157]	0.1043 (0.1025)	0.0644 (0.0634)	1.177 (0.919)	59.38 (71.80)
[90/157]	0.1014 (0.1026)	0.0608 (0.0635)	0.927 (0.926)	65.62 (71.46)
[100/157]	0.1049 (0.1026)	0.0639 (0.0635)	1.412 (0.941)	46.88 (70.67)
[110/157]	0.0935 (0.1021)	0.0560 (0.0631)	0.817 (0.936)	75.00 (70.69)
[120/157]	0.0963 (0.1015)	0.0576 (0.0627)	0.959 (0.938)	78.12 (70.51)
[130/157]	0.0960 (0.1011)	0.0576 (0.0624)	0.935 (0.934)	71.88 (70.59)
[140/157]	0.1149 (0.1008)	0.0740 (0.0622)	0.989 (0.930)	78.12 (70.81)
[150/157]	0.1034 (0.1015)	0.0642 (0.0628)	0.725 (0.931)	81.25 (71.09)
[156/157]	0.0853 (0.1015)	0.0581 (0.0628)	0.708 (0.934)	87.50 (71.04)
 * Train Acc 71.040
 * Val Acc 70.100, Total time 0.62
 * Val loss 0.872, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0092 (0.0092)	1.009 (1.009)	62.50 (62.50)
[10/157]	0.1021 (0.0969)	0.0627 (0.0586)	1.048 (0.945)	68.75 (70.17)
[20/157]	0.0952 (0.0990)	0.0571 (0.0607)	0.937 (0.940)	68.75 (69.79)
[30/157]	0.0959 (0.0977)	0.0585 (0.0598)	1.221 (0.918)	62.50 (71.07)
[40/157]	0.0964 (0.0971)	0.0584 (0.0594)	1.041 (0.927)	59.38 (70.43)
[50/157]	0.1002 (0.0977)	0.0608 (0.0597)	0.828 (0.916)	75.00 (71.14)
[60/157]	0.1005 (0.0981)	0.0611 (0.0599)	0.880 (0.919)	68.75 (71.21)
[70/157]	0.1011 (0.0985)	0.0617 (0.0603)	0.775 (0.922)	75.00 (70.73)
[80/157]	0.1002 (0.0988)	0.0615 (0.0605)	0.758 (0.921)	75.00 (70.79)
[90/157]	0.1022 (0.0990)	0.0630 (0.0607)	0.969 (0.918)	75.00 (70.88)
[100/157]	0.1004 (0.0992)	0.0609 (0.0608)	0.779 (0.915)	81.25 (70.82)
[110/157]	0.1004 (0.0993)	0.0610 (0.0608)	1.020 (0.917)	75.00 (71.11)
[120/157]	0.1005 (0.0994)	0.0618 (0.0609)	0.957 (0.916)	71.88 (70.95)
[130/157]	0.1002 (0.0995)	0.0612 (0.0610)	0.842 (0.918)	71.88 (71.11)
[140/157]	0.1020 (0.0996)	0.0634 (0.0611)	0.942 (0.922)	84.38 (71.08)
[150/157]	0.1008 (0.0997)	0.0614 (0.0612)	1.163 (0.928)	59.38 (70.90)
[156/157]	0.0836 (0.0996)	0.0561 (0.0612)	2.675 (0.931)	0.00 (70.76)
 * Train Acc 70.760
 * Val Acc 70.200, Total time 0.60
 * Val loss 0.882, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0099 (0.0099)	0.832 (0.832)	81.25 (81.25)
[10/157]	0.1001 (0.0951)	0.0601 (0.0567)	1.101 (0.924)	56.25 (71.88)
[20/157]	0.1006 (0.0975)	0.0614 (0.0589)	0.979 (0.977)	71.88 (70.24)
[30/157]	0.1026 (0.0987)	0.0621 (0.0601)	0.945 (0.969)	75.00 (70.77)
[40/157]	0.1005 (0.0991)	0.0614 (0.0605)	0.948 (0.966)	71.88 (70.43)
[50/157]	0.1018 (0.0994)	0.0624 (0.0608)	0.905 (0.955)	71.88 (70.65)
[60/157]	0.1003 (0.0997)	0.0613 (0.0610)	1.134 (0.940)	71.88 (71.31)
[70/157]	0.0998 (0.0998)	0.0603 (0.0611)	0.786 (0.944)	71.88 (70.86)
[80/157]	0.0950 (0.0994)	0.0566 (0.0608)	0.989 (0.936)	65.62 (71.18)
[90/157]	0.0980 (0.1003)	0.0591 (0.0617)	0.806 (0.938)	75.00 (71.19)
[100/157]	0.0971 (0.0999)	0.0583 (0.0613)	0.802 (0.943)	71.88 (70.76)
[110/157]	0.0965 (0.0995)	0.0590 (0.0611)	0.738 (0.946)	71.88 (70.61)
[120/157]	0.0967 (0.0992)	0.0594 (0.0609)	1.174 (0.943)	62.50 (70.66)
[130/157]	0.0952 (0.0989)	0.0579 (0.0607)	1.166 (0.949)	53.12 (70.30)
[140/157]	0.1122 (0.1001)	0.0708 (0.0618)	0.864 (0.947)	71.88 (70.23)
[150/157]	0.0961 (0.0999)	0.0586 (0.0617)	1.073 (0.947)	65.62 (70.26)
[156/157]	0.0809 (0.0996)	0.0543 (0.0616)	1.078 (0.950)	50.00 (70.16)
 * Train Acc 70.160
 * Val Acc 71.000, Total time 0.59
 * Val loss 0.861, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0090 (0.0090)	0.835 (0.835)	81.25 (81.25)
[10/157]	0.0946 (0.0910)	0.0573 (0.0537)	0.805 (0.976)	65.62 (68.18)
[20/157]	0.1028 (0.0945)	0.0628 (0.0569)	1.007 (1.003)	78.12 (68.45)
[30/157]	0.0998 (0.0964)	0.0595 (0.0582)	0.833 (0.980)	75.00 (69.86)
[40/157]	0.0996 (0.0973)	0.0604 (0.0587)	0.757 (0.970)	71.88 (69.59)
[50/157]	0.1003 (0.0978)	0.0612 (0.0592)	0.796 (0.955)	75.00 (69.98)
[60/157]	0.1009 (0.0982)	0.0606 (0.0595)	0.980 (0.945)	65.62 (70.24)
[70/157]	0.0997 (0.0984)	0.0610 (0.0597)	0.687 (0.935)	81.25 (70.42)
[80/157]	0.1024 (0.0986)	0.0625 (0.0599)	0.753 (0.934)	84.38 (70.87)
[90/157]	0.0990 (0.0988)	0.0596 (0.0599)	1.142 (0.930)	62.50 (71.19)
[100/157]	0.1008 (0.0989)	0.0611 (0.0600)	0.820 (0.944)	71.88 (70.64)
[110/157]	0.0990 (0.0990)	0.0595 (0.0601)	0.821 (0.944)	81.25 (70.72)
[120/157]	0.0998 (0.0991)	0.0608 (0.0602)	1.122 (0.946)	62.50 (70.74)
[130/157]	0.1010 (0.0991)	0.0604 (0.0603)	1.108 (0.949)	59.38 (70.61)
[140/157]	0.1003 (0.0992)	0.0605 (0.0603)	0.988 (0.948)	71.88 (70.59)
[150/157]	0.0994 (0.0992)	0.0608 (0.0604)	0.883 (0.947)	81.25 (70.76)
[156/157]	0.0830 (0.0991)	0.0562 (0.0604)	0.642 (0.941)	62.50 (70.90)
 * Train Acc 70.900
 * Val Acc 70.700, Total time 0.59
 * Val loss 0.873, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0082 (0.0082)	0.603 (0.603)	84.38 (84.38)
[10/157]	0.0974 (0.0904)	0.0595 (0.0528)	1.037 (0.931)	68.75 (71.02)
[20/157]	0.0995 (0.0942)	0.0608 (0.0565)	0.870 (0.944)	68.75 (70.54)
[30/157]	0.1027 (0.0963)	0.0633 (0.0586)	1.117 (0.946)	68.75 (70.46)
[40/157]	0.1020 (0.0977)	0.0624 (0.0596)	0.885 (0.936)	87.50 (70.81)
[50/157]	0.1000 (0.0984)	0.0609 (0.0601)	0.755 (0.947)	78.12 (70.89)
[60/157]	0.1004 (0.0987)	0.0622 (0.0605)	0.712 (0.935)	75.00 (71.11)
[70/157]	0.1011 (0.0991)	0.0625 (0.0608)	0.967 (0.930)	75.00 (71.04)
[80/157]	0.1005 (0.0992)	0.0617 (0.0610)	0.838 (0.919)	75.00 (71.60)
[90/157]	0.1020 (0.0994)	0.0627 (0.0611)	0.919 (0.921)	81.25 (71.70)
[100/157]	0.1004 (0.0995)	0.0610 (0.0612)	1.239 (0.926)	59.38 (71.47)
[110/157]	0.0999 (0.0996)	0.0604 (0.0613)	1.305 (0.929)	53.12 (71.31)
[120/157]	0.1019 (0.0997)	0.0622 (0.0614)	0.787 (0.930)	71.88 (70.95)
[130/157]	0.0998 (0.0998)	0.0609 (0.0614)	0.733 (0.923)	75.00 (71.30)
[140/157]	0.1002 (0.0999)	0.0612 (0.0615)	1.057 (0.931)	62.50 (70.99)
[150/157]	0.1007 (0.1000)	0.0619 (0.0616)	0.981 (0.928)	68.75 (71.01)
[156/157]	0.0865 (0.0999)	0.0588 (0.0616)	0.932 (0.927)	75.00 (70.96)
 * Train Acc 70.960
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.857, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0089 (0.0089)	0.940 (0.940)	75.00 (75.00)
[10/157]	0.1017 (0.0952)	0.0632 (0.0566)	0.466 (0.964)	90.62 (69.32)
[20/157]	0.1011 (0.0982)	0.0612 (0.0593)	1.100 (0.942)	68.75 (69.35)
[30/157]	0.1001 (0.0993)	0.0605 (0.0605)	0.724 (0.937)	84.38 (69.46)
[40/157]	0.1012 (0.1000)	0.0617 (0.0611)	0.884 (0.935)	71.88 (69.51)
[50/157]	0.1014 (0.1001)	0.0627 (0.0614)	0.879 (0.938)	75.00 (69.00)
[60/157]	0.1007 (0.1003)	0.0617 (0.0616)	0.891 (0.924)	59.38 (69.77)
[70/157]	0.1015 (0.1003)	0.0623 (0.0616)	0.907 (0.919)	71.88 (70.11)
[80/157]	0.1007 (0.1005)	0.0619 (0.0618)	0.977 (0.924)	75.00 (70.29)
[90/157]	0.1008 (0.1006)	0.0616 (0.0620)	1.008 (0.932)	71.88 (69.99)
[100/157]	0.1001 (0.1006)	0.0609 (0.0619)	1.424 (0.940)	53.12 (69.93)
[110/157]	0.1010 (0.1006)	0.0616 (0.0620)	0.702 (0.928)	78.12 (70.30)
[120/157]	0.1008 (0.1006)	0.0609 (0.0620)	0.844 (0.928)	68.75 (70.38)
[130/157]	0.1015 (0.1006)	0.0623 (0.0620)	1.011 (0.925)	71.88 (70.44)
[140/157]	0.1025 (0.1006)	0.0625 (0.0620)	1.085 (0.927)	71.88 (70.46)
[150/157]	0.1003 (0.1006)	0.0612 (0.0620)	0.756 (0.929)	81.25 (70.59)
[156/157]	0.0862 (0.1005)	0.0582 (0.0620)	1.072 (0.932)	50.00 (70.66)
 * Train Acc 70.660
 * Val Acc 72.400, Total time 0.59
 * Val loss 0.854, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0091 (0.0091)	0.761 (0.761)	81.25 (81.25)
[10/157]	0.1007 (0.0950)	0.0604 (0.0565)	1.114 (0.946)	59.38 (70.45)
[20/157]	0.1017 (0.0978)	0.0621 (0.0593)	1.113 (0.934)	56.25 (70.68)
[30/157]	0.0998 (0.0988)	0.0607 (0.0603)	0.920 (0.943)	68.75 (70.16)
[40/157]	0.1004 (0.0991)	0.0609 (0.0606)	1.217 (0.922)	53.12 (70.73)
[50/157]	0.1012 (0.0994)	0.0623 (0.0610)	0.752 (0.916)	90.62 (71.20)
[60/157]	0.1013 (0.0997)	0.0613 (0.0613)	1.139 (0.926)	65.62 (70.85)
[70/157]	0.1025 (0.0999)	0.0632 (0.0614)	1.077 (0.927)	62.50 (71.26)
[80/157]	0.1012 (0.1000)	0.0617 (0.0616)	1.050 (0.935)	71.88 (71.18)
[90/157]	0.0995 (0.1001)	0.0606 (0.0616)	1.192 (0.936)	59.38 (70.91)
[100/157]	0.0988 (0.1002)	0.0600 (0.0616)	1.358 (0.936)	56.25 (70.79)
[110/157]	0.1001 (0.1002)	0.0608 (0.0617)	0.686 (0.927)	84.38 (71.14)
[120/157]	0.0999 (0.1002)	0.0619 (0.0617)	0.846 (0.930)	75.00 (71.07)
[130/157]	0.0995 (0.1003)	0.0604 (0.0617)	0.861 (0.928)	71.88 (71.16)
[140/157]	0.1002 (0.1003)	0.0616 (0.0617)	0.986 (0.930)	71.88 (71.14)
[150/157]	0.1036 (0.1004)	0.0639 (0.0618)	1.213 (0.931)	71.88 (71.30)
[156/157]	0.0832 (0.1002)	0.0563 (0.0618)	1.827 (0.931)	37.50 (71.16)
 * Train Acc 71.160
 * Val Acc 70.700, Total time 0.60
 * Val loss 0.869, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0090 (0.0090)	0.860 (0.860)	75.00 (75.00)
[10/157]	0.1009 (0.0950)	0.0619 (0.0570)	0.628 (0.860)	87.50 (71.88)
[20/157]	0.0992 (0.0974)	0.0603 (0.0594)	1.058 (0.876)	68.75 (71.13)
[30/157]	0.1017 (0.0985)	0.0628 (0.0604)	1.023 (0.900)	65.62 (70.97)
[40/157]	0.1001 (0.0992)	0.0611 (0.0609)	1.170 (0.909)	56.25 (70.12)
[50/157]	0.1025 (0.0996)	0.0629 (0.0613)	1.270 (0.917)	62.50 (69.98)
[60/157]	0.1012 (0.0998)	0.0617 (0.0614)	0.951 (0.921)	71.88 (70.54)
[70/157]	0.1021 (0.0999)	0.0626 (0.0616)	0.781 (0.913)	78.12 (71.04)
[80/157]	0.1021 (0.1001)	0.0615 (0.0616)	0.931 (0.921)	68.75 (70.64)
[90/157]	0.0997 (0.1003)	0.0608 (0.0617)	0.856 (0.912)	78.12 (70.95)
[100/157]	0.1037 (0.1004)	0.0636 (0.0619)	1.360 (0.913)	62.50 (70.85)
[110/157]	0.1001 (0.1004)	0.0608 (0.0619)	1.257 (0.916)	65.62 (70.97)
[120/157]	0.1009 (0.1004)	0.0618 (0.0619)	0.887 (0.915)	68.75 (71.10)
[130/157]	0.1016 (0.1005)	0.0618 (0.0619)	0.903 (0.913)	68.75 (71.02)
[140/157]	0.1003 (0.1005)	0.0614 (0.0619)	0.983 (0.917)	56.25 (70.72)
[150/157]	0.1028 (0.1006)	0.0617 (0.0619)	0.912 (0.918)	65.62 (70.65)
[156/157]	0.0859 (0.1004)	0.0581 (0.0619)	1.029 (0.918)	62.50 (70.74)
 * Train Acc 70.740
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.859, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0100 (0.0100)	0.873 (0.873)	68.75 (68.75)
[10/157]	0.1006 (0.0954)	0.0615 (0.0576)	0.898 (0.978)	68.75 (67.61)
[20/157]	0.1012 (0.0976)	0.0619 (0.0596)	0.687 (0.941)	78.12 (68.90)
[30/157]	0.1003 (0.0984)	0.0609 (0.0604)	0.685 (0.920)	87.50 (69.86)
[40/157]	0.1007 (0.0992)	0.0618 (0.0610)	0.923 (0.902)	68.75 (70.43)
[50/157]	0.1010 (0.0995)	0.0624 (0.0613)	1.307 (0.908)	56.25 (70.40)
[60/157]	0.1010 (0.0998)	0.0618 (0.0615)	0.820 (0.923)	75.00 (70.49)
[70/157]	0.1038 (0.1001)	0.0640 (0.0617)	0.893 (0.910)	68.75 (70.99)
[80/157]	0.1001 (0.1001)	0.0610 (0.0618)	1.177 (0.918)	68.75 (70.83)
[90/157]	0.1020 (0.1002)	0.0626 (0.0619)	0.985 (0.918)	75.00 (71.09)
[100/157]	0.1004 (0.1003)	0.0611 (0.0619)	0.809 (0.914)	81.25 (71.29)
[110/157]	0.1018 (0.1003)	0.0626 (0.0620)	1.351 (0.919)	59.38 (71.03)
[120/157]	0.1007 (0.1004)	0.0615 (0.0620)	1.069 (0.927)	71.88 (70.64)
[130/157]	0.1017 (0.1004)	0.0628 (0.0621)	1.040 (0.930)	62.50 (70.56)
[140/157]	0.1009 (0.1005)	0.0619 (0.0621)	1.289 (0.932)	53.12 (70.46)
[150/157]	0.1010 (0.1005)	0.0623 (0.0621)	0.767 (0.928)	75.00 (70.70)
[156/157]	0.0838 (0.1004)	0.0581 (0.0621)	1.099 (0.928)	62.50 (70.66)
 * Train Acc 70.660
 * Val Acc 71.000, Total time 0.60
 * Val loss 0.859, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0095 (0.0095)	0.786 (0.786)	87.50 (87.50)
[10/157]	0.1025 (0.0951)	0.0630 (0.0576)	0.915 (0.937)	71.88 (73.30)
[20/157]	0.1001 (0.0977)	0.0610 (0.0596)	1.237 (0.960)	65.62 (71.28)
[30/157]	0.1031 (0.0987)	0.0627 (0.0605)	0.704 (0.937)	81.25 (72.38)
[40/157]	0.1006 (0.0993)	0.0618 (0.0611)	0.879 (0.932)	71.88 (72.71)
[50/157]	0.1005 (0.0995)	0.0617 (0.0613)	0.601 (0.933)	87.50 (72.79)
[60/157]	0.1001 (0.0998)	0.0616 (0.0616)	0.775 (0.930)	71.88 (72.23)
[70/157]	0.1021 (0.1000)	0.0626 (0.0617)	1.087 (0.926)	62.50 (72.45)
[80/157]	0.1010 (0.1002)	0.0613 (0.0619)	0.853 (0.925)	68.75 (72.38)
[90/157]	0.1016 (0.1003)	0.0617 (0.0619)	1.026 (0.927)	62.50 (72.05)
[100/157]	0.1001 (0.1003)	0.0610 (0.0619)	1.069 (0.927)	56.25 (71.72)
[110/157]	0.1009 (0.1003)	0.0615 (0.0619)	0.901 (0.920)	78.12 (71.99)
[120/157]	0.1018 (0.1004)	0.0617 (0.0619)	0.938 (0.923)	78.12 (71.85)
[130/157]	0.1010 (0.1004)	0.0620 (0.0619)	0.979 (0.922)	71.88 (71.92)
[140/157]	0.1020 (0.1005)	0.0619 (0.0620)	1.111 (0.927)	68.75 (71.63)
[150/157]	0.1029 (0.1005)	0.0631 (0.0620)	1.004 (0.927)	71.88 (71.69)
[156/157]	0.0837 (0.1004)	0.0564 (0.0620)	1.819 (0.928)	75.00 (71.74)
 * Train Acc 71.740
 * Val Acc 70.800, Total time 0.60
 * Val loss 0.875, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0093 (0.0093)	0.745 (0.745)	87.50 (87.50)
[10/157]	0.1015 (0.0954)	0.0613 (0.0563)	0.886 (0.954)	71.88 (71.59)
[20/157]	0.1000 (0.0984)	0.0609 (0.0593)	0.825 (0.971)	71.88 (70.68)
[30/157]	0.1011 (0.0991)	0.0618 (0.0600)	0.856 (0.961)	68.75 (71.47)
[40/157]	0.0978 (0.0996)	0.0599 (0.0605)	0.750 (0.949)	71.88 (70.43)
[50/157]	0.1011 (0.0998)	0.0619 (0.0607)	0.831 (0.937)	81.25 (70.77)
[60/157]	0.0997 (0.1001)	0.0607 (0.0611)	1.193 (0.935)	50.00 (70.75)
[70/157]	0.0995 (0.1001)	0.0603 (0.0611)	0.827 (0.928)	68.75 (70.82)
[80/157]	0.1033 (0.1002)	0.0632 (0.0613)	0.651 (0.918)	90.62 (71.26)
[90/157]	0.1006 (0.1003)	0.0612 (0.0614)	0.800 (0.917)	75.00 (71.39)
[100/157]	0.1015 (0.1003)	0.0620 (0.0615)	0.947 (0.916)	71.88 (71.38)
[110/157]	0.1026 (0.1004)	0.0623 (0.0616)	0.824 (0.915)	81.25 (71.26)
[120/157]	0.0995 (0.1004)	0.0614 (0.0616)	0.648 (0.913)	78.12 (71.41)
[130/157]	0.1028 (0.1005)	0.0627 (0.0618)	0.874 (0.923)	71.88 (70.94)
[140/157]	0.1024 (0.1005)	0.0630 (0.0618)	0.873 (0.928)	65.62 (70.74)
[150/157]	0.1015 (0.1006)	0.0617 (0.0618)	0.722 (0.928)	68.75 (70.57)
[156/157]	0.0851 (0.1004)	0.0568 (0.0618)	1.354 (0.930)	62.50 (70.64)
 * Train Acc 70.640
 * Val Acc 73.400, Total time 0.61
 * Val loss 0.842, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0095 (0.0095)	1.130 (1.130)	65.62 (65.62)
[10/157]	0.1008 (0.0956)	0.0611 (0.0566)	1.112 (0.963)	59.38 (68.18)
[20/157]	0.1008 (0.0979)	0.0621 (0.0593)	1.063 (0.960)	62.50 (69.35)
[30/157]	0.1009 (0.0990)	0.0620 (0.0604)	1.095 (0.938)	68.75 (70.87)
[40/157]	0.1006 (0.0993)	0.0622 (0.0609)	0.744 (0.957)	84.38 (70.20)
[50/157]	0.1006 (0.0997)	0.0614 (0.0613)	1.046 (0.942)	65.62 (70.59)
[60/157]	0.1006 (0.0998)	0.0619 (0.0614)	1.198 (0.941)	53.12 (70.75)
[70/157]	0.1019 (0.1000)	0.0625 (0.0616)	1.134 (0.942)	62.50 (70.77)
[80/157]	0.1003 (0.1001)	0.0613 (0.0617)	1.021 (0.936)	68.75 (70.91)
[90/157]	0.1016 (0.1001)	0.0629 (0.0617)	0.871 (0.927)	75.00 (71.15)
[100/157]	0.1000 (0.1003)	0.0616 (0.0618)	0.628 (0.914)	84.38 (71.53)
[110/157]	0.1022 (0.1003)	0.0635 (0.0619)	0.729 (0.913)	84.38 (71.68)
[120/157]	0.1012 (0.1004)	0.0626 (0.0620)	1.042 (0.912)	62.50 (71.62)
[130/157]	0.1022 (0.1005)	0.0620 (0.0621)	0.948 (0.915)	71.88 (71.59)
[140/157]	0.1027 (0.1006)	0.0622 (0.0621)	0.650 (0.912)	87.50 (71.74)
[150/157]	0.1022 (0.1006)	0.0635 (0.0621)	0.957 (0.916)	75.00 (71.71)
[156/157]	0.0843 (0.1005)	0.0574 (0.0621)	1.619 (0.918)	25.00 (71.60)
 * Train Acc 71.600
 * Val Acc 71.200, Total time 0.60
 * Val loss 0.857, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0089 (0.0089)	0.922 (0.922)	65.62 (65.62)
[10/157]	0.1000 (0.0954)	0.0603 (0.0572)	0.829 (0.808)	75.00 (73.30)
[20/157]	0.0990 (0.0977)	0.0596 (0.0594)	0.850 (0.837)	71.88 (72.77)
[30/157]	0.1029 (0.0989)	0.0626 (0.0604)	1.066 (0.852)	78.12 (72.18)
[40/157]	0.1045 (0.0996)	0.0628 (0.0609)	0.619 (0.847)	81.25 (72.87)
[50/157]	0.1012 (0.0998)	0.0618 (0.0610)	0.941 (0.854)	68.75 (72.86)
[60/157]	0.1029 (0.1001)	0.0626 (0.0614)	0.896 (0.871)	75.00 (72.75)
[70/157]	0.1011 (0.1002)	0.0618 (0.0614)	0.963 (0.875)	65.62 (72.54)
[80/157]	0.0993 (0.1003)	0.0598 (0.0616)	0.884 (0.881)	68.75 (72.26)
[90/157]	0.1011 (0.1004)	0.0620 (0.0616)	0.818 (0.886)	68.75 (72.25)
[100/157]	0.1008 (0.1005)	0.0610 (0.0617)	1.132 (0.904)	65.62 (71.35)
[110/157]	0.1037 (0.1005)	0.0642 (0.0618)	0.981 (0.914)	71.88 (71.17)
[120/157]	0.1002 (0.1006)	0.0619 (0.0618)	0.813 (0.916)	78.12 (71.20)
[130/157]	0.0999 (0.1007)	0.0594 (0.0619)	1.142 (0.912)	59.38 (71.35)
[140/157]	0.1011 (0.1006)	0.0619 (0.0619)	0.883 (0.911)	68.75 (71.32)
[150/157]	0.1007 (0.1007)	0.0617 (0.0619)	1.021 (0.912)	71.88 (71.38)
[156/157]	0.0845 (0.1006)	0.0573 (0.0619)	1.729 (0.914)	37.50 (71.22)
 * Train Acc 71.220
 * Val Acc 72.700, Total time 0.60
 * Val loss 0.861, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0085 (0.0085)	0.962 (0.962)	65.62 (65.62)
[10/157]	0.1002 (0.0954)	0.0611 (0.0569)	0.793 (0.887)	81.25 (72.16)
[20/157]	0.0999 (0.0976)	0.0611 (0.0593)	0.816 (0.897)	78.12 (72.32)
[30/157]	0.1011 (0.0985)	0.0626 (0.0603)	1.091 (0.910)	56.25 (71.47)
[40/157]	0.1000 (0.0992)	0.0609 (0.0609)	1.035 (0.905)	56.25 (71.11)
[50/157]	0.1022 (0.0995)	0.0622 (0.0610)	0.873 (0.881)	78.12 (72.92)
[60/157]	0.1021 (0.0998)	0.0626 (0.0613)	0.989 (0.889)	68.75 (72.69)
[70/157]	0.1030 (0.1000)	0.0628 (0.0615)	0.995 (0.910)	65.62 (71.70)
[80/157]	0.1010 (0.1001)	0.0617 (0.0616)	1.016 (0.916)	65.62 (71.37)
[90/157]	0.1016 (0.1003)	0.0619 (0.0618)	0.734 (0.909)	78.12 (71.70)
[100/157]	0.1007 (0.1004)	0.0615 (0.0618)	0.798 (0.921)	78.12 (71.07)
[110/157]	0.1000 (0.1004)	0.0612 (0.0619)	1.011 (0.919)	53.12 (71.20)
[120/157]	0.1007 (0.1004)	0.0616 (0.0619)	1.156 (0.922)	65.62 (71.23)
[130/157]	0.1002 (0.1005)	0.0612 (0.0620)	0.686 (0.923)	81.25 (71.37)
[140/157]	0.1012 (0.1005)	0.0625 (0.0620)	1.055 (0.922)	65.62 (71.30)
[150/157]	0.1004 (0.1006)	0.0614 (0.0620)	1.026 (0.931)	71.88 (70.94)
[156/157]	0.0848 (0.1004)	0.0568 (0.0620)	1.071 (0.929)	50.00 (70.92)
 * Train Acc 70.920
 * Val Acc 72.300, Total time 0.59
 * Val loss 0.844, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0468 (0.0468)	0.0098 (0.0098)	0.711 (0.711)	87.50 (87.50)
[10/157]	0.1003 (0.0954)	0.0605 (0.0571)	1.261 (0.981)	59.38 (68.18)
[20/157]	0.1025 (0.0984)	0.0626 (0.0597)	0.842 (0.975)	78.12 (68.45)
[30/157]	0.1027 (0.0994)	0.0620 (0.0605)	0.766 (0.941)	78.12 (70.26)
[40/157]	0.1006 (0.0997)	0.0608 (0.0608)	0.712 (0.935)	81.25 (70.96)
[50/157]	0.1014 (0.1001)	0.0619 (0.0612)	1.074 (0.932)	75.00 (70.77)
[60/157]	0.1017 (0.1002)	0.0621 (0.0613)	0.946 (0.935)	68.75 (70.54)
[70/157]	0.1000 (0.1004)	0.0608 (0.0614)	1.083 (0.942)	71.88 (70.38)
[80/157]	0.1019 (0.1004)	0.0621 (0.0615)	0.946 (0.931)	71.88 (70.87)
[90/157]	0.1008 (0.1005)	0.0613 (0.0616)	1.051 (0.930)	75.00 (70.91)
[100/157]	0.0996 (0.1006)	0.0601 (0.0617)	0.679 (0.923)	84.38 (71.26)
[110/157]	0.1000 (0.1006)	0.0610 (0.0617)	0.947 (0.922)	71.88 (71.34)
[120/157]	0.1025 (0.1007)	0.0606 (0.0618)	0.705 (0.921)	75.00 (71.18)
[130/157]	0.1037 (0.1007)	0.0631 (0.0619)	0.701 (0.921)	71.88 (71.25)
[140/157]	0.1013 (0.1007)	0.0622 (0.0619)	0.838 (0.920)	71.88 (71.56)
[150/157]	0.1013 (0.1008)	0.0623 (0.0619)	0.997 (0.923)	65.62 (71.38)
[156/157]	0.0857 (0.1007)	0.0582 (0.0619)	1.119 (0.923)	62.50 (71.38)
 * Train Acc 71.380
 * Val Acc 71.500, Total time 0.60
 * Val loss 0.852, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0094 (0.0094)	0.892 (0.892)	78.12 (78.12)
[10/157]	0.1004 (0.0956)	0.0612 (0.0572)	0.941 (0.949)	62.50 (69.60)
[20/157]	0.1024 (0.0985)	0.0623 (0.0599)	0.956 (0.942)	68.75 (70.09)
[30/157]	0.1020 (0.0992)	0.0625 (0.0605)	1.106 (0.913)	68.75 (71.37)
[40/157]	0.1003 (0.0998)	0.0611 (0.0610)	1.202 (0.893)	65.62 (72.94)
[50/157]	0.1012 (0.0999)	0.0616 (0.0612)	1.022 (0.921)	65.62 (71.32)
[60/157]	0.1010 (0.1002)	0.0618 (0.0614)	0.762 (0.927)	81.25 (71.31)
[70/157]	0.1016 (0.1004)	0.0621 (0.0616)	0.818 (0.919)	78.12 (71.70)
[80/157]	0.1016 (0.1005)	0.0619 (0.0616)	0.691 (0.926)	84.38 (71.22)
[90/157]	0.1002 (0.1005)	0.0608 (0.0616)	0.727 (0.916)	90.62 (71.77)
[100/157]	0.1030 (0.1006)	0.0628 (0.0617)	1.050 (0.919)	75.00 (71.78)
[110/157]	0.1018 (0.1007)	0.0637 (0.0618)	0.966 (0.921)	62.50 (71.59)
[120/157]	0.0984 (0.1007)	0.0591 (0.0618)	1.126 (0.920)	59.38 (71.69)
[130/157]	0.1018 (0.1008)	0.0624 (0.0619)	0.923 (0.917)	75.00 (71.85)
[140/157]	0.1011 (0.1008)	0.0628 (0.0619)	0.792 (0.916)	71.88 (71.74)
[150/157]	0.1015 (0.1008)	0.0626 (0.0620)	0.602 (0.912)	81.25 (71.88)
[156/157]	0.0847 (0.1007)	0.0577 (0.0620)	0.915 (0.920)	75.00 (71.70)
 * Train Acc 71.700
 * Val Acc 72.600, Total time 0.60
 * Val loss 0.851, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0087 (0.0087)	0.973 (0.973)	62.50 (62.50)
[10/157]	0.1006 (0.0946)	0.0615 (0.0571)	0.950 (0.871)	75.00 (71.88)
[20/157]	0.1012 (0.0974)	0.0628 (0.0597)	0.853 (0.878)	75.00 (71.58)
[30/157]	0.1002 (0.0987)	0.0612 (0.0607)	0.719 (0.877)	78.12 (72.28)
[40/157]	0.1008 (0.0991)	0.0623 (0.0611)	0.996 (0.883)	65.62 (72.18)
[50/157]	0.1016 (0.0995)	0.0620 (0.0615)	0.881 (0.888)	75.00 (71.63)
[60/157]	0.1004 (0.0997)	0.0619 (0.0616)	0.885 (0.899)	68.75 (71.36)
[70/157]	0.1017 (0.0999)	0.0626 (0.0618)	1.050 (0.904)	62.50 (70.99)
[80/157]	0.1009 (0.1000)	0.0620 (0.0619)	1.030 (0.916)	75.00 (70.52)
[90/157]	0.1021 (0.1001)	0.0624 (0.0620)	0.823 (0.913)	65.62 (70.67)
[100/157]	0.1005 (0.1002)	0.0612 (0.0620)	1.163 (0.909)	68.75 (70.92)
[110/157]	0.1012 (0.1002)	0.0621 (0.0620)	0.993 (0.914)	68.75 (70.86)
[120/157]	0.1005 (0.1003)	0.0611 (0.0621)	0.762 (0.918)	75.00 (70.97)
[130/157]	0.1027 (0.1003)	0.0637 (0.0621)	0.891 (0.920)	75.00 (71.06)
[140/157]	0.1006 (0.1004)	0.0615 (0.0621)	0.804 (0.921)	68.75 (70.97)
[150/157]	0.0999 (0.1005)	0.0608 (0.0622)	0.948 (0.920)	75.00 (71.15)
[156/157]	0.0833 (0.1004)	0.0555 (0.0621)	0.619 (0.927)	75.00 (70.86)
 * Train Acc 70.860
 * Val Acc 72.100, Total time 0.59
 * Val loss 0.854, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0422 (0.0422)	0.0085 (0.0085)	1.081 (1.081)	65.62 (65.62)
[10/157]	0.1010 (0.0950)	0.0621 (0.0569)	1.061 (0.935)	56.25 (69.60)
[20/157]	0.1020 (0.0980)	0.0618 (0.0598)	0.602 (0.922)	84.38 (70.54)
[30/157]	0.1011 (0.0989)	0.0612 (0.0604)	1.011 (0.934)	75.00 (70.26)
[40/157]	0.1004 (0.0995)	0.0611 (0.0608)	1.257 (0.947)	59.38 (70.43)
[50/157]	0.1024 (0.0999)	0.0622 (0.0612)	0.836 (0.945)	71.88 (70.22)
[60/157]	0.1014 (0.1001)	0.0622 (0.0613)	0.984 (0.933)	71.88 (70.75)
[70/157]	0.1007 (0.1003)	0.0615 (0.0614)	0.709 (0.912)	75.00 (71.52)
[80/157]	0.1009 (0.1003)	0.0621 (0.0614)	0.800 (0.914)	81.25 (71.18)
[90/157]	0.0983 (0.1004)	0.0593 (0.0615)	1.273 (0.917)	56.25 (71.29)
[100/157]	0.1014 (0.1004)	0.0620 (0.0616)	1.139 (0.912)	53.12 (71.10)
[110/157]	0.1028 (0.1005)	0.0621 (0.0617)	0.978 (0.915)	71.88 (71.26)
[120/157]	0.0949 (0.1005)	0.0573 (0.0618)	0.937 (0.913)	71.88 (71.28)
[130/157]	0.1207 (0.1008)	0.0800 (0.0621)	0.715 (0.908)	81.25 (71.66)
[140/157]	0.0960 (0.1009)	0.0580 (0.0623)	1.096 (0.910)	65.62 (71.56)
[150/157]	0.0951 (0.1006)	0.0574 (0.0620)	0.886 (0.912)	65.62 (71.25)
[156/157]	0.0970 (0.1011)	0.0681 (0.0626)	0.406 (0.909)	87.50 (71.42)
 * Train Acc 71.420
 * Val Acc 71.600, Total time 0.58
 * Val loss 0.849, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0087 (0.0087)	1.027 (1.027)	62.50 (62.50)
[10/157]	0.1200 (0.0946)	0.0793 (0.0567)	1.070 (0.862)	71.88 (74.15)
[20/157]	0.0989 (0.0999)	0.0595 (0.0617)	0.973 (0.936)	75.00 (71.73)
[30/157]	0.0987 (0.0987)	0.0598 (0.0609)	0.950 (0.945)	59.38 (70.46)
[40/157]	0.0972 (0.0981)	0.0592 (0.0604)	0.653 (0.924)	78.12 (71.19)
[50/157]	0.0950 (0.0979)	0.0568 (0.0601)	1.101 (0.933)	65.62 (70.59)
[60/157]	0.0967 (0.0976)	0.0574 (0.0598)	0.887 (0.923)	75.00 (70.80)
[70/157]	0.0964 (0.0988)	0.0580 (0.0609)	1.024 (0.924)	71.88 (70.99)
[80/157]	0.0962 (0.0986)	0.0580 (0.0606)	0.788 (0.920)	71.88 (70.68)
[90/157]	0.0932 (0.0998)	0.0561 (0.0617)	0.751 (0.918)	84.38 (70.64)
[100/157]	0.0970 (0.0994)	0.0588 (0.0614)	0.664 (0.922)	90.62 (70.48)
[110/157]	0.0968 (0.0991)	0.0579 (0.0611)	0.655 (0.923)	90.62 (70.50)
[120/157]	0.1004 (0.0991)	0.0613 (0.0610)	1.084 (0.930)	68.75 (70.20)
[130/157]	0.1002 (0.0993)	0.0615 (0.0612)	1.083 (0.924)	56.25 (70.25)
[140/157]	0.1007 (0.0995)	0.0617 (0.0613)	1.078 (0.924)	68.75 (70.52)
[150/157]	0.1027 (0.0997)	0.0628 (0.0615)	1.201 (0.924)	68.75 (70.80)
[156/157]	0.0854 (0.0996)	0.0575 (0.0615)	1.060 (0.920)	75.00 (70.98)
 * Train Acc 70.980
 * Val Acc 70.500, Total time 0.59
 * Val loss 0.859, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0090 (0.0090)	1.081 (1.081)	78.12 (78.12)
[10/157]	0.1005 (0.0947)	0.0619 (0.0573)	1.037 (0.935)	65.62 (71.59)
[20/157]	0.1025 (0.0982)	0.0620 (0.0603)	0.994 (0.887)	68.75 (73.96)
[30/157]	0.1010 (0.0994)	0.0617 (0.0611)	0.952 (0.849)	75.00 (76.01)
[40/157]	0.1019 (0.1000)	0.0613 (0.0615)	1.220 (0.875)	59.38 (74.54)
[50/157]	0.1034 (0.1003)	0.0634 (0.0617)	0.895 (0.879)	75.00 (73.71)
[60/157]	0.1006 (0.1004)	0.0615 (0.0618)	0.788 (0.874)	75.00 (74.03)
[70/157]	0.1022 (0.1006)	0.0632 (0.0620)	0.817 (0.881)	71.88 (73.64)
[80/157]	0.1030 (0.1008)	0.0639 (0.0621)	1.138 (0.894)	68.75 (73.19)
[90/157]	0.1017 (0.1009)	0.0623 (0.0621)	1.077 (0.894)	62.50 (72.84)
[100/157]	0.1015 (0.1010)	0.0624 (0.0622)	1.074 (0.898)	62.50 (72.49)
[110/157]	0.1009 (0.1011)	0.0617 (0.0622)	0.890 (0.900)	68.75 (72.33)
[120/157]	0.1023 (0.1011)	0.0619 (0.0623)	0.946 (0.906)	78.12 (72.16)
[130/157]	0.1006 (0.1011)	0.0624 (0.0623)	0.701 (0.906)	75.00 (72.14)
[140/157]	0.0994 (0.1011)	0.0613 (0.0624)	0.906 (0.903)	65.62 (72.23)
[150/157]	0.1021 (0.1011)	0.0629 (0.0624)	0.901 (0.905)	68.75 (72.08)
[156/157]	0.0855 (0.1010)	0.0581 (0.0624)	0.521 (0.907)	75.00 (72.04)
 * Train Acc 72.040
 * Val Acc 71.500, Total time 0.60
 * Val loss 0.848, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0095 (0.0095)	0.859 (0.859)	65.62 (65.62)
[10/157]	0.1001 (0.0955)	0.0612 (0.0575)	0.782 (0.932)	78.12 (71.31)
[20/157]	0.1024 (0.0984)	0.0631 (0.0602)	1.255 (1.000)	68.75 (69.64)
[30/157]	0.1033 (0.0994)	0.0635 (0.0612)	0.805 (0.962)	78.12 (69.66)
[40/157]	0.1017 (0.0998)	0.0619 (0.0614)	1.122 (0.939)	65.62 (70.43)
[50/157]	0.1005 (0.1002)	0.0609 (0.0616)	0.851 (0.917)	78.12 (71.69)
[60/157]	0.1012 (0.1005)	0.0616 (0.0618)	0.845 (0.910)	71.88 (71.88)
[70/157]	0.1010 (0.1007)	0.0607 (0.0618)	0.782 (0.924)	84.38 (71.21)
[80/157]	0.1023 (0.1008)	0.0621 (0.0619)	1.203 (0.921)	62.50 (71.30)
[90/157]	0.1029 (0.1008)	0.0630 (0.0619)	0.848 (0.915)	71.88 (71.67)
[100/157]	0.1015 (0.1009)	0.0624 (0.0619)	0.866 (0.912)	68.75 (71.94)
[110/157]	0.1007 (0.1010)	0.0616 (0.0620)	0.660 (0.915)	78.12 (71.85)
[120/157]	0.1001 (0.1010)	0.0615 (0.0621)	0.693 (0.910)	75.00 (71.85)
[130/157]	0.1023 (0.1011)	0.0619 (0.0622)	0.843 (0.916)	65.62 (71.61)
[140/157]	0.1008 (0.1012)	0.0618 (0.0622)	0.918 (0.919)	68.75 (71.34)
[150/157]	0.1034 (0.1012)	0.0631 (0.0623)	1.041 (0.919)	65.62 (71.44)
[156/157]	0.0854 (0.1011)	0.0577 (0.0623)	0.912 (0.921)	50.00 (71.44)
 * Train Acc 71.440
 * Val Acc 72.300, Total time 0.60
 * Val loss 0.850, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0089 (0.0089)	0.922 (0.922)	65.62 (65.62)
[10/157]	0.1019 (0.0957)	0.0623 (0.0579)	0.823 (0.933)	71.88 (69.32)
[20/157]	0.1039 (0.0984)	0.0638 (0.0603)	0.864 (0.928)	68.75 (70.68)
[30/157]	0.1028 (0.0995)	0.0635 (0.0611)	1.171 (0.945)	68.75 (69.86)
[40/157]	0.1010 (0.1000)	0.0619 (0.0616)	0.626 (0.923)	81.25 (70.88)
[50/157]	0.1016 (0.1003)	0.0620 (0.0618)	0.778 (0.927)	78.12 (71.26)
[60/157]	0.1028 (0.1005)	0.0633 (0.0620)	0.946 (0.921)	71.88 (71.16)
[70/157]	0.1016 (0.1006)	0.0623 (0.0621)	0.793 (0.922)	84.38 (71.26)
[80/157]	0.1018 (0.1007)	0.0621 (0.0622)	0.622 (0.918)	87.50 (71.68)
[90/157]	0.0996 (0.1008)	0.0613 (0.0623)	0.892 (0.916)	65.62 (71.26)
[100/157]	0.1004 (0.1009)	0.0607 (0.0624)	0.870 (0.917)	75.00 (71.23)
[110/157]	0.1018 (0.1009)	0.0633 (0.0624)	0.836 (0.912)	75.00 (71.42)
[120/157]	0.1034 (0.1010)	0.0617 (0.0624)	0.712 (0.916)	81.25 (71.49)
[130/157]	0.1003 (0.1010)	0.0611 (0.0625)	0.952 (0.914)	68.75 (71.56)
[140/157]	0.1010 (0.1011)	0.0615 (0.0625)	0.714 (0.917)	78.12 (71.45)
[150/157]	0.1014 (0.1011)	0.0623 (0.0625)	1.002 (0.912)	65.62 (71.50)
[156/157]	0.0861 (0.1010)	0.0581 (0.0625)	1.151 (0.909)	62.50 (71.58)
 * Train Acc 71.580
 * Val Acc 72.900, Total time 0.59
 * Val loss 0.850, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0085 (0.0085)	0.750 (0.750)	75.00 (75.00)
[10/157]	0.0999 (0.0951)	0.0616 (0.0572)	1.036 (0.955)	71.88 (71.59)
[20/157]	0.1003 (0.0982)	0.0616 (0.0600)	1.173 (0.918)	62.50 (72.77)
[30/157]	0.1011 (0.0994)	0.0616 (0.0612)	0.784 (0.920)	75.00 (72.08)
[40/157]	0.1040 (0.0998)	0.0642 (0.0616)	0.944 (0.914)	62.50 (71.42)
[50/157]	0.1014 (0.1001)	0.0622 (0.0618)	1.134 (0.947)	59.38 (70.10)
[60/157]	0.1010 (0.1003)	0.0622 (0.0619)	1.077 (0.927)	71.88 (71.06)
[70/157]	0.1003 (0.1006)	0.0617 (0.0621)	0.905 (0.932)	68.75 (70.60)
[80/157]	0.1034 (0.1007)	0.0634 (0.0622)	1.186 (0.925)	62.50 (71.18)
[90/157]	0.1019 (0.1007)	0.0629 (0.0623)	1.082 (0.916)	59.38 (71.29)
[100/157]	0.1006 (0.1008)	0.0615 (0.0623)	0.845 (0.918)	75.00 (71.23)
[110/157]	0.1016 (0.1009)	0.0623 (0.0623)	0.906 (0.918)	81.25 (71.23)
[120/157]	0.1028 (0.1010)	0.0636 (0.0624)	0.916 (0.918)	71.88 (71.26)
[130/157]	0.1037 (0.1010)	0.0639 (0.0624)	0.898 (0.916)	68.75 (71.06)
[140/157]	0.1017 (0.1010)	0.0631 (0.0624)	0.693 (0.914)	81.25 (71.23)
[150/157]	0.1008 (0.1010)	0.0615 (0.0625)	0.876 (0.919)	75.00 (71.03)
[156/157]	0.0906 (0.1010)	0.0595 (0.0625)	0.863 (0.919)	50.00 (70.98)
 * Train Acc 70.980
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.847, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0461 (0.0461)	0.0092 (0.0092)	0.969 (0.969)	71.88 (71.88)
[10/157]	0.1009 (0.0960)	0.0623 (0.0569)	0.747 (0.868)	78.12 (71.59)
[20/157]	0.1003 (0.0987)	0.0606 (0.0594)	1.013 (0.887)	65.62 (71.13)
[30/157]	0.0999 (0.0996)	0.0616 (0.0605)	1.197 (0.929)	65.62 (70.16)
[40/157]	0.1003 (0.0998)	0.0615 (0.0609)	0.856 (0.908)	78.12 (72.10)
[50/157]	0.1011 (0.1002)	0.0616 (0.0614)	1.208 (0.920)	59.38 (71.32)
[60/157]	0.1010 (0.1005)	0.0620 (0.0617)	0.867 (0.912)	71.88 (71.26)
[70/157]	0.1030 (0.1006)	0.0635 (0.0618)	0.982 (0.910)	65.62 (71.35)
[80/157]	0.1021 (0.1007)	0.0630 (0.0619)	1.005 (0.907)	68.75 (71.49)
[90/157]	0.1041 (0.1009)	0.0627 (0.0620)	1.231 (0.911)	62.50 (71.39)
[100/157]	0.1008 (0.1009)	0.0622 (0.0621)	1.113 (0.915)	68.75 (71.41)
[110/157]	0.1017 (0.1010)	0.0622 (0.0622)	1.090 (0.920)	59.38 (71.11)
[120/157]	0.1017 (0.1010)	0.0625 (0.0622)	1.039 (0.918)	59.38 (71.15)
[130/157]	0.1007 (0.1011)	0.0612 (0.0623)	0.978 (0.917)	62.50 (71.14)
[140/157]	0.1028 (0.1011)	0.0637 (0.0623)	1.013 (0.918)	68.75 (70.92)
[150/157]	0.1015 (0.1011)	0.0617 (0.0623)	0.635 (0.914)	75.00 (70.94)
[156/157]	0.0856 (0.1011)	0.0587 (0.0623)	0.751 (0.911)	75.00 (71.06)
 * Train Acc 71.060
 * Val Acc 73.200, Total time 0.60
 * Val loss 0.845, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 5/27; cond: 428467.46875, radio:3.542999183991924e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0041,  0.1073, -0.1581],
          [-0.1381, -0.0746,  0.0502],
          [ 0.0052,  0.1572, -0.0150]],

         [[ 0.0548, -0.0551, -0.0375],
          [-0.1811, -0.1278, -0.0790],
          [ 0.0137,  0.0788,  0.1180]],

         [[-0.1300, -0.0833,  0.0677],
          [ 0.1596, -0.0408,  0.1445],
          [-0.0280,  0.0216,  0.1758]]],


        [[[-0.1678, -0.1051, -0.0261],
          [-0.0667,  0.1735, -0.1106],
          [-0.0792, -0.1349, -0.1716]],

         [[-0.1063,  0.1792,  0.1032],
          [ 0.0971,  0.0175, -0.0890],
          [ 0.0344, -0.1830, -0.1366]],

         [[-0.0978,  0.1278,  0.1212],
          [-0.0830, -0.0013,  0.1283],
          [ 0.1914,  0.0733,  0.0266]]],


        [[[ 0.1332, -0.1135,  0.0364],
          [-0.1475, -0.1349, -0.0990],
          [ 0.0869,  0.0754, -0.1128]],

         [[ 0.0591,  0.1011, -0.0280],
          [ 0.0059,  0.0401,  0.1156],
          [ 0.1802, -0.1534, -0.0733]],

         [[ 0.0775,  0.1566,  0.1639],
          [ 0.1683,  0.0344, -0.1717],
          [ 0.0118, -0.1272, -0.1840]]],


        ...,


        [[[ 0.0682, -0.0764, -0.1206],
          [ 0.0763,  0.0335, -0.1410],
          [-0.1323,  0.0946,  0.1445]],

         [[ 0.0183, -0.0975,  0.1003],
          [ 0.0360, -0.1679, -0.1243],
          [ 0.0343,  0.1229, -0.0629]],

         [[ 0.0807, -0.1307,  0.1822],
          [ 0.1275,  0.0268,  0.0759],
          [ 0.0453, -0.1181, -0.0130]]],


        [[[ 0.0627, -0.0406,  0.1924],
          [-0.0632, -0.1655, -0.0865],
          [-0.1704,  0.1684, -0.0841]],

         [[-0.1291, -0.0211, -0.1255],
          [-0.0926,  0.1033, -0.1425],
          [ 0.0978,  0.1511,  0.0007]],

         [[ 0.0847, -0.1124,  0.1498],
          [ 0.1871,  0.0452,  0.1721],
          [-0.1955, -0.0192,  0.0318]]],


        [[[-0.1669,  0.0944, -0.0724],
          [-0.0708, -0.1543,  0.0819],
          [-0.1406, -0.0459,  0.0854]],

         [[ 0.1453,  0.0465, -0.0711],
          [-0.0836,  0.1102,  0.0390],
          [ 0.1595,  0.0674,  0.1515]],

         [[-0.1560,  0.1532, -0.0941],
          [-0.0553, -0.0043, -0.0430],
          [ 0.0336,  0.1605, -0.1260]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.0421e+06, 1.9618e+05, 1.9589e+05, 7.6582e+04, 7.2050e+04, 5.9989e+04,
        1.1541e+04, 7.5760e+03, 2.3821e+03, 2.2673e+03, 2.2587e+03, 2.0121e+03,
        8.2000e+02, 4.6739e+02, 4.6303e+02, 3.9035e+02, 3.6083e+02, 2.2478e+02,
        9.7177e+01, 8.8265e+01, 7.0082e+01, 4.9579e+01, 4.4703e+01, 1.7164e+01,
        1.5594e+01, 1.2511e+01, 4.7661e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 5]) 

NULL SPACE BASIS :  tensor([[ 1.9356e-01,  1.2949e-01, -1.3306e-01,  1.2158e-01,  8.4266e-02],
        [ 1.9607e-02, -2.3851e-01,  5.5064e-03, -2.2920e-01, -1.5404e-01],
        [-2.1553e-01,  1.3187e-01,  1.2756e-01,  1.2376e-01,  8.5162e-02],
        [-3.8401e-01,  4.9788e-03,  2.3967e-01, -2.2478e-01, -1.5159e-01],
        [ 2.1658e-04, -6.2845e-03, -5.2734e-03,  4.2143e-01,  2.7517e-01],
        [ 3.8529e-01,  8.7936e-04, -2.3415e-01, -2.2980e-01, -1.5288e-01],
        [ 2.1845e-01, -1.3414e-01, -1.2995e-01,  1.2246e-01,  8.4047e-02],
        [-2.0851e-02,  2.4412e-01,  5.8124e-04, -2.3018e-01, -1.5340e-01],
        [-1.9670e-01, -1.3239e-01,  1.2912e-01,  1.2582e-01,  8.4398e-02],
        [-1.1156e-02, -2.4163e-01,  2.4650e-01, -2.2971e-03, -1.5086e-01],
        [ 5.8843e-04,  4.4178e-01, -4.3803e-03,  9.2664e-03,  2.7554e-01],
        [ 1.2984e-02, -2.4468e-01, -2.4250e-01, -9.1142e-03, -1.5204e-01],
        [ 2.0825e-02, -3.4708e-03, -4.4248e-01,  3.9654e-04,  2.7046e-01],
        [-3.4326e-04,  2.1935e-03,  1.1558e-03, -6.9706e-03, -4.9126e-01],
        [-2.3538e-02,  2.9228e-03,  4.4188e-01,  1.0328e-02,  2.7252e-01],
        [-8.1519e-03,  2.4477e-01,  2.4151e-01,  1.9518e-03, -1.4954e-01],
        [ 1.0375e-03, -4.4357e-01,  2.2747e-03, -7.5885e-04,  2.7341e-01],
        [ 7.7291e-03,  2.4163e-01, -2.4391e-01, -2.3807e-03, -1.5040e-01],
        [-2.0623e-01,  1.3330e-01, -1.3484e-01, -1.3584e-01,  7.8967e-02],
        [-2.3219e-02, -2.4192e-01, -1.1492e-03,  2.5004e-01, -1.4422e-01],
        [ 2.2947e-01,  1.3433e-01,  1.3645e-01, -1.3071e-01,  7.9406e-02],
        [ 4.0867e-01, -1.3231e-03,  2.4099e-01,  2.5430e-01, -1.4106e-01],
        [ 9.8959e-04,  4.3149e-03,  4.5345e-03, -4.6875e-01,  2.5674e-01],
        [-4.0804e-01, -4.4922e-03, -2.4650e-01,  2.4890e-01, -1.4219e-01],
        [-2.3702e-01, -1.3190e-01, -1.3222e-01, -1.4101e-01,  7.7680e-02],
        [ 2.1751e-02,  2.3789e-01, -3.3828e-03,  2.6154e-01, -1.4248e-01],
        [ 2.1362e-01, -1.3002e-01,  1.3609e-01, -1.4000e-01,  7.8386e-02]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0420, -0.0307, -0.0087, -0.0651,  0.0333,  0.0291,  0.0287, -0.0060,
         -0.0224, -0.0354,  0.0368, -0.0048,  0.0381, -0.0189, -0.0173, -0.0064,
         -0.0155,  0.0234, -0.0065, -0.0078,  0.0154,  0.0295, -0.0157, -0.0127,
         -0.0250,  0.0247, -0.0018],
        [-0.0307,  0.0597, -0.0342,  0.0302, -0.0615,  0.0368, -0.0024,  0.0079,
         -0.0060,  0.0369, -0.0671,  0.0370, -0.0192,  0.0343, -0.0193, -0.0155,
          0.0286, -0.0157, -0.0079,  0.0099, -0.0041, -0.0120,  0.0299, -0.0194,
          0.0208, -0.0422,  0.0250],
        [-0.0087, -0.0342,  0.0459,  0.0328,  0.0331, -0.0690, -0.0264, -0.0021,
          0.0287, -0.0050,  0.0368, -0.0358, -0.0171, -0.0189,  0.0386,  0.0234,
         -0.0158, -0.0063,  0.0155, -0.0037, -0.0106, -0.0170, -0.0157,  0.0334,
          0.0027,  0.0208, -0.0253],
        [-0.0651,  0.0302,  0.0328,  0.1245, -0.0616, -0.0578, -0.0697,  0.0377,
          0.0290,  0.0383, -0.0192, -0.0175, -0.0694,  0.0342,  0.0319,  0.0378,
         -0.0194, -0.0165,  0.0296, -0.0120, -0.0167, -0.0604,  0.0300,  0.0283,
          0.0351, -0.0202, -0.0136],
        [ 0.0333, -0.0615,  0.0331, -0.0616,  0.1133, -0.0615,  0.0341, -0.0629,
          0.0341, -0.0189,  0.0344, -0.0192,  0.0344, -0.0618,  0.0344, -0.0193,
          0.0347, -0.0191, -0.0160,  0.0301, -0.0155,  0.0300, -0.0568,  0.0300,
         -0.0164,  0.0311, -0.0167],
        [ 0.0291,  0.0368, -0.0690, -0.0578, -0.0615,  0.1250,  0.0329,  0.0306,
         -0.0662, -0.0173, -0.0191,  0.0389,  0.0314,  0.0341, -0.0700, -0.0166,
         -0.0188,  0.0375, -0.0128, -0.0198,  0.0333,  0.0287,  0.0303, -0.0604,
         -0.0179, -0.0129,  0.0315],
        [ 0.0287, -0.0024, -0.0264, -0.0697,  0.0341,  0.0329,  0.0468, -0.0351,
         -0.0087, -0.0067, -0.0153,  0.0238,  0.0381, -0.0191, -0.0173, -0.0350,
          0.0368, -0.0053, -0.0248,  0.0206,  0.0023,  0.0346, -0.0164, -0.0170,
         -0.0124, -0.0030,  0.0160],
        [-0.0060,  0.0079, -0.0021,  0.0377, -0.0629,  0.0306, -0.0351,  0.0611,
         -0.0313, -0.0156,  0.0284, -0.0155, -0.0193,  0.0347, -0.0191,  0.0369,
         -0.0671,  0.0368,  0.0250, -0.0420,  0.0206, -0.0204,  0.0311, -0.0126,
         -0.0030,  0.0086, -0.0071],
        [-0.0224, -0.0060,  0.0287,  0.0290,  0.0341, -0.0662, -0.0087, -0.0313,
          0.0429,  0.0237, -0.0155, -0.0069, -0.0169, -0.0190,  0.0383, -0.0054,
          0.0366, -0.0349, -0.0022,  0.0249, -0.0246, -0.0130, -0.0168,  0.0306,
          0.0160, -0.0069, -0.0082],
        [-0.0354,  0.0369, -0.0050,  0.0383, -0.0189, -0.0173, -0.0067, -0.0156,
          0.0237,  0.0635, -0.0668,  0.0099, -0.0668,  0.0330,  0.0301,  0.0103,
          0.0297, -0.0429, -0.0334,  0.0356, -0.0058,  0.0339, -0.0168, -0.0153,
         -0.0042, -0.0168,  0.0228],
        [ 0.0368, -0.0671,  0.0368, -0.0192,  0.0344, -0.0191, -0.0153,  0.0284,
         -0.0155, -0.0668,  0.1213, -0.0666,  0.0335, -0.0601,  0.0333,  0.0295,
         -0.0540,  0.0297,  0.0357, -0.0645,  0.0356, -0.0170,  0.0305, -0.0170,
         -0.0169,  0.0305, -0.0168],
        [-0.0048,  0.0370, -0.0358, -0.0175, -0.0192,  0.0389,  0.0238, -0.0155,
         -0.0069,  0.0099, -0.0666,  0.0635,  0.0301,  0.0331, -0.0670, -0.0429,
          0.0297,  0.0103, -0.0060,  0.0352, -0.0330, -0.0151, -0.0165,  0.0335,
          0.0227, -0.0169, -0.0041],
        [ 0.0381, -0.0192, -0.0171, -0.0694,  0.0344,  0.0314,  0.0381, -0.0193,
         -0.0169, -0.0668,  0.0335,  0.0301,  0.1205, -0.0597, -0.0547, -0.0663,
          0.0333,  0.0298,  0.0341, -0.0170, -0.0155, -0.0609,  0.0301,  0.0278,
          0.0335, -0.0167, -0.0153],
        [-0.0189,  0.0343, -0.0189,  0.0342, -0.0618,  0.0341, -0.0191,  0.0347,
         -0.0190,  0.0330, -0.0601,  0.0331, -0.0597,  0.1080, -0.0597,  0.0332,
         -0.0605,  0.0332, -0.0168,  0.0307, -0.0169,  0.0303, -0.0549,  0.0304,
         -0.0168,  0.0307, -0.0169],
        [-0.0173, -0.0193,  0.0386,  0.0319,  0.0344, -0.0700, -0.0173, -0.0191,
          0.0383,  0.0301,  0.0333, -0.0670, -0.0547, -0.0597,  0.1208,  0.0299,
          0.0332, -0.0663, -0.0153, -0.0167,  0.0338,  0.0273,  0.0300, -0.0606,
         -0.0150, -0.0167,  0.0334],
        [-0.0064, -0.0155,  0.0234,  0.0378, -0.0193, -0.0166, -0.0350,  0.0369,
         -0.0054,  0.0103,  0.0295, -0.0429, -0.0663,  0.0332,  0.0299,  0.0629,
         -0.0666,  0.0101, -0.0046, -0.0167,  0.0232,  0.0341, -0.0166, -0.0159,
         -0.0332,  0.0354, -0.0057],
        [-0.0155,  0.0286, -0.0158, -0.0194,  0.0347, -0.0188,  0.0368, -0.0671,
          0.0366,  0.0297, -0.0540,  0.0297,  0.0333, -0.0605,  0.0332, -0.0666,
          0.1214, -0.0666, -0.0170,  0.0303, -0.0166, -0.0166,  0.0307, -0.0170,
          0.0355, -0.0647,  0.0357],
        [ 0.0234, -0.0157, -0.0063, -0.0165, -0.0191,  0.0375, -0.0053,  0.0368,
         -0.0349, -0.0429,  0.0297,  0.0103,  0.0298,  0.0332, -0.0663,  0.0101,
         -0.0666,  0.0629,  0.0232, -0.0167, -0.0048, -0.0158, -0.0168,  0.0343,
         -0.0057,  0.0355, -0.0333],
        [-0.0065, -0.0079,  0.0155,  0.0296, -0.0160, -0.0128, -0.0248,  0.0250,
         -0.0022, -0.0334,  0.0357, -0.0060,  0.0341, -0.0168, -0.0153, -0.0046,
         -0.0170,  0.0232,  0.0461, -0.0325, -0.0106, -0.0727,  0.0374,  0.0321,
          0.0333, -0.0085, -0.0244],
        [-0.0078,  0.0099, -0.0037, -0.0120,  0.0301, -0.0198,  0.0206, -0.0420,
          0.0249,  0.0356, -0.0645,  0.0352, -0.0170,  0.0307, -0.0167, -0.0167,
          0.0303, -0.0167, -0.0325,  0.0637, -0.0367,  0.0333, -0.0695,  0.0419,
         -0.0040,  0.0125, -0.0089],
        [ 0.0154, -0.0041, -0.0106, -0.0167, -0.0155,  0.0333,  0.0023,  0.0206,
         -0.0246, -0.0058,  0.0356, -0.0330, -0.0155, -0.0169,  0.0338,  0.0232,
         -0.0166, -0.0048, -0.0106, -0.0367,  0.0504,  0.0367,  0.0372, -0.0768,
         -0.0293, -0.0040,  0.0334],
        [ 0.0295, -0.0120, -0.0170, -0.0604,  0.0300,  0.0287,  0.0346, -0.0204,
         -0.0130,  0.0339, -0.0170, -0.0151, -0.0609,  0.0303,  0.0273,  0.0341,
         -0.0166, -0.0158, -0.0727,  0.0333,  0.0367,  0.1385, -0.0688, -0.0639,
         -0.0784,  0.0422,  0.0329],
        [-0.0157,  0.0299, -0.0157,  0.0300, -0.0568,  0.0303, -0.0164,  0.0311,
         -0.0168, -0.0168,  0.0305, -0.0165,  0.0301, -0.0549,  0.0300, -0.0166,
          0.0307, -0.0168,  0.0374, -0.0695,  0.0372, -0.0688,  0.1278, -0.0692,
          0.0379, -0.0707,  0.0385],
        [-0.0127, -0.0194,  0.0334,  0.0283,  0.0300, -0.0604, -0.0170, -0.0126,
          0.0306, -0.0153, -0.0170,  0.0335,  0.0278,  0.0304, -0.0606, -0.0159,
         -0.0170,  0.0343,  0.0321,  0.0419, -0.0768, -0.0639, -0.0692,  0.1384,
          0.0375,  0.0341, -0.0743],
        [-0.0250,  0.0208,  0.0027,  0.0351, -0.0164, -0.0179, -0.0124, -0.0030,
          0.0160, -0.0042, -0.0169,  0.0227,  0.0335, -0.0168, -0.0150, -0.0332,
          0.0355, -0.0057,  0.0333, -0.0040, -0.0293, -0.0784,  0.0379,  0.0375,
          0.0523, -0.0376, -0.0115],
        [ 0.0247, -0.0422,  0.0208, -0.0202,  0.0311, -0.0129, -0.0030,  0.0086,
         -0.0069, -0.0168,  0.0305, -0.0169, -0.0167,  0.0307, -0.0167,  0.0354,
         -0.0647,  0.0355, -0.0085,  0.0125, -0.0040,  0.0422, -0.0707,  0.0341,
         -0.0376,  0.0652, -0.0333],
        [-0.0018,  0.0250, -0.0253, -0.0136, -0.0167,  0.0315,  0.0160, -0.0071,
         -0.0082,  0.0228, -0.0168, -0.0041, -0.0153, -0.0169,  0.0334, -0.0057,
          0.0357, -0.0333, -0.0244, -0.0089,  0.0334,  0.0329,  0.0385, -0.0743,
         -0.0115, -0.0333,  0.0478]], device='cuda:0') 

reserving basis 76/576; cond: 9850891.0, radio:3.577147799660452e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1403e-02, -3.9739e-02, -7.2800e-03],
          [ 3.8436e-02,  2.6447e-02,  3.1681e-02],
          [ 2.2138e-02, -1.7261e-03,  8.7264e-03]],

         [[-1.4065e-02,  3.7794e-02,  1.8553e-02],
          [-3.0016e-02,  3.6872e-02, -3.1761e-03],
          [-6.9138e-03, -1.7051e-02, -2.0399e-02]],

         [[-3.0399e-03, -2.0810e-02, -3.6751e-02],
          [-2.9212e-02, -1.0369e-02,  3.3319e-02],
          [ 2.0517e-02, -3.1888e-02, -9.4195e-03]],

         ...,

         [[-2.5485e-02,  1.1632e-02,  9.8576e-06],
          [ 1.4868e-02, -6.5511e-03, -9.4259e-03],
          [-2.0564e-03,  8.6973e-03, -3.1115e-02]],

         [[-4.1533e-02,  1.3741e-02, -3.4925e-02],
          [-6.7151e-03,  1.2106e-02,  1.1021e-03],
          [ 2.0803e-02,  8.2504e-05,  4.1162e-03]],

         [[-1.5990e-02,  1.9737e-03,  1.1437e-02],
          [ 2.3163e-02, -3.9611e-02, -1.6172e-02],
          [-4.1597e-02, -1.1406e-02,  2.9627e-02]]],


        [[[-4.2272e-02, -4.7225e-02,  1.4219e-02],
          [-2.3758e-02,  2.0052e-02,  2.7560e-03],
          [ 8.1566e-03,  2.3263e-02, -4.2234e-02]],

         [[ 3.2911e-02,  8.0336e-03, -3.0303e-02],
          [ 1.1819e-02, -1.1411e-02, -1.7351e-02],
          [ 3.4499e-03, -1.1691e-02,  4.0712e-02]],

         [[ 2.6465e-02, -1.6331e-02, -4.4931e-02],
          [-1.7234e-02, -1.1043e-02,  1.0105e-02],
          [-1.2283e-02, -2.3453e-03, -6.4475e-03]],

         ...,

         [[-5.3910e-03, -3.8438e-03,  8.6902e-03],
          [ 1.5663e-02, -3.3969e-02,  1.7236e-02],
          [ 3.3153e-02,  7.4381e-03,  3.0871e-02]],

         [[-2.1738e-02,  6.7331e-03, -4.5001e-04],
          [-1.1112e-02,  2.3797e-02, -4.6474e-02],
          [-1.0346e-03, -1.4392e-02, -2.0127e-02]],

         [[-1.4755e-02, -3.2090e-02,  4.2493e-03],
          [-6.8437e-03, -3.1525e-02, -8.1298e-03],
          [-8.7274e-03,  1.5978e-02, -4.1645e-02]]],


        [[[-3.0812e-03,  1.8389e-02,  2.0644e-02],
          [ 7.9978e-03,  1.3029e-02,  4.1745e-02],
          [-1.3417e-02,  2.5634e-02,  3.2370e-02]],

         [[-3.1274e-02,  2.0017e-02,  2.9013e-02],
          [-2.3417e-03,  3.2309e-02,  6.1504e-03],
          [ 3.5380e-02,  6.7329e-03, -2.0917e-02]],

         [[ 3.8722e-02,  1.5193e-02,  3.4430e-02],
          [ 1.8328e-02,  2.5696e-02,  2.0172e-02],
          [ 1.6739e-02, -5.9932e-03,  9.5106e-03]],

         ...,

         [[ 2.1117e-02, -1.6892e-02,  4.4105e-02],
          [-3.2617e-03,  3.8409e-02, -3.4187e-03],
          [ 2.4445e-02,  3.3231e-02, -3.6193e-02]],

         [[-2.0646e-02, -3.6384e-02,  3.6761e-02],
          [ 4.5202e-02, -1.3484e-02, -1.8871e-02],
          [ 1.1979e-02,  3.0990e-02,  1.3887e-02]],

         [[-1.4943e-02, -2.3180e-02,  3.0110e-02],
          [-2.9681e-02, -4.1790e-02, -2.1359e-02],
          [ 3.5998e-03,  5.9173e-03,  6.1681e-03]]],


        ...,


        [[[-4.4342e-02, -1.8218e-02,  9.4862e-03],
          [ 2.4337e-02, -1.7162e-02, -8.7119e-03],
          [-4.2118e-03,  2.0895e-02, -3.4554e-02]],

         [[-4.1720e-02,  2.7748e-02,  5.7495e-03],
          [-1.3494e-02, -1.1909e-02,  8.7588e-03],
          [-1.9361e-02,  2.8401e-02,  1.2229e-02]],

         [[ 2.4045e-02,  3.2010e-02,  1.0487e-03],
          [ 4.7738e-02, -7.7254e-03, -3.4068e-02],
          [ 3.8611e-02, -1.4896e-02, -3.0850e-03]],

         ...,

         [[-2.5753e-02, -5.3294e-02,  2.3080e-02],
          [-2.0527e-02,  2.2699e-02, -4.7226e-02],
          [ 1.6439e-02,  4.0685e-02,  2.7453e-02]],

         [[-4.6401e-02, -5.5661e-02,  1.6269e-02],
          [-1.3021e-02, -1.4882e-02, -6.4282e-03],
          [-2.0967e-02,  9.8634e-03, -9.3697e-03]],

         [[ 3.6295e-02, -2.2360e-02,  4.1174e-02],
          [ 9.5249e-03, -5.7641e-03,  4.7798e-02],
          [ 3.1238e-02,  2.7899e-02,  4.8148e-02]]],


        [[[ 2.5296e-02,  1.4019e-02, -1.3574e-02],
          [-1.3509e-02, -3.2026e-02, -2.0576e-02],
          [ 1.5957e-02, -5.8614e-02, -4.2030e-02]],

         [[-8.1867e-03,  2.0763e-02,  8.9293e-03],
          [ 9.9843e-03,  1.1435e-03, -4.0605e-02],
          [-9.4056e-03,  3.7801e-02,  1.2352e-02]],

         [[-9.3504e-03,  8.4201e-03,  4.0407e-02],
          [ 3.1019e-04,  5.4661e-03,  3.0311e-02],
          [-3.1340e-02,  2.5724e-02,  3.2533e-02]],

         ...,

         [[ 4.0194e-02,  1.9609e-02,  3.8757e-02],
          [-3.5707e-02,  9.9989e-03,  7.1889e-04],
          [-1.2470e-03, -4.1722e-02, -3.9175e-02]],

         [[ 1.6754e-02,  2.3290e-02, -9.1791e-03],
          [ 2.6037e-02,  8.1159e-03,  1.8209e-02],
          [-4.8322e-02, -2.4970e-02, -5.2987e-03]],

         [[ 2.7795e-02,  2.5242e-02, -3.1755e-02],
          [-3.3100e-02, -2.8767e-02, -1.2087e-02],
          [-8.0581e-03, -3.9902e-02,  2.2228e-02]]],


        [[[ 2.7670e-02,  1.0042e-02, -1.6696e-02],
          [-6.0664e-03,  6.6090e-03, -1.2671e-02],
          [-1.9061e-03, -3.5063e-02, -7.6696e-03]],

         [[-4.3825e-02, -2.8668e-02, -2.9404e-02],
          [ 1.0632e-02, -5.6315e-02, -1.2028e-03],
          [ 2.6748e-02, -3.5973e-02, -6.9387e-03]],

         [[-1.7150e-02,  1.3754e-02,  2.2088e-02],
          [ 1.5419e-02, -1.3571e-02,  1.0562e-02],
          [-2.2098e-02,  3.6026e-02,  3.6855e-03]],

         ...,

         [[-2.1363e-02, -9.0682e-03,  1.0035e-02],
          [ 4.0384e-03,  3.7261e-02, -4.2749e-02],
          [-2.2907e-02,  1.5695e-02,  2.4623e-02]],

         [[-3.1386e-02, -3.8667e-02,  5.5158e-03],
          [-1.9641e-02, -1.0674e-02, -4.8789e-02],
          [-2.8310e-02, -5.1264e-02, -3.9112e-02]],

         [[-4.0120e-03, -1.0381e-02,  3.1075e-02],
          [ 3.3961e-02,  4.4786e-03,  3.5771e-02],
          [-2.8862e-02,  4.5870e-02, -3.2580e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.5492e+07, 1.9966e+06, 1.6179e+06, 1.5177e+06, 1.1514e+06, 8.3665e+05,
        7.7486e+05, 7.2010e+05, 6.1691e+05, 4.7477e+05, 4.1432e+05, 3.7660e+05,
        3.1535e+05, 2.3853e+05, 1.4587e+05, 1.2381e+05, 1.2038e+05, 1.0615e+05,
        9.1450e+04, 8.2709e+04, 7.6421e+04, 7.0263e+04, 5.4179e+04, 4.8539e+04,
        4.5937e+04, 4.5261e+04, 4.0664e+04, 3.8896e+04, 3.7910e+04, 3.4644e+04,
        3.3315e+04, 2.9549e+04, 2.7443e+04, 2.5624e+04, 2.4810e+04, 2.2728e+04,
        2.1025e+04, 1.9594e+04, 1.8490e+04, 1.7089e+04, 1.6170e+04, 1.5383e+04,
        1.4841e+04, 1.4506e+04, 1.3741e+04, 1.3033e+04, 1.1845e+04, 1.1731e+04,
        1.0970e+04, 1.0675e+04, 1.0268e+04, 9.9530e+03, 9.6128e+03, 9.5732e+03,
        9.1579e+03, 8.4958e+03, 8.0903e+03, 7.8811e+03, 7.6559e+03, 7.5522e+03,
        7.3767e+03, 6.9809e+03, 6.9453e+03, 6.7129e+03, 6.5407e+03, 6.4460e+03,
        6.2717e+03, 6.1287e+03, 5.9538e+03, 5.7412e+03, 5.6932e+03, 5.4261e+03,
        5.2999e+03, 5.1969e+03, 5.1310e+03, 4.9502e+03, 4.8129e+03, 4.6026e+03,
        4.5149e+03, 4.3346e+03, 4.2645e+03, 4.1398e+03, 4.1304e+03, 4.0788e+03,
        3.9708e+03, 3.9223e+03, 3.7043e+03, 3.6156e+03, 3.5496e+03, 3.4952e+03,
        3.3748e+03, 3.3270e+03, 3.2525e+03, 3.1801e+03, 3.0536e+03, 3.0174e+03,
        2.8986e+03, 2.8242e+03, 2.7492e+03, 2.6939e+03, 2.6108e+03, 2.6032e+03,
        2.5430e+03, 2.4759e+03, 2.4347e+03, 2.3065e+03, 2.2777e+03, 2.2674e+03,
        2.2212e+03, 2.1787e+03, 2.1204e+03, 2.1054e+03, 2.0183e+03, 2.0047e+03,
        1.9703e+03, 1.9365e+03, 1.9156e+03, 1.8703e+03, 1.8296e+03, 1.7950e+03,
        1.7834e+03, 1.7608e+03, 1.7288e+03, 1.6704e+03, 1.6368e+03, 1.6160e+03,
        1.6003e+03, 1.5323e+03, 1.5044e+03, 1.4947e+03, 1.4683e+03, 1.4485e+03,
        1.4353e+03, 1.4207e+03, 1.4129e+03, 1.3725e+03, 1.3386e+03, 1.3211e+03,
        1.3086e+03, 1.2975e+03, 1.2608e+03, 1.2465e+03, 1.2349e+03, 1.2204e+03,
        1.2130e+03, 1.1970e+03, 1.1599e+03, 1.1515e+03, 1.1325e+03, 1.1229e+03,
        1.1108e+03, 1.0823e+03, 1.0761e+03, 1.0604e+03, 1.0489e+03, 1.0252e+03,
        1.0198e+03, 1.0156e+03, 1.0011e+03, 9.7444e+02, 9.6404e+02, 9.5073e+02,
        9.3712e+02, 9.2581e+02, 9.1691e+02, 8.8901e+02, 8.8566e+02, 8.6318e+02,
        8.5518e+02, 8.4803e+02, 8.3011e+02, 8.2559e+02, 8.0552e+02, 7.9003e+02,
        7.8622e+02, 7.8481e+02, 7.6813e+02, 7.5689e+02, 7.4813e+02, 7.3638e+02,
        7.3417e+02, 7.2574e+02, 7.0542e+02, 6.9653e+02, 6.9303e+02, 6.8104e+02,
        6.6945e+02, 6.6780e+02, 6.6546e+02, 6.5175e+02, 6.4351e+02, 6.3989e+02,
        6.2945e+02, 6.2192e+02, 6.1633e+02, 6.1302e+02, 6.0529e+02, 6.0298e+02,
        5.9308e+02, 5.9021e+02, 5.8407e+02, 5.6736e+02, 5.6346e+02, 5.5398e+02,
        5.5242e+02, 5.4599e+02, 5.3908e+02, 5.3578e+02, 5.2760e+02, 5.2241e+02,
        5.1744e+02, 5.1036e+02, 5.0857e+02, 5.0651e+02, 4.9747e+02, 4.9189e+02,
        4.8367e+02, 4.8012e+02, 4.7806e+02, 4.7011e+02, 4.6222e+02, 4.6092e+02,
        4.5432e+02, 4.4839e+02, 4.4500e+02, 4.3997e+02, 4.3982e+02, 4.3162e+02,
        4.2522e+02, 4.1816e+02, 4.1439e+02, 4.1114e+02, 4.0932e+02, 4.0499e+02,
        4.0393e+02, 3.9928e+02, 3.9552e+02, 3.9264e+02, 3.8637e+02, 3.7793e+02,
        3.7737e+02, 3.7302e+02, 3.6775e+02, 3.6699e+02, 3.6368e+02, 3.6174e+02,
        3.5700e+02, 3.5303e+02, 3.4925e+02, 3.4543e+02, 3.4455e+02, 3.3822e+02,
        3.3638e+02, 3.3476e+02, 3.2854e+02, 3.2571e+02, 3.2504e+02, 3.2122e+02,
        3.1807e+02, 3.1702e+02, 3.1482e+02, 3.0805e+02, 3.0676e+02, 3.0442e+02,
        3.0217e+02, 2.9824e+02, 2.9324e+02, 2.8983e+02, 2.8720e+02, 2.8596e+02,
        2.8315e+02, 2.8106e+02, 2.7757e+02, 2.7660e+02, 2.7401e+02, 2.7253e+02,
        2.7004e+02, 2.6944e+02, 2.6570e+02, 2.6185e+02, 2.5857e+02, 2.5568e+02,
        2.5495e+02, 2.5281e+02, 2.5043e+02, 2.4922e+02, 2.4638e+02, 2.4564e+02,
        2.4170e+02, 2.4017e+02, 2.3849e+02, 2.3516e+02, 2.3449e+02, 2.3316e+02,
        2.3136e+02, 2.2767e+02, 2.2651e+02, 2.2435e+02, 2.2292e+02, 2.2138e+02,
        2.1874e+02, 2.1755e+02, 2.1362e+02, 2.1303e+02, 2.1186e+02, 2.0944e+02,
        2.0736e+02, 2.0689e+02, 2.0623e+02, 2.0343e+02, 2.0169e+02, 2.0054e+02,
        1.9854e+02, 1.9737e+02, 1.9479e+02, 1.9404e+02, 1.9345e+02, 1.9194e+02,
        1.8956e+02, 1.8785e+02, 1.8546e+02, 1.8440e+02, 1.8123e+02, 1.8014e+02,
        1.7935e+02, 1.7794e+02, 1.7701e+02, 1.7648e+02, 1.7453e+02, 1.7342e+02,
        1.7167e+02, 1.7065e+02, 1.6958e+02, 1.6700e+02, 1.6647e+02, 1.6551e+02,
        1.6428e+02, 1.6251e+02, 1.6118e+02, 1.5906e+02, 1.5767e+02, 1.5618e+02,
        1.5576e+02, 1.5486e+02, 1.5377e+02, 1.5191e+02, 1.5082e+02, 1.4959e+02,
        1.4841e+02, 1.4760e+02, 1.4606e+02, 1.4566e+02, 1.4559e+02, 1.4379e+02,
        1.4247e+02, 1.4193e+02, 1.4097e+02, 1.3951e+02, 1.3797e+02, 1.3744e+02,
        1.3677e+02, 1.3572e+02, 1.3445e+02, 1.3401e+02, 1.3240e+02, 1.3053e+02,
        1.2997e+02, 1.2930e+02, 1.2798e+02, 1.2667e+02, 1.2643e+02, 1.2606e+02,
        1.2534e+02, 1.2339e+02, 1.2270e+02, 1.2186e+02, 1.2118e+02, 1.1989e+02,
        1.1939e+02, 1.1837e+02, 1.1749e+02, 1.1701e+02, 1.1514e+02, 1.1500e+02,
        1.1462e+02, 1.1371e+02, 1.1315e+02, 1.1271e+02, 1.1162e+02, 1.1105e+02,
        1.1026e+02, 1.0957e+02, 1.0831e+02, 1.0720e+02, 1.0588e+02, 1.0539e+02,
        1.0484e+02, 1.0410e+02, 1.0322e+02, 1.0298e+02, 1.0273e+02, 1.0120e+02,
        1.0093e+02, 1.0006e+02, 9.9698e+01, 9.8847e+01, 9.8088e+01, 9.6946e+01,
        9.6213e+01, 9.5883e+01, 9.5619e+01, 9.4864e+01, 9.4246e+01, 9.3751e+01,
        9.2765e+01, 9.1759e+01, 9.1240e+01, 9.0587e+01, 8.9893e+01, 8.9465e+01,
        8.9061e+01, 8.8905e+01, 8.7892e+01, 8.7094e+01, 8.6352e+01, 8.5231e+01,
        8.5042e+01, 8.4002e+01, 8.3506e+01, 8.3112e+01, 8.2980e+01, 8.2167e+01,
        8.1091e+01, 8.1019e+01, 7.9393e+01, 7.9162e+01, 7.8266e+01, 7.7554e+01,
        7.6913e+01, 7.6681e+01, 7.6265e+01, 7.5694e+01, 7.5281e+01, 7.4610e+01,
        7.4312e+01, 7.3695e+01, 7.3097e+01, 7.2583e+01, 7.1861e+01, 7.1776e+01,
        7.0588e+01, 7.0097e+01, 6.9603e+01, 6.9355e+01, 6.8273e+01, 6.7827e+01,
        6.7575e+01, 6.6594e+01, 6.6181e+01, 6.5687e+01, 6.5434e+01, 6.4964e+01,
        6.4434e+01, 6.3861e+01, 6.3401e+01, 6.3195e+01, 6.2347e+01, 6.1497e+01,
        6.1066e+01, 6.0561e+01, 6.0163e+01, 5.9917e+01, 5.9241e+01, 5.8623e+01,
        5.8342e+01, 5.7874e+01, 5.7489e+01, 5.6399e+01, 5.6180e+01, 5.5584e+01,
        5.5138e+01, 5.4687e+01, 5.3975e+01, 5.3527e+01, 5.2594e+01, 5.2426e+01,
        5.2242e+01, 5.1794e+01, 5.1164e+01, 5.1059e+01, 5.0434e+01, 4.9791e+01,
        4.9659e+01, 4.9089e+01, 4.8005e+01, 4.7711e+01, 4.7341e+01, 4.7022e+01,
        4.6881e+01, 4.6408e+01, 4.5971e+01, 4.5572e+01, 4.4989e+01, 4.4568e+01,
        4.4398e+01, 4.3698e+01, 4.3143e+01, 4.3063e+01, 4.2353e+01, 4.2155e+01,
        4.2087e+01, 4.1311e+01, 4.0857e+01, 4.0213e+01, 3.9382e+01, 3.9245e+01,
        3.8988e+01, 3.8387e+01, 3.7889e+01, 3.7188e+01, 3.6847e+01, 3.6723e+01,
        3.6037e+01, 3.5831e+01, 3.5203e+01, 3.4688e+01, 3.4511e+01, 3.3896e+01,
        3.2999e+01, 3.2564e+01, 3.2229e+01, 3.1214e+01, 3.0411e+01, 3.0310e+01,
        2.9899e+01, 2.9442e+01, 2.9238e+01, 2.8533e+01, 2.8463e+01, 2.7862e+01,
        2.7536e+01, 2.6899e+01, 2.6310e+01, 2.5396e+01, 2.5206e+01, 2.5044e+01,
        2.4268e+01, 2.4256e+01, 2.3657e+01, 2.2684e+01, 2.2245e+01, 2.1510e+01,
        2.1005e+01, 2.0258e+01, 1.9438e+01, 1.8185e+01, 1.8087e+01, 1.6973e+01,
        1.6135e+01, 1.5920e+01, 1.5314e+01, 1.4839e+01, 1.4427e+01, 1.4041e+01,
        1.3675e+01, 1.3378e+01, 1.2380e+01, 1.1792e+01, 1.0534e+01, 9.9496e+00,
        9.4915e+00, 7.4560e+00, 6.6954e+00, 6.3645e+00, 5.7015e+00, 4.6181e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 76]) 

NULL SPACE BASIS :  tensor([[ 0.0534, -0.0103, -0.0578,  ...,  0.0134, -0.0053,  0.0055],
        [ 0.0069,  0.0151,  0.0098,  ..., -0.0045,  0.0029, -0.0054],
        [-0.0742,  0.0355,  0.0441,  ..., -0.0038, -0.0009,  0.0019],
        ...,
        [ 0.0049,  0.0141,  0.0112,  ...,  0.0035,  0.0007, -0.0024],
        [-0.0200,  0.0273, -0.0106,  ..., -0.0021,  0.0001,  0.0021],
        [ 0.0326, -0.0200, -0.0073,  ..., -0.0039, -0.0015,  0.0030]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.9747e-03, -6.7270e-03,  6.6667e-04,  ..., -3.6898e-04,
          6.4923e-04, -2.3117e-04],
        [-6.7270e-03,  1.4534e-02, -8.2980e-03,  ...,  1.2794e-03,
         -1.2069e-04, -3.9129e-04],
        [ 6.6667e-04, -8.2980e-03,  9.2434e-03,  ..., -9.5555e-04,
          2.2838e-04,  4.4105e-04],
        ...,
        [-3.6898e-04,  1.2794e-03, -9.5555e-04,  ...,  1.9928e-03,
         -1.4353e-03, -4.7204e-05],
        [ 6.4923e-04, -1.2069e-04,  2.2838e-04,  ..., -1.4353e-03,
          2.8945e-03, -1.1759e-03],
        [-2.3117e-04, -3.9129e-04,  4.4105e-04,  ..., -4.7204e-05,
         -1.1759e-03,  1.5827e-03]], device='cuda:0') 

reserving basis 170/576; cond: 1048728.75, radio:0.0007002358324825764
PARAMETER       :  Parameter containing:
tensor([[[[-0.0140,  0.0039,  0.0141],
          [ 0.0236,  0.0075,  0.0079],
          [-0.0523, -0.0492, -0.0113]],

         [[ 0.0031, -0.0118, -0.0065],
          [ 0.0270, -0.0293,  0.0264],
          [-0.0625, -0.0075, -0.0569]],

         [[-0.0004,  0.0193,  0.0253],
          [-0.0142, -0.0341,  0.0467],
          [-0.0164, -0.0083,  0.0515]],

         ...,

         [[-0.0399,  0.0276,  0.0215],
          [-0.0110,  0.0077, -0.0070],
          [-0.0351,  0.0125,  0.0238]],

         [[-0.0175,  0.0127,  0.0018],
          [-0.0338,  0.0199, -0.0045],
          [ 0.0176, -0.0393,  0.0341]],

         [[ 0.0214, -0.0014,  0.0240],
          [ 0.0102,  0.0108,  0.0408],
          [-0.0294,  0.0203, -0.0058]]],


        [[[ 0.0323,  0.0112, -0.0229],
          [-0.0092,  0.0011,  0.0106],
          [ 0.0036, -0.0463,  0.0084]],

         [[ 0.0343,  0.0249,  0.0225],
          [-0.0202, -0.0088, -0.0240],
          [ 0.0083, -0.0297,  0.0155]],

         [[-0.0307, -0.0146, -0.0277],
          [-0.0039,  0.0132,  0.0101],
          [ 0.0351,  0.0009, -0.0428]],

         ...,

         [[ 0.0166,  0.0328,  0.0059],
          [ 0.0363,  0.0390, -0.0078],
          [-0.0085, -0.0293,  0.0359]],

         [[-0.0055,  0.0128, -0.0182],
          [-0.0052, -0.0202,  0.0153],
          [-0.0406, -0.0232, -0.0348]],

         [[ 0.0349, -0.0177,  0.0104],
          [ 0.0101,  0.0149,  0.0030],
          [ 0.0236, -0.0129,  0.0230]]],


        [[[ 0.0306, -0.0055, -0.0454],
          [ 0.0318,  0.0374,  0.0301],
          [-0.0148, -0.0073,  0.0391]],

         [[ 0.0142,  0.0034,  0.0092],
          [ 0.0190, -0.0288, -0.0029],
          [ 0.0300,  0.0279, -0.0159]],

         [[ 0.0417,  0.0021,  0.0097],
          [ 0.0141, -0.0183,  0.0390],
          [-0.0336, -0.0382,  0.0078]],

         ...,

         [[-0.0004,  0.0307, -0.0124],
          [-0.0160, -0.0450,  0.0061],
          [-0.0102, -0.0252, -0.0254]],

         [[-0.0084,  0.0243, -0.0459],
          [ 0.0329,  0.0238, -0.0223],
          [-0.0274, -0.0247,  0.0117]],

         [[ 0.0376, -0.0036, -0.0087],
          [-0.0255, -0.0259,  0.0093],
          [ 0.0031,  0.0420,  0.0343]]],


        ...,


        [[[ 0.0370, -0.0317, -0.0338],
          [ 0.0362, -0.0351, -0.0468],
          [-0.0152, -0.0219,  0.0140]],

         [[ 0.0383,  0.0164,  0.0284],
          [-0.0414, -0.0431,  0.0034],
          [-0.0480,  0.0249, -0.0390]],

         [[ 0.0129,  0.0451,  0.0054],
          [-0.0449, -0.0423,  0.0211],
          [-0.0213,  0.0176,  0.0274]],

         ...,

         [[-0.0250, -0.0244,  0.0181],
          [ 0.0025,  0.0296,  0.0367],
          [ 0.0342, -0.0487,  0.0121]],

         [[ 0.0009, -0.0232,  0.0332],
          [-0.0379, -0.0054, -0.0032],
          [-0.0084, -0.0444,  0.0062]],

         [[-0.0037, -0.0197, -0.0075],
          [ 0.0308,  0.0484, -0.0029],
          [-0.0029,  0.0315,  0.0074]]],


        [[[-0.0004, -0.0414,  0.0144],
          [-0.0407,  0.0209,  0.0333],
          [ 0.0135,  0.0278, -0.0118]],

         [[ 0.0130, -0.0208, -0.0189],
          [ 0.0071, -0.0446,  0.0017],
          [-0.0219, -0.0201, -0.0053]],

         [[ 0.0268,  0.0252,  0.0153],
          [-0.0049,  0.0060, -0.0200],
          [ 0.0331, -0.0353, -0.0013]],

         ...,

         [[-0.0215,  0.0429,  0.0049],
          [ 0.0251,  0.0238, -0.0309],
          [-0.0044, -0.0387,  0.0145]],

         [[-0.0230, -0.0143, -0.0362],
          [-0.0039,  0.0208,  0.0029],
          [ 0.0096, -0.0308,  0.0332]],

         [[ 0.0104,  0.0433,  0.0122],
          [-0.0028,  0.0223,  0.0184],
          [-0.0130,  0.0046,  0.0230]]],


        [[[ 0.0086, -0.0099, -0.0072],
          [-0.0157,  0.0161, -0.0078],
          [-0.0392, -0.0432,  0.0426]],

         [[-0.0251,  0.0046, -0.0186],
          [-0.0319, -0.0280,  0.0008],
          [-0.0333,  0.0117,  0.0107]],

         [[ 0.0004,  0.0141, -0.0149],
          [ 0.0068, -0.0373, -0.0085],
          [-0.0235,  0.0281, -0.0471]],

         ...,

         [[ 0.0403, -0.0362, -0.0336],
          [-0.0111,  0.0118,  0.0402],
          [-0.0142,  0.0308, -0.0371]],

         [[-0.0121, -0.0312, -0.0107],
          [ 0.0155,  0.0208, -0.0294],
          [ 0.0316, -0.0314,  0.0298]],

         [[-0.0033, -0.0311, -0.0275],
          [-0.0214,  0.0329,  0.0006],
          [-0.0163,  0.0396,  0.0060]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.9569e+07, 1.5466e+06, 1.2744e+06, 1.1917e+06, 1.1644e+06, 8.0625e+05,
        6.1022e+05, 5.5732e+05, 4.9825e+05, 4.2471e+05, 3.3581e+05, 3.3266e+05,
        3.0190e+05, 2.6978e+05, 2.1125e+05, 1.5155e+05, 1.4276e+05, 1.3294e+05,
        1.1884e+05, 1.0325e+05, 9.7904e+04, 7.7600e+04, 7.4047e+04, 7.3252e+04,
        6.9212e+04, 6.1095e+04, 6.0209e+04, 5.5262e+04, 5.1844e+04, 4.4944e+04,
        4.2335e+04, 4.0308e+04, 3.9830e+04, 3.6482e+04, 3.4075e+04, 3.2438e+04,
        3.1748e+04, 2.9638e+04, 2.6644e+04, 2.5758e+04, 2.4359e+04, 2.2792e+04,
        2.1656e+04, 2.1257e+04, 1.9797e+04, 1.9425e+04, 1.8960e+04, 1.7899e+04,
        1.6861e+04, 1.6561e+04, 1.5985e+04, 1.5575e+04, 1.5179e+04, 1.4947e+04,
        1.4399e+04, 1.3419e+04, 1.3378e+04, 1.3125e+04, 1.2315e+04, 1.1970e+04,
        1.1595e+04, 1.1485e+04, 1.1115e+04, 1.0722e+04, 1.0538e+04, 1.0134e+04,
        9.9403e+03, 9.6479e+03, 9.4198e+03, 9.1099e+03, 9.0668e+03, 9.0204e+03,
        8.6801e+03, 8.5227e+03, 8.4606e+03, 8.2855e+03, 7.9880e+03, 7.8462e+03,
        7.6967e+03, 7.6349e+03, 7.3780e+03, 7.1882e+03, 7.0977e+03, 6.9206e+03,
        6.7931e+03, 6.6891e+03, 6.5933e+03, 6.4255e+03, 6.2881e+03, 6.2332e+03,
        6.1087e+03, 5.9889e+03, 5.9524e+03, 5.7728e+03, 5.7345e+03, 5.6267e+03,
        5.5322e+03, 5.4396e+03, 5.3859e+03, 5.2800e+03, 5.2649e+03, 5.1796e+03,
        5.0132e+03, 4.9705e+03, 4.8718e+03, 4.8390e+03, 4.7800e+03, 4.7080e+03,
        4.6259e+03, 4.5167e+03, 4.5080e+03, 4.4345e+03, 4.3627e+03, 4.2834e+03,
        4.2595e+03, 4.2144e+03, 4.1849e+03, 4.0880e+03, 4.0182e+03, 3.9223e+03,
        3.8961e+03, 3.8039e+03, 3.7813e+03, 3.7331e+03, 3.7048e+03, 3.6688e+03,
        3.5807e+03, 3.5691e+03, 3.5074e+03, 3.4746e+03, 3.4515e+03, 3.3379e+03,
        3.3092e+03, 3.2738e+03, 3.2315e+03, 3.1831e+03, 3.1446e+03, 3.0992e+03,
        3.0731e+03, 3.0552e+03, 3.0103e+03, 2.9882e+03, 2.9388e+03, 2.8980e+03,
        2.8614e+03, 2.8039e+03, 2.7274e+03, 2.7060e+03, 2.6984e+03, 2.6620e+03,
        2.6426e+03, 2.6165e+03, 2.5848e+03, 2.5256e+03, 2.5041e+03, 2.5006e+03,
        2.4860e+03, 2.4198e+03, 2.4066e+03, 2.3619e+03, 2.3513e+03, 2.3313e+03,
        2.3170e+03, 2.2920e+03, 2.2468e+03, 2.2258e+03, 2.2054e+03, 2.1772e+03,
        2.1637e+03, 2.1387e+03, 2.1283e+03, 2.0983e+03, 2.0811e+03, 2.0751e+03,
        2.0442e+03, 2.0355e+03, 2.0313e+03, 2.0065e+03, 1.9542e+03, 1.9523e+03,
        1.9206e+03, 1.9157e+03, 1.8996e+03, 1.8820e+03, 1.8529e+03, 1.8345e+03,
        1.8172e+03, 1.7983e+03, 1.7905e+03, 1.7726e+03, 1.7605e+03, 1.7446e+03,
        1.7185e+03, 1.6941e+03, 1.6877e+03, 1.6756e+03, 1.6643e+03, 1.6560e+03,
        1.6408e+03, 1.6184e+03, 1.6085e+03, 1.5820e+03, 1.5781e+03, 1.5516e+03,
        1.5297e+03, 1.5216e+03, 1.5081e+03, 1.5026e+03, 1.4927e+03, 1.4850e+03,
        1.4780e+03, 1.4528e+03, 1.4486e+03, 1.4372e+03, 1.4165e+03, 1.3954e+03,
        1.3811e+03, 1.3733e+03, 1.3665e+03, 1.3581e+03, 1.3442e+03, 1.3426e+03,
        1.3245e+03, 1.3107e+03, 1.3007e+03, 1.2952e+03, 1.2854e+03, 1.2715e+03,
        1.2553e+03, 1.2373e+03, 1.2342e+03, 1.2219e+03, 1.2114e+03, 1.2041e+03,
        1.2016e+03, 1.1911e+03, 1.1830e+03, 1.1777e+03, 1.1582e+03, 1.1559e+03,
        1.1429e+03, 1.1363e+03, 1.1303e+03, 1.1221e+03, 1.1168e+03, 1.0942e+03,
        1.0869e+03, 1.0787e+03, 1.0685e+03, 1.0645e+03, 1.0591e+03, 1.0577e+03,
        1.0362e+03, 1.0336e+03, 1.0288e+03, 1.0230e+03, 1.0131e+03, 1.0114e+03,
        1.0039e+03, 9.9016e+02, 9.7971e+02, 9.7513e+02, 9.6790e+02, 9.6289e+02,
        9.5774e+02, 9.5281e+02, 9.4026e+02, 9.3638e+02, 9.1901e+02, 9.1663e+02,
        9.1410e+02, 9.0517e+02, 9.0405e+02, 8.9607e+02, 8.8553e+02, 8.7918e+02,
        8.7288e+02, 8.6617e+02, 8.5784e+02, 8.5459e+02, 8.4350e+02, 8.3821e+02,
        8.3432e+02, 8.2764e+02, 8.2377e+02, 8.1963e+02, 8.1912e+02, 8.1265e+02,
        8.0879e+02, 8.0233e+02, 7.9222e+02, 7.8377e+02, 7.7640e+02, 7.7313e+02,
        7.6814e+02, 7.6414e+02, 7.6305e+02, 7.5094e+02, 7.4891e+02, 7.3974e+02,
        7.3842e+02, 7.3473e+02, 7.3053e+02, 7.2594e+02, 7.1661e+02, 7.1373e+02,
        7.1199e+02, 7.0575e+02, 7.0094e+02, 6.9866e+02, 6.9057e+02, 6.8638e+02,
        6.8542e+02, 6.7648e+02, 6.7396e+02, 6.6889e+02, 6.6217e+02, 6.5914e+02,
        6.5041e+02, 6.4771e+02, 6.4708e+02, 6.4331e+02, 6.4068e+02, 6.3625e+02,
        6.3233e+02, 6.3157e+02, 6.2311e+02, 6.2148e+02, 6.1787e+02, 6.1429e+02,
        6.1050e+02, 6.0392e+02, 5.9949e+02, 5.9914e+02, 5.9467e+02, 5.9191e+02,
        5.8460e+02, 5.8267e+02, 5.8114e+02, 5.7069e+02, 5.6883e+02, 5.6537e+02,
        5.5975e+02, 5.5544e+02, 5.5421e+02, 5.5154e+02, 5.4936e+02, 5.4715e+02,
        5.4651e+02, 5.3867e+02, 5.3513e+02, 5.3128e+02, 5.3061e+02, 5.2724e+02,
        5.2165e+02, 5.1928e+02, 5.1909e+02, 5.1282e+02, 5.0683e+02, 5.0391e+02,
        5.0044e+02, 4.9836e+02, 4.9657e+02, 4.9339e+02, 4.9010e+02, 4.8697e+02,
        4.8609e+02, 4.8188e+02, 4.8004e+02, 4.7696e+02, 4.7248e+02, 4.6978e+02,
        4.6738e+02, 4.6562e+02, 4.6278e+02, 4.6053e+02, 4.5867e+02, 4.5564e+02,
        4.5370e+02, 4.5221e+02, 4.4805e+02, 4.4424e+02, 4.4088e+02, 4.3889e+02,
        4.3395e+02, 4.3345e+02, 4.2984e+02, 4.2744e+02, 4.2448e+02, 4.2025e+02,
        4.1815e+02, 4.1379e+02, 4.1030e+02, 4.0959e+02, 4.0698e+02, 4.0446e+02,
        3.9979e+02, 3.9722e+02, 3.9626e+02, 3.9403e+02, 3.9201e+02, 3.8811e+02,
        3.8534e+02, 3.8249e+02, 3.8108e+02, 3.7850e+02, 3.7597e+02, 3.7302e+02,
        3.7246e+02, 3.6849e+02, 3.6614e+02, 3.6344e+02, 3.6199e+02, 3.5914e+02,
        3.5616e+02, 3.5424e+02, 3.5304e+02, 3.5074e+02, 3.4840e+02, 3.4751e+02,
        3.4462e+02, 3.4423e+02, 3.4277e+02, 3.4073e+02, 3.3817e+02, 3.3587e+02,
        3.3336e+02, 3.3105e+02, 3.2887e+02, 3.2575e+02, 3.2460e+02, 3.2155e+02,
        3.1850e+02, 3.1769e+02, 3.1312e+02, 3.1244e+02, 3.1200e+02, 3.0975e+02,
        3.0643e+02, 3.0375e+02, 3.0163e+02, 2.9965e+02, 2.9958e+02, 2.9800e+02,
        2.9466e+02, 2.9230e+02, 2.8894e+02, 2.8717e+02, 2.8593e+02, 2.8436e+02,
        2.8348e+02, 2.8095e+02, 2.7992e+02, 2.7829e+02, 2.7562e+02, 2.7492e+02,
        2.7403e+02, 2.7035e+02, 2.6964e+02, 2.6605e+02, 2.6239e+02, 2.6138e+02,
        2.5899e+02, 2.5840e+02, 2.5636e+02, 2.5479e+02, 2.5404e+02, 2.5211e+02,
        2.5150e+02, 2.4872e+02, 2.4669e+02, 2.4549e+02, 2.4324e+02, 2.4163e+02,
        2.4002e+02, 2.3880e+02, 2.3634e+02, 2.3455e+02, 2.3405e+02, 2.3367e+02,
        2.3042e+02, 2.2904e+02, 2.2785e+02, 2.2377e+02, 2.2297e+02, 2.2220e+02,
        2.1944e+02, 2.1887e+02, 2.1682e+02, 2.1483e+02, 2.1306e+02, 2.1245e+02,
        2.1023e+02, 2.0812e+02, 2.0748e+02, 2.0527e+02, 2.0306e+02, 2.0185e+02,
        1.9846e+02, 1.9622e+02, 1.9513e+02, 1.9441e+02, 1.9312e+02, 1.9110e+02,
        1.8900e+02, 1.8805e+02, 1.8668e+02, 1.8506e+02, 1.8395e+02, 1.8100e+02,
        1.8030e+02, 1.7830e+02, 1.7794e+02, 1.7516e+02, 1.7413e+02, 1.7264e+02,
        1.6932e+02, 1.6817e+02, 1.6637e+02, 1.6303e+02, 1.6120e+02, 1.5989e+02,
        1.5803e+02, 1.5699e+02, 1.5649e+02, 1.5432e+02, 1.5180e+02, 1.5047e+02,
        1.4918e+02, 1.4766e+02, 1.4605e+02, 1.4298e+02, 1.3983e+02, 1.3963e+02,
        1.3762e+02, 1.3592e+02, 1.3485e+02, 1.3134e+02, 1.3025e+02, 1.3006e+02,
        1.2769e+02, 1.2579e+02, 1.2340e+02, 1.2246e+02, 1.2038e+02, 1.1805e+02,
        1.1708e+02, 1.1579e+02, 1.1433e+02, 1.1293e+02, 1.1054e+02, 1.0915e+02,
        1.0638e+02, 1.0450e+02, 1.0345e+02, 1.0144e+02, 9.8529e+01, 9.7272e+01,
        9.5974e+01, 9.1786e+01, 9.0526e+01, 8.8927e+01, 8.6911e+01, 8.3591e+01,
        7.7709e+01, 7.7106e+01, 7.4411e+01, 7.0945e+01, 6.8654e+01, 6.6886e+01,
        6.6408e+01, 6.2229e+01, 5.9818e+01, 5.6961e+01, 4.8250e+01, 3.7731e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 170]) 

NULL SPACE BASIS :  tensor([[ 2.3253e-02, -5.9609e-02, -3.6693e-02,  ..., -6.5991e-03,
          7.6533e-03, -1.9203e-02],
        [ 3.2982e-03,  5.8460e-02, -5.0653e-02,  ..., -2.1011e-03,
         -6.6394e-04,  3.4741e-02],
        [ 4.2847e-02,  2.9473e-02,  4.4142e-02,  ...,  8.9306e-03,
          5.5369e-03, -1.9502e-02],
        ...,
        [-3.8211e-02, -4.7432e-02, -7.7426e-02,  ..., -2.1246e-02,
         -2.0042e-01, -4.7395e-02],
        [-1.3695e-02, -8.7275e-03, -2.9352e-03,  ..., -2.2157e-02,
          3.2581e-01,  5.3229e-02],
        [ 3.7157e-02, -2.1192e-04,  6.0665e-02,  ...,  3.7825e-02,
         -1.4966e-01, -2.0908e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.6697e-02, -1.9632e-02, -6.8825e-03,  ...,  1.1951e-03,
          8.0609e-05, -4.2449e-04],
        [-1.9632e-02,  4.2402e-02, -1.9399e-02,  ..., -3.2507e-04,
         -4.8890e-05,  2.0012e-04],
        [-6.8825e-03, -1.9399e-02,  3.5751e-02,  ..., -9.0609e-04,
          5.6795e-04,  5.3551e-04],
        ...,
        [ 1.1951e-03, -3.2507e-04, -9.0609e-04,  ...,  4.5392e-02,
         -2.0814e-02, -9.6120e-03],
        [ 8.0609e-05, -4.8890e-05,  5.6795e-04,  ..., -2.0814e-02,
          4.9350e-02, -1.9080e-02],
        [-4.2449e-04,  2.0012e-04,  5.3551e-04,  ..., -9.6120e-03,
         -1.9080e-02,  4.3199e-02]], device='cuda:0') 

reserving basis 229/576; cond: 1057594.25, radio:0.0008622026070952415
PARAMETER       :  Parameter containing:
tensor([[[[ 1.9037e-02,  4.0032e-02,  3.1709e-02],
          [ 3.3890e-03, -2.5575e-03, -8.6906e-03],
          [-3.5146e-02,  2.8715e-02,  1.6402e-02]],

         [[-4.2760e-02, -1.9537e-02, -2.2027e-02],
          [-4.7624e-02, -4.5888e-02, -2.7720e-02],
          [-2.3524e-02,  5.3118e-03, -5.9542e-03]],

         [[ 1.7568e-02,  2.4263e-02, -5.0998e-02],
          [-1.4650e-02, -2.9699e-02, -1.9161e-02],
          [ 1.0967e-02,  1.0065e-02,  1.9654e-02]],

         ...,

         [[-5.5284e-02,  3.1932e-02,  4.5163e-02],
          [-2.9739e-02, -7.0920e-03, -1.5214e-02],
          [ 2.3870e-02, -2.3369e-02, -3.3653e-02]],

         [[ 1.0867e-02, -2.8642e-02,  3.4836e-02],
          [ 4.0599e-02, -2.6038e-02, -3.4527e-03],
          [-1.9116e-02, -3.2812e-02,  1.9966e-02]],

         [[-2.6696e-02, -2.0951e-02,  5.8275e-03],
          [ 1.6912e-02,  1.4730e-02, -2.3897e-02],
          [-2.5621e-02,  3.6861e-03,  4.1021e-03]]],


        [[[-3.0122e-02, -2.0262e-02, -1.1456e-02],
          [ 3.6926e-02,  2.0658e-02,  2.9040e-02],
          [-1.4026e-02,  4.5979e-02,  1.6879e-03]],

         [[-3.8542e-02, -3.2577e-02, -3.1227e-02],
          [-7.2423e-06, -4.0622e-02, -2.4131e-02],
          [-1.4601e-02,  2.0023e-02,  2.1961e-02]],

         [[-9.2864e-03,  1.5220e-02, -2.3040e-02],
          [-3.5372e-02, -3.0571e-02,  1.4004e-02],
          [ 3.0666e-02,  1.1640e-02, -1.8409e-02]],

         ...,

         [[ 2.4643e-02, -4.6008e-02,  1.7446e-02],
          [-1.2235e-02,  1.2064e-02, -2.0292e-02],
          [ 2.6765e-02,  2.5392e-02, -7.1965e-03]],

         [[ 1.1892e-02, -4.0563e-02, -7.5144e-03],
          [-1.3678e-02, -1.8741e-02, -4.1110e-02],
          [-3.9825e-03,  5.4256e-03, -5.6437e-04]],

         [[-1.2975e-03, -2.7565e-02, -1.0808e-02],
          [-3.0671e-02, -2.7840e-02, -1.5537e-02],
          [ 3.4244e-02,  2.1391e-02, -1.8240e-02]]],


        [[[-4.0344e-02, -8.9471e-03, -2.0860e-03],
          [-5.7809e-02, -4.3169e-02, -3.4428e-02],
          [-1.3069e-03,  3.5089e-02,  3.8214e-02]],

         [[-1.8478e-02, -9.5354e-03, -3.9904e-03],
          [-2.3420e-02, -4.0467e-02, -3.9032e-02],
          [-1.7080e-02, -1.7892e-02, -9.7881e-03]],

         [[ 8.0672e-03, -1.4663e-02,  4.8220e-03],
          [-3.5975e-02,  5.0836e-02, -8.6316e-03],
          [-5.3880e-03,  2.7025e-02,  3.7242e-02]],

         ...,

         [[-4.5161e-02,  2.2951e-02,  3.6698e-02],
          [-1.8581e-02, -4.0340e-02,  3.3729e-02],
          [-1.1311e-02, -2.9677e-02,  1.3054e-02]],

         [[ 2.7053e-02,  1.8253e-02,  4.0050e-02],
          [ 2.3103e-02, -3.0046e-02, -2.2017e-02],
          [-3.4424e-02, -4.9861e-02,  1.9812e-02]],

         [[-2.9053e-02,  5.0365e-02,  3.2104e-02],
          [ 2.0185e-02,  8.5464e-03, -1.1662e-02],
          [-1.5508e-02, -5.1241e-04, -3.7703e-02]]],


        ...,


        [[[ 3.4672e-02,  5.4737e-03, -6.5011e-03],
          [ 3.9011e-02,  4.4941e-02,  1.5824e-03],
          [-1.5035e-02,  1.1920e-02, -3.7878e-02]],

         [[-3.1142e-02,  1.9846e-03, -2.8784e-02],
          [-1.0264e-02,  3.4440e-02,  1.4255e-02],
          [-4.6319e-03, -1.7795e-02,  2.0937e-02]],

         [[-4.1874e-02, -4.1225e-02, -3.6564e-02],
          [-2.7723e-02,  3.6296e-02,  4.2783e-02],
          [-1.1354e-02, -3.3796e-02, -2.7858e-02]],

         ...,

         [[ 3.3901e-02, -6.2049e-03,  3.3737e-02],
          [-1.1689e-02,  4.2933e-02, -1.2315e-02],
          [-3.9229e-02,  2.6078e-02, -2.3690e-02]],

         [[-1.8221e-02, -4.2313e-02, -1.4409e-02],
          [-1.1568e-02,  1.8889e-02, -3.5110e-02],
          [ 2.4424e-02,  2.2347e-02,  1.3338e-02]],

         [[ 2.1958e-02, -1.5545e-02,  1.0113e-02],
          [ 3.2856e-02, -3.8921e-03, -2.6487e-02],
          [-4.0707e-02,  1.4302e-02, -1.1350e-02]]],


        [[[-3.7860e-02,  2.3941e-02, -4.7817e-02],
          [ 3.7576e-03, -4.1774e-02,  2.0286e-02],
          [-1.6658e-02, -6.9815e-03,  3.6763e-02]],

         [[ 1.8324e-02, -3.0418e-02,  7.4121e-03],
          [-8.0854e-03,  2.7180e-02,  5.7614e-03],
          [ 2.4164e-02, -1.2222e-02,  1.6905e-02]],

         [[-3.4244e-02, -4.0994e-02, -2.4390e-02],
          [-2.4298e-02, -1.6919e-02,  1.8269e-02],
          [ 7.0140e-03, -3.3948e-02, -3.7569e-02]],

         ...,

         [[-9.1077e-03, -1.6223e-02, -3.0572e-02],
          [-2.0878e-02,  2.8591e-02, -2.3046e-02],
          [-4.0432e-02,  3.4224e-02,  3.1339e-02]],

         [[-3.0152e-02, -1.0608e-02, -2.9226e-02],
          [ 3.7065e-02,  2.4195e-03, -1.6544e-02],
          [ 2.9649e-02,  3.2324e-02, -2.0459e-02]],

         [[ 1.4594e-02, -2.6502e-02, -7.6131e-03],
          [-4.0315e-02, -3.4482e-02,  3.8587e-02],
          [ 3.8797e-02,  2.2516e-02,  3.9076e-02]]],


        [[[-3.8497e-02,  3.1847e-02, -4.2004e-03],
          [-3.3469e-02,  3.4567e-02, -7.6781e-03],
          [-3.7727e-02, -2.3041e-02,  3.0182e-02]],

         [[-3.4240e-03, -2.5287e-02,  8.6681e-03],
          [ 1.8265e-02,  2.1409e-03, -9.0796e-03],
          [-1.4850e-02, -2.7090e-03,  3.1507e-02]],

         [[-2.2611e-02, -3.7425e-02, -6.2132e-03],
          [-1.5798e-03, -1.6195e-03,  2.6987e-03],
          [ 3.5218e-02, -2.6440e-02,  3.0023e-03]],

         ...,

         [[ 1.2585e-02, -1.6688e-02, -1.9697e-02],
          [-2.0331e-02,  3.0853e-02, -4.4471e-02],
          [-4.2818e-06,  1.5657e-02, -3.0066e-02]],

         [[ 3.5618e-02, -6.4958e-03,  3.1736e-02],
          [ 4.9931e-03, -2.0821e-02, -3.8477e-02],
          [-2.0545e-02,  8.3954e-03, -3.6380e-02]],

         [[ 1.0347e-02,  4.1386e-02, -1.7761e-02],
          [-3.1959e-02,  4.1812e-02,  3.6413e-02],
          [ 2.6930e-02,  1.5332e-02,  7.2644e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.1050e+07, 1.5655e+06, 1.3704e+06, 1.1354e+06, 9.4469e+05, 6.6076e+05,
        6.1134e+05, 4.9664e+05, 4.5321e+05, 3.6076e+05, 3.3252e+05, 2.9702e+05,
        2.3985e+05, 1.9209e+05, 1.0199e+05, 9.2891e+04, 8.5613e+04, 8.0756e+04,
        7.7860e+04, 7.0033e+04, 6.7089e+04, 5.9487e+04, 5.4529e+04, 5.0749e+04,
        4.8604e+04, 4.3200e+04, 4.1893e+04, 3.8962e+04, 3.8623e+04, 3.6931e+04,
        3.4154e+04, 3.0890e+04, 3.0702e+04, 2.9743e+04, 2.7224e+04, 2.5208e+04,
        2.4178e+04, 2.3162e+04, 2.2777e+04, 2.1659e+04, 2.1317e+04, 2.0548e+04,
        2.0159e+04, 1.9347e+04, 1.8499e+04, 1.7512e+04, 1.7076e+04, 1.6462e+04,
        1.6059e+04, 1.5412e+04, 1.4486e+04, 1.3804e+04, 1.3495e+04, 1.2642e+04,
        1.2590e+04, 1.1826e+04, 1.1432e+04, 1.1110e+04, 1.0801e+04, 1.0491e+04,
        9.7667e+03, 9.5916e+03, 9.4129e+03, 9.2612e+03, 9.0423e+03, 8.9661e+03,
        8.6118e+03, 8.4279e+03, 8.2925e+03, 8.2299e+03, 8.1069e+03, 7.6293e+03,
        7.4129e+03, 7.2725e+03, 7.0291e+03, 6.7678e+03, 6.6735e+03, 6.5890e+03,
        6.5202e+03, 6.3016e+03, 6.1943e+03, 6.1166e+03, 6.0192e+03, 5.9634e+03,
        5.6876e+03, 5.6193e+03, 5.5524e+03, 5.3336e+03, 5.3165e+03, 5.2259e+03,
        5.1790e+03, 5.1334e+03, 5.0069e+03, 4.9515e+03, 4.7804e+03, 4.6085e+03,
        4.5354e+03, 4.5113e+03, 4.4592e+03, 4.3702e+03, 4.3343e+03, 4.2176e+03,
        4.0894e+03, 4.0401e+03, 4.0236e+03, 3.9351e+03, 3.8584e+03, 3.7951e+03,
        3.7046e+03, 3.6671e+03, 3.5980e+03, 3.5370e+03, 3.4912e+03, 3.4107e+03,
        3.3671e+03, 3.3112e+03, 3.2963e+03, 3.2140e+03, 3.1496e+03, 3.1178e+03,
        3.0697e+03, 3.0428e+03, 2.9553e+03, 2.9405e+03, 2.9122e+03, 2.8506e+03,
        2.8226e+03, 2.7866e+03, 2.7396e+03, 2.7310e+03, 2.6464e+03, 2.6228e+03,
        2.5747e+03, 2.5305e+03, 2.4878e+03, 2.4737e+03, 2.4381e+03, 2.3714e+03,
        2.3645e+03, 2.3103e+03, 2.2672e+03, 2.2354e+03, 2.2107e+03, 2.1983e+03,
        2.1568e+03, 2.1429e+03, 2.1327e+03, 2.0951e+03, 2.0843e+03, 2.0698e+03,
        2.0181e+03, 1.9820e+03, 1.9588e+03, 1.9406e+03, 1.9177e+03, 1.8921e+03,
        1.8814e+03, 1.8454e+03, 1.8359e+03, 1.8199e+03, 1.7894e+03, 1.7763e+03,
        1.7553e+03, 1.7490e+03, 1.7426e+03, 1.7255e+03, 1.7101e+03, 1.6803e+03,
        1.6589e+03, 1.6372e+03, 1.6225e+03, 1.6060e+03, 1.5950e+03, 1.5791e+03,
        1.5580e+03, 1.5455e+03, 1.5315e+03, 1.5165e+03, 1.5014e+03, 1.4694e+03,
        1.4647e+03, 1.4595e+03, 1.4258e+03, 1.4242e+03, 1.3970e+03, 1.3818e+03,
        1.3765e+03, 1.3723e+03, 1.3611e+03, 1.3455e+03, 1.3424e+03, 1.3350e+03,
        1.3019e+03, 1.2969e+03, 1.2739e+03, 1.2550e+03, 1.2451e+03, 1.2247e+03,
        1.2152e+03, 1.2047e+03, 1.1988e+03, 1.1699e+03, 1.1642e+03, 1.1593e+03,
        1.1451e+03, 1.1385e+03, 1.1296e+03, 1.1166e+03, 1.1138e+03, 1.1046e+03,
        1.0884e+03, 1.0781e+03, 1.0704e+03, 1.0597e+03, 1.0521e+03, 1.0379e+03,
        1.0322e+03, 1.0301e+03, 1.0237e+03, 1.0091e+03, 1.0032e+03, 1.0005e+03,
        9.8599e+02, 9.8362e+02, 9.5709e+02, 9.5500e+02, 9.4648e+02, 9.4319e+02,
        9.3946e+02, 9.3091e+02, 9.2562e+02, 9.1811e+02, 9.0746e+02, 8.9548e+02,
        8.8749e+02, 8.7974e+02, 8.7880e+02, 8.6939e+02, 8.6432e+02, 8.6413e+02,
        8.5379e+02, 8.3904e+02, 8.2975e+02, 8.2606e+02, 8.2159e+02, 8.1359e+02,
        8.0752e+02, 8.0099e+02, 7.9172e+02, 7.8501e+02, 7.7748e+02, 7.7424e+02,
        7.6721e+02, 7.5843e+02, 7.5606e+02, 7.4878e+02, 7.4745e+02, 7.3763e+02,
        7.3323e+02, 7.2257e+02, 7.2192e+02, 7.1889e+02, 7.0653e+02, 7.0501e+02,
        6.9616e+02, 6.9481e+02, 6.8441e+02, 6.8008e+02, 6.7779e+02, 6.7231e+02,
        6.6665e+02, 6.6334e+02, 6.5587e+02, 6.5220e+02, 6.4969e+02, 6.4375e+02,
        6.4061e+02, 6.2984e+02, 6.2806e+02, 6.2211e+02, 6.1896e+02, 6.1523e+02,
        6.1405e+02, 6.0976e+02, 6.0879e+02, 6.0342e+02, 5.9701e+02, 5.9400e+02,
        5.9045e+02, 5.8172e+02, 5.8095e+02, 5.7816e+02, 5.7191e+02, 5.6436e+02,
        5.6301e+02, 5.5915e+02, 5.5778e+02, 5.5422e+02, 5.4866e+02, 5.4502e+02,
        5.4028e+02, 5.3621e+02, 5.3178e+02, 5.2695e+02, 5.2221e+02, 5.1933e+02,
        5.1870e+02, 5.1472e+02, 5.0864e+02, 5.0504e+02, 5.0457e+02, 4.9577e+02,
        4.9492e+02, 4.9215e+02, 4.9082e+02, 4.8818e+02, 4.8386e+02, 4.7759e+02,
        4.7560e+02, 4.7318e+02, 4.6592e+02, 4.6353e+02, 4.5882e+02, 4.5522e+02,
        4.5071e+02, 4.4493e+02, 4.4319e+02, 4.4149e+02, 4.3933e+02, 4.3829e+02,
        4.3477e+02, 4.3183e+02, 4.3092e+02, 4.2548e+02, 4.2206e+02, 4.1897e+02,
        4.1641e+02, 4.1419e+02, 4.0979e+02, 4.0637e+02, 4.0420e+02, 4.0158e+02,
        3.9907e+02, 3.9662e+02, 3.9498e+02, 3.9364e+02, 3.9296e+02, 3.8769e+02,
        3.8334e+02, 3.8185e+02, 3.7978e+02, 3.7696e+02, 3.7572e+02, 3.7350e+02,
        3.7116e+02, 3.6901e+02, 3.6679e+02, 3.6453e+02, 3.6147e+02, 3.6087e+02,
        3.5644e+02, 3.5394e+02, 3.5110e+02, 3.4942e+02, 3.4789e+02, 3.4512e+02,
        3.4144e+02, 3.3912e+02, 3.3482e+02, 3.3305e+02, 3.3007e+02, 3.2830e+02,
        3.2766e+02, 3.2611e+02, 3.2222e+02, 3.2086e+02, 3.1877e+02, 3.1796e+02,
        3.1567e+02, 3.1417e+02, 3.1243e+02, 3.0895e+02, 3.0648e+02, 3.0555e+02,
        3.0287e+02, 3.0131e+02, 2.9934e+02, 2.9762e+02, 2.9461e+02, 2.9329e+02,
        2.9101e+02, 2.8923e+02, 2.8736e+02, 2.8607e+02, 2.8331e+02, 2.8176e+02,
        2.8044e+02, 2.7829e+02, 2.7586e+02, 2.7520e+02, 2.7368e+02, 2.7344e+02,
        2.7017e+02, 2.6957e+02, 2.6773e+02, 2.6479e+02, 2.6348e+02, 2.6292e+02,
        2.6155e+02, 2.5998e+02, 2.5800e+02, 2.5677e+02, 2.5449e+02, 2.5338e+02,
        2.5122e+02, 2.5021e+02, 2.4909e+02, 2.4724e+02, 2.4560e+02, 2.4349e+02,
        2.4170e+02, 2.3771e+02, 2.3658e+02, 2.3493e+02, 2.3340e+02, 2.3140e+02,
        2.3051e+02, 2.2999e+02, 2.2896e+02, 2.2750e+02, 2.2724e+02, 2.2529e+02,
        2.2269e+02, 2.2228e+02, 2.2108e+02, 2.1922e+02, 2.1746e+02, 2.1545e+02,
        2.1424e+02, 2.1286e+02, 2.1203e+02, 2.1032e+02, 2.0972e+02, 2.0831e+02,
        2.0749e+02, 2.0602e+02, 2.0272e+02, 2.0178e+02, 2.0085e+02, 2.0013e+02,
        1.9965e+02, 1.9913e+02, 1.9686e+02, 1.9552e+02, 1.9406e+02, 1.9239e+02,
        1.9155e+02, 1.9057e+02, 1.8798e+02, 1.8642e+02, 1.8515e+02, 1.8490e+02,
        1.8287e+02, 1.8198e+02, 1.7969e+02, 1.7758e+02, 1.7666e+02, 1.7449e+02,
        1.7427e+02, 1.7248e+02, 1.7161e+02, 1.7078e+02, 1.6866e+02, 1.6761e+02,
        1.6670e+02, 1.6569e+02, 1.6447e+02, 1.6383e+02, 1.6229e+02, 1.6090e+02,
        1.5895e+02, 1.5839e+02, 1.5700e+02, 1.5672e+02, 1.5546e+02, 1.5361e+02,
        1.5268e+02, 1.5185e+02, 1.5090e+02, 1.4956e+02, 1.4804e+02, 1.4774e+02,
        1.4581e+02, 1.4441e+02, 1.4416e+02, 1.4330e+02, 1.4179e+02, 1.4010e+02,
        1.3919e+02, 1.3830e+02, 1.3743e+02, 1.3534e+02, 1.3412e+02, 1.3322e+02,
        1.3255e+02, 1.3135e+02, 1.3042e+02, 1.2876e+02, 1.2775e+02, 1.2601e+02,
        1.2525e+02, 1.2449e+02, 1.2281e+02, 1.2154e+02, 1.2125e+02, 1.2034e+02,
        1.1895e+02, 1.1833e+02, 1.1708e+02, 1.1653e+02, 1.1520e+02, 1.1380e+02,
        1.1194e+02, 1.1087e+02, 1.0966e+02, 1.0864e+02, 1.0852e+02, 1.0781e+02,
        1.0706e+02, 1.0603e+02, 1.0502e+02, 1.0422e+02, 1.0327e+02, 1.0179e+02,
        1.0001e+02, 9.9171e+01, 9.7396e+01, 9.6676e+01, 9.5202e+01, 9.4224e+01,
        9.3965e+01, 9.3076e+01, 9.1531e+01, 9.1268e+01, 8.8946e+01, 8.7867e+01,
        8.7795e+01, 8.7371e+01, 8.6444e+01, 8.5877e+01, 8.5010e+01, 8.3287e+01,
        8.2476e+01, 8.2142e+01, 8.0184e+01, 7.9214e+01, 7.8074e+01, 7.6975e+01,
        7.6797e+01, 7.3868e+01, 7.2896e+01, 7.0808e+01, 6.8197e+01, 6.6615e+01,
        6.6563e+01, 6.4546e+01, 6.2100e+01, 5.9985e+01, 5.8618e+01, 5.7108e+01,
        5.5460e+01, 5.4230e+01, 5.1401e+01, 5.0030e+01, 4.1328e+01, 3.8815e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 229]) 

NULL SPACE BASIS :  tensor([[-0.0144,  0.0542,  0.0098,  ...,  0.0018,  0.0126, -0.0164],
        [ 0.0397, -0.0295, -0.0018,  ..., -0.0028, -0.0179,  0.0225],
        [-0.0444,  0.0479,  0.0758,  ...,  0.0029,  0.0055, -0.0077],
        ...,
        [-0.0431, -0.0057, -0.0084,  ...,  0.0001, -0.0122, -0.0096],
        [-0.0306, -0.0262,  0.0621,  ..., -0.0189,  0.0113,  0.0126],
        [-0.0266,  0.0592,  0.0670,  ...,  0.0219, -0.0113, -0.0048]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0226, -0.0168, -0.0012,  ...,  0.0003,  0.0002,  0.0005],
        [-0.0168,  0.0332, -0.0136,  ..., -0.0015,  0.0005, -0.0018],
        [-0.0012, -0.0136,  0.0191,  ...,  0.0006, -0.0012,  0.0008],
        ...,
        [ 0.0003, -0.0015,  0.0006,  ...,  0.0200, -0.0100, -0.0024],
        [ 0.0002,  0.0005, -0.0012,  ..., -0.0100,  0.0262, -0.0100],
        [ 0.0005, -0.0018,  0.0008,  ..., -0.0024, -0.0100,  0.0223]],
       device='cuda:0') 

reserving basis 316/576; cond: 268444.09375, radio:0.0043146051466465
PARAMETER       :  Parameter containing:
tensor([[[[-2.7013e-02, -4.1018e-03,  1.9197e-02],
          [-2.4276e-02, -2.7084e-02, -1.3059e-02],
          [ 1.0933e-02,  1.1924e-02,  7.9916e-03]],

         [[ 5.3030e-04,  1.4939e-02,  5.1634e-03],
          [-4.3121e-03,  1.4048e-03,  1.8568e-02],
          [-1.9430e-02, -1.6243e-02, -4.7253e-02]],

         [[ 2.0437e-03,  4.2659e-02,  4.2855e-02],
          [-3.5718e-03, -3.7797e-02, -4.8829e-03],
          [-3.5109e-02, -4.5170e-03, -2.4835e-02]],

         ...,

         [[-1.8639e-02,  4.4060e-02,  2.8183e-03],
          [-2.6454e-02, -8.7264e-03,  4.1788e-02],
          [-1.4123e-02, -3.8590e-02, -1.6914e-02]],

         [[ 2.9419e-02,  1.6232e-02, -4.4796e-02],
          [ 2.8343e-02,  6.0013e-03,  4.6140e-02],
          [-3.3374e-02,  2.1614e-02, -2.1402e-02]],

         [[ 3.5191e-02,  2.2630e-02,  2.1696e-02],
          [-3.6144e-02, -9.2573e-03,  2.5882e-02],
          [ 3.0355e-02, -1.5599e-02, -2.7135e-02]]],


        [[[-2.7448e-02, -2.3887e-02, -4.7154e-02],
          [ 9.4441e-03, -9.3788e-03,  8.4627e-03],
          [-5.2142e-02,  7.1022e-03,  1.4831e-03]],

         [[-3.2210e-02,  1.8688e-02,  5.1481e-02],
          [ 2.1930e-03,  2.2626e-02, -1.2649e-02],
          [-1.3899e-02,  2.9537e-02,  4.6444e-02]],

         [[-1.6130e-03, -1.6941e-02,  3.0449e-02],
          [-4.0072e-04, -3.1418e-02,  3.6726e-02],
          [ 3.4864e-02,  3.3541e-02, -2.6801e-03]],

         ...,

         [[-6.7360e-03,  2.1036e-02, -2.2319e-02],
          [-2.7034e-02,  2.2622e-02,  1.4373e-02],
          [-3.9352e-02,  2.7855e-02, -4.4288e-02]],

         [[-8.8468e-03,  2.1838e-02, -2.1962e-02],
          [-1.8817e-02, -2.9577e-02, -3.6159e-02],
          [-4.5031e-02, -1.0776e-02, -3.8523e-02]],

         [[ 2.2650e-02, -3.8288e-02, -3.5366e-02],
          [-2.0850e-02, -3.4537e-02, -1.6217e-02],
          [-2.2729e-02,  1.3182e-03,  9.8021e-03]]],


        [[[-5.8546e-03, -5.7353e-03, -1.6703e-02],
          [-1.1506e-04, -4.4139e-02, -3.9232e-02],
          [-4.0782e-02, -1.5767e-03, -1.3740e-02]],

         [[-2.4907e-02, -2.2494e-02, -1.9531e-02],
          [ 4.2589e-04, -2.7051e-02, -1.2310e-02],
          [ 1.5955e-02, -2.6424e-02,  7.6918e-04]],

         [[ 4.8155e-02,  4.1268e-02,  2.6718e-03],
          [ 2.9169e-02,  4.1905e-03, -3.9741e-02],
          [ 1.1981e-02,  8.2100e-03, -1.5014e-03]],

         ...,

         [[ 1.3674e-02, -1.3425e-02, -2.2171e-03],
          [ 1.4874e-02,  1.6638e-02, -4.0357e-02],
          [-3.4768e-02,  9.3336e-03, -1.2707e-02]],

         [[ 1.1607e-02,  6.0694e-03, -2.5403e-02],
          [-2.8107e-02,  9.3696e-03,  2.5789e-02],
          [ 4.9917e-03, -2.2266e-02,  8.6732e-04]],

         [[ 1.8016e-02, -1.1717e-02, -3.1967e-02],
          [ 1.1377e-02, -3.3590e-02, -3.3609e-02],
          [ 3.1384e-02, -1.5431e-02,  1.3306e-03]]],


        ...,


        [[[ 1.2679e-02,  3.2446e-02,  1.4809e-02],
          [-1.5980e-03, -1.6055e-02, -1.8848e-02],
          [ 5.0168e-02,  2.2889e-02, -5.4498e-02]],

         [[ 3.9607e-03, -2.6336e-02,  2.5296e-03],
          [-3.0585e-02,  2.5705e-02, -2.0170e-02],
          [ 1.8274e-02, -2.5015e-02,  4.4134e-02]],

         [[ 2.5714e-02, -1.9402e-02,  4.3242e-02],
          [ 3.2659e-02, -3.8528e-02, -3.9743e-02],
          [ 1.5905e-02, -1.2509e-02, -4.0364e-03]],

         ...,

         [[-1.6567e-02, -1.3956e-02,  3.7981e-03],
          [ 1.3458e-02,  8.3399e-03,  1.4054e-02],
          [ 2.7558e-03, -1.1222e-02,  3.4413e-03]],

         [[ 1.4650e-02,  2.1554e-02, -4.6686e-02],
          [-3.0476e-02, -2.0099e-02, -3.1975e-03],
          [ 9.6503e-03,  8.7568e-03,  1.5283e-02]],

         [[ 3.0573e-02,  6.2294e-03,  2.9135e-02],
          [ 1.6550e-02,  4.3589e-02,  2.6009e-03],
          [ 3.9297e-02, -1.0275e-02,  1.9765e-02]]],


        [[[-1.2385e-02,  1.9821e-02,  3.5065e-02],
          [ 1.6114e-02,  2.4412e-02,  7.1076e-03],
          [-3.1474e-03,  2.8214e-02, -5.1149e-02]],

         [[ 3.4838e-02, -1.4406e-02, -2.6972e-02],
          [-3.7508e-02,  5.8811e-03, -3.5042e-02],
          [-3.9864e-02, -3.6623e-03,  4.0778e-02]],

         [[ 2.0643e-02, -4.2113e-02, -2.3413e-02],
          [-3.5632e-02,  2.8184e-02, -1.6639e-02],
          [-3.5955e-02,  1.4295e-02, -4.1206e-03]],

         ...,

         [[-1.7656e-02, -2.8672e-02,  1.4430e-02],
          [ 1.2742e-02,  3.7652e-02,  8.7472e-03],
          [-2.0495e-02, -3.3706e-02, -5.6040e-03]],

         [[-2.0628e-02,  3.7623e-02, -2.4615e-02],
          [ 6.0018e-03, -1.5386e-02,  3.8838e-02],
          [ 4.5930e-02, -1.8798e-03, -1.0031e-02]],

         [[-2.2450e-02,  5.4141e-03,  2.8803e-02],
          [-3.2993e-02, -2.2497e-02,  1.0211e-02],
          [ 1.0523e-02,  1.0751e-02, -3.0520e-03]]],


        [[[ 3.8403e-02,  4.3463e-02,  2.3236e-04],
          [ 2.0203e-02, -1.1775e-02,  2.4237e-02],
          [ 6.1994e-03, -3.5010e-02, -5.0534e-02]],

         [[ 4.5808e-02, -5.3507e-03,  3.2674e-02],
          [ 1.9314e-02,  3.4289e-02,  9.4388e-03],
          [-9.4610e-04, -8.7181e-03,  3.7394e-02]],

         [[-2.3780e-02,  5.0603e-04, -4.2587e-02],
          [-3.6531e-03, -7.7239e-03,  4.0157e-02],
          [ 2.0755e-02, -1.7411e-02,  2.6780e-02]],

         ...,

         [[-8.1392e-03,  3.7864e-02, -3.3682e-02],
          [-9.6990e-03, -3.8266e-02,  2.7538e-02],
          [-3.7865e-02,  6.4697e-03,  3.1982e-05]],

         [[ 3.2761e-03, -3.6318e-02, -1.6160e-02],
          [-3.6938e-02,  2.5808e-02, -2.1220e-02],
          [-2.3462e-02, -3.9965e-02,  7.3316e-03]],

         [[ 4.2096e-02, -5.3969e-03,  3.5172e-02],
          [ 3.3923e-02, -1.4773e-02,  2.4139e-02],
          [ 4.5487e-05,  2.5126e-02,  2.4068e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.8264e+07, 1.3228e+06, 1.1465e+06, 9.9294e+05, 8.7610e+05, 5.9644e+05,
        5.7793e+05, 4.8796e+05, 4.6616e+05, 4.1481e+05, 3.6481e+05, 3.4279e+05,
        2.9280e+05, 2.2858e+05, 1.7704e+05, 1.5119e+05, 1.3611e+05, 1.1455e+05,
        1.0586e+05, 9.7725e+04, 9.4183e+04, 8.9768e+04, 8.6383e+04, 7.7645e+04,
        7.3169e+04, 6.9591e+04, 6.6031e+04, 6.1877e+04, 5.9234e+04, 5.3539e+04,
        5.1209e+04, 4.9745e+04, 4.5777e+04, 4.4416e+04, 3.9734e+04, 3.8406e+04,
        3.6863e+04, 3.5271e+04, 3.3473e+04, 3.2477e+04, 3.2112e+04, 3.0105e+04,
        2.9769e+04, 2.7174e+04, 2.6464e+04, 2.5778e+04, 2.4418e+04, 2.4001e+04,
        2.3202e+04, 2.2686e+04, 2.1889e+04, 2.1137e+04, 1.9681e+04, 1.9484e+04,
        1.8942e+04, 1.8610e+04, 1.8467e+04, 1.7938e+04, 1.7338e+04, 1.6821e+04,
        1.6742e+04, 1.6279e+04, 1.5941e+04, 1.5048e+04, 1.4894e+04, 1.4444e+04,
        1.3979e+04, 1.3769e+04, 1.3651e+04, 1.3183e+04, 1.3105e+04, 1.2732e+04,
        1.2510e+04, 1.2453e+04, 1.1934e+04, 1.1910e+04, 1.1763e+04, 1.1435e+04,
        1.1239e+04, 1.1056e+04, 1.0782e+04, 1.0606e+04, 1.0379e+04, 1.0219e+04,
        1.0133e+04, 9.9133e+03, 9.6728e+03, 9.5246e+03, 9.3031e+03, 9.2489e+03,
        9.0718e+03, 8.7669e+03, 8.7036e+03, 8.5782e+03, 8.2642e+03, 8.1875e+03,
        8.0858e+03, 8.0210e+03, 7.7684e+03, 7.6935e+03, 7.6066e+03, 7.5454e+03,
        7.4392e+03, 7.4081e+03, 7.2958e+03, 7.2311e+03, 7.0866e+03, 7.0274e+03,
        6.9318e+03, 6.8476e+03, 6.5663e+03, 6.5256e+03, 6.4190e+03, 6.3413e+03,
        6.2706e+03, 6.2048e+03, 6.1385e+03, 6.1057e+03, 6.0593e+03, 6.0040e+03,
        5.9309e+03, 5.8610e+03, 5.7627e+03, 5.7220e+03, 5.6290e+03, 5.5927e+03,
        5.5347e+03, 5.4590e+03, 5.4140e+03, 5.2889e+03, 5.2100e+03, 5.1962e+03,
        5.1636e+03, 5.0718e+03, 4.9779e+03, 4.9513e+03, 4.8924e+03, 4.8412e+03,
        4.7738e+03, 4.7300e+03, 4.6577e+03, 4.6153e+03, 4.5932e+03, 4.5741e+03,
        4.5213e+03, 4.4744e+03, 4.3922e+03, 4.3875e+03, 4.3225e+03, 4.2822e+03,
        4.2568e+03, 4.1539e+03, 4.1346e+03, 4.0652e+03, 4.0421e+03, 3.9915e+03,
        3.9574e+03, 3.9328e+03, 3.8850e+03, 3.8650e+03, 3.7975e+03, 3.7887e+03,
        3.7537e+03, 3.7369e+03, 3.7052e+03, 3.6734e+03, 3.6495e+03, 3.6069e+03,
        3.5741e+03, 3.5459e+03, 3.5065e+03, 3.4881e+03, 3.4403e+03, 3.4186e+03,
        3.4104e+03, 3.3966e+03, 3.3651e+03, 3.3426e+03, 3.3000e+03, 3.2859e+03,
        3.2542e+03, 3.2237e+03, 3.2139e+03, 3.1625e+03, 3.1509e+03, 3.1358e+03,
        3.0921e+03, 3.0758e+03, 3.0389e+03, 3.0173e+03, 2.9694e+03, 2.9594e+03,
        2.9156e+03, 2.9041e+03, 2.8930e+03, 2.8808e+03, 2.8327e+03, 2.8146e+03,
        2.7948e+03, 2.7850e+03, 2.7497e+03, 2.7301e+03, 2.7198e+03, 2.6956e+03,
        2.6857e+03, 2.6643e+03, 2.6303e+03, 2.6252e+03, 2.5998e+03, 2.5790e+03,
        2.5555e+03, 2.5258e+03, 2.4935e+03, 2.4768e+03, 2.4645e+03, 2.4393e+03,
        2.4266e+03, 2.4082e+03, 2.3939e+03, 2.3823e+03, 2.3582e+03, 2.3385e+03,
        2.3249e+03, 2.3028e+03, 2.2912e+03, 2.2724e+03, 2.2545e+03, 2.2385e+03,
        2.2237e+03, 2.1961e+03, 2.1903e+03, 2.1681e+03, 2.1627e+03, 2.1322e+03,
        2.1312e+03, 2.1193e+03, 2.1072e+03, 2.0846e+03, 2.0736e+03, 2.0649e+03,
        2.0458e+03, 2.0402e+03, 2.0272e+03, 2.0150e+03, 1.9952e+03, 1.9757e+03,
        1.9664e+03, 1.9503e+03, 1.9453e+03, 1.9298e+03, 1.9214e+03, 1.9129e+03,
        1.8907e+03, 1.8821e+03, 1.8705e+03, 1.8671e+03, 1.8356e+03, 1.8284e+03,
        1.8095e+03, 1.8057e+03, 1.7881e+03, 1.7816e+03, 1.7761e+03, 1.7722e+03,
        1.7609e+03, 1.7424e+03, 1.7240e+03, 1.7174e+03, 1.7128e+03, 1.6994e+03,
        1.6724e+03, 1.6654e+03, 1.6509e+03, 1.6499e+03, 1.6442e+03, 1.6294e+03,
        1.6239e+03, 1.6147e+03, 1.6031e+03, 1.5925e+03, 1.5843e+03, 1.5749e+03,
        1.5605e+03, 1.5508e+03, 1.5457e+03, 1.5327e+03, 1.5181e+03, 1.5096e+03,
        1.5075e+03, 1.4976e+03, 1.4882e+03, 1.4761e+03, 1.4710e+03, 1.4666e+03,
        1.4579e+03, 1.4508e+03, 1.4403e+03, 1.4355e+03, 1.4176e+03, 1.4119e+03,
        1.4050e+03, 1.3912e+03, 1.3805e+03, 1.3727e+03, 1.3667e+03, 1.3556e+03,
        1.3502e+03, 1.3424e+03, 1.3325e+03, 1.3240e+03, 1.3223e+03, 1.3090e+03,
        1.3055e+03, 1.3017e+03, 1.2927e+03, 1.2835e+03, 1.2745e+03, 1.2664e+03,
        1.2604e+03, 1.2522e+03, 1.2424e+03, 1.2401e+03, 1.2392e+03, 1.2318e+03,
        1.2211e+03, 1.2168e+03, 1.2104e+03, 1.2075e+03, 1.1955e+03, 1.1910e+03,
        1.1859e+03, 1.1786e+03, 1.1689e+03, 1.1615e+03, 1.1583e+03, 1.1502e+03,
        1.1440e+03, 1.1374e+03, 1.1286e+03, 1.1211e+03, 1.1186e+03, 1.1085e+03,
        1.1054e+03, 1.1037e+03, 1.0971e+03, 1.0930e+03, 1.0868e+03, 1.0767e+03,
        1.0732e+03, 1.0654e+03, 1.0637e+03, 1.0576e+03, 1.0538e+03, 1.0493e+03,
        1.0434e+03, 1.0375e+03, 1.0243e+03, 1.0212e+03, 1.0183e+03, 1.0108e+03,
        1.0036e+03, 9.9763e+02, 9.9145e+02, 9.8508e+02, 9.8232e+02, 9.7749e+02,
        9.7180e+02, 9.6697e+02, 9.6013e+02, 9.5651e+02, 9.5282e+02, 9.4580e+02,
        9.4370e+02, 9.3589e+02, 9.3182e+02, 9.2873e+02, 9.2241e+02, 9.1558e+02,
        9.1346e+02, 9.1039e+02, 9.0439e+02, 9.0180e+02, 8.9535e+02, 8.8738e+02,
        8.8617e+02, 8.7983e+02, 8.7765e+02, 8.7402e+02, 8.7135e+02, 8.6817e+02,
        8.6074e+02, 8.5369e+02, 8.5073e+02, 8.4143e+02, 8.3895e+02, 8.3418e+02,
        8.2611e+02, 8.2303e+02, 8.1838e+02, 8.1399e+02, 8.1179e+02, 8.0915e+02,
        8.0836e+02, 8.0056e+02, 7.9701e+02, 7.9603e+02, 7.9143e+02, 7.8494e+02,
        7.7757e+02, 7.7257e+02, 7.7176e+02, 7.6719e+02, 7.6588e+02, 7.5744e+02,
        7.5671e+02, 7.5052e+02, 7.4499e+02, 7.4104e+02, 7.3980e+02, 7.3437e+02,
        7.3009e+02, 7.2650e+02, 7.1833e+02, 7.1686e+02, 7.1158e+02, 7.0663e+02,
        7.0473e+02, 7.0028e+02, 6.9963e+02, 6.9587e+02, 6.9462e+02, 6.9161e+02,
        6.8594e+02, 6.7889e+02, 6.7356e+02, 6.7171e+02, 6.6774e+02, 6.6559e+02,
        6.6433e+02, 6.5464e+02, 6.5296e+02, 6.4945e+02, 6.4745e+02, 6.4076e+02,
        6.3554e+02, 6.3309e+02, 6.3198e+02, 6.2760e+02, 6.2343e+02, 6.2248e+02,
        6.1470e+02, 6.1204e+02, 6.0948e+02, 6.0612e+02, 6.0478e+02, 6.0175e+02,
        6.0050e+02, 5.9631e+02, 5.9255e+02, 5.8876e+02, 5.8431e+02, 5.8360e+02,
        5.7676e+02, 5.7502e+02, 5.7056e+02, 5.6837e+02, 5.6386e+02, 5.6156e+02,
        5.5610e+02, 5.5194e+02, 5.5047e+02, 5.4822e+02, 5.4441e+02, 5.4137e+02,
        5.3303e+02, 5.3214e+02, 5.2902e+02, 5.2753e+02, 5.2279e+02, 5.2132e+02,
        5.1887e+02, 5.1631e+02, 5.1089e+02, 5.0826e+02, 5.0356e+02, 5.0067e+02,
        4.9526e+02, 4.9153e+02, 4.9078e+02, 4.8821e+02, 4.8441e+02, 4.8135e+02,
        4.7700e+02, 4.6971e+02, 4.6928e+02, 4.6718e+02, 4.6666e+02, 4.6075e+02,
        4.5634e+02, 4.5168e+02, 4.4940e+02, 4.4773e+02, 4.4441e+02, 4.4347e+02,
        4.3912e+02, 4.3781e+02, 4.3516e+02, 4.3322e+02, 4.2736e+02, 4.2134e+02,
        4.1918e+02, 4.1854e+02, 4.1570e+02, 4.1180e+02, 4.0749e+02, 4.0591e+02,
        4.0296e+02, 4.0086e+02, 3.9641e+02, 3.9130e+02, 3.8901e+02, 3.8361e+02,
        3.8248e+02, 3.7818e+02, 3.7111e+02, 3.7065e+02, 3.6738e+02, 3.6506e+02,
        3.5908e+02, 3.5731e+02, 3.5482e+02, 3.5062e+02, 3.4991e+02, 3.4687e+02,
        3.4460e+02, 3.4171e+02, 3.4059e+02, 3.3600e+02, 3.3430e+02, 3.2815e+02,
        3.2446e+02, 3.2215e+02, 3.2052e+02, 3.1915e+02, 3.1262e+02, 3.1111e+02,
        3.0829e+02, 3.0750e+02, 3.0392e+02, 3.0146e+02, 2.9538e+02, 2.9149e+02,
        2.8695e+02, 2.8239e+02, 2.7968e+02, 2.7420e+02, 2.7234e+02, 2.7142e+02,
        2.6514e+02, 2.6304e+02, 2.5695e+02, 2.5613e+02, 2.5345e+02, 2.4687e+02,
        2.4157e+02, 2.3843e+02, 2.3210e+02, 2.2399e+02, 2.2076e+02, 2.1666e+02,
        2.1089e+02, 2.0580e+02, 2.0155e+02, 1.9235e+02, 1.8660e+02, 1.7979e+02],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 316]) 

NULL SPACE BASIS :  tensor([[-0.0061,  0.0215,  0.0806,  ...,  0.0169, -0.0125,  0.0021],
        [ 0.0589,  0.0160, -0.0206,  ..., -0.0250,  0.0287,  0.0117],
        [ 0.0373,  0.0114, -0.0603,  ...,  0.0161, -0.0153, -0.0085],
        ...,
        [ 0.0591, -0.0915, -0.0155,  ..., -0.0180,  0.0102,  0.0292],
        [ 0.0385, -0.0147, -0.0175,  ...,  0.0081, -0.0045, -0.0550],
        [ 0.0697, -0.0640,  0.0364,  ...,  0.0011, -0.0049,  0.0204]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.7531e-02, -9.0207e-03, -2.3753e-03,  ...,  9.1529e-04,
         -1.2577e-04,  2.3831e-05],
        [-9.0207e-03,  3.2017e-02, -9.2661e-03,  ..., -9.3053e-04,
         -3.8417e-04, -8.9387e-06],
        [-2.3753e-03, -9.2661e-03,  2.3741e-02,  ...,  3.0969e-04,
         -8.9909e-04, -1.2944e-03],
        ...,
        [ 9.1529e-04, -9.3053e-04,  3.0969e-04,  ...,  3.2774e-02,
         -8.2108e-03, -3.7390e-03],
        [-1.2577e-04, -3.8417e-04, -8.9909e-04,  ..., -8.2108e-03,
          3.7131e-02, -7.6729e-03],
        [ 2.3831e-05, -8.9387e-06, -1.2944e-03,  ..., -3.7390e-03,
         -7.6729e-03,  3.0477e-02]], device='cuda:0') 

reserving basis 279/576; cond: 501333.9375, radio:0.002179686911404133
PARAMETER       :  Parameter containing:
tensor([[[[-0.0280, -0.0121, -0.0246],
          [ 0.0330, -0.0163, -0.0241],
          [ 0.0161, -0.0068,  0.0452]],

         [[ 0.0354, -0.0077, -0.0266],
          [ 0.0326,  0.0061,  0.0357],
          [ 0.0085, -0.0209,  0.0026]],

         [[-0.0071, -0.0220, -0.0173],
          [ 0.0255, -0.0465,  0.0200],
          [-0.0325,  0.0033, -0.0107]],

         ...,

         [[-0.0173,  0.0152,  0.0254],
          [ 0.0147, -0.0133, -0.0102],
          [ 0.0151, -0.0146, -0.0329]],

         [[ 0.0027,  0.0316,  0.0427],
          [ 0.0040,  0.0245,  0.0240],
          [ 0.0061,  0.0098,  0.0341]],

         [[-0.0405, -0.0228, -0.0131],
          [-0.0215, -0.0224,  0.0230],
          [-0.0357, -0.0022, -0.0377]]],


        [[[ 0.0055, -0.0175, -0.0502],
          [-0.0116, -0.0234, -0.0451],
          [-0.0057, -0.0186,  0.0122]],

         [[-0.0140, -0.0053,  0.0093],
          [ 0.0204,  0.0401,  0.0255],
          [-0.0130, -0.0044,  0.0079]],

         [[-0.0217,  0.0105, -0.0170],
          [ 0.0278, -0.0339,  0.0381],
          [-0.0122,  0.0072, -0.0008]],

         ...,

         [[ 0.0041,  0.0424, -0.0138],
          [-0.0016, -0.0009, -0.0206],
          [-0.0259, -0.0104, -0.0121]],

         [[-0.0515,  0.0033,  0.0148],
          [ 0.0138, -0.0013, -0.0112],
          [ 0.0482, -0.0094,  0.0001]],

         [[-0.0421,  0.0082,  0.0148],
          [-0.0142,  0.0273, -0.0096],
          [ 0.0233,  0.0360,  0.0373]]],


        [[[-0.0388,  0.0225,  0.0134],
          [ 0.0309,  0.0177,  0.0304],
          [ 0.0332,  0.0158,  0.0227]],

         [[-0.0058,  0.0040, -0.0083],
          [ 0.0243, -0.0215,  0.0205],
          [-0.0087, -0.0369, -0.0256]],

         [[ 0.0430, -0.0152, -0.0097],
          [ 0.0103,  0.0363, -0.0053],
          [-0.0225,  0.0127,  0.0387]],

         ...,

         [[ 0.0195, -0.0245,  0.0237],
          [ 0.0431,  0.0123, -0.0203],
          [ 0.0343,  0.0250, -0.0136]],

         [[ 0.0203, -0.0404, -0.0178],
          [-0.0298,  0.0277,  0.0048],
          [ 0.0033,  0.0297,  0.0213]],

         [[-0.0239,  0.0081,  0.0075],
          [-0.0244,  0.0040, -0.0341],
          [ 0.0038,  0.0165,  0.0050]]],


        ...,


        [[[-0.0152, -0.0284, -0.0350],
          [-0.0007,  0.0118, -0.0492],
          [-0.0101, -0.0096, -0.0386]],

         [[-0.0210,  0.0266,  0.0387],
          [-0.0154, -0.0301, -0.0325],
          [-0.0123,  0.0382,  0.0429]],

         [[-0.0028, -0.0185,  0.0403],
          [-0.0057,  0.0298,  0.0393],
          [ 0.0022,  0.0421,  0.0184]],

         ...,

         [[ 0.0305, -0.0433, -0.0358],
          [ 0.0162,  0.0214, -0.0098],
          [ 0.0016, -0.0047,  0.0087]],

         [[ 0.0227, -0.0002,  0.0093],
          [ 0.0266,  0.0091, -0.0402],
          [ 0.0453,  0.0371, -0.0388]],

         [[ 0.0028,  0.0427, -0.0002],
          [ 0.0305, -0.0344, -0.0069],
          [ 0.0033, -0.0331,  0.0182]]],


        [[[ 0.0365, -0.0120,  0.0455],
          [-0.0472,  0.0199, -0.0143],
          [ 0.0305, -0.0137, -0.0206]],

         [[ 0.0267, -0.0227, -0.0128],
          [ 0.0238,  0.0081, -0.0229],
          [ 0.0523, -0.0239,  0.0463]],

         [[-0.0310, -0.0357,  0.0275],
          [ 0.0171, -0.0368, -0.0536],
          [-0.0383, -0.0005, -0.0122]],

         ...,

         [[-0.0198,  0.0192,  0.0320],
          [-0.0353,  0.0096, -0.0092],
          [ 0.0315, -0.0338,  0.0362]],

         [[-0.0050, -0.0179, -0.0323],
          [ 0.0239, -0.0105,  0.0128],
          [-0.0004,  0.0348,  0.0131]],

         [[-0.0304, -0.0145, -0.0418],
          [ 0.0266, -0.0423, -0.0296],
          [-0.0362,  0.0439,  0.0165]]],


        [[[-0.0014, -0.0048,  0.0155],
          [-0.0390, -0.0057, -0.0076],
          [ 0.0250,  0.0044,  0.0371]],

         [[-0.0236,  0.0285,  0.0154],
          [ 0.0390,  0.0264,  0.0193],
          [-0.0051,  0.0236,  0.0116]],

         [[-0.0149, -0.0270,  0.0352],
          [-0.0375,  0.0198,  0.0168],
          [ 0.0038, -0.0267, -0.0329]],

         ...,

         [[ 0.0514, -0.0200,  0.0275],
          [ 0.0334, -0.0390,  0.0274],
          [-0.0079, -0.0419, -0.0197]],

         [[-0.0161, -0.0008, -0.0189],
          [-0.0309, -0.0482, -0.0387],
          [ 0.0219,  0.0190, -0.0144]],

         [[-0.0086, -0.0136,  0.0475],
          [ 0.0232,  0.0181,  0.0024],
          [ 0.0118,  0.0306, -0.0008]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([9.5418e+06, 4.7165e+05, 4.2954e+05, 2.6887e+05, 1.9715e+05, 1.1564e+05,
        1.0765e+05, 7.1477e+04, 5.7407e+04, 5.1588e+04, 3.4069e+04, 3.1293e+04,
        2.6633e+04, 2.4402e+04, 2.2013e+04, 2.0414e+04, 1.9102e+04, 1.8483e+04,
        1.7215e+04, 1.5865e+04, 1.5492e+04, 1.3711e+04, 1.2974e+04, 1.2008e+04,
        1.1558e+04, 1.0964e+04, 1.0165e+04, 9.8998e+03, 9.0202e+03, 8.6700e+03,
        8.1625e+03, 7.9675e+03, 7.5595e+03, 7.0197e+03, 6.6413e+03, 6.5577e+03,
        5.9457e+03, 5.6190e+03, 5.2692e+03, 5.0161e+03, 4.5748e+03, 4.4953e+03,
        4.3954e+03, 4.2442e+03, 4.2125e+03, 4.0566e+03, 3.8627e+03, 3.6660e+03,
        3.5226e+03, 3.4804e+03, 3.4166e+03, 3.1838e+03, 3.0169e+03, 2.9660e+03,
        2.8780e+03, 2.8151e+03, 2.7390e+03, 2.6806e+03, 2.6482e+03, 2.5264e+03,
        2.5113e+03, 2.4475e+03, 2.4107e+03, 2.3986e+03, 2.3005e+03, 2.2323e+03,
        2.2040e+03, 2.1681e+03, 2.1155e+03, 2.0701e+03, 1.9873e+03, 1.9720e+03,
        1.9622e+03, 1.8675e+03, 1.8181e+03, 1.7963e+03, 1.7683e+03, 1.7261e+03,
        1.6797e+03, 1.6558e+03, 1.6456e+03, 1.5951e+03, 1.5739e+03, 1.4891e+03,
        1.4760e+03, 1.4684e+03, 1.4166e+03, 1.3993e+03, 1.3777e+03, 1.3384e+03,
        1.3363e+03, 1.3295e+03, 1.3220e+03, 1.2792e+03, 1.2737e+03, 1.2395e+03,
        1.2085e+03, 1.1878e+03, 1.1505e+03, 1.1412e+03, 1.1359e+03, 1.1302e+03,
        1.0768e+03, 1.0736e+03, 1.0555e+03, 1.0461e+03, 1.0324e+03, 1.0017e+03,
        9.9234e+02, 9.9015e+02, 9.8474e+02, 9.7338e+02, 9.4898e+02, 9.2335e+02,
        9.1543e+02, 9.0998e+02, 8.9690e+02, 8.7741e+02, 8.6538e+02, 8.5360e+02,
        8.4485e+02, 8.4228e+02, 8.2406e+02, 8.0214e+02, 7.9783e+02, 7.9456e+02,
        7.7911e+02, 7.7344e+02, 7.6664e+02, 7.5976e+02, 7.4948e+02, 7.4418e+02,
        7.3214e+02, 7.3067e+02, 7.2335e+02, 7.1508e+02, 7.0348e+02, 6.9680e+02,
        6.8590e+02, 6.7493e+02, 6.7060e+02, 6.6009e+02, 6.5179e+02, 6.4055e+02,
        6.3847e+02, 6.3172e+02, 6.2339e+02, 6.2073e+02, 6.1214e+02, 6.0935e+02,
        6.0207e+02, 5.9106e+02, 5.8705e+02, 5.7593e+02, 5.7395e+02, 5.6947e+02,
        5.6347e+02, 5.5792e+02, 5.4825e+02, 5.4686e+02, 5.3900e+02, 5.3738e+02,
        5.2815e+02, 5.2371e+02, 5.2152e+02, 5.1698e+02, 5.0621e+02, 5.0432e+02,
        5.0043e+02, 4.9500e+02, 4.9388e+02, 4.8866e+02, 4.8717e+02, 4.7863e+02,
        4.7270e+02, 4.6859e+02, 4.6551e+02, 4.5929e+02, 4.5406e+02, 4.5217e+02,
        4.4934e+02, 4.4536e+02, 4.3894e+02, 4.3637e+02, 4.2930e+02, 4.2675e+02,
        4.2568e+02, 4.2007e+02, 4.1640e+02, 4.1270e+02, 4.0931e+02, 4.0756e+02,
        4.0226e+02, 3.9649e+02, 3.9386e+02, 3.9362e+02, 3.8778e+02, 3.8252e+02,
        3.8143e+02, 3.7886e+02, 3.7633e+02, 3.7083e+02, 3.6965e+02, 3.6600e+02,
        3.6448e+02, 3.6322e+02, 3.6147e+02, 3.5662e+02, 3.5354e+02, 3.5054e+02,
        3.4494e+02, 3.4325e+02, 3.4047e+02, 3.3847e+02, 3.3779e+02, 3.3547e+02,
        3.3075e+02, 3.2971e+02, 3.2747e+02, 3.2557e+02, 3.2382e+02, 3.2273e+02,
        3.2147e+02, 3.1769e+02, 3.1629e+02, 3.1435e+02, 3.1302e+02, 3.0504e+02,
        3.0331e+02, 3.0216e+02, 2.9901e+02, 2.9847e+02, 2.9332e+02, 2.9205e+02,
        2.9077e+02, 2.8840e+02, 2.8733e+02, 2.8679e+02, 2.8451e+02, 2.8296e+02,
        2.7758e+02, 2.7706e+02, 2.7517e+02, 2.7461e+02, 2.7019e+02, 2.6833e+02,
        2.6596e+02, 2.6405e+02, 2.6354e+02, 2.6204e+02, 2.5889e+02, 2.5611e+02,
        2.5392e+02, 2.5252e+02, 2.5166e+02, 2.5118e+02, 2.4847e+02, 2.4593e+02,
        2.4337e+02, 2.4267e+02, 2.4010e+02, 2.3940e+02, 2.3778e+02, 2.3571e+02,
        2.3464e+02, 2.3196e+02, 2.3102e+02, 2.2951e+02, 2.2867e+02, 2.2754e+02,
        2.2720e+02, 2.2665e+02, 2.2348e+02, 2.2036e+02, 2.1981e+02, 2.1874e+02,
        2.1782e+02, 2.1597e+02, 2.1510e+02, 2.1343e+02, 2.1294e+02, 2.1132e+02,
        2.0989e+02, 2.0936e+02, 2.0703e+02, 2.0666e+02, 2.0490e+02, 2.0238e+02,
        2.0216e+02, 2.0084e+02, 1.9987e+02, 1.9958e+02, 1.9666e+02, 1.9582e+02,
        1.9521e+02, 1.9436e+02, 1.9123e+02, 1.8994e+02, 1.8965e+02, 1.8847e+02,
        1.8781e+02, 1.8709e+02, 1.8571e+02, 1.8454e+02, 1.8361e+02, 1.8204e+02,
        1.8060e+02, 1.7966e+02, 1.7852e+02, 1.7772e+02, 1.7689e+02, 1.7575e+02,
        1.7497e+02, 1.7434e+02, 1.7216e+02, 1.7203e+02, 1.7100e+02, 1.7003e+02,
        1.6868e+02, 1.6803e+02, 1.6681e+02, 1.6607e+02, 1.6514e+02, 1.6449e+02,
        1.6412e+02, 1.6291e+02, 1.6167e+02, 1.5975e+02, 1.5887e+02, 1.5784e+02,
        1.5671e+02, 1.5615e+02, 1.5547e+02, 1.5490e+02, 1.5444e+02, 1.5223e+02,
        1.5189e+02, 1.5119e+02, 1.5023e+02, 1.5007e+02, 1.4875e+02, 1.4781e+02,
        1.4604e+02, 1.4514e+02, 1.4481e+02, 1.4329e+02, 1.4205e+02, 1.4122e+02,
        1.4099e+02, 1.4004e+02, 1.3984e+02, 1.3817e+02, 1.3727e+02, 1.3665e+02,
        1.3514e+02, 1.3476e+02, 1.3419e+02, 1.3348e+02, 1.3315e+02, 1.3292e+02,
        1.3211e+02, 1.3071e+02, 1.3060e+02, 1.3003e+02, 1.2894e+02, 1.2836e+02,
        1.2672e+02, 1.2662e+02, 1.2591e+02, 1.2517e+02, 1.2434e+02, 1.2379e+02,
        1.2323e+02, 1.2277e+02, 1.2198e+02, 1.2161e+02, 1.2047e+02, 1.1956e+02,
        1.1886e+02, 1.1840e+02, 1.1776e+02, 1.1760e+02, 1.1615e+02, 1.1612e+02,
        1.1573e+02, 1.1544e+02, 1.1444e+02, 1.1359e+02, 1.1350e+02, 1.1298e+02,
        1.1215e+02, 1.1143e+02, 1.1065e+02, 1.0994e+02, 1.0954e+02, 1.0856e+02,
        1.0829e+02, 1.0743e+02, 1.0710e+02, 1.0648e+02, 1.0579e+02, 1.0540e+02,
        1.0430e+02, 1.0412e+02, 1.0344e+02, 1.0246e+02, 1.0185e+02, 1.0164e+02,
        1.0089e+02, 1.0037e+02, 9.9187e+01, 9.8747e+01, 9.8453e+01, 9.8156e+01,
        9.7860e+01, 9.7687e+01, 9.7088e+01, 9.6735e+01, 9.6148e+01, 9.5423e+01,
        9.4958e+01, 9.4352e+01, 9.3893e+01, 9.3400e+01, 9.2906e+01, 9.1971e+01,
        9.1364e+01, 9.1277e+01, 9.0454e+01, 9.0397e+01, 9.0026e+01, 8.8582e+01,
        8.8339e+01, 8.7780e+01, 8.7636e+01, 8.7210e+01, 8.6603e+01, 8.6040e+01,
        8.5595e+01, 8.5458e+01, 8.5018e+01, 8.4672e+01, 8.4344e+01, 8.3599e+01,
        8.3178e+01, 8.2747e+01, 8.2542e+01, 8.1841e+01, 8.1678e+01, 8.1071e+01,
        8.0647e+01, 7.9784e+01, 7.9541e+01, 7.9318e+01, 7.8201e+01, 7.7940e+01,
        7.7222e+01, 7.6834e+01, 7.6485e+01, 7.5966e+01, 7.5730e+01, 7.5093e+01,
        7.4899e+01, 7.4155e+01, 7.3731e+01, 7.3432e+01, 7.2469e+01, 7.2388e+01,
        7.2302e+01, 7.1444e+01, 7.1232e+01, 7.0971e+01, 7.0348e+01, 7.0221e+01,
        6.9498e+01, 6.8585e+01, 6.8309e+01, 6.8074e+01, 6.7711e+01, 6.7566e+01,
        6.6904e+01, 6.6386e+01, 6.5654e+01, 6.5201e+01, 6.4805e+01, 6.4522e+01,
        6.4107e+01, 6.3394e+01, 6.2610e+01, 6.2157e+01, 6.1718e+01, 6.1484e+01,
        6.0911e+01, 6.0440e+01, 6.0094e+01, 5.9989e+01, 5.9294e+01, 5.8506e+01,
        5.8242e+01, 5.7755e+01, 5.7501e+01, 5.7280e+01, 5.6732e+01, 5.6587e+01,
        5.6390e+01, 5.5479e+01, 5.5376e+01, 5.4821e+01, 5.4549e+01, 5.3990e+01,
        5.3592e+01, 5.3319e+01, 5.2961e+01, 5.2553e+01, 5.1875e+01, 5.1601e+01,
        5.1139e+01, 5.0868e+01, 5.0590e+01, 5.0079e+01, 4.9687e+01, 4.9501e+01,
        4.9031e+01, 4.8787e+01, 4.8368e+01, 4.8233e+01, 4.7674e+01, 4.7089e+01,
        4.6823e+01, 4.6225e+01, 4.6103e+01, 4.5584e+01, 4.5278e+01, 4.4995e+01,
        4.4739e+01, 4.4452e+01, 4.3969e+01, 4.3339e+01, 4.3021e+01, 4.2520e+01,
        4.2160e+01, 4.1709e+01, 4.1391e+01, 4.0919e+01, 4.0086e+01, 3.9774e+01,
        3.9734e+01, 3.9226e+01, 3.8834e+01, 3.8294e+01, 3.8145e+01, 3.7559e+01,
        3.6450e+01, 3.6123e+01, 3.5963e+01, 3.4804e+01, 3.4469e+01, 3.4107e+01,
        3.3879e+01, 3.3132e+01, 3.2303e+01, 3.2150e+01, 3.0695e+01, 3.0380e+01,
        2.9906e+01, 2.8994e+01, 2.8679e+01, 2.8082e+01, 2.7560e+01, 2.6743e+01,
        2.6224e+01, 2.5750e+01, 2.4723e+01, 2.4176e+01, 2.2587e+01, 1.9033e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 279]) 

NULL SPACE BASIS :  tensor([[-0.0042, -0.0089,  0.0170,  ...,  0.0017,  0.0004,  0.0122],
        [ 0.0282,  0.0034,  0.0583,  ..., -0.0017,  0.0042, -0.0137],
        [ 0.0624,  0.0175, -0.0064,  ...,  0.0029, -0.0081,  0.0028],
        ...,
        [-0.0111,  0.0030, -0.0414,  ...,  0.0022,  0.0032,  0.0018],
        [-0.0222,  0.0045,  0.0735,  ...,  0.0041,  0.0001,  0.0143],
        [ 0.0232, -0.0051, -0.0331,  ..., -0.0002,  0.0009, -0.0127]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.0192e-02, -1.6811e-02,  2.0350e-03,  ..., -1.5450e-03,
          1.4107e-03, -9.4515e-06],
        [-1.6811e-02,  3.1621e-02, -1.4603e-02,  ..., -1.1693e-04,
          1.9999e-05, -5.3368e-04],
        [ 2.0350e-03, -1.4603e-02,  1.7150e-02,  ...,  7.9752e-04,
         -4.0752e-04,  3.2818e-04],
        ...,
        [-1.5450e-03, -1.1693e-04,  7.9752e-04,  ...,  1.3097e-02,
         -7.5458e-03, -8.6692e-04],
        [ 1.4107e-03,  1.9999e-05, -4.0752e-04,  ..., -7.5458e-03,
          1.7432e-02, -6.6675e-03],
        [-9.4515e-06, -5.3368e-04,  3.2818e-04,  ..., -8.6692e-04,
         -6.6675e-03,  1.2977e-02]], device='cuda:0') 

reserving basis 786/1152; cond: 305542.8125, radio:0.00897060427814722
PARAMETER       :  Parameter containing:
tensor([[[[ 1.4720e-02,  3.0304e-02, -1.8034e-02],
          [-8.5733e-04, -1.6494e-02,  9.5482e-03],
          [ 1.8040e-02,  1.8167e-02, -4.3305e-03]],

         [[-1.8856e-02, -2.3206e-02, -2.6715e-02],
          [-2.2373e-02, -2.2539e-02,  6.3913e-03],
          [-1.4863e-02,  9.2843e-03, -2.8949e-02]],

         [[ 1.1748e-03, -1.3824e-02,  4.3421e-03],
          [ 8.0032e-03,  1.5250e-02, -1.5431e-02],
          [-1.4748e-02, -2.5176e-02,  2.6797e-02]],

         ...,

         [[-1.2726e-02,  1.6356e-03,  5.2816e-03],
          [-3.0147e-02, -2.1002e-02, -1.3470e-03],
          [ 6.8407e-03,  1.5309e-02,  2.8903e-02]],

         [[ 9.3765e-03, -3.4089e-02, -1.8516e-02],
          [ 2.2872e-02,  1.5226e-02,  3.9239e-05],
          [ 1.5294e-02, -1.5428e-02,  3.4825e-02]],

         [[ 1.8439e-02,  5.5841e-03,  1.4981e-02],
          [-2.2717e-02, -3.4066e-02, -2.5912e-02],
          [-2.9235e-02,  1.8885e-02, -1.5106e-02]]],


        [[[ 2.5219e-02,  8.7441e-04,  8.0554e-03],
          [-1.4997e-02, -1.2681e-02,  2.2336e-02],
          [-2.6668e-02, -2.9459e-02, -1.5152e-02]],

         [[ 8.0132e-03, -2.4113e-02, -2.3221e-02],
          [ 9.6910e-03, -1.2811e-02,  1.2286e-03],
          [-2.6046e-02,  6.4047e-03, -1.2785e-02]],

         [[ 9.0755e-05, -1.7610e-02, -1.8625e-02],
          [-4.9436e-03,  1.3815e-02, -2.9224e-02],
          [ 2.5602e-02,  1.8511e-02,  2.8507e-03]],

         ...,

         [[ 4.8564e-03,  1.3678e-02, -3.9614e-03],
          [-2.1012e-02,  2.5897e-02,  2.6808e-02],
          [ 1.5001e-02,  2.0352e-02,  2.1220e-02]],

         [[ 2.5099e-02,  1.6129e-03,  7.0563e-03],
          [-8.0308e-03, -2.1733e-02, -2.7214e-02],
          [ 4.1743e-02, -1.9352e-02, -2.9883e-02]],

         [[-1.5255e-02, -1.8366e-02, -3.2616e-02],
          [-2.2824e-02, -1.3297e-02, -8.2894e-03],
          [ 7.4100e-03, -1.0888e-02, -1.6259e-03]]],


        [[[-1.6743e-02,  1.8695e-02, -2.8556e-02],
          [-4.4401e-03,  1.7836e-02, -2.4659e-03],
          [ 5.8210e-04,  2.8150e-02,  1.6486e-02]],

         [[ 4.9005e-03, -1.2227e-02,  2.3402e-02],
          [-4.0489e-03, -9.3521e-03,  5.0228e-03],
          [-2.1423e-02,  4.8774e-03,  8.8278e-03]],

         [[-1.8192e-02, -1.5320e-02, -1.3108e-02],
          [-2.5893e-02, -2.7035e-02, -3.1547e-02],
          [-1.2663e-02, -1.7889e-02,  1.6621e-02]],

         ...,

         [[ 2.6546e-02,  8.1425e-03, -1.7728e-02],
          [-3.0883e-02,  1.4513e-02,  3.8679e-03],
          [ 1.9936e-02, -1.6378e-02, -3.2435e-02]],

         [[-4.0691e-02,  2.3941e-03, -1.1227e-02],
          [-1.6410e-02,  1.7627e-02,  8.8495e-03],
          [-1.3915e-02,  8.3411e-03, -1.4135e-02]],

         [[-1.3799e-02, -1.9307e-02,  1.0672e-02],
          [-4.0107e-03, -1.0044e-02,  5.4209e-03],
          [ 1.0733e-02, -1.7619e-02,  3.0883e-02]]],


        ...,


        [[[ 2.7038e-02,  5.3201e-04,  3.0865e-02],
          [ 2.4367e-02,  1.7800e-02, -3.0033e-02],
          [ 4.1743e-03,  4.9889e-04,  2.7852e-02]],

         [[ 9.4668e-03, -7.6958e-03,  1.2530e-02],
          [ 5.0908e-03, -2.5417e-02,  4.8361e-03],
          [-1.6102e-02, -7.8683e-03,  1.5951e-02]],

         [[ 1.6350e-02,  1.2360e-02, -1.0437e-02],
          [-1.3338e-02,  2.7975e-02,  5.8885e-03],
          [ 6.9719e-03, -6.2190e-03, -1.3748e-02]],

         ...,

         [[-2.9241e-02,  1.4568e-02, -1.4064e-02],
          [ 6.6578e-03, -2.8738e-02, -1.5551e-02],
          [-1.9365e-02,  2.7185e-03,  8.4066e-03]],

         [[ 1.9217e-02,  1.7496e-02,  2.8459e-02],
          [-2.2211e-02,  5.2690e-04,  1.6315e-02],
          [-5.5735e-03,  1.0373e-02,  5.3685e-03]],

         [[-1.4832e-02, -3.1942e-03,  2.3019e-02],
          [ 1.5489e-02,  8.7443e-03,  3.0616e-02],
          [ 5.4709e-03,  8.6165e-03,  1.8870e-02]]],


        [[[-4.2106e-03, -1.1877e-02,  5.0718e-03],
          [-2.8767e-02, -8.5941e-03,  1.9865e-02],
          [ 1.0153e-02, -2.7119e-02, -2.2130e-02]],

         [[-1.2561e-02, -1.5814e-02,  2.4108e-02],
          [-1.9267e-02, -3.5575e-02, -4.9192e-03],
          [-2.4080e-02, -7.6313e-04, -2.2505e-02]],

         [[-1.5978e-02, -2.2917e-03, -2.1142e-02],
          [-1.1983e-02,  1.4275e-02,  2.2009e-02],
          [ 8.1710e-03, -2.0326e-02,  1.9210e-02]],

         ...,

         [[-1.4670e-02,  1.1075e-02,  4.4859e-03],
          [-2.9665e-02, -2.1204e-02, -1.4108e-02],
          [-2.3369e-02, -4.9868e-02, -1.7152e-02]],

         [[-3.1755e-03, -6.7138e-03, -1.2893e-02],
          [-3.0877e-02,  3.0789e-02,  2.9778e-02],
          [-3.6747e-02,  2.2340e-02, -8.1989e-03]],

         [[ 2.1746e-02, -2.2424e-02, -8.5959e-03],
          [ 3.0141e-02, -1.5945e-02,  3.2240e-02],
          [-4.6873e-03,  1.8188e-02,  1.4791e-03]]],


        [[[ 4.1868e-03, -2.6883e-02,  2.9287e-02],
          [-2.4638e-02,  4.1969e-03, -3.6390e-02],
          [-1.8696e-03, -2.8435e-02, -1.8206e-02]],

         [[ 1.5715e-02,  1.4739e-02, -1.5838e-02],
          [ 3.9177e-03,  1.2923e-02,  8.6062e-03],
          [-1.6825e-02, -2.7658e-02,  2.2920e-02]],

         [[-8.5503e-03, -8.6296e-03,  7.1624e-03],
          [-1.8175e-02,  1.9066e-03,  2.6457e-02],
          [-1.6564e-02,  2.4453e-02, -4.4617e-03]],

         ...,

         [[ 2.7349e-02,  1.1364e-02, -7.4931e-03],
          [-1.6873e-02,  3.1095e-02,  1.4829e-02],
          [ 2.8319e-02, -1.0244e-02, -5.1634e-04]],

         [[-1.5602e-02,  1.0070e-02, -4.6099e-03],
          [-2.3127e-02,  3.1782e-02,  2.8284e-02],
          [ 1.6282e-02,  1.2861e-02,  7.7286e-03]],

         [[ 1.1542e-02,  3.8722e-02, -1.7861e-02],
          [-1.3801e-02,  1.2579e-02,  3.2223e-02],
          [ 5.6826e-03, -2.5207e-02, -1.7933e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.8388e+07, 7.0695e+05, 6.7662e+05,  ..., 7.7251e+01, 7.1314e+01,
        6.0180e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 786]) 

NULL SPACE BASIS :  tensor([[-0.0381, -0.0118,  0.0140,  ...,  0.0505, -0.0218,  0.0114],
        [-0.0320,  0.0029, -0.0213,  ..., -0.0849,  0.0277, -0.0045],
        [-0.0150, -0.0032, -0.0245,  ...,  0.0343, -0.0078, -0.0012],
        ...,
        [-0.0070,  0.0087, -0.0018,  ...,  0.0188,  0.0030, -0.0052],
        [-0.0104, -0.0237, -0.0043,  ..., -0.0173, -0.0147, -0.0095],
        [-0.0069,  0.0097,  0.0024,  ..., -0.0025,  0.0143,  0.0123]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.9334e-02, -1.0759e-03, -7.1243e-04,  ..., -4.5841e-04,
          4.1749e-04, -5.0314e-05],
        [-1.0759e-03,  3.0007e-02, -7.8236e-04,  ...,  1.2438e-04,
          2.8110e-04,  1.3689e-04],
        [-7.1243e-04, -7.8236e-04,  2.9616e-02,  ..., -1.0406e-04,
          2.6154e-04,  1.9867e-04],
        ...,
        [-4.5841e-04,  1.2438e-04, -1.0406e-04,  ...,  2.2762e-02,
         -2.7202e-03, -8.0724e-04],
        [ 4.1749e-04,  2.8110e-04,  2.6154e-04,  ..., -2.7202e-03,
          2.3781e-02, -2.6526e-03],
        [-5.0314e-05,  1.3689e-04,  1.9867e-04,  ..., -8.0724e-04,
         -2.6526e-03,  2.4944e-02]], device='cuda:0') 

reserving basis 44/64; cond: 7129.83203125, radio:0.017696522176265717
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0130]],

         [[ 0.0930]],

         [[ 0.0556]],

         ...,

         [[-0.0836]],

         [[-0.0896]],

         [[-0.0502]]],


        [[[ 0.0285]],

         [[-0.0264]],

         [[-0.0903]],

         ...,

         [[-0.0452]],

         [[ 0.0207]],

         [[ 0.0227]]],


        [[[ 0.0270]],

         [[-0.0654]],

         [[-0.0007]],

         ...,

         [[-0.1426]],

         [[ 0.0927]],

         [[ 0.0186]]],


        ...,


        [[[ 0.0301]],

         [[-0.0816]],

         [[-0.1018]],

         ...,

         [[-0.0248]],

         [[ 0.0613]],

         [[-0.0173]]],


        [[[ 0.0150]],

         [[-0.0646]],

         [[ 0.1164]],

         ...,

         [[-0.0042]],

         [[ 0.0333]],

         [[ 0.0667]]],


        [[[ 0.0020]],

         [[ 0.0030]],

         [[ 0.0788]],

         ...,

         [[-0.0049]],

         [[ 0.0588]],

         [[ 0.0887]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.1997e+06, 6.5098e+04, 5.5183e+04, 2.5014e+04, 1.7681e+04, 1.4347e+04,
        1.0050e+04, 7.5693e+03, 5.2742e+03, 5.0720e+03, 4.5405e+03, 3.9957e+03,
        3.3804e+03, 3.0888e+03, 2.7032e+03, 2.6646e+03, 2.3658e+03, 2.3122e+03,
        2.0356e+03, 1.9264e+03, 1.6081e+03, 1.4660e+03, 1.2879e+03, 1.2271e+03,
        1.1774e+03, 1.1581e+03, 1.0838e+03, 9.4670e+02, 9.1958e+02, 8.4341e+02,
        7.8649e+02, 7.4401e+02, 7.2378e+02, 7.0189e+02, 6.6062e+02, 6.2402e+02,
        6.0962e+02, 5.8149e+02, 5.5696e+02, 5.2569e+02, 5.1609e+02, 4.8725e+02,
        4.5476e+02, 4.3970e+02, 4.0598e+02, 4.0435e+02, 3.7355e+02, 3.4645e+02,
        3.3989e+02, 3.3496e+02, 3.2435e+02, 3.2121e+02, 3.0273e+02, 2.8845e+02,
        2.7477e+02, 2.6198e+02, 2.5596e+02, 2.3932e+02, 2.3352e+02, 2.2512e+02,
        2.1075e+02, 1.9789e+02, 1.9362e+02, 1.6826e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 44]) 

NULL SPACE BASIS :  tensor([[ 0.2071,  0.0921, -0.1848,  ...,  0.0595,  0.1362, -0.1099],
        [ 0.0652,  0.0971,  0.1238,  ...,  0.0344, -0.0431, -0.0362],
        [ 0.0557,  0.1598,  0.0333,  ..., -0.2775, -0.0623,  0.0387],
        ...,
        [ 0.0968, -0.1419,  0.0096,  ..., -0.0160, -0.0062,  0.0219],
        [-0.0581, -0.0887, -0.0526,  ...,  0.0075,  0.0420, -0.0327],
        [-0.2470, -0.0047,  0.0036,  ..., -0.0149,  0.0108,  0.0230]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0988,  0.0028,  0.0169,  ...,  0.0005, -0.0039, -0.0128],
        [ 0.0028,  0.1219, -0.0112,  ...,  0.0109, -0.0103, -0.0035],
        [ 0.0169, -0.0112,  0.0962,  ..., -0.0061,  0.0019,  0.0044],
        ...,
        [ 0.0005,  0.0109, -0.0061,  ...,  0.0365, -0.0326, -0.0026],
        [-0.0039, -0.0103,  0.0019,  ..., -0.0326,  0.1130,  0.0038],
        [-0.0128, -0.0035,  0.0044,  ..., -0.0026,  0.0038,  0.0839]],
       device='cuda:0') 

reserving basis 782/1152; cond: 301219.53125, radio:0.009208467788994312
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1852e-02, -3.3030e-03,  9.9312e-03],
          [-3.2207e-03,  1.6133e-02,  1.1077e-02],
          [-1.0460e-02, -1.2602e-02, -1.3432e-02]],

         [[ 4.3022e-03, -1.7720e-02, -2.2518e-02],
          [ 2.6302e-02, -2.1626e-03,  3.5634e-04],
          [ 6.4741e-03,  1.9201e-02, -1.6722e-02]],

         [[ 6.8759e-03, -1.8822e-04,  6.1628e-03],
          [ 1.6760e-02,  1.7157e-02,  1.6146e-02],
          [-2.4084e-02, -1.4251e-02,  1.2813e-02]],

         ...,

         [[ 1.6587e-02,  6.0203e-03, -7.2056e-04],
          [-1.1635e-02,  2.6715e-02,  6.1361e-03],
          [ 1.5137e-02, -1.7358e-03, -6.9268e-03]],

         [[ 8.5242e-03,  1.1921e-02,  6.6666e-03],
          [ 2.3087e-03,  2.2858e-02,  2.0603e-02],
          [ 2.4022e-02,  2.8315e-02, -5.6506e-03]],

         [[ 1.0451e-03,  2.6650e-02,  4.6792e-02],
          [-1.8788e-02,  1.4470e-02,  2.0418e-02],
          [ 5.8907e-03, -2.0170e-02,  1.7244e-02]]],


        [[[-1.1214e-02,  7.2767e-03, -2.1453e-02],
          [ 1.3653e-02, -6.9964e-03,  5.5675e-03],
          [ 2.4959e-03, -2.4239e-02,  1.9517e-02]],

         [[-1.9719e-03,  1.8709e-02,  2.6421e-02],
          [ 2.1243e-02,  1.6746e-02,  3.5309e-04],
          [-8.9536e-03,  1.7051e-02,  1.8970e-02]],

         [[ 1.8771e-02,  1.7575e-02,  9.9206e-03],
          [ 2.6200e-02, -5.2981e-03,  2.1819e-02],
          [-1.6694e-02,  1.3568e-03,  9.5559e-03]],

         ...,

         [[-3.3614e-03,  1.6162e-02, -1.7622e-02],
          [-2.7047e-02, -2.2306e-02, -3.0895e-02],
          [-2.1240e-02, -2.1631e-02, -2.3603e-02]],

         [[-2.2755e-02,  2.1880e-03,  3.1874e-02],
          [-1.5304e-02, -1.7714e-02, -1.8992e-03],
          [ 1.5101e-02, -1.0505e-02, -1.7625e-02]],

         [[ 2.2923e-02, -2.3787e-02,  1.3165e-02],
          [-7.2983e-03, -1.3600e-02, -2.8362e-02],
          [ 2.4053e-03,  1.6342e-02, -3.1780e-02]]],


        [[[ 1.8862e-02,  2.0577e-02,  1.4668e-02],
          [-6.0043e-03, -2.9211e-02,  2.7985e-02],
          [-1.2009e-02, -1.4831e-04, -2.4408e-02]],

         [[-1.0035e-02,  5.8864e-03, -1.5716e-02],
          [ 3.2343e-02,  6.7598e-03, -5.0087e-03],
          [-7.0349e-03, -1.8426e-02,  1.8971e-02]],

         [[-3.0371e-02, -1.5944e-02, -2.2672e-02],
          [-1.5442e-02, -1.5384e-02,  2.0563e-03],
          [ 2.4344e-02, -1.3398e-02, -7.5944e-03]],

         ...,

         [[-3.5614e-02, -6.9160e-03, -2.1495e-02],
          [-7.4286e-03, -3.5012e-02, -2.6928e-02],
          [-4.0178e-03, -2.8016e-02, -1.4031e-03]],

         [[ 2.4989e-02, -1.3537e-02,  5.0224e-03],
          [ 1.9482e-02, -1.7084e-02, -2.0404e-02],
          [-3.6548e-03, -3.0003e-02,  1.0132e-02]],

         [[ 1.1040e-02,  1.3929e-02,  9.4117e-05],
          [ 5.6939e-03,  1.0785e-02, -1.0752e-02],
          [ 4.3456e-03,  2.3999e-02, -1.2034e-02]]],


        ...,


        [[[-1.4774e-02, -2.3790e-02, -3.4177e-02],
          [-1.0637e-02, -1.5409e-02,  1.8031e-03],
          [ 1.5485e-02,  7.3466e-03,  2.3625e-02]],

         [[-3.1489e-02, -7.2308e-03, -2.5200e-02],
          [-1.2685e-02,  1.5265e-02,  1.0821e-02],
          [-1.2670e-02,  3.1610e-02,  2.8284e-02]],

         [[ 2.9856e-02,  6.0323e-03,  6.3320e-04],
          [-1.6136e-02,  2.4894e-03, -2.7580e-02],
          [ 3.1126e-02,  2.3129e-02,  8.6851e-03]],

         ...,

         [[-2.1828e-02, -2.7346e-02,  6.9957e-03],
          [ 1.5893e-02, -1.4623e-02,  1.6156e-02],
          [-1.2100e-02,  3.3709e-02, -6.3305e-03]],

         [[ 1.1592e-02,  3.5240e-02,  1.1226e-02],
          [-1.0862e-02, -1.9121e-02,  2.4872e-02],
          [ 1.6888e-02, -2.1442e-03, -4.5029e-03]],

         [[ 3.7817e-02,  7.9952e-03,  1.7460e-02],
          [ 2.0966e-02, -1.4994e-02,  1.6318e-02],
          [-1.1033e-02, -3.4079e-02, -3.7969e-02]]],


        [[[-2.6925e-02, -1.1873e-03,  1.4196e-02],
          [ 2.1466e-02,  1.4668e-02, -2.0520e-02],
          [ 2.9039e-02,  2.5295e-02, -1.8230e-03]],

         [[ 8.0825e-03, -4.2157e-03,  1.7373e-03],
          [ 2.0200e-02,  3.8399e-02,  1.8473e-02],
          [ 2.2417e-02,  3.7546e-02, -1.1480e-02]],

         [[-2.0211e-02, -1.6649e-02, -2.7665e-02],
          [-1.7458e-02, -1.2395e-03, -9.3687e-03],
          [-2.0585e-03,  5.7215e-03, -2.7825e-02]],

         ...,

         [[-2.7053e-02,  1.6094e-02, -2.5452e-02],
          [ 1.5962e-02, -1.1709e-02, -6.5656e-03],
          [-1.0912e-02,  2.7353e-02,  1.2360e-02]],

         [[-2.7530e-02,  2.1787e-03,  1.4859e-02],
          [-2.1601e-03, -5.1451e-03,  3.4564e-03],
          [ 2.1671e-02,  2.2803e-02,  2.2118e-03]],

         [[-2.8582e-02, -7.3039e-03,  9.3388e-03],
          [-2.6737e-02,  1.2456e-02,  2.7998e-02],
          [-2.9243e-02,  8.7395e-04,  1.4756e-02]]],


        [[[-1.5387e-02,  1.5024e-02, -3.0473e-03],
          [-3.5180e-03,  1.1831e-02,  4.9063e-03],
          [-4.8877e-03, -1.7376e-03,  1.9632e-02]],

         [[-1.0881e-03, -2.1673e-02, -1.8843e-02],
          [-2.4340e-02, -1.2789e-02, -3.8688e-02],
          [ 3.8505e-02,  1.5979e-02,  3.0906e-03]],

         [[ 2.8926e-02,  8.2004e-04, -9.0158e-03],
          [-1.7299e-02, -1.3399e-02, -1.5555e-02],
          [ 2.6028e-02, -7.1955e-03, -2.1075e-02]],

         ...,

         [[ 9.8680e-03, -4.6158e-03,  9.7332e-03],
          [-2.0724e-02, -1.1041e-02,  4.9868e-03],
          [ 1.3357e-02,  5.6526e-05, -1.3441e-02]],

         [[-1.6501e-02, -9.9986e-03,  2.5920e-03],
          [ 1.7108e-02,  1.0572e-02, -3.1918e-02],
          [-1.6493e-02, -6.3649e-03, -9.1759e-03]],

         [[-2.6835e-03, -3.0058e-02, -1.6524e-02],
          [ 2.9947e-03,  2.3608e-02, -2.7377e-02],
          [ 2.5623e-02,  1.2237e-02,  2.1475e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.7878e+07, 6.9533e+05, 6.6297e+05,  ..., 7.3723e+01, 7.2282e+01,
        5.9353e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 782]) 

NULL SPACE BASIS :  tensor([[ 0.0236,  0.0180, -0.0220,  ...,  0.0177, -0.0033,  0.0179],
        [ 0.0358, -0.0252,  0.0164,  ..., -0.0322,  0.0062, -0.0162],
        [-0.0065,  0.0039,  0.0218,  ...,  0.0074, -0.0069,  0.0064],
        ...,
        [ 0.0252, -0.0289,  0.0218,  ..., -0.0091,  0.0025, -0.0014],
        [-0.0209, -0.0054,  0.0396,  ...,  0.0035,  0.0030,  0.0035],
        [-0.0503, -0.0603,  0.0376,  ...,  0.0025, -0.0040,  0.0002]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2098e-02, -5.6353e-03, -2.6952e-03,  ..., -3.8999e-04,
         -5.2432e-04,  6.7676e-05],
        [-5.6353e-03,  2.3286e-02, -5.2953e-03,  ..., -2.3396e-05,
          7.0545e-05, -2.5363e-04],
        [-2.6952e-03, -5.2953e-03,  2.1893e-02,  ..., -1.0941e-04,
         -5.2489e-06,  7.9562e-05],
        ...,
        [-3.8999e-04, -2.3396e-05, -1.0941e-04,  ...,  1.9861e-02,
         -6.3772e-03, -2.7386e-03],
        [-5.2432e-04,  7.0545e-05, -5.2489e-06,  ..., -6.3772e-03,
          2.0510e-02, -6.4676e-03],
        [ 6.7676e-05, -2.5363e-04,  7.9562e-05,  ..., -2.7386e-03,
         -6.4676e-03,  2.0215e-02]], device='cuda:0') 

reserving basis 527/1152; cond: 522624.03125, radio:0.0045410506427288055
PARAMETER       :  Parameter containing:
tensor([[[[ 1.9517e-03, -1.3015e-02, -7.0720e-03],
          [ 1.7970e-02,  1.7741e-02,  3.7480e-02],
          [-1.1521e-02, -5.8649e-03,  1.6019e-02]],

         [[-1.6152e-02,  1.9408e-02, -9.8012e-03],
          [-1.1990e-02, -2.3538e-02, -2.5410e-02],
          [-3.6244e-04, -1.6137e-02, -3.3201e-03]],

         [[-2.0163e-02, -1.4195e-02,  3.0675e-02],
          [ 3.7804e-02,  2.4372e-02,  7.0211e-05],
          [ 5.2753e-03,  6.7314e-03, -6.4363e-03]],

         ...,

         [[ 3.7977e-02,  1.2790e-02, -6.0475e-03],
          [ 4.0015e-02,  1.5350e-02,  1.6051e-02],
          [-1.5794e-02,  1.4500e-03, -1.5776e-02]],

         [[ 4.3012e-02,  5.0023e-03,  2.8854e-02],
          [ 3.0735e-02,  1.3101e-02,  1.9715e-02],
          [-2.2411e-02, -3.6962e-03,  1.9145e-02]],

         [[ 1.1332e-02, -1.3414e-02, -1.4063e-02],
          [ 2.0583e-02,  1.8511e-02,  3.7750e-02],
          [-1.4765e-02,  1.4769e-02,  9.4990e-04]]],


        [[[ 1.9760e-02,  3.6243e-02, -2.5876e-02],
          [ 2.4271e-02, -3.5621e-03, -2.7495e-02],
          [ 8.9845e-03, -2.5035e-02, -1.8511e-02]],

         [[-3.1374e-02, -3.4353e-02, -4.2170e-02],
          [-1.9696e-02, -2.3521e-02, -7.0285e-03],
          [ 1.8098e-02, -1.4026e-02,  5.5636e-03]],

         [[ 1.1176e-02,  1.8845e-02,  2.8239e-02],
          [-4.1333e-03,  1.3906e-02, -6.0333e-03],
          [ 1.2579e-02, -9.2397e-03, -2.9879e-02]],

         ...,

         [[-1.0599e-02,  3.0310e-02, -1.6722e-02],
          [ 3.1504e-02, -8.0476e-03,  1.1912e-02],
          [ 9.1677e-03,  1.8721e-02, -2.0661e-02]],

         [[-2.5662e-02,  1.8795e-02, -1.8144e-02],
          [-1.6231e-02, -3.1908e-03, -2.0204e-02],
          [ 2.8812e-02, -9.8266e-03,  2.1041e-02]],

         [[ 1.3131e-02, -2.5800e-02,  3.4032e-02],
          [-1.9258e-02,  2.2733e-02,  1.1817e-03],
          [ 1.5443e-02, -6.1087e-04, -7.5871e-03]]],


        [[[-1.9707e-03, -1.0898e-03,  9.1071e-03],
          [-2.0409e-02,  6.1544e-03, -1.7450e-02],
          [-1.6288e-02,  1.8795e-03, -5.3456e-03]],

         [[ 1.3995e-02, -1.6946e-02, -1.5017e-02],
          [ 2.5841e-02,  9.1353e-05,  3.0952e-02],
          [ 3.3631e-02,  2.0290e-02, -1.4566e-02]],

         [[-1.4739e-02,  1.0760e-02, -2.4577e-02],
          [ 6.7066e-03,  1.7653e-02,  2.4731e-03],
          [-1.7909e-04, -6.5383e-03,  3.0887e-02]],

         ...,

         [[ 1.8875e-02,  2.3195e-02,  1.6463e-02],
          [-1.6518e-02, -2.5227e-02,  4.0756e-03],
          [ 2.8009e-03,  1.7485e-02, -1.0302e-02]],

         [[ 1.5659e-02,  1.6768e-02,  8.5465e-03],
          [ 8.3921e-03, -9.4170e-03, -2.0408e-02],
          [ 4.2285e-03,  1.0268e-02,  3.1842e-02]],

         [[-1.2582e-02, -1.4819e-02, -4.1656e-03],
          [-2.0369e-02,  2.2188e-03,  6.2241e-04],
          [-1.9931e-03,  6.8544e-03,  4.0919e-02]]],


        ...,


        [[[-9.9913e-03, -2.4475e-02,  1.1692e-02],
          [ 1.9445e-02, -7.3195e-03,  1.6928e-02],
          [ 2.7645e-03, -1.5557e-02,  2.4519e-02]],

         [[-2.2393e-02, -2.3676e-02, -8.5226e-03],
          [-1.9310e-02, -1.7965e-02,  1.8488e-03],
          [-3.2787e-02,  2.3312e-02, -5.9737e-03]],

         [[-6.0614e-03,  3.9040e-03, -5.9084e-03],
          [-6.9583e-03,  1.9301e-02,  3.0813e-03],
          [ 3.0475e-02,  9.9986e-03,  1.8460e-02]],

         ...,

         [[ 3.6480e-04, -1.2155e-02,  3.9176e-02],
          [-1.7187e-03,  2.0640e-02,  2.1013e-02],
          [ 1.4444e-02,  1.0209e-02,  9.0121e-03]],

         [[-2.1089e-02,  1.9049e-03, -5.6369e-03],
          [-1.7368e-02,  1.2621e-02,  5.2466e-03],
          [ 1.2271e-02, -1.4626e-03,  2.0088e-02]],

         [[-4.1194e-03, -5.4749e-03, -9.4965e-03],
          [ 5.6076e-03,  2.3257e-02, -1.7679e-02],
          [-1.7910e-03, -1.3240e-02, -5.6354e-03]]],


        [[[-8.5039e-03,  3.8514e-02,  1.0790e-02],
          [-2.2119e-03, -5.9153e-04, -8.6613e-04],
          [-2.3534e-02,  2.3702e-02,  2.2642e-02]],

         [[ 1.6134e-02, -8.5798e-03, -1.3544e-04],
          [-3.4210e-03,  3.2561e-02,  1.1769e-02],
          [ 5.2589e-03, -7.9613e-03, -2.4739e-02]],

         [[-8.7286e-03, -1.4189e-02,  6.3472e-03],
          [-1.3328e-02, -3.2160e-02, -4.0769e-03],
          [ 1.5351e-02, -1.4582e-02,  2.6911e-02]],

         ...,

         [[-2.1229e-03,  1.1657e-02, -2.0250e-02],
          [ 1.5669e-02,  7.3003e-04,  1.7126e-02],
          [ 3.0229e-02,  1.2639e-02,  2.3954e-02]],

         [[ 9.5969e-03, -2.7844e-02,  3.4091e-03],
          [-1.6901e-02,  1.1611e-03, -1.9684e-02],
          [ 1.7730e-02,  1.9535e-02,  2.4501e-02]],

         [[-3.9010e-03, -3.0144e-02, -1.9111e-02],
          [ 1.2478e-02,  1.0607e-02,  3.1447e-03],
          [ 1.6046e-02,  5.3619e-03, -2.8073e-02]]],


        [[[ 2.2199e-02, -2.5854e-02,  4.0907e-04],
          [ 1.1186e-02,  7.4504e-03, -2.8441e-02],
          [ 2.0827e-02,  2.4520e-02,  1.1606e-02]],

         [[-4.1249e-02, -1.7273e-02, -3.0792e-02],
          [ 2.2435e-04,  1.2920e-02, -1.3308e-03],
          [ 3.8712e-03, -2.6811e-02,  1.0623e-02]],

         [[-1.0232e-02,  9.8694e-04, -7.9494e-03],
          [-2.5340e-02, -1.6260e-02, -2.1570e-02],
          [ 2.3224e-03, -1.0793e-02,  1.5185e-02]],

         ...,

         [[-1.4962e-02, -2.8069e-02, -2.8283e-02],
          [ 1.0826e-02,  1.6161e-03,  9.2637e-04],
          [ 1.9040e-02, -1.5551e-02, -1.5073e-02]],

         [[-3.4500e-03, -1.7583e-02, -7.0560e-03],
          [ 2.4483e-03,  4.3045e-03,  1.8819e-02],
          [ 1.4080e-03,  1.9157e-03,  3.0015e-03]],

         [[ 2.0588e-02,  1.2730e-02,  9.2798e-04],
          [ 2.0770e-02, -7.2276e-03,  2.0653e-02],
          [-1.4802e-02, -1.5269e-02,  1.4324e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.9307e+07, 7.2490e+05, 6.4195e+05,  ..., 4.5564e+01, 4.1649e+01,
        3.6942e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 527]) 

NULL SPACE BASIS :  tensor([[-3.3481e-02,  6.1886e-02,  4.8620e-03,  ..., -5.0128e-03,
         -5.3743e-04, -4.8475e-04],
        [ 5.6259e-02,  1.1603e-02, -3.8467e-02,  ...,  3.7817e-04,
          3.4831e-03, -2.4347e-03],
        [ 1.8790e-02, -2.1835e-02, -3.4532e-04,  ...,  2.3592e-03,
          2.2684e-04,  3.4234e-03],
        ...,
        [ 3.7322e-02, -1.4447e-02, -2.3604e-04,  ...,  1.2240e-02,
          2.6953e-05, -1.2203e-03],
        [ 1.9889e-02, -6.2441e-03,  4.0722e-02,  ..., -7.6267e-03,
          3.5023e-03,  2.1890e-03],
        [ 1.3385e-02, -1.4838e-02,  2.0359e-03,  ...,  8.8649e-04,
          2.4641e-03, -2.9043e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.7372e-03, -2.1748e-03, -8.9156e-04,  ..., -2.4143e-04,
         -3.1746e-04,  2.5152e-04],
        [-2.1748e-03,  8.8634e-03, -1.7919e-03,  ..., -2.3407e-04,
          2.6531e-04,  3.9930e-05],
        [-8.9156e-04, -1.7919e-03,  7.2730e-03,  ..., -3.4183e-04,
         -7.2013e-04, -8.6345e-05],
        ...,
        [-2.4143e-04, -2.3407e-04, -3.4183e-04,  ...,  2.9979e-02,
         -6.5509e-03, -2.4906e-03],
        [-3.1746e-04,  2.6531e-04, -7.2013e-04,  ..., -6.5509e-03,
          2.9220e-02, -5.8887e-03],
        [ 2.5152e-04,  3.9930e-05, -8.6345e-05,  ..., -2.4906e-03,
         -5.8887e-03,  2.9036e-02]], device='cuda:0') 

reserving basis 782/1152; cond: 332944.96875, radio:0.00843292661011219
PARAMETER       :  Parameter containing:
tensor([[[[-1.9818e-02, -1.4734e-02,  4.1590e-02],
          [ 2.1680e-02,  8.0616e-03, -2.5604e-03],
          [ 1.6376e-02,  2.1149e-02, -2.7982e-02]],

         [[ 7.0477e-03,  9.1685e-04,  3.3321e-02],
          [-2.3590e-03,  1.6828e-02,  2.9884e-02],
          [ 2.6908e-02, -2.4468e-02, -7.0001e-03]],

         [[ 1.7784e-02,  5.9511e-03,  8.2421e-03],
          [-5.7525e-03, -8.2537e-03,  2.5745e-02],
          [ 1.0997e-02,  1.9963e-02,  2.5791e-02]],

         ...,

         [[ 7.0692e-03,  1.5975e-02,  2.6293e-02],
          [ 3.3342e-02, -2.0350e-02,  2.3721e-02],
          [ 3.2763e-04, -1.4035e-02, -1.3414e-02]],

         [[-7.6618e-03, -7.2721e-03, -1.9133e-02],
          [-2.2539e-03,  3.7494e-03,  5.0541e-03],
          [ 1.4235e-03, -8.6685e-04, -7.7147e-03]],

         [[-3.8054e-02,  8.9048e-03, -1.7744e-02],
          [ 1.0089e-02,  2.6433e-03,  1.7409e-02],
          [ 2.6448e-02, -4.5321e-03,  2.3669e-02]]],


        [[[ 5.2460e-03, -3.4116e-03,  1.5527e-02],
          [ 2.0732e-02,  1.2692e-02, -1.7215e-03],
          [-3.1012e-03, -1.0391e-02, -3.1832e-03]],

         [[ 3.2208e-03,  1.2158e-02,  1.7012e-02],
          [-2.5598e-02, -2.3712e-03, -2.5062e-02],
          [-1.3792e-02,  1.9023e-02, -2.3478e-03]],

         [[-1.8184e-02, -6.2819e-03,  2.7495e-02],
          [ 3.2229e-03, -1.1608e-02, -1.1533e-02],
          [ 1.5221e-04,  1.2606e-02, -1.3150e-02]],

         ...,

         [[-2.0691e-02, -1.6482e-02, -2.1924e-02],
          [-2.6079e-02,  7.6880e-03, -2.0977e-02],
          [ 3.2485e-02,  1.8593e-03, -3.4773e-02]],

         [[ 1.7844e-02,  2.6551e-02,  3.4469e-02],
          [-1.4206e-02,  1.9557e-02, -9.7648e-03],
          [ 2.2607e-02,  1.9400e-02,  4.0581e-02]],

         [[ 2.1114e-04,  3.3354e-03,  5.8807e-04],
          [-4.5002e-03, -9.1688e-03,  2.0900e-02],
          [ 6.3172e-03,  4.6939e-02,  4.2384e-02]]],


        [[[-2.0986e-02, -2.1087e-02,  4.1724e-03],
          [-2.2975e-02,  3.9274e-03,  1.0868e-03],
          [ 5.7735e-03,  3.0955e-03,  4.7431e-03]],

         [[-1.7099e-03,  8.1458e-03,  9.7128e-03],
          [-2.1760e-04,  2.2407e-02,  6.9323e-03],
          [ 2.0416e-03,  3.3739e-02, -1.8919e-02]],

         [[-4.5865e-03, -1.7889e-02, -2.2129e-02],
          [-2.2426e-02, -2.9682e-02, -5.6924e-03],
          [-7.5064e-03, -2.8187e-02, -1.0617e-02]],

         ...,

         [[ 1.2465e-02,  1.0491e-02,  4.1657e-02],
          [-2.2288e-02,  7.8540e-03, -1.6504e-02],
          [-1.8931e-02,  2.8679e-02, -2.2986e-02]],

         [[-3.6479e-02, -1.1008e-02, -2.1273e-03],
          [ 6.4736e-03, -1.1076e-02,  8.1509e-03],
          [-9.4466e-03,  1.7301e-02, -2.6701e-02]],

         [[ 2.1987e-03, -8.7203e-03, -1.5349e-02],
          [-3.0041e-02,  3.7671e-03,  2.3959e-02],
          [-2.1243e-02,  1.0789e-02,  1.9339e-02]]],


        ...,


        [[[-1.4691e-03,  1.0809e-02, -2.0812e-02],
          [-1.9209e-02, -1.3540e-02, -2.3623e-02],
          [-4.1759e-03, -1.5861e-02, -2.4052e-02]],

         [[ 3.0872e-02,  3.0762e-03,  7.0310e-03],
          [-7.9308e-03,  5.9477e-03, -2.3477e-02],
          [-3.5102e-02, -3.9391e-02, -1.6785e-02]],

         [[ 9.5254e-03,  1.1768e-02, -2.1493e-03],
          [-5.1029e-03, -1.6435e-02,  3.4835e-03],
          [-4.2000e-03,  3.6474e-02,  2.3173e-02]],

         ...,

         [[ 2.0083e-02, -7.1843e-03,  8.7993e-03],
          [-1.2886e-02, -2.3391e-02,  2.0385e-02],
          [ 1.2080e-02, -2.5897e-02, -1.7735e-02]],

         [[-7.6216e-03, -1.1784e-02,  2.9551e-02],
          [-3.5472e-02,  2.5472e-02,  1.5735e-02],
          [-1.8759e-02,  4.2072e-03,  1.5813e-02]],

         [[-2.0471e-02, -1.9916e-02,  3.8566e-03],
          [-2.2036e-03, -1.8716e-02, -2.3950e-02],
          [-5.5007e-03,  1.3535e-02, -1.8851e-02]]],


        [[[-3.0422e-02,  7.9807e-03,  9.7060e-03],
          [-8.6729e-03, -1.5786e-02, -2.5381e-02],
          [-2.4908e-02, -1.0031e-02, -3.2476e-05]],

         [[ 1.0422e-02,  1.8085e-02,  1.2488e-02],
          [-1.8052e-02, -2.1126e-02, -2.2869e-02],
          [-8.5330e-03, -8.1482e-03, -1.7446e-02]],

         [[-2.7871e-02, -2.0023e-02, -1.8365e-02],
          [ 1.8688e-02,  2.1949e-02,  3.0642e-02],
          [ 1.7545e-02, -1.6511e-02, -1.6678e-02]],

         ...,

         [[-3.6461e-02,  1.4923e-02,  1.0595e-02],
          [-3.1852e-02,  1.2698e-03, -5.7457e-03],
          [-2.2611e-03,  1.4308e-03,  1.1285e-02]],

         [[ 2.2266e-03, -1.2976e-02, -4.7004e-04],
          [ 4.4364e-03, -2.9582e-02, -1.0611e-03],
          [ 6.1891e-03, -4.6229e-03,  9.8795e-03]],

         [[-3.5149e-03,  6.5539e-03,  4.2161e-03],
          [-1.1551e-02,  5.5434e-04, -2.1882e-02],
          [ 5.7238e-03,  1.0001e-02, -1.7633e-02]]],


        [[[ 7.0169e-03, -7.5945e-03,  1.8151e-02],
          [ 1.2584e-02, -2.0133e-02,  9.4119e-03],
          [ 7.5875e-03,  2.0836e-02,  4.6404e-03]],

         [[ 3.1297e-02,  6.0155e-03,  9.5433e-03],
          [-9.8501e-03,  3.1594e-02,  1.7802e-02],
          [ 8.6943e-03,  1.4477e-03,  1.4008e-02]],

         [[ 6.4649e-03, -1.3086e-02,  4.2426e-03],
          [-7.0340e-03, -3.5933e-02, -1.8694e-02],
          [-3.6319e-02, -1.2111e-02,  4.5658e-03]],

         ...,

         [[ 9.1822e-03, -3.1881e-02,  1.3323e-02],
          [-3.6315e-02,  1.7910e-02,  1.7232e-03],
          [-1.5844e-02,  5.5493e-03, -1.2609e-02]],

         [[ 1.5972e-02, -2.4518e-03,  8.8835e-04],
          [ 2.8401e-03,  2.7757e-02,  2.7901e-02],
          [ 9.0377e-03,  1.4878e-02,  2.4320e-02]],

         [[ 2.0943e-02,  3.0978e-02,  2.2816e-02],
          [-6.1190e-03,  7.1720e-03, -5.5165e-03],
          [-2.1582e-02,  7.9660e-03, -1.5523e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.7712e+06, 2.8080e+05, 2.6533e+05,  ..., 1.8049e+01, 1.6442e+01,
        1.4330e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 782]) 

NULL SPACE BASIS :  tensor([[-2.3879e-03,  4.1390e-03,  4.5548e-02,  ..., -9.0873e-03,
          1.7229e-02, -6.5040e-03],
        [-5.5623e-02,  4.9599e-03,  2.2219e-02,  ...,  1.6420e-02,
         -1.4622e-02,  4.2057e-03],
        [-5.5751e-02,  2.7656e-03, -3.1008e-02,  ..., -5.6675e-03,
          7.1497e-03,  4.1394e-03],
        ...,
        [-2.3469e-02,  1.6351e-02,  2.4375e-02,  ...,  4.4279e-03,
          7.0702e-03,  2.2701e-03],
        [-5.2375e-02,  7.6711e-03,  1.9079e-03,  ...,  5.5897e-04,
         -2.2459e-02, -7.6961e-04],
        [-4.6649e-03, -1.8819e-02,  9.1767e-03,  ..., -6.0959e-05,
          4.3489e-03, -5.5316e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.4430e-02, -4.4995e-03, -1.8613e-03,  ..., -2.3550e-04,
         -2.9193e-04,  5.8991e-05],
        [-4.4995e-03,  2.4572e-02, -4.6063e-03,  ..., -5.7796e-06,
         -1.0616e-04,  2.3234e-04],
        [-1.8613e-03, -4.6063e-03,  2.4365e-02,  ..., -4.7759e-04,
         -4.8191e-06,  7.4481e-05],
        ...,
        [-2.3550e-04, -5.7796e-06, -4.7759e-04,  ...,  2.3648e-02,
         -6.5422e-03, -2.7380e-03],
        [-2.9193e-04, -1.0616e-04, -4.8191e-06,  ..., -6.5422e-03,
          2.2900e-02, -6.7748e-03],
        [ 5.8991e-05,  2.3234e-04,  7.4481e-05,  ..., -2.7380e-03,
         -6.7748e-03,  2.1407e-02]], device='cuda:0') 

reserving basis 1571/2304; cond: 582988.0625, radio:0.008866941556334496
PARAMETER       :  Parameter containing:
tensor([[[[ 3.9873e-03, -4.5922e-03,  2.1104e-04],
          [-2.3664e-02, -7.0349e-03,  1.4335e-02],
          [-2.4863e-02, -6.5733e-03, -1.7352e-02]],

         [[ 6.6309e-03,  2.1520e-02,  2.5723e-03],
          [-6.1846e-03, -1.8505e-02,  2.0444e-02],
          [-1.1619e-02, -2.0465e-03,  1.3166e-02]],

         [[-1.8044e-02, -1.7060e-02, -1.1862e-02],
          [-2.2646e-03, -1.5788e-02, -2.1228e-02],
          [ 5.5259e-03,  7.5199e-03, -1.2422e-02]],

         ...,

         [[ 1.4189e-02,  1.1283e-02,  1.5239e-02],
          [-1.6607e-02, -1.4047e-02,  1.0018e-02],
          [ 2.6727e-03, -5.1685e-04,  1.2329e-03]],

         [[ 2.1567e-02,  6.2393e-04,  1.4921e-02],
          [-1.3984e-02,  4.4588e-04,  2.3592e-02],
          [ 2.0929e-02, -1.9391e-03,  8.4991e-03]],

         [[ 7.5761e-03,  8.4470e-03, -2.0760e-02],
          [ 6.0946e-03, -3.7620e-03,  2.8984e-03],
          [ 2.4139e-02,  5.8768e-03,  1.2006e-02]]],


        [[[ 8.8222e-03, -4.9866e-03, -1.9904e-03],
          [ 5.2907e-03,  7.5594e-03, -9.6757e-03],
          [ 2.3644e-02, -9.5018e-03, -1.4036e-02]],

         [[-1.0586e-02,  1.1798e-02, -1.5533e-03],
          [-1.8843e-02,  2.2188e-02,  1.2023e-02],
          [ 1.1996e-03, -5.1667e-03,  2.0347e-04]],

         [[-1.5815e-02, -1.0923e-03,  1.2119e-02],
          [ 2.4806e-02,  5.4866e-03,  7.6858e-03],
          [-6.8769e-03,  2.3329e-02, -1.2100e-02]],

         ...,

         [[-1.2752e-02,  2.2286e-02,  1.7207e-03],
          [ 2.0753e-02, -1.4001e-02,  1.5744e-02],
          [ 2.6275e-03, -1.9706e-02, -1.2556e-02]],

         [[-1.5341e-02, -2.6086e-02, -5.5659e-03],
          [-8.7561e-03,  3.4450e-03, -5.6947e-03],
          [-1.5527e-02,  1.6330e-02,  1.8043e-02]],

         [[ 7.9259e-03, -4.9867e-03, -1.9589e-02],
          [-7.7284e-03, -1.3578e-02,  7.0299e-03],
          [-1.9189e-02,  9.8012e-03, -6.1776e-03]]],


        [[[ 1.6319e-02, -2.2061e-02, -1.2029e-02],
          [ 1.6556e-02,  1.8649e-02, -2.3355e-03],
          [ 1.7486e-02,  2.1727e-02, -2.4749e-02]],

         [[-1.0474e-03,  1.2466e-02,  1.8344e-03],
          [-1.0473e-02, -1.3583e-02, -1.9905e-02],
          [-7.9078e-03,  2.3063e-02,  1.6059e-03]],

         [[-4.3727e-03, -1.0979e-02,  7.4959e-04],
          [-5.3827e-03, -5.1847e-03,  1.0092e-02],
          [-4.6938e-04, -1.3974e-02,  9.3694e-03]],

         ...,

         [[-1.6603e-02,  2.2091e-02, -7.7526e-03],
          [ 1.7442e-02, -1.5724e-02, -3.6608e-03],
          [-1.6948e-02, -1.3761e-02, -2.1641e-02]],

         [[-3.4655e-03,  1.8269e-02, -1.1831e-02],
          [-1.7524e-02, -2.1317e-02, -2.0530e-02],
          [ 1.8335e-02, -1.8808e-02, -1.0147e-02]],

         [[ 8.5170e-03,  3.1821e-03, -1.9665e-02],
          [ 2.6897e-03, -4.4015e-03,  1.1116e-02],
          [-9.4808e-03,  1.2785e-04, -8.9497e-03]]],


        ...,


        [[[ 7.5838e-03,  1.8031e-02,  7.9072e-03],
          [ 1.9910e-02, -1.5464e-02, -1.5611e-02],
          [-4.2366e-03, -1.3376e-02, -1.1512e-02]],

         [[-1.5299e-02, -1.0420e-02,  2.9725e-02],
          [-5.4595e-05,  9.2595e-03, -3.2537e-03],
          [ 2.4060e-02, -2.0051e-02,  1.0793e-02]],

         [[-1.6998e-02, -7.0934e-04, -1.4589e-02],
          [ 4.9149e-03,  2.5747e-02, -2.5107e-02],
          [ 9.7907e-03,  3.1604e-02,  1.2952e-02]],

         ...,

         [[-6.8092e-03,  1.1259e-02,  1.9621e-03],
          [-1.3105e-02, -1.1301e-02,  6.6779e-03],
          [ 9.5508e-04,  1.1249e-03, -2.8789e-02]],

         [[-1.1902e-02,  2.4413e-03, -1.1349e-02],
          [-1.6282e-02, -1.0372e-02, -1.7605e-03],
          [-8.4662e-03,  4.1853e-03,  2.0349e-02]],

         [[-2.5291e-02, -4.1224e-02, -2.7545e-02],
          [ 1.0620e-02, -2.7199e-02, -1.9231e-02],
          [ 1.2484e-02, -1.0885e-02,  1.0324e-02]]],


        [[[-9.0814e-03,  1.6173e-02,  1.1227e-03],
          [-1.2627e-02, -2.0050e-02,  3.9552e-03],
          [ 1.0553e-02, -3.7641e-03, -1.4112e-02]],

         [[-3.3526e-03,  6.3287e-03, -4.0099e-03],
          [ 5.5783e-03, -1.4656e-02,  1.4723e-02],
          [ 8.9433e-03, -1.3548e-02, -1.5902e-02]],

         [[-2.1438e-02, -2.8615e-02, -2.0669e-02],
          [ 2.5034e-02,  1.7512e-02, -8.1382e-04],
          [-3.5628e-03,  8.6592e-03, -5.8182e-03]],

         ...,

         [[ 7.5964e-03, -1.4162e-02, -3.0949e-02],
          [ 1.5355e-02,  1.4093e-02, -2.4330e-02],
          [-1.5321e-02, -2.3350e-02,  1.2583e-02]],

         [[ 4.7908e-03,  8.6918e-04,  1.0812e-02],
          [-1.4309e-02, -8.4140e-03, -1.0541e-02],
          [-8.4745e-03,  7.8506e-03,  4.3540e-03]],

         [[-2.3729e-02,  1.6789e-02,  1.7263e-02],
          [-2.6548e-02, -9.3350e-03,  2.6207e-02],
          [-1.5240e-02,  3.7031e-02,  4.0497e-02]]],


        [[[-1.6895e-03, -3.0562e-02,  9.1204e-03],
          [-1.5295e-02, -7.8235e-03,  2.7307e-02],
          [-1.9717e-02, -8.6710e-03,  9.3479e-03]],

         [[-1.9690e-02, -1.6560e-02,  1.3915e-02],
          [-1.0760e-02,  7.2522e-03, -6.3422e-03],
          [ 5.2677e-03,  1.2405e-02,  1.7008e-03]],

         [[ 9.4040e-03, -6.4982e-03, -5.4739e-03],
          [-1.1820e-02,  2.3025e-02, -2.7210e-03],
          [-2.1739e-02,  1.2590e-03, -4.8742e-03]],

         ...,

         [[-2.5863e-03, -2.8768e-02, -1.0888e-03],
          [-2.3620e-02, -2.6227e-02,  8.1723e-03],
          [ 1.2011e-02, -1.4300e-02,  1.9194e-02]],

         [[ 9.7304e-03,  2.8933e-03,  1.7735e-02],
          [ 7.1833e-03, -1.2059e-02, -6.9440e-04],
          [-2.2434e-02,  1.4629e-02, -2.6168e-02]],

         [[-2.4804e-02,  8.6510e-03, -3.3986e-02],
          [ 1.0529e-02, -1.3982e-02, -5.5729e-03],
          [ 1.2844e-02, -2.5138e-02, -1.9329e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([6.9261e+06, 4.1933e+05, 3.9299e+05,  ..., 1.2958e+01, 1.2385e+01,
        1.1880e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1571]) 

NULL SPACE BASIS :  tensor([[ 8.3340e-03, -2.0411e-02, -2.7493e-02,  ..., -3.3061e-03,
          3.7302e-03, -2.2804e-03],
        [-2.6808e-02, -3.1091e-02, -1.6248e-03,  ..., -4.7772e-03,
          7.5369e-04, -3.1316e-03],
        [ 2.5823e-02, -1.4198e-02, -1.5765e-02,  ...,  5.5398e-03,
          3.5341e-03,  2.5039e-03],
        ...,
        [-1.7447e-03, -6.2195e-03,  1.6327e-02,  ...,  6.0027e-03,
          1.8816e-05,  1.7858e-02],
        [ 1.9661e-02, -2.2766e-03,  3.4765e-03,  ...,  4.8558e-03,
         -4.1555e-03, -1.0814e-02],
        [-2.1423e-03, -1.6444e-02, -8.5372e-03,  ..., -1.7641e-02,
          2.4637e-03,  5.6748e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.7436e-02, -4.6588e-04, -2.4853e-04,  ...,  1.6163e-05,
         -1.1160e-04,  1.4227e-04],
        [-4.6588e-04,  1.7137e-02, -7.3392e-04,  ...,  2.5395e-05,
         -2.0286e-04, -6.5259e-05],
        [-2.4853e-04, -7.3392e-04,  1.7662e-02,  ...,  1.4618e-04,
          1.2180e-04, -1.0454e-04],
        ...,
        [ 1.6163e-05,  2.5395e-05,  1.4618e-04,  ...,  2.0123e-02,
         -1.6852e-04, -9.0536e-05],
        [-1.1160e-04, -2.0286e-04,  1.2180e-04,  ..., -1.6852e-04,
          1.9752e-02, -1.7024e-04],
        [ 1.4227e-04, -6.5259e-05, -1.0454e-04,  ..., -9.0536e-05,
         -1.7024e-04,  2.0400e-02]], device='cuda:0') 

reserving basis 98/128; cond: 12272.64453125, radio:0.021302150562405586
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0230]],

         [[-0.0340]],

         [[-0.0297]],

         ...,

         [[-0.0350]],

         [[-0.0691]],

         [[ 0.0783]]],


        [[[-0.0474]],

         [[ 0.0589]],

         [[ 0.0719]],

         ...,

         [[-0.0514]],

         [[ 0.0010]],

         [[-0.0156]]],


        [[[ 0.0568]],

         [[-0.0240]],

         [[-0.0312]],

         ...,

         [[ 0.0554]],

         [[ 0.0162]],

         [[-0.0239]]],


        ...,


        [[[-0.0667]],

         [[ 0.0267]],

         [[ 0.0804]],

         ...,

         [[-0.0774]],

         [[-0.0579]],

         [[ 0.0298]]],


        [[[-0.0279]],

         [[-0.0724]],

         [[ 0.0051]],

         ...,

         [[ 0.0342]],

         [[ 0.0512]],

         [[ 0.0246]]],


        [[[ 0.0395]],

         [[-0.0324]],

         [[-0.0659]],

         ...,

         [[-0.0532]],

         [[ 0.0624]],

         [[-0.0327]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([6.7003e+05, 3.5710e+04, 2.9498e+04, 1.1950e+04, 7.3829e+03, 6.0467e+03,
        5.7740e+03, 4.3077e+03, 3.2878e+03, 3.2272e+03, 2.9670e+03, 2.2184e+03,
        2.0136e+03, 1.7224e+03, 1.4583e+03, 1.3908e+03, 1.1857e+03, 1.1417e+03,
        1.0411e+03, 9.3718e+02, 9.0390e+02, 8.6519e+02, 7.9041e+02, 7.7071e+02,
        7.2214e+02, 6.9675e+02, 6.2492e+02, 6.0671e+02, 5.6278e+02, 5.5034e+02,
        4.9823e+02, 4.8715e+02, 4.6032e+02, 4.4886e+02, 4.2132e+02, 4.1143e+02,
        3.8690e+02, 3.6789e+02, 3.5521e+02, 3.4756e+02, 3.4051e+02, 3.2768e+02,
        3.2057e+02, 3.0913e+02, 3.0152e+02, 2.9035e+02, 2.8091e+02, 2.7735e+02,
        2.6502e+02, 2.5909e+02, 2.5407e+02, 2.4637e+02, 2.3702e+02, 2.3089e+02,
        2.2919e+02, 2.2206e+02, 2.1288e+02, 2.1134e+02, 2.0578e+02, 2.0357e+02,
        1.9812e+02, 1.9449e+02, 1.9016e+02, 1.8522e+02, 1.8227e+02, 1.8184e+02,
        1.7481e+02, 1.7317e+02, 1.7031e+02, 1.6771e+02, 1.6618e+02, 1.6238e+02,
        1.5684e+02, 1.5480e+02, 1.5252e+02, 1.4884e+02, 1.4539e+02, 1.4233e+02,
        1.3874e+02, 1.3813e+02, 1.3635e+02, 1.3476e+02, 1.3321e+02, 1.3081e+02,
        1.2848e+02, 1.2631e+02, 1.2412e+02, 1.2206e+02, 1.2117e+02, 1.1976e+02,
        1.1729e+02, 1.1609e+02, 1.1426e+02, 1.1250e+02, 1.1150e+02, 1.1045e+02,
        1.0760e+02, 1.0680e+02, 1.0510e+02, 1.0426e+02, 1.0188e+02, 1.0036e+02,
        9.9000e+01, 9.8228e+01, 9.6768e+01, 9.5525e+01, 9.3950e+01, 9.2532e+01,
        9.1882e+01, 8.9545e+01, 8.7484e+01, 8.6547e+01, 8.3181e+01, 8.3150e+01,
        8.0717e+01, 7.8355e+01, 7.7771e+01, 7.6916e+01, 7.6512e+01, 7.5924e+01,
        7.4470e+01, 7.1934e+01, 7.1467e+01, 6.9462e+01, 6.7987e+01, 6.5562e+01,
        6.2142e+01, 5.4596e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 98]) 

NULL SPACE BASIS :  tensor([[-0.0387,  0.2169,  0.0138,  ...,  0.0621,  0.0119,  0.0610],
        [ 0.0437, -0.0879, -0.0795,  ..., -0.1603,  0.0069, -0.0306],
        [-0.1112, -0.1891, -0.0566,  ..., -0.0372,  0.2065, -0.1001],
        ...,
        [-0.0439, -0.0576,  0.0629,  ..., -0.0453, -0.1018, -0.0228],
        [ 0.0255,  0.0808,  0.0861,  ...,  0.0740,  0.0003,  0.0528],
        [-0.0171, -0.0730, -0.0359,  ...,  0.0760,  0.0186,  0.0845]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0783,  0.0011,  0.0004,  ..., -0.0047,  0.0028, -0.0089],
        [ 0.0011,  0.0661,  0.0012,  ..., -0.0018, -0.0050,  0.0081],
        [ 0.0004,  0.0012,  0.0862,  ...,  0.0004, -0.0011, -0.0034],
        ...,
        [-0.0047, -0.0018,  0.0004,  ...,  0.0763,  0.0011, -0.0070],
        [ 0.0028, -0.0050, -0.0011,  ...,  0.0011,  0.0782, -0.0012],
        [-0.0089,  0.0081, -0.0034,  ..., -0.0070, -0.0012,  0.0701]],
       device='cuda:0') 

reserving basis 1483/2304; cond: 606117.8125, radio:0.008305312134325504
PARAMETER       :  Parameter containing:
tensor([[[[ 5.1249e-03,  1.3306e-03, -1.0381e-02],
          [ 4.6783e-03, -2.0125e-02,  8.3215e-04],
          [-6.3949e-03,  2.1083e-02, -5.7438e-03]],

         [[-2.2489e-03,  1.2360e-02,  2.1806e-03],
          [ 2.6751e-02, -6.0783e-03, -2.0730e-02],
          [-3.0743e-03,  6.1363e-03,  1.0945e-02]],

         [[ 1.4656e-02,  2.6636e-02, -1.3319e-02],
          [ 1.3546e-02,  9.9289e-03,  1.2272e-02],
          [-4.6316e-03,  9.1916e-03,  5.3027e-03]],

         ...,

         [[-5.9557e-04,  2.0438e-02,  2.8748e-03],
          [-2.8173e-02, -6.2218e-03, -3.1481e-02],
          [-8.3427e-04, -1.6417e-02, -2.6864e-03]],

         [[-2.3566e-02,  1.3944e-02,  9.7976e-03],
          [ 4.4111e-03, -2.5831e-02, -1.8538e-02],
          [-1.9454e-02, -1.5254e-02, -1.5599e-02]],

         [[ 1.3494e-03, -1.2591e-03, -2.1507e-02],
          [ 7.9524e-03,  1.1789e-02,  5.3483e-03],
          [ 2.3488e-02,  2.0800e-02, -1.3104e-02]]],


        [[[-5.3050e-03,  2.3090e-02,  3.0812e-02],
          [ 2.0733e-03, -1.9100e-02,  1.5997e-02],
          [ 6.6253e-03,  1.7836e-02, -2.5152e-02]],

         [[-1.4100e-02, -2.2236e-03, -3.9206e-02],
          [-7.4753e-05, -5.5415e-03,  8.7136e-03],
          [ 1.3356e-03,  1.8327e-02,  2.5142e-02]],

         [[-7.2082e-03, -2.2679e-02,  1.5381e-02],
          [-1.0499e-02,  6.6464e-04, -1.4333e-02],
          [ 6.9919e-03,  7.4437e-03,  1.9448e-02]],

         ...,

         [[-1.1699e-02, -2.6077e-02, -1.0866e-02],
          [-8.5953e-03,  6.1134e-03,  1.7131e-02],
          [ 3.4307e-03,  1.3632e-02, -8.4647e-03]],

         [[ 1.5687e-03, -1.7123e-02, -1.0706e-03],
          [ 1.0654e-02,  1.7802e-02,  2.2517e-02],
          [-1.6463e-02, -1.9191e-02,  1.9650e-02]],

         [[ 6.6147e-03, -1.1918e-02, -2.9002e-02],
          [-7.4081e-03,  5.2784e-03,  3.8182e-03],
          [-1.3214e-02, -2.5649e-02, -1.3580e-02]]],


        [[[-1.3693e-02,  3.6787e-04,  3.2960e-03],
          [ 1.2562e-02, -2.0703e-02, -1.7211e-02],
          [-1.0268e-02,  2.1326e-02,  3.7113e-03]],

         [[-1.6834e-02,  7.5167e-03,  3.3801e-03],
          [-1.1107e-02, -4.1773e-03,  1.8484e-02],
          [-1.8496e-02,  1.5572e-02,  1.2468e-03]],

         [[ 2.0902e-02, -9.5120e-03,  1.9118e-02],
          [-1.5442e-02, -3.7391e-03,  2.5594e-02],
          [-1.7087e-02, -1.1979e-02,  2.2241e-02]],

         ...,

         [[ 1.6589e-02, -6.1760e-03, -1.7195e-02],
          [ 1.1176e-02, -2.9206e-02, -3.5863e-03],
          [ 8.1962e-03, -2.9233e-02, -3.3294e-02]],

         [[-8.4081e-03,  1.0703e-03, -1.4379e-02],
          [ 1.3350e-02,  6.1953e-03,  4.1375e-03],
          [ 2.5074e-02, -3.1439e-02, -8.7282e-03]],

         [[-2.1763e-03, -1.1655e-02, -1.1672e-02],
          [-1.0256e-02, -1.4886e-02,  3.2050e-03],
          [ 1.6922e-02,  3.0238e-02,  1.8147e-02]]],


        ...,


        [[[-2.0059e-02,  1.1220e-02, -2.7303e-02],
          [ 4.3791e-03, -1.1491e-02, -2.9783e-02],
          [ 1.0504e-02,  5.4803e-04, -5.5890e-03]],

         [[-1.0027e-02, -2.4650e-02, -2.4873e-03],
          [ 1.3869e-02,  1.5915e-02,  7.1134e-03],
          [-1.6995e-02, -2.5150e-02, -1.2474e-02]],

         [[ 3.8384e-03, -1.7712e-02,  1.2297e-02],
          [-2.0026e-02,  1.7862e-04,  2.1309e-02],
          [ 1.6395e-02,  5.2383e-03, -1.6553e-02]],

         ...,

         [[ 1.5845e-02, -3.6600e-02,  1.2774e-02],
          [-1.2648e-02, -1.3157e-03,  6.1966e-03],
          [-1.3488e-02, -2.3541e-02,  3.5748e-03]],

         [[ 1.4972e-02, -1.4192e-03,  3.1071e-03],
          [-1.4955e-02,  1.1659e-02,  1.1826e-03],
          [-2.1418e-02,  1.9888e-03,  3.3471e-03]],

         [[-2.0829e-02, -1.2476e-02, -2.4701e-02],
          [ 8.4459e-03,  1.2679e-02, -5.0816e-03],
          [-2.2640e-02,  2.4919e-02,  3.2323e-03]]],


        [[[ 8.0206e-03, -4.6675e-03,  3.6093e-02],
          [ 6.2541e-03,  1.9227e-02,  1.1421e-02],
          [ 2.6372e-02, -5.3521e-03,  3.4570e-02]],

         [[ 4.3232e-03,  2.0437e-02, -1.7464e-02],
          [-1.3676e-02,  3.7428e-03,  2.1747e-02],
          [ 3.4971e-03,  9.7114e-03,  2.7022e-02]],

         [[-1.2140e-02,  2.9266e-03,  1.4982e-02],
          [-1.3954e-02,  7.9931e-03, -2.2617e-02],
          [ 3.2102e-03,  1.8682e-02,  1.0142e-02]],

         ...,

         [[ 9.9688e-03,  1.2346e-02, -1.4247e-02],
          [ 1.3796e-02,  1.7526e-02, -1.7958e-02],
          [ 5.3428e-04,  1.8334e-02,  3.1525e-02]],

         [[ 4.3164e-03, -9.8393e-03,  1.9192e-02],
          [ 1.0122e-02,  1.6007e-02,  2.0850e-02],
          [-1.3304e-02,  8.1043e-03,  1.4501e-02]],

         [[-1.3094e-02, -9.7998e-03,  4.5227e-03],
          [ 6.9105e-03, -4.6269e-03,  2.4267e-03],
          [-1.6330e-02, -3.0924e-03,  1.8364e-02]]],


        [[[ 2.8582e-02,  4.2406e-03, -6.7530e-03],
          [ 8.6978e-03, -5.0496e-03,  7.8499e-03],
          [-2.4853e-03, -1.2425e-02, -7.7650e-03]],

         [[-1.0153e-02, -6.4888e-03,  1.4106e-02],
          [ 1.7523e-02, -1.9895e-03,  2.4010e-02],
          [-2.2680e-03,  2.5734e-02,  1.0176e-02]],

         [[-1.2223e-02, -2.1570e-02, -3.7132e-03],
          [-1.3486e-02, -2.7178e-02, -1.3654e-02],
          [ 1.5639e-03,  1.0199e-03,  4.9584e-03]],

         ...,

         [[ 1.0262e-02,  8.0846e-03,  1.7927e-02],
          [ 1.5790e-02,  1.4037e-02, -1.2276e-02],
          [-6.2067e-04, -1.3590e-02, -1.5183e-02]],

         [[ 1.3396e-02, -1.0707e-02, -1.3614e-02],
          [-6.5084e-04, -2.3294e-02, -1.4412e-02],
          [ 4.0373e-04, -2.0945e-02, -2.0161e-02]],

         [[ 5.9049e-03, -2.8272e-03, -1.1423e-02],
          [-5.6406e-04, -8.6034e-03, -1.5755e-02],
          [ 2.1722e-02,  3.1251e-03,  1.4560e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.9793e+06, 4.0289e+05, 3.8788e+05,  ..., 1.1935e+01, 1.1130e+01,
        9.8650e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1483]) 

NULL SPACE BASIS :  tensor([[-1.0891e-03,  2.2031e-04,  6.9953e-03,  ...,  7.7973e-03,
         -4.4506e-03, -4.8121e-03],
        [-6.1681e-03,  6.3228e-03, -5.3688e-03,  ..., -8.4028e-03,
         -4.7298e-03,  7.3290e-03],
        [ 5.2666e-02,  1.9849e-02,  8.9127e-04,  ...,  7.0354e-03,
          2.0980e-03, -1.5707e-03],
        ...,
        [ 3.4079e-02, -1.3578e-05,  6.3448e-03,  ..., -2.1202e-02,
          2.3514e-02, -9.5824e-04],
        [ 1.9892e-03, -1.3141e-02, -5.2349e-04,  ...,  5.0556e-03,
         -3.0086e-02, -1.6153e-03],
        [-1.2417e-02, -3.3260e-02,  1.4640e-02,  ...,  1.6427e-02,
          1.2267e-02, -6.2461e-04]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.3456e-02, -4.4670e-03, -8.9291e-04,  ...,  1.2575e-05,
         -1.3571e-04, -5.0312e-05],
        [-4.4670e-03,  1.4062e-02, -4.5124e-03,  ...,  2.9593e-05,
          3.3721e-04, -3.8631e-05],
        [-8.9291e-04, -4.5124e-03,  1.3481e-02,  ...,  1.0085e-04,
         -1.4440e-04,  3.2928e-04],
        ...,
        [ 1.2575e-05,  2.9593e-05,  1.0085e-04,  ...,  2.0911e-02,
         -4.7038e-04, -4.0924e-04],
        [-1.3571e-04,  3.3721e-04, -1.4440e-04,  ..., -4.7038e-04,
          2.0362e-02, -4.5433e-04],
        [-5.0312e-05, -3.8631e-05,  3.2928e-04,  ..., -4.0924e-04,
         -4.5433e-04,  2.0264e-02]], device='cuda:0') 

reserving basis 1158/2304; cond: 932474.75, radio:0.004287535324692726
PARAMETER       :  Parameter containing:
tensor([[[[-1.3350e-02, -6.4871e-03, -1.5232e-02],
          [-3.8776e-03, -2.9395e-02,  6.3501e-03],
          [-1.4980e-02, -1.2943e-02, -4.7869e-03]],

         [[ 2.4265e-02,  2.7878e-02,  2.9981e-02],
          [-1.0156e-02,  1.3073e-02, -1.8325e-02],
          [-7.2684e-03,  2.5656e-02,  2.7239e-02]],

         [[-4.5771e-03, -1.1887e-02, -2.7856e-02],
          [-1.4324e-02, -1.7567e-02, -3.4565e-03],
          [ 1.4641e-02, -2.1393e-02, -1.5896e-02]],

         ...,

         [[-1.8103e-02,  2.2343e-02, -1.8962e-02],
          [ 4.9324e-03,  2.5850e-03, -2.2777e-02],
          [ 5.1885e-04,  2.3842e-03,  9.2066e-03]],

         [[ 2.0541e-02,  1.9516e-03, -2.0248e-02],
          [-1.2298e-02,  2.7501e-03, -8.6291e-03],
          [-2.6715e-02,  1.5563e-02,  9.9913e-03]],

         [[ 4.0877e-03,  2.5433e-02,  7.5055e-03],
          [-1.0548e-02,  1.4505e-02, -1.7944e-02],
          [ 9.1978e-03, -4.7680e-03,  1.5695e-02]]],


        [[[ 1.1293e-03,  2.0835e-02,  3.3161e-02],
          [-2.4400e-02,  1.9452e-02,  7.7495e-03],
          [ 3.7896e-03, -4.3447e-03,  7.5445e-03]],

         [[ 2.0565e-02,  1.6325e-02, -2.0786e-02],
          [ 1.2066e-02,  3.0803e-03, -4.2682e-03],
          [-2.9556e-02, -1.2082e-02, -1.7999e-02]],

         [[ 2.3924e-03, -2.2445e-03,  9.9990e-03],
          [ 4.4029e-03, -2.6215e-02, -1.6743e-02],
          [ 1.4913e-02, -8.8733e-03,  1.6215e-02]],

         ...,

         [[-1.8650e-02, -3.6221e-03, -1.4367e-02],
          [ 7.8896e-03,  1.2422e-02, -3.4996e-04],
          [-1.8747e-03,  1.4199e-02, -9.4791e-03]],

         [[ 2.6240e-03, -7.1275e-03,  2.0903e-02],
          [-1.1657e-03, -4.3229e-03,  2.0530e-02],
          [ 6.2751e-03,  9.8463e-03,  1.4720e-02]],

         [[-2.3103e-02, -1.5866e-02, -2.2225e-02],
          [-1.8497e-02, -1.5519e-02, -1.7873e-02],
          [-1.9482e-02, -1.7096e-02,  2.9421e-03]]],


        [[[-2.8015e-02, -1.9831e-02, -1.6084e-02],
          [ 1.6405e-03,  1.7050e-02,  7.3927e-03],
          [ 1.1181e-02, -8.6366e-03, -1.5869e-02]],

         [[-1.4749e-02,  1.5842e-02,  4.3500e-03],
          [-8.3476e-03,  3.6291e-03,  3.1026e-03],
          [-3.3770e-03, -1.4548e-02,  1.8065e-02]],

         [[-7.7981e-03,  7.8048e-03,  1.1140e-03],
          [-7.8646e-04, -1.6683e-02, -2.4555e-02],
          [ 9.2126e-03, -5.2336e-03,  2.3638e-03]],

         ...,

         [[-5.4286e-03,  2.7438e-02, -9.0313e-03],
          [-1.8112e-02,  1.6146e-03, -1.5911e-02],
          [-2.0738e-02, -2.3656e-02,  7.6344e-03]],

         [[ 1.9919e-05,  2.4886e-03, -6.0980e-04],
          [-3.2107e-03,  1.3094e-02,  8.9751e-03],
          [-1.8541e-02, -1.6946e-02,  1.2047e-03]],

         [[-1.9931e-02, -1.0546e-02, -3.1591e-04],
          [ 1.0707e-02, -8.4628e-03,  1.9287e-02],
          [-6.2378e-03, -1.8428e-03, -1.2835e-02]]],


        ...,


        [[[-1.4033e-02, -1.6505e-02,  2.1302e-03],
          [-1.4324e-03,  1.1207e-02,  1.1903e-02],
          [-1.2479e-02, -1.1818e-02,  4.7352e-03]],

         [[ 4.8467e-04,  2.1293e-02, -2.8132e-03],
          [ 1.4907e-02, -2.0745e-03,  2.8992e-02],
          [-2.0502e-03, -1.3244e-02, -1.0745e-02]],

         [[-1.9838e-03, -1.9346e-02,  6.8145e-03],
          [-1.0075e-03,  1.9528e-02, -4.4015e-03],
          [-3.6329e-02, -6.1198e-03,  2.3220e-02]],

         ...,

         [[ 2.3978e-02,  4.4914e-03,  1.3853e-02],
          [ 9.1976e-03,  1.7509e-02,  1.6368e-02],
          [-1.8090e-03, -1.0617e-02,  6.6186e-03]],

         [[ 3.5604e-03, -1.4514e-02, -2.3324e-02],
          [-4.7321e-03,  1.8810e-02, -5.9819e-03],
          [ 1.1243e-02, -1.2932e-02,  1.6620e-02]],

         [[ 1.2241e-02,  1.3888e-02, -6.6807e-03],
          [-1.1837e-02, -2.1606e-03,  1.7955e-02],
          [ 1.2837e-02, -2.0328e-03,  6.6222e-03]]],


        [[[ 8.7629e-03, -1.7752e-02,  6.6963e-03],
          [ 2.6741e-02,  4.5347e-03, -1.0233e-02],
          [-2.9730e-03,  2.0716e-03, -1.7361e-02]],

         [[ 3.1904e-03, -8.4889e-03,  1.4723e-02],
          [ 1.8264e-02,  1.9912e-02,  3.5792e-02],
          [ 3.0236e-02,  2.0979e-02,  3.4142e-02]],

         [[-1.3147e-02,  1.6634e-02, -7.0681e-03],
          [ 2.6928e-02,  2.9999e-02,  1.1550e-02],
          [-1.7120e-02,  4.4321e-04,  2.4101e-02]],

         ...,

         [[-1.3872e-02,  6.5415e-03,  2.0421e-02],
          [ 1.7308e-02,  9.4864e-03,  1.1712e-02],
          [-1.4019e-02,  9.5402e-03, -1.4892e-02]],

         [[-2.1478e-02, -2.0246e-02,  8.6497e-03],
          [-1.0829e-02,  9.4365e-03, -6.7366e-04],
          [ 1.6884e-02, -1.4665e-02, -8.2723e-03]],

         [[-1.8856e-02,  2.2138e-03,  2.2357e-02],
          [ 1.3597e-02,  2.0650e-02, -1.1709e-02],
          [ 1.3124e-02,  1.2100e-02, -7.2313e-03]]],


        [[[ 1.5188e-02, -1.1493e-02, -1.9189e-03],
          [-6.7394e-03, -1.1222e-02, -2.5009e-03],
          [-2.1485e-02, -1.8797e-02, -1.1981e-03]],

         [[ 3.9792e-03, -7.2814e-03, -1.4593e-02],
          [-7.3982e-03, -2.9780e-03, -1.8346e-02],
          [ 1.6189e-04, -2.1233e-02, -1.0134e-02]],

         [[ 3.0511e-02,  1.6276e-03,  2.0899e-03],
          [ 2.5102e-02,  4.6988e-03,  9.6788e-03],
          [-2.1091e-03, -5.7917e-03, -1.3547e-02]],

         ...,

         [[ 1.4865e-02,  1.2668e-02,  2.1169e-03],
          [ 1.9934e-02, -7.2238e-03,  5.6842e-03],
          [-6.4686e-03, -3.7010e-03,  1.2645e-04]],

         [[-6.9047e-03,  3.9356e-03, -1.9915e-02],
          [ 2.0734e-02,  1.8520e-02,  1.2569e-02],
          [ 8.0373e-04,  3.8377e-02,  3.3167e-02]],

         [[ 2.0388e-02, -7.5211e-04, -1.1578e-02],
          [ 7.7612e-03,  7.7576e-03, -1.2478e-02],
          [-3.1052e-02, -3.9548e-03, -1.7371e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.3757e+06, 3.6761e+05, 3.5525e+05,  ..., 6.8172e+00, 6.6476e+00,
        5.7649e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1158]) 

NULL SPACE BASIS :  tensor([[ 0.0574, -0.0083,  0.0210,  ..., -0.0044,  0.0035,  0.0075],
        [ 0.0200,  0.0046,  0.0009,  ...,  0.0056, -0.0055, -0.0032],
        [-0.0110,  0.0401, -0.0163,  ..., -0.0004, -0.0016, -0.0036],
        ...,
        [-0.0585, -0.0187, -0.0195,  ...,  0.0031,  0.0050, -0.0001],
        [ 0.0289, -0.0304, -0.0043,  ...,  0.0006, -0.0036,  0.0008],
        [ 0.0176,  0.0056, -0.0115,  ...,  0.0004,  0.0001,  0.0005]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.8343e-02, -3.6650e-03, -5.8575e-04,  ...,  2.2379e-04,
         -6.8791e-05,  2.3025e-04],
        [-3.6650e-03,  1.8036e-02, -3.9876e-03,  ...,  4.5297e-06,
          1.5836e-04, -1.9548e-04],
        [-5.8575e-04, -3.9876e-03,  1.9916e-02,  ...,  1.6413e-04,
         -1.0708e-04, -6.3205e-06],
        ...,
        [ 2.2379e-04,  4.5297e-06,  1.6413e-04,  ...,  7.1360e-03,
         -2.2280e-03, -1.4427e-04],
        [-6.8791e-05,  1.5836e-04, -1.0708e-04,  ..., -2.2280e-03,
          6.8799e-03, -1.7922e-03],
        [ 2.3025e-04, -1.9548e-04, -6.3205e-06,  ..., -1.4427e-04,
         -1.7922e-03,  6.3376e-03]], device='cuda:0') 

reserving basis 1216/2304; cond: 1102893.5, radio:0.003799762576818466
PARAMETER       :  Parameter containing:
tensor([[[[-5.9838e-03, -9.1068e-03, -6.6463e-03],
          [-8.0566e-03,  1.5548e-02,  1.3009e-02],
          [-2.3841e-02, -4.0665e-03, -2.7953e-03]],

         [[ 1.3129e-03,  1.8972e-03, -1.1496e-02],
          [-7.9478e-03,  1.1749e-02, -5.0027e-03],
          [ 1.7772e-02, -2.4920e-02,  5.7221e-03]],

         [[ 1.5237e-02, -4.6506e-03,  9.0388e-03],
          [ 1.2607e-02, -1.0250e-02,  2.1272e-03],
          [-1.8091e-02, -1.1697e-02, -6.9838e-03]],

         ...,

         [[ 1.4737e-02, -1.5445e-02,  1.6832e-03],
          [ 1.0420e-02,  1.0566e-02,  1.3975e-02],
          [-1.5018e-03, -2.5971e-02,  1.2790e-02]],

         [[ 2.0090e-02,  1.1044e-02, -1.8975e-03],
          [-1.0259e-02,  2.7448e-03, -1.0832e-02],
          [-2.5858e-03,  1.7861e-02, -1.7087e-02]],

         [[-6.4372e-03, -4.6733e-03,  7.9174e-03],
          [-3.1387e-02,  2.7740e-03, -3.3321e-04],
          [ 5.0325e-03, -1.0819e-02,  2.0120e-02]]],


        [[[-1.1857e-02,  8.0026e-03,  7.5102e-03],
          [ 1.0627e-02, -7.2357e-03,  1.5271e-02],
          [ 1.5778e-02, -6.1780e-03, -8.8062e-04]],

         [[ 1.8624e-02, -1.3418e-03, -1.8753e-02],
          [ 1.2215e-03, -4.5480e-03,  8.5219e-03],
          [ 1.4073e-02, -2.0939e-02, -1.4855e-02]],

         [[ 1.5704e-02, -2.5644e-02,  2.8824e-03],
          [-2.2877e-02,  5.8051e-04,  8.8190e-03],
          [-1.1011e-02, -1.7961e-02,  4.5121e-03]],

         ...,

         [[-2.1967e-02, -1.8333e-02, -8.1380e-03],
          [ 6.4201e-04, -1.1905e-02, -2.5843e-02],
          [ 4.2538e-03, -4.8187e-03, -2.1856e-04]],

         [[-9.0999e-03,  5.7402e-03, -6.1977e-03],
          [ 8.6268e-03,  2.4166e-02, -1.0107e-02],
          [-1.5186e-02,  1.1552e-02, -8.8995e-03]],

         [[ 1.0930e-02,  2.0993e-02,  4.0734e-03],
          [ 1.1997e-03,  6.5408e-03, -9.0632e-03],
          [-7.9877e-03,  2.6445e-02, -2.7386e-03]]],


        [[[ 3.4676e-03, -2.4529e-02, -4.2362e-03],
          [-3.2362e-03,  1.0720e-02, -5.1216e-03],
          [ 7.2261e-03,  1.0456e-02,  3.3853e-03]],

         [[ 2.1999e-02,  1.8674e-02,  2.0725e-02],
          [ 2.0553e-02,  9.3106e-03,  5.8526e-03],
          [ 8.8942e-03, -4.0492e-03,  9.0867e-03]],

         [[-1.7684e-02, -1.4205e-02,  2.7045e-03],
          [ 6.3468e-03, -1.9833e-02, -5.7413e-03],
          [-1.3208e-02, -8.8302e-03, -3.1322e-02]],

         ...,

         [[ 1.2444e-03, -4.4156e-03, -3.1995e-02],
          [-5.0995e-03,  3.2872e-03, -8.2613e-03],
          [ 1.8820e-02,  1.0901e-03, -3.1029e-03]],

         [[ 1.2225e-02,  4.7399e-03, -4.7479e-03],
          [-5.7542e-03, -1.6960e-03,  6.3706e-03],
          [-2.2267e-02, -2.2264e-02, -1.4782e-02]],

         [[ 4.4511e-04,  4.9523e-03, -6.6501e-03],
          [ 8.0225e-04,  1.6521e-02,  1.9668e-05],
          [ 1.0778e-02,  1.0825e-02, -9.8396e-03]]],


        ...,


        [[[-5.5100e-05,  1.1754e-03, -1.4174e-02],
          [-1.4370e-02,  2.3342e-03,  4.7524e-04],
          [ 7.6531e-03, -1.0747e-02, -8.1790e-03]],

         [[ 1.7334e-02, -3.5730e-03, -1.1204e-02],
          [ 2.1716e-02,  1.5158e-02, -1.5150e-02],
          [-6.7288e-03,  3.1254e-03, -1.5220e-02]],

         [[-2.0984e-02, -2.0635e-03,  7.8501e-03],
          [-6.6839e-03, -1.5183e-02,  6.3443e-03],
          [ 4.8956e-03,  3.2866e-03, -2.5052e-02]],

         ...,

         [[ 8.1130e-03, -1.3296e-02,  7.5406e-03],
          [ 1.1699e-03,  5.0264e-03, -5.6505e-03],
          [-5.6931e-03, -7.4593e-03, -2.9778e-02]],

         [[-1.5534e-02,  6.7386e-03, -2.7602e-03],
          [ 7.9285e-03, -2.3544e-02, -2.1396e-02],
          [-7.5405e-03, -1.3464e-02, -1.1489e-02]],

         [[ 1.0387e-02,  1.8559e-02,  6.8542e-03],
          [-1.0068e-02,  1.1824e-02, -1.0358e-02],
          [-2.3754e-03,  9.4925e-03, -5.8897e-03]]],


        [[[-1.8914e-02, -4.0088e-03, -1.2799e-03],
          [-1.0244e-02,  2.4045e-03,  7.5146e-03],
          [ 1.7842e-03,  1.1362e-02,  7.9346e-03]],

         [[-1.1791e-02,  2.1600e-02,  1.1208e-02],
          [-3.1683e-03,  2.7585e-03,  1.1603e-02],
          [ 8.8663e-03, -1.0458e-02,  1.2195e-02]],

         [[-8.0852e-03,  4.4941e-03,  1.8167e-02],
          [ 1.3493e-02, -5.6380e-03,  1.4368e-02],
          [-5.3874e-03, -1.0903e-02, -1.2260e-02]],

         ...,

         [[ 1.1524e-02, -1.8365e-02, -1.7295e-03],
          [-3.0074e-03, -1.5268e-02,  1.0496e-02],
          [-1.5672e-02, -1.2259e-02,  3.9126e-03]],

         [[-2.5346e-03,  6.7930e-03, -2.6333e-02],
          [-1.5641e-02, -5.7953e-03, -3.3867e-02],
          [-1.4915e-02, -2.0154e-02, -5.8508e-03]],

         [[ 1.1383e-02,  1.7779e-02,  2.7579e-02],
          [ 1.4047e-02,  2.4237e-03,  2.9491e-03],
          [ 7.4822e-03, -1.6203e-03,  1.2005e-02]]],


        [[[ 2.1318e-03,  8.0278e-03,  2.3013e-02],
          [-3.6264e-03, -3.1640e-03, -1.6523e-02],
          [ 6.2663e-03,  4.9826e-03, -8.1930e-03]],

         [[ 4.4089e-04, -3.9912e-03, -1.0552e-02],
          [-9.9241e-03, -1.0691e-02, -1.6656e-02],
          [ 6.5057e-03, -2.6362e-02,  3.1316e-03]],

         [[ 1.8276e-03, -1.7753e-02,  7.8648e-03],
          [-1.2884e-02,  9.7566e-03,  4.5524e-03],
          [-2.0558e-02,  5.7488e-04, -9.1105e-03]],

         ...,

         [[-1.5187e-02, -1.6403e-02,  1.9677e-02],
          [-1.2256e-02, -1.2349e-02, -2.3007e-02],
          [ 8.7698e-03, -1.3756e-02,  1.8666e-02]],

         [[ 2.1200e-02,  8.5871e-03, -8.3960e-03],
          [ 1.3960e-02,  1.1255e-02,  1.6244e-02],
          [-9.9064e-03, -2.1542e-02, -8.5587e-04]],

         [[-9.0507e-03, -9.7094e-03,  1.7594e-03],
          [-1.6882e-02,  2.8264e-02,  1.5901e-02],
          [-2.0597e-02, -4.3566e-03, -1.7912e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.6553e+06, 1.8527e+05, 1.7792e+05,  ..., 1.6242e+00, 1.5825e+00,
        1.5009e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1216]) 

NULL SPACE BASIS :  tensor([[-0.0133,  0.0459, -0.0615,  ...,  0.0004,  0.0075, -0.0097],
        [ 0.0253, -0.0349,  0.0344,  ..., -0.0027, -0.0032,  0.0003],
        [-0.0401, -0.0194, -0.0161,  ...,  0.0041,  0.0024, -0.0034],
        ...,
        [-0.0086, -0.0183,  0.0065,  ...,  0.0253, -0.0065,  0.0022],
        [-0.0179,  0.0183,  0.0064,  ..., -0.0343,  0.0254,  0.0064],
        [ 0.0030,  0.0111, -0.0015,  ...,  0.0005, -0.0222, -0.0077]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.4179e-03, -3.2318e-03,  3.1591e-04,  ...,  1.8826e-04,
         -5.6918e-05,  1.3116e-05],
        [-3.2318e-03,  7.9936e-03, -3.5652e-03,  ...,  2.2142e-05,
         -8.3888e-06,  1.9943e-04],
        [ 3.1591e-04, -3.5652e-03,  6.0323e-03,  ...,  1.3800e-04,
          2.9379e-04, -2.0297e-04],
        ...,
        [ 1.8826e-04,  2.2142e-05,  1.3800e-04,  ...,  2.0170e-02,
         -2.7668e-03, -8.8421e-04],
        [-5.6918e-05, -8.3888e-06,  2.9379e-04,  ..., -2.7668e-03,
          1.9416e-02, -3.4032e-03],
        [ 1.3116e-05,  1.9943e-04, -2.0297e-04,  ..., -8.8421e-04,
         -3.4032e-03,  1.8349e-02]], device='cuda:0') 

reserving basis 1445/4608; cond: 6136330.5, radio:0.0007009913097135723
PARAMETER       :  Parameter containing:
tensor([[[[ 4.5197e-03,  4.0835e-03,  1.2965e-02],
          [-7.4008e-03, -8.5275e-03,  1.2701e-02],
          [-4.4153e-03, -5.0302e-03,  9.7661e-03]],

         [[ 4.2986e-03, -6.3911e-03,  1.6717e-03],
          [ 5.5795e-03,  9.7831e-03,  2.0145e-03],
          [ 1.7411e-02, -7.1797e-03,  8.6998e-03]],

         [[-2.4731e-03, -1.4177e-02, -4.0513e-04],
          [-5.0270e-03, -6.5305e-03, -2.3045e-02],
          [-1.1430e-02,  9.7320e-03, -2.1021e-02]],

         ...,

         [[-4.8810e-03,  2.1780e-03,  3.9597e-03],
          [-1.9883e-03, -1.5523e-03, -1.7584e-02],
          [-1.0442e-02, -1.2391e-03, -2.5680e-02]],

         [[-4.9041e-03,  1.0572e-02,  1.3671e-02],
          [ 2.4914e-03,  1.6409e-02,  3.7160e-03],
          [ 3.1667e-04,  5.0903e-03,  5.1373e-03]],

         [[-1.4110e-02, -3.1410e-02, -2.8348e-02],
          [-6.4913e-03, -2.9641e-04, -2.0023e-02],
          [ 7.3374e-03,  9.2435e-04, -8.6035e-04]]],


        [[[ 8.4083e-03, -1.5935e-02,  8.5809e-03],
          [-3.9540e-03,  5.4549e-03,  6.6672e-03],
          [ 5.0102e-03, -1.3016e-02, -1.1735e-02]],

         [[-1.4868e-02,  7.8270e-03, -9.2697e-03],
          [-3.3185e-03, -2.5520e-03, -3.4612e-03],
          [ 7.1913e-03,  4.6631e-03, -1.0431e-02]],

         [[-1.1651e-03,  9.3264e-05,  1.1394e-02],
          [ 7.4514e-04,  1.3033e-02,  1.6277e-03],
          [ 1.0992e-02,  2.3305e-02,  1.8379e-02]],

         ...,

         [[-2.5981e-03,  3.0868e-03, -9.2312e-03],
          [ 8.2321e-04, -6.3916e-03, -1.8708e-03],
          [-1.7154e-03,  7.2802e-03,  1.3923e-02]],

         [[ 5.2375e-03, -3.8031e-03, -1.2369e-02],
          [-4.2118e-03, -3.2526e-03,  3.4723e-03],
          [-5.0479e-03,  6.7685e-04, -1.2051e-02]],

         [[ 4.1291e-04,  1.2290e-03, -5.0439e-03],
          [-1.5411e-02, -1.9025e-03, -1.2108e-04],
          [ 1.0699e-02, -6.3513e-03, -4.3699e-03]]],


        [[[ 3.0258e-03,  1.0652e-02,  3.0606e-03],
          [ 8.0257e-03,  1.5148e-02,  1.9297e-02],
          [-6.8660e-03, -2.0406e-03,  4.4162e-03]],

         [[ 1.1055e-03,  1.3358e-02,  8.4739e-03],
          [-1.2152e-02, -7.0523e-03,  3.6915e-03],
          [-1.0183e-02, -7.5403e-03, -5.7623e-03]],

         [[-3.9551e-03, -6.8416e-03,  6.1216e-03],
          [-6.2023e-03, -1.1289e-02,  9.4393e-03],
          [-1.2708e-03, -5.0564e-03, -1.2931e-03]],

         ...,

         [[ 8.0452e-03,  2.9548e-03, -9.0430e-03],
          [ 7.0802e-03, -1.5512e-03, -1.1485e-02],
          [ 8.6900e-03,  3.9184e-03,  1.5794e-03]],

         [[ 8.8685e-03,  5.4404e-03,  2.0685e-02],
          [-7.0947e-03, -5.4290e-03,  1.8769e-02],
          [-4.5010e-03,  3.3287e-03, -3.4366e-03]],

         [[-4.6570e-03,  4.8533e-03, -1.2891e-02],
          [-1.1951e-02,  3.5020e-03, -6.8975e-03],
          [-7.4273e-03,  1.0735e-02,  8.5738e-03]]],


        ...,


        [[[ 5.6277e-03, -4.0942e-03, -1.6239e-02],
          [-1.0568e-02,  5.0896e-03, -3.4830e-03],
          [ 1.4748e-03,  1.0740e-02, -1.3953e-02]],

         [[-6.4791e-03,  3.7350e-04,  4.0938e-03],
          [-8.0664e-03,  1.5202e-02, -4.5820e-04],
          [-6.8328e-03,  9.0454e-03, -8.2099e-03]],

         [[ 1.1136e-03, -9.7471e-03,  1.0811e-03],
          [-3.2007e-03, -2.6439e-02, -1.1299e-02],
          [ 1.2465e-02, -1.9991e-02, -1.7804e-02]],

         ...,

         [[-1.8067e-02,  6.1096e-03, -7.5154e-03],
          [-7.1921e-03, -5.1542e-03, -1.2390e-02],
          [-6.6801e-03,  1.0748e-02,  6.2050e-03]],

         [[-1.0868e-03, -3.9994e-03, -3.9878e-03],
          [-6.0428e-03,  5.6463e-03,  1.3251e-02],
          [ 9.5957e-03, -5.6411e-03, -1.0023e-02]],

         [[ 1.4044e-02,  1.3601e-02,  5.4602e-03],
          [-1.1821e-03,  4.6310e-03,  2.2159e-03],
          [-7.2754e-04,  3.3675e-04, -1.4014e-03]]],


        [[[-1.3743e-02, -3.5355e-03,  3.4425e-03],
          [-1.2903e-02,  8.4768e-03, -1.8463e-02],
          [-6.3978e-03, -2.5569e-03,  2.7324e-03]],

         [[ 1.3192e-03, -1.4168e-02, -2.2513e-02],
          [ 9.9252e-03,  4.1988e-03, -1.6329e-02],
          [ 8.9237e-03,  4.3527e-03,  4.6817e-03]],

         [[-7.0966e-03,  7.4695e-04,  1.0734e-02],
          [ 1.7045e-03, -1.1696e-03, -6.0451e-03],
          [-6.8747e-03, -4.9619e-03, -7.3283e-03]],

         ...,

         [[ 2.8562e-03,  1.1286e-03, -7.1437e-03],
          [-2.4284e-02, -1.7741e-02, -1.7406e-02],
          [-2.4570e-04, -2.6426e-03,  1.3775e-02]],

         [[ 4.1124e-03, -3.7746e-03,  2.9656e-03],
          [-3.5581e-03,  2.7742e-05, -4.8343e-04],
          [ 1.0412e-02,  9.7386e-03,  4.4018e-03]],

         [[ 7.5387e-03, -1.4083e-02, -1.3483e-02],
          [ 1.8419e-03, -3.4433e-03,  1.3966e-02],
          [-1.3635e-02,  3.0979e-03, -4.9349e-03]]],


        [[[ 8.5866e-03, -6.5549e-03, -3.3560e-03],
          [-1.4630e-02, -8.8866e-04,  4.6939e-03],
          [ 1.3179e-03, -9.6223e-03, -1.2027e-02]],

         [[-4.4588e-03,  1.3965e-02,  2.2817e-03],
          [ 5.4433e-03, -1.0212e-02,  4.3258e-04],
          [ 4.2384e-03,  7.1066e-03, -5.4959e-03]],

         [[-1.2089e-02, -3.4918e-03,  1.6767e-02],
          [-2.8027e-03,  4.6356e-03,  1.8160e-02],
          [-8.4352e-03, -1.1853e-03,  2.3874e-03]],

         ...,

         [[ 1.3591e-02,  5.5045e-03, -5.4980e-04],
          [ 1.1458e-02, -1.4152e-02,  5.7855e-03],
          [ 1.5644e-03, -1.5187e-02,  1.0480e-02]],

         [[ 2.0003e-02,  2.4073e-02,  2.6462e-03],
          [ 1.6801e-02,  2.1506e-03, -4.9396e-03],
          [-9.5483e-03,  1.2808e-02, -1.1993e-02]],

         [[-4.2817e-03, -1.3604e-02,  7.7454e-03],
          [-1.1571e-02, -5.5971e-03, -5.4109e-03],
          [ 1.0702e-02, -2.7055e-03,  1.9211e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.9548e+06, 3.1535e+05, 3.0496e+05,  ..., 3.2801e-01, 3.2163e-01,
        3.1856e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1445]) 

NULL SPACE BASIS :  tensor([[ 2.6683e-02, -2.7650e-02, -3.8730e-02,  ..., -3.3799e-03,
          1.6484e-02, -4.7616e-05],
        [ 2.2767e-03,  2.6630e-02,  1.5266e-02,  ..., -5.2980e-03,
          6.0097e-04, -2.6619e-03],
        [ 6.7271e-03, -3.5597e-02,  5.4901e-03,  ...,  2.7925e-03,
          3.9226e-03, -4.5739e-03],
        ...,
        [ 1.6148e-02,  6.7469e-03,  1.7570e-02,  ..., -8.4266e-03,
          3.0255e-03, -3.2694e-03],
        [ 7.0584e-03,  7.6574e-03, -2.1764e-03,  ...,  3.9939e-03,
          5.9479e-03, -8.1216e-04],
        [ 1.4231e-02,  1.7011e-02,  9.0633e-04,  ..., -1.6285e-03,
         -4.3986e-03, -1.0921e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 8.0249e-03, -4.6835e-04, -4.4419e-04,  ..., -2.3791e-04,
         -2.7190e-04,  4.9996e-05],
        [-4.6835e-04,  5.2667e-03, -1.7956e-04,  ..., -1.0946e-04,
          5.8097e-05, -2.1457e-04],
        [-4.4419e-04, -1.7956e-04,  6.8555e-03,  ..., -7.0165e-05,
         -8.7409e-06,  2.1099e-04],
        ...,
        [-2.3791e-04, -1.0946e-04, -7.0165e-05,  ...,  1.0156e-02,
         -6.3541e-04, -8.4044e-05],
        [-2.7190e-04,  5.8097e-05, -8.7409e-06,  ..., -6.3541e-04,
          7.5986e-03, -7.0170e-04],
        [ 4.9996e-05, -2.1457e-04,  2.1099e-04,  ..., -8.4044e-05,
         -7.0170e-04,  8.5074e-03]], device='cuda:0') 

reserving basis 195/256; cond: 46577.8984375, radio:0.01269786711782217
PARAMETER       :  Parameter containing:
tensor([[[[-0.0170]],

         [[-0.0339]],

         [[-0.0382]],

         ...,

         [[ 0.0083]],

         [[-0.0450]],

         [[-0.0536]]],


        [[[ 0.0649]],

         [[-0.0593]],

         [[ 0.0166]],

         ...,

         [[ 0.0062]],

         [[ 0.0075]],

         [[ 0.0146]]],


        [[[ 0.0407]],

         [[ 0.0259]],

         [[-0.0080]],

         ...,

         [[ 0.0204]],

         [[ 0.0373]],

         [[-0.0327]]],


        ...,


        [[[-0.0268]],

         [[ 0.0091]],

         [[ 0.0486]],

         ...,

         [[ 0.0179]],

         [[-0.0167]],

         [[-0.0112]]],


        [[[ 0.0175]],

         [[ 0.0207]],

         [[-0.0265]],

         ...,

         [[ 0.0410]],

         [[ 0.0101]],

         [[ 0.0439]]],


        [[[-0.0377]],

         [[ 0.0250]],

         [[ 0.0383]],

         ...,

         [[ 0.0540]],

         [[-0.0619]],

         [[ 0.0371]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.2969e+05, 1.9482e+04, 1.6114e+04, 6.4709e+03, 5.2711e+03, 4.4482e+03,
        3.1470e+03, 1.6436e+03, 1.2104e+03, 1.0059e+03, 8.5384e+02, 7.8309e+02,
        6.4843e+02, 6.1988e+02, 5.8350e+02, 5.2366e+02, 5.1497e+02, 4.7862e+02,
        4.4948e+02, 4.2028e+02, 3.9868e+02, 3.7660e+02, 3.4137e+02, 3.3389e+02,
        2.8214e+02, 2.7336e+02, 2.5221e+02, 2.2685e+02, 2.2128e+02, 2.1296e+02,
        2.0089e+02, 1.8969e+02, 1.8622e+02, 1.7451e+02, 1.7001e+02, 1.6750e+02,
        1.5127e+02, 1.4241e+02, 1.3820e+02, 1.3591e+02, 1.2983e+02, 1.2522e+02,
        1.2006e+02, 1.1336e+02, 1.1100e+02, 1.0838e+02, 1.0604e+02, 1.0232e+02,
        1.0093e+02, 9.7985e+01, 9.3629e+01, 8.9312e+01, 8.8111e+01, 8.4212e+01,
        8.3452e+01, 8.3181e+01, 8.0151e+01, 7.7799e+01, 7.5319e+01, 7.2812e+01,
        7.2490e+01, 6.8118e+01, 6.7522e+01, 6.7118e+01, 6.5360e+01, 6.4008e+01,
        6.2577e+01, 6.1276e+01, 6.0759e+01, 5.9982e+01, 5.9161e+01, 5.8819e+01,
        5.6637e+01, 5.5742e+01, 5.5388e+01, 5.3515e+01, 5.3425e+01, 5.2229e+01,
        5.1108e+01, 5.0004e+01, 4.9785e+01, 4.8477e+01, 4.7622e+01, 4.7149e+01,
        4.6462e+01, 4.6176e+01, 4.5184e+01, 4.4831e+01, 4.4211e+01, 4.3601e+01,
        4.3279e+01, 4.2698e+01, 4.2540e+01, 4.1785e+01, 4.1057e+01, 4.0751e+01,
        4.0519e+01, 4.0010e+01, 3.8858e+01, 3.8660e+01, 3.8130e+01, 3.7650e+01,
        3.7497e+01, 3.7094e+01, 3.6881e+01, 3.6489e+01, 3.6069e+01, 3.5652e+01,
        3.5353e+01, 3.4402e+01, 3.4064e+01, 3.3773e+01, 3.3323e+01, 3.2944e+01,
        3.2643e+01, 3.2267e+01, 3.2166e+01, 3.1391e+01, 3.1325e+01, 3.0872e+01,
        3.0578e+01, 3.0360e+01, 3.0129e+01, 3.0030e+01, 2.9851e+01, 2.9480e+01,
        2.8834e+01, 2.8823e+01, 2.8709e+01, 2.8190e+01, 2.7797e+01, 2.7612e+01,
        2.7468e+01, 2.7288e+01, 2.7070e+01, 2.6739e+01, 2.6587e+01, 2.6220e+01,
        2.6112e+01, 2.5887e+01, 2.5527e+01, 2.5400e+01, 2.5134e+01, 2.5056e+01,
        2.4930e+01, 2.4878e+01, 2.4472e+01, 2.4252e+01, 2.4052e+01, 2.3837e+01,
        2.3542e+01, 2.3458e+01, 2.3238e+01, 2.3197e+01, 2.2928e+01, 2.2627e+01,
        2.2588e+01, 2.2482e+01, 2.2095e+01, 2.1886e+01, 2.1804e+01, 2.1655e+01,
        2.1554e+01, 2.1458e+01, 2.1317e+01, 2.0953e+01, 2.0807e+01, 2.0680e+01,
        2.0522e+01, 2.0343e+01, 2.0235e+01, 2.0137e+01, 2.0013e+01, 1.9831e+01,
        1.9640e+01, 1.9338e+01, 1.9246e+01, 1.9154e+01, 1.8935e+01, 1.8786e+01,
        1.8653e+01, 1.8418e+01, 1.8246e+01, 1.8095e+01, 1.7852e+01, 1.7834e+01,
        1.7785e+01, 1.7594e+01, 1.7490e+01, 1.7292e+01, 1.7224e+01, 1.7006e+01,
        1.6839e+01, 1.6816e+01, 1.6685e+01, 1.6397e+01, 1.6248e+01, 1.6158e+01,
        1.6011e+01, 1.5718e+01, 1.5623e+01, 1.5567e+01, 1.5454e+01, 1.5382e+01,
        1.5161e+01, 1.5119e+01, 1.4952e+01, 1.4782e+01, 1.4693e+01, 1.4680e+01,
        1.4481e+01, 1.4398e+01, 1.4357e+01, 1.4118e+01, 1.3964e+01, 1.3848e+01,
        1.3693e+01, 1.3600e+01, 1.3583e+01, 1.3439e+01, 1.3396e+01, 1.3062e+01,
        1.2996e+01, 1.2706e+01, 1.2637e+01, 1.2440e+01, 1.2354e+01, 1.2348e+01,
        1.2066e+01, 1.1947e+01, 1.1825e+01, 1.1785e+01, 1.1715e+01, 1.1572e+01,
        1.1474e+01, 1.1357e+01, 1.1298e+01, 1.1071e+01, 1.1012e+01, 1.0854e+01,
        1.0769e+01, 1.0432e+01, 1.0378e+01, 1.0280e+01, 1.0099e+01, 9.7937e+00,
        9.7173e+00, 9.4947e+00, 9.3307e+00, 9.1580e+00, 8.9542e+00, 8.7776e+00,
        8.7147e+00, 8.3356e+00, 8.1270e+00, 7.0784e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 195]) 

NULL SPACE BASIS :  tensor([[-7.8763e-02,  4.4676e-02,  1.1802e-01,  ..., -1.0493e-02,
         -1.4619e-02,  4.9331e-03],
        [ 1.9754e-02, -1.9304e-02,  1.0561e-01,  ..., -3.2624e-04,
          1.5228e-02,  4.1239e-02],
        [-5.3089e-02, -3.5315e-02,  2.8976e-02,  ...,  1.0890e-02,
         -9.3717e-02,  2.8053e-02],
        ...,
        [-1.8715e-02, -2.8136e-02, -2.7795e-02,  ..., -7.5115e-04,
          4.4767e-02,  8.4544e-03],
        [ 4.9710e-05, -5.9031e-02,  3.6333e-02,  ...,  3.7854e-02,
         -5.7251e-03, -5.2516e-03],
        [-1.7703e-02, -8.3651e-03, -1.7811e-03,  ...,  7.3787e-02,
          1.6265e-01,  4.0277e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0312,  0.0028, -0.0017,  ...,  0.0010, -0.0030,  0.0022],
        [ 0.0028,  0.0574, -0.0027,  ..., -0.0057,  0.0026,  0.0014],
        [-0.0017, -0.0027,  0.0618,  ..., -0.0027,  0.0030, -0.0007],
        ...,
        [ 0.0010, -0.0057, -0.0027,  ...,  0.0556, -0.0016,  0.0015],
        [-0.0030,  0.0026,  0.0030,  ..., -0.0016,  0.0447, -0.0009],
        [ 0.0022,  0.0014, -0.0007,  ...,  0.0015, -0.0009,  0.0646]],
       device='cuda:0') 

reserving basis 1626/4608; cond: 7178528.0, radio:0.0006606150418519974
PARAMETER       :  Parameter containing:
tensor([[[[-4.0554e-03,  4.9257e-03, -1.2809e-02],
          [-7.8241e-03,  5.7656e-03, -1.6170e-02],
          [ 9.8295e-03, -3.0722e-03,  6.9298e-03]],

         [[-8.0681e-03, -2.7778e-03,  3.5264e-03],
          [ 8.9437e-03,  4.8192e-03, -3.6353e-03],
          [ 4.8612e-03, -5.2144e-03,  6.0508e-03]],

         [[ 1.3005e-03, -1.3859e-02, -1.4533e-02],
          [-4.1671e-03, -1.7666e-03, -1.2448e-02],
          [ 1.6966e-03,  1.4257e-02, -4.4336e-03]],

         ...,

         [[-3.6789e-03,  9.3938e-03, -1.0468e-03],
          [ 1.0388e-02,  4.0417e-04,  5.9383e-03],
          [ 4.8702e-03,  1.0818e-02, -5.0367e-03]],

         [[-1.2645e-02,  6.1885e-04, -1.1183e-02],
          [ 1.9154e-03, -7.1754e-03, -2.4748e-03],
          [-8.1708e-04, -1.0684e-02,  1.0212e-02]],

         [[-6.1675e-04,  1.0403e-02,  7.9493e-03],
          [ 6.9559e-03,  9.5708e-03,  1.2950e-02],
          [-1.2800e-02,  3.5158e-03, -9.7514e-03]]],


        [[[ 6.0887e-03, -4.1016e-03,  7.7803e-03],
          [-1.3545e-02, -9.0286e-03,  5.4921e-03],
          [-2.3063e-02,  1.0115e-03,  6.8374e-03]],

         [[-7.4163e-03,  1.6102e-03,  2.7833e-03],
          [-6.7588e-03,  1.6396e-02, -1.9901e-03],
          [ 1.1613e-02,  8.1197e-03,  1.6136e-02]],

         [[ 5.4322e-03, -7.0143e-03,  5.6511e-03],
          [ 1.2171e-03, -1.1046e-02,  5.1954e-03],
          [ 1.1797e-02,  1.6434e-05,  7.3317e-03]],

         ...,

         [[ 9.0779e-03,  3.5347e-03,  8.1827e-03],
          [ 1.1826e-02, -3.4896e-03, -4.3320e-03],
          [-6.4121e-03, -1.0615e-03,  2.6575e-03]],

         [[ 2.2427e-03, -1.8687e-03, -9.9699e-03],
          [ 7.9110e-04, -5.1914e-03, -2.2973e-03],
          [-3.2910e-03,  1.2604e-02,  2.8411e-03]],

         [[ 1.2347e-02, -7.3024e-03, -9.5883e-03],
          [ 1.2901e-02, -5.2587e-03,  1.7879e-03],
          [-1.8341e-03,  5.8986e-03, -1.0554e-02]]],


        [[[-5.7770e-03, -1.8172e-02, -7.9530e-03],
          [-2.6419e-03,  7.7607e-03, -1.5088e-02],
          [ 1.5356e-02,  1.7943e-02,  5.7105e-03]],

         [[ 6.8642e-03, -1.0650e-02,  1.1335e-03],
          [-5.8426e-03, -3.2977e-03, -2.1332e-02],
          [-4.3057e-03, -4.1782e-03, -2.1018e-02]],

         [[-1.5124e-03, -2.1961e-03,  5.1120e-03],
          [ 1.3801e-03,  2.2721e-03, -5.3392e-04],
          [-5.9724e-03,  1.0171e-02, -2.0971e-03]],

         ...,

         [[ 7.1804e-03,  1.3809e-02,  8.3149e-03],
          [-5.4380e-03,  9.5406e-03, -8.2072e-03],
          [ 4.7471e-04, -1.2658e-02, -2.2554e-03]],

         [[ 5.2857e-03, -1.1294e-02, -5.8206e-03],
          [ 4.1225e-03, -6.9071e-03, -1.9648e-02],
          [-3.5542e-03,  1.6163e-03, -1.3623e-02]],

         [[-1.2538e-02, -1.9033e-02, -3.7679e-03],
          [ 7.5079e-03,  6.1917e-03, -1.1366e-02],
          [-5.0631e-03, -1.1392e-02,  6.3469e-03]]],


        ...,


        [[[-1.1218e-02,  8.7571e-03, -8.5749e-03],
          [ 9.0434e-03,  7.1630e-03,  1.0371e-02],
          [-1.2583e-02, -1.7608e-02,  1.0138e-03]],

         [[-1.6191e-02, -1.6899e-03, -1.9938e-03],
          [ 4.2375e-03,  2.8662e-03, -1.8480e-03],
          [-9.6598e-03,  4.6958e-03, -3.1773e-03]],

         [[-1.5358e-02,  1.4013e-03,  1.7106e-03],
          [-1.3382e-02, -5.5611e-03, -1.6082e-02],
          [-8.1306e-03, -1.4342e-02, -3.0206e-03]],

         ...,

         [[ 1.0177e-02, -1.4918e-02,  5.6053e-03],
          [-3.3545e-03,  3.8906e-03, -1.0809e-02],
          [-8.3619e-03, -1.3888e-03,  5.2990e-03]],

         [[-1.5861e-03, -2.4197e-03, -1.6141e-03],
          [-1.2531e-02, -8.0020e-03, -1.9279e-02],
          [-1.4614e-02, -4.4769e-03, -4.0048e-03]],

         [[-3.3288e-03, -3.9772e-03, -1.0492e-02],
          [ 8.9094e-03,  9.8649e-03, -1.3825e-03],
          [ 1.0444e-02,  5.9695e-03,  1.0314e-02]]],


        [[[-1.4671e-02, -2.7776e-03,  9.6792e-03],
          [ 6.6469e-03, -9.2743e-04,  2.2711e-02],
          [-6.2285e-03, -7.6199e-03,  2.0985e-02]],

         [[ 1.0869e-02, -7.5281e-03, -1.8453e-03],
          [-4.0650e-03, -4.6532e-03,  8.6575e-03],
          [-1.9242e-02, -7.1429e-03, -1.8967e-02]],

         [[-1.5744e-02, -1.4502e-02, -1.3512e-02],
          [ 1.3704e-02,  1.4135e-03, -1.5434e-02],
          [ 8.5854e-03, -1.2021e-02, -3.0456e-03]],

         ...,

         [[ 1.2685e-02,  7.4289e-03, -2.5948e-03],
          [ 1.3068e-02,  1.0119e-02,  1.5672e-02],
          [-5.9572e-03,  9.8212e-03, -1.4989e-02]],

         [[-8.4036e-03,  1.9133e-03, -4.7055e-03],
          [ 5.1767e-03, -4.5661e-03,  1.5530e-03],
          [ 1.5965e-02,  5.5654e-03,  6.6180e-03]],

         [[ 3.6991e-03,  1.1875e-02,  1.0432e-02],
          [-7.8484e-04,  4.4225e-03,  3.2004e-03],
          [-1.7706e-03,  1.6778e-02,  2.1899e-03]]],


        [[[ 1.9482e-02,  4.7489e-03, -1.7576e-03],
          [ 2.8358e-03, -5.6055e-03, -7.1768e-03],
          [-1.0402e-02,  2.4402e-03,  3.4845e-03]],

         [[ 1.5746e-02,  7.6369e-04, -2.7807e-03],
          [-1.3268e-03,  1.7260e-02, -2.6745e-03],
          [ 6.8231e-03,  2.5963e-03,  1.4369e-02]],

         [[ 1.8251e-02,  2.4418e-03,  6.1010e-03],
          [-5.5248e-03, -9.6563e-03, -9.4178e-03],
          [-1.0758e-02, -5.9338e-04, -9.3105e-03]],

         ...,

         [[ 1.0443e-02, -1.4387e-02,  2.0815e-03],
          [ 3.6101e-03,  9.1813e-03,  4.5554e-03],
          [ 1.9566e-02,  2.3631e-02,  2.6465e-02]],

         [[-1.2155e-03,  1.1502e-02, -8.1679e-03],
          [-1.3512e-02,  9.7308e-04, -1.7752e-02],
          [ 7.5065e-03,  6.2409e-04, -1.5424e-03]],

         [[ 1.0756e-04,  3.9865e-03, -1.3599e-02],
          [-1.0368e-02,  6.2988e-03, -1.0917e-02],
          [-5.2656e-03, -1.1809e-02,  1.0597e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.5310e+06, 5.0655e+05, 4.9243e+05,  ..., 3.6746e-01, 3.6429e-01,
        3.5258e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1626]) 

NULL SPACE BASIS :  tensor([[-0.0015,  0.0228,  0.0087,  ..., -0.0049, -0.0103,  0.0226],
        [-0.0046, -0.0260, -0.0080,  ...,  0.0012,  0.0180, -0.0138],
        [ 0.0039, -0.0013, -0.0021,  ..., -0.0060, -0.0066,  0.0003],
        ...,
        [ 0.0070, -0.0020, -0.0071,  ...,  0.0037,  0.0031,  0.0039],
        [ 0.0088,  0.0212, -0.0117,  ..., -0.0047, -0.0068,  0.0005],
        [-0.0111,  0.0035,  0.0167,  ..., -0.0007,  0.0053,  0.0014]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 8.4921e-03, -1.7922e-03, -1.2987e-04,  ...,  3.8558e-05,
         -9.1596e-06,  3.7696e-05],
        [-1.7922e-03,  6.8006e-03, -1.7620e-03,  ...,  1.5135e-04,
          1.0029e-04, -6.6208e-05],
        [-1.2987e-04, -1.7620e-03,  7.6602e-03,  ..., -2.9670e-05,
         -9.9534e-05,  2.7840e-05],
        ...,
        [ 3.8558e-05,  1.5135e-04, -2.9670e-05,  ...,  4.9371e-03,
         -1.6722e-04,  9.2600e-05],
        [-9.1596e-06,  1.0029e-04, -9.9534e-05,  ..., -1.6722e-04,
          3.8902e-03, -1.3146e-04],
        [ 3.7696e-05, -6.6208e-05,  2.7840e-05,  ...,  9.2600e-05,
         -1.3146e-04,  4.2174e-03]], device='cuda:0') 

reserving basis 717/4608; cond: 49109016.0, radio:3.680617373902351e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 8.3533e-03,  5.0654e-03, -4.6268e-03],
          [-1.8819e-03, -8.6739e-03, -2.3916e-03],
          [ 1.3518e-03, -7.9430e-03,  5.8276e-03]],

         [[-4.7213e-03, -1.1140e-02,  1.1575e-03],
          [-4.5677e-03, -1.1908e-03, -2.7311e-03],
          [ 3.1377e-03,  9.5825e-03, -1.7814e-03]],

         [[-1.0951e-02,  6.1497e-03,  1.1864e-02],
          [ 1.5226e-02,  1.3107e-02,  3.2471e-03],
          [-1.1700e-03,  1.7554e-02,  5.4142e-03]],

         ...,

         [[ 1.1880e-02,  3.6566e-03, -1.4215e-04],
          [ 2.4413e-03, -8.5607e-03,  1.3733e-03],
          [ 6.5969e-03, -5.8246e-03,  7.7283e-03]],

         [[ 7.3699e-03, -9.4775e-04,  3.1557e-03],
          [ 3.7541e-03,  1.4190e-03,  7.2603e-03],
          [ 8.2023e-03,  8.3589e-03, -1.9859e-04]],

         [[-7.8515e-03,  1.2673e-02, -3.0967e-04],
          [-1.0006e-02, -1.3031e-02,  1.4327e-03],
          [-1.1875e-02,  4.8186e-03, -1.4413e-02]]],


        [[[-8.5624e-04,  8.0188e-03, -2.1754e-03],
          [ 1.5401e-03, -2.7897e-03, -9.0995e-03],
          [-5.5806e-03, -1.5010e-02,  6.5761e-03]],

         [[ 4.0089e-03,  6.6685e-03,  3.0200e-03],
          [ 1.0337e-02,  1.3690e-02, -1.2724e-02],
          [ 1.6972e-02,  7.5843e-03, -9.7629e-03]],

         [[ 7.0801e-03,  5.9002e-03,  2.8709e-03],
          [ 3.7333e-03, -4.3763e-03,  6.6668e-03],
          [-2.9129e-03, -7.4900e-03,  3.6224e-04]],

         ...,

         [[-1.9686e-03,  8.9025e-03,  4.8373e-03],
          [-8.2103e-03, -8.0005e-03,  7.4614e-03],
          [-7.3034e-03,  6.2602e-03, -3.1850e-03]],

         [[ 5.0009e-03, -3.0266e-03,  9.4707e-03],
          [-2.6980e-03, -1.4131e-02, -4.6730e-03],
          [ 4.7717e-03, -1.5985e-03, -1.4154e-02]],

         [[ 1.5428e-02,  1.9666e-02, -2.0132e-03],
          [ 4.2297e-04,  8.3054e-05,  5.8451e-03],
          [ 4.2548e-03, -2.5321e-03, -9.1187e-04]]],


        [[[-4.0888e-03, -4.6126e-03, -6.7326e-03],
          [-3.1850e-03, -1.2547e-02, -7.5236e-03],
          [-7.4371e-03, -2.4524e-03, -1.4046e-03]],

         [[-3.9284e-03,  5.3946e-03, -1.2144e-02],
          [ 1.2701e-03, -1.0102e-02, -1.3268e-02],
          [-5.5201e-03, -3.6457e-03, -9.0128e-03]],

         [[-1.5411e-03, -1.2509e-02,  4.2627e-03],
          [-5.6861e-03, -7.6608e-03, -1.3829e-02],
          [-5.0070e-03,  1.3459e-02, -6.4582e-03]],

         ...,

         [[ 3.0520e-03, -1.8271e-02, -6.3806e-04],
          [ 1.5722e-02,  7.9850e-04, -6.2122e-03],
          [-4.9215e-03,  8.1193e-03, -2.2735e-03]],

         [[ 3.6134e-03,  1.8583e-02,  4.5391e-03],
          [-2.7169e-03, -5.2731e-03,  8.9233e-03],
          [ 3.2177e-04, -3.9726e-03,  4.1841e-04]],

         [[-1.3578e-02, -1.5207e-02, -1.6328e-02],
          [-1.2527e-02, -2.4945e-04,  1.2938e-03],
          [-1.2479e-02, -1.1208e-02, -1.6953e-02]]],


        ...,


        [[[-1.7324e-03,  7.9738e-04,  9.1201e-03],
          [ 7.9790e-05,  9.0329e-03, -8.3032e-03],
          [ 7.9501e-03, -2.8133e-04,  4.6123e-03]],

         [[-8.1489e-03, -3.8336e-03,  9.1656e-03],
          [ 8.6799e-03,  7.2658e-03, -7.7345e-03],
          [ 1.2330e-02, -4.1218e-03, -2.9317e-03]],

         [[-1.4475e-02, -1.5350e-03, -1.1540e-02],
          [-1.2520e-02, -1.1421e-02, -3.5394e-03],
          [ 1.1714e-02,  1.6486e-02, -7.1178e-03]],

         ...,

         [[-3.8983e-03,  6.9408e-03,  5.9848e-03],
          [ 9.4894e-03,  2.0579e-03,  5.7338e-03],
          [-5.0527e-03, -3.4218e-03, -2.4104e-03]],

         [[-8.8291e-03,  1.1952e-02, -9.1338e-03],
          [ 9.7399e-03, -1.2211e-02, -8.2146e-03],
          [ 3.3764e-04,  1.4429e-03, -1.6879e-02]],

         [[ 1.6865e-02,  2.0385e-02,  2.4048e-02],
          [ 1.1208e-02, -1.9939e-03,  3.8255e-03],
          [ 1.0662e-02,  1.5999e-02,  6.5782e-03]]],


        [[[ 1.5857e-03, -2.0913e-03, -9.7016e-05],
          [-3.2478e-03, -1.1146e-02, -9.9861e-03],
          [-1.1295e-02,  1.3879e-03, -2.8727e-03]],

         [[-4.3289e-03,  9.9473e-03,  2.5327e-03],
          [-9.0997e-04,  5.9970e-03,  1.3263e-02],
          [-6.8458e-03, -1.0806e-02,  1.2108e-03]],

         [[ 4.4381e-03,  7.9689e-04,  4.4680e-03],
          [-1.2426e-02, -8.3325e-03,  9.8209e-03],
          [ 3.4482e-03,  9.7081e-03, -6.2351e-03]],

         ...,

         [[ 4.6206e-03,  4.5278e-03, -4.3551e-03],
          [ 8.6921e-03,  4.1718e-03,  3.0383e-03],
          [ 2.3334e-03,  1.7449e-03, -1.4551e-02]],

         [[-8.8339e-03, -3.4906e-03, -5.1582e-03],
          [ 2.4452e-03,  1.4866e-03, -1.4661e-02],
          [-7.2492e-04, -8.2613e-03,  2.8829e-03]],

         [[-1.3464e-02,  2.6841e-04,  3.6989e-05],
          [ 2.9390e-03, -4.5593e-03,  4.5072e-03],
          [-1.1587e-02, -1.1663e-02,  1.4294e-03]]],


        [[[-6.3027e-03, -1.0073e-02, -8.6964e-03],
          [ 3.1817e-03, -6.9616e-03,  2.0354e-03],
          [ 3.8771e-03,  2.6734e-03, -4.8758e-03]],

         [[-5.9602e-03,  9.3036e-03,  7.2016e-04],
          [ 1.8154e-03,  7.8229e-03,  1.9971e-03],
          [ 2.4727e-03,  1.1350e-02,  2.5667e-03]],

         [[-2.1196e-03,  5.8999e-03,  6.1678e-03],
          [-7.2032e-03,  1.0421e-02, -1.3058e-02],
          [ 2.9613e-04,  2.2487e-03,  2.6872e-03]],

         ...,

         [[ 4.2713e-03, -6.0494e-04,  1.3293e-02],
          [ 1.0462e-02,  3.3168e-03,  1.4427e-02],
          [ 7.2547e-03,  1.1082e-02,  3.8198e-03]],

         [[-8.1291e-03, -1.9583e-02, -7.3887e-04],
          [-1.1334e-02, -1.2159e-02,  4.2458e-03],
          [ 5.8086e-03, -8.9041e-05,  4.8348e-03]],

         [[-7.1116e-03,  4.8961e-03, -1.5339e-02],
          [-4.7935e-03,  1.0261e-02, -1.2424e-02],
          [ 7.0739e-03, -3.8341e-03, -5.5793e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([6.4581e+05, 1.4376e+05, 1.2885e+05,  ..., 1.7326e-02, 1.6350e-02,
        1.3151e-02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 717]) 

NULL SPACE BASIS :  tensor([[-0.0018,  0.0068,  0.0626,  ...,  0.0118, -0.0087,  0.0200],
        [ 0.0016,  0.0021, -0.0151,  ..., -0.0124, -0.0079, -0.0197],
        [-0.0228,  0.0039, -0.0204,  ...,  0.0149, -0.0062, -0.0062],
        ...,
        [ 0.0117, -0.0070, -0.0041,  ...,  0.0035,  0.0008,  0.0035],
        [-0.0289,  0.0070,  0.0037,  ..., -0.0074,  0.0011, -0.0003],
        [ 0.0177, -0.0165, -0.0004,  ...,  0.0028, -0.0031, -0.0030]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.7559e-02, -2.4498e-03, -1.6228e-05,  ..., -1.4142e-04,
          3.5701e-04,  5.1198e-05],
        [-2.4498e-03,  1.4768e-02, -2.5319e-03,  ...,  2.6847e-04,
         -1.8652e-04,  1.4477e-04],
        [-1.6228e-05, -2.5319e-03,  1.6565e-02,  ..., -5.7571e-06,
          1.0314e-04, -1.3842e-04],
        ...,
        [-1.4142e-04,  2.6847e-04, -5.7571e-06,  ...,  1.6092e-03,
         -7.5506e-04,  1.7774e-04],
        [ 3.5701e-04, -1.8652e-04,  1.0314e-04,  ..., -7.5506e-04,
          1.8701e-03, -7.7264e-04],
        [ 5.1198e-05,  1.4477e-04, -1.3842e-04,  ...,  1.7774e-04,
         -7.7264e-04,  1.5307e-03]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 82.400, Total time 0.56
 * Val loss 0.953, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 97.280, Total time 3.17
 * Val loss 0.080, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 71.600, Total time 0.57
 * Val loss 0.871, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 72.160, Total time 3.18
 * Val loss 0.796, Total time 0.00
**************************************************
validation split name: 3
 * Val Acc 66.800, Total time 0.57
 * Val loss 0.991, Total time 0.00
**************************************************
training split name: 3
 * Val Acc 70.360, Total time 3.19
 * Val loss 0.872, Total time 0.00
**************************************************
validation split name: 4
 * Val Acc 73.200, Total time 0.58
 * Val loss 0.845, Total time 0.00
**************************************************
training split name: 4
 * Val Acc 73.880, Total time 3.19
 * Val loss 0.783, Total time 0.00
**************************************************
====================== 5 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0085 (0.0085)	2.349 (2.349)	12.50 (12.50)
[10/157]	0.0963 (0.0915)	0.0579 (0.0544)	2.057 (2.237)	37.50 (30.97)
[20/157]	0.0970 (0.0935)	0.0583 (0.0563)	1.909 (2.091)	40.62 (37.80)
[30/157]	0.0959 (0.0939)	0.0585 (0.0570)	1.661 (1.979)	46.88 (42.04)
[40/157]	0.0957 (0.0942)	0.0577 (0.0574)	1.674 (1.908)	50.00 (43.06)
[50/157]	0.0962 (0.0943)	0.0592 (0.0575)	1.430 (1.831)	53.12 (45.83)
[60/157]	0.0972 (0.0944)	0.0589 (0.0576)	1.271 (1.772)	65.62 (47.18)
[70/157]	0.0954 (0.0945)	0.0581 (0.0577)	1.428 (1.720)	59.38 (48.28)
[80/157]	0.0952 (0.0946)	0.0575 (0.0578)	1.208 (1.688)	75.00 (49.15)
[90/157]	0.0936 (0.0946)	0.0575 (0.0578)	1.231 (1.644)	65.62 (50.48)
[100/157]	0.1030 (0.0951)	0.0624 (0.0582)	1.041 (1.609)	71.88 (51.45)
[110/157]	0.1039 (0.0958)	0.0647 (0.0587)	1.276 (1.579)	75.00 (52.42)
[120/157]	0.1034 (0.0963)	0.0641 (0.0591)	1.428 (1.554)	56.25 (53.33)
[130/157]	0.1044 (0.0968)	0.0644 (0.0595)	1.322 (1.527)	53.12 (54.20)
[140/157]	0.1038 (0.0972)	0.0635 (0.0598)	1.152 (1.514)	71.88 (54.61)
[150/157]	0.1020 (0.0975)	0.0615 (0.0600)	1.208 (1.497)	65.62 (55.17)
[156/157]	0.0867 (0.0976)	0.0598 (0.0601)	0.978 (1.491)	75.00 (55.22)
 * Train Acc 55.220
 * Val Acc 60.700, Total time 0.58
 * Val loss 1.173, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0085 (0.0085)	1.211 (1.211)	78.12 (78.12)
[10/157]	0.0960 (0.0920)	0.0585 (0.0551)	1.306 (1.254)	53.12 (62.50)
[20/157]	0.0951 (0.0934)	0.0581 (0.0566)	1.092 (1.228)	68.75 (63.69)
[30/157]	0.0955 (0.0940)	0.0576 (0.0573)	1.134 (1.259)	62.50 (61.79)
[40/157]	0.0962 (0.0943)	0.0579 (0.0575)	1.384 (1.263)	50.00 (61.05)
[50/157]	0.0973 (0.0944)	0.0586 (0.0576)	1.159 (1.248)	59.38 (61.58)
[60/157]	0.0955 (0.0946)	0.0567 (0.0578)	1.322 (1.256)	62.50 (60.71)
[70/157]	0.0947 (0.0947)	0.0570 (0.0579)	1.212 (1.244)	56.25 (61.09)
[80/157]	0.1001 (0.0952)	0.0611 (0.0582)	1.379 (1.240)	53.12 (61.11)
[90/157]	0.1012 (0.0958)	0.0623 (0.0586)	1.324 (1.236)	46.88 (61.13)
[100/157]	0.0995 (0.0964)	0.0604 (0.0590)	1.189 (1.225)	62.50 (61.54)
[110/157]	0.1017 (0.0968)	0.0607 (0.0593)	1.234 (1.223)	62.50 (61.51)
[120/157]	0.1014 (0.0971)	0.0619 (0.0595)	1.154 (1.226)	65.62 (61.29)
[130/157]	0.1009 (0.0974)	0.0620 (0.0597)	1.179 (1.224)	59.38 (61.31)
[140/157]	0.1011 (0.0977)	0.0616 (0.0599)	1.455 (1.226)	56.25 (61.15)
[150/157]	0.1013 (0.0979)	0.0607 (0.0600)	1.033 (1.225)	65.62 (61.07)
[156/157]	0.0829 (0.0979)	0.0549 (0.0600)	1.460 (1.231)	62.50 (60.82)
 * Train Acc 60.820
 * Val Acc 64.000, Total time 0.61
 * Val loss 1.089, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0091 (0.0091)	1.063 (1.063)	71.88 (71.88)
[10/157]	0.1022 (0.0952)	0.0625 (0.0563)	0.910 (1.179)	78.12 (64.20)
[20/157]	0.1006 (0.0981)	0.0609 (0.0588)	1.067 (1.154)	68.75 (64.29)
[30/157]	0.1017 (0.0990)	0.0623 (0.0596)	1.283 (1.132)	68.75 (65.22)
[40/157]	0.1012 (0.0995)	0.0612 (0.0602)	1.148 (1.152)	56.25 (64.56)
[50/157]	0.0999 (0.0999)	0.0602 (0.0607)	0.930 (1.152)	65.62 (63.97)
[60/157]	0.1013 (0.1001)	0.0614 (0.0609)	1.061 (1.161)	62.50 (63.63)
[70/157]	0.1005 (0.1002)	0.0613 (0.0611)	1.147 (1.150)	62.50 (63.82)
[80/157]	0.1011 (0.1002)	0.0623 (0.0612)	1.250 (1.153)	59.38 (63.73)
[90/157]	0.1013 (0.1004)	0.0619 (0.0614)	1.129 (1.148)	62.50 (63.87)
[100/157]	0.1014 (0.1004)	0.0619 (0.0615)	1.288 (1.146)	56.25 (63.58)
[110/157]	0.1024 (0.1005)	0.0624 (0.0616)	1.216 (1.141)	65.62 (63.94)
[120/157]	0.0999 (0.1005)	0.0609 (0.0616)	1.170 (1.144)	59.38 (63.69)
[130/157]	0.1004 (0.1004)	0.0613 (0.0616)	1.375 (1.154)	62.50 (63.38)
[140/157]	0.1002 (0.1004)	0.0612 (0.0616)	1.160 (1.161)	68.75 (63.10)
[150/157]	0.1010 (0.1004)	0.0621 (0.0617)	1.264 (1.164)	53.12 (62.98)
[156/157]	0.0838 (0.1003)	0.0566 (0.0617)	1.385 (1.164)	37.50 (63.04)
 * Train Acc 63.040
 * Val Acc 63.200, Total time 0.59
 * Val loss 1.067, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0454 (0.0454)	0.0086 (0.0086)	1.067 (1.067)	68.75 (68.75)
[10/157]	0.1031 (0.0952)	0.0629 (0.0574)	1.565 (1.148)	53.12 (63.92)
[20/157]	0.1010 (0.0979)	0.0622 (0.0597)	1.135 (1.111)	68.75 (63.24)
[30/157]	0.1014 (0.0990)	0.0617 (0.0606)	1.372 (1.126)	46.88 (62.30)
[40/157]	0.0986 (0.0994)	0.0592 (0.0607)	1.244 (1.116)	62.50 (63.19)
[50/157]	0.1035 (0.0998)	0.0634 (0.0610)	1.385 (1.123)	56.25 (62.81)
[60/157]	0.1014 (0.0999)	0.0621 (0.0611)	1.474 (1.120)	43.75 (63.06)
[70/157]	0.1016 (0.1001)	0.0621 (0.0612)	1.514 (1.118)	53.12 (63.73)
[80/157]	0.1006 (0.1001)	0.0617 (0.0613)	1.152 (1.117)	68.75 (63.77)
[90/157]	0.1016 (0.1002)	0.0622 (0.0615)	1.271 (1.122)	59.38 (63.60)
[100/157]	0.1000 (0.1003)	0.0604 (0.0615)	0.957 (1.133)	68.75 (63.18)
[110/157]	0.1016 (0.1004)	0.0613 (0.0616)	0.942 (1.138)	71.88 (63.15)
[120/157]	0.1002 (0.1004)	0.0605 (0.0615)	0.951 (1.138)	68.75 (63.20)
[130/157]	0.1043 (0.1005)	0.0613 (0.0616)	1.010 (1.142)	65.62 (62.91)
[140/157]	0.0996 (0.1005)	0.0605 (0.0615)	0.865 (1.139)	81.25 (63.10)
[150/157]	0.1004 (0.1004)	0.0616 (0.0615)	0.908 (1.132)	71.88 (63.64)
[156/157]	0.0843 (0.1003)	0.0573 (0.0615)	1.138 (1.132)	62.50 (63.62)
 * Train Acc 63.620
 * Val Acc 65.100, Total time 0.60
 * Val loss 1.027, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0464 (0.0464)	0.0084 (0.0084)	0.965 (0.965)	71.88 (71.88)
[10/157]	0.1024 (0.0954)	0.0630 (0.0571)	1.206 (1.023)	65.62 (69.03)
[20/157]	0.1008 (0.0977)	0.0612 (0.0592)	1.152 (1.078)	59.38 (65.92)
[30/157]	0.1006 (0.0986)	0.0614 (0.0601)	0.873 (1.086)	75.00 (65.83)
[40/157]	0.0994 (0.0992)	0.0603 (0.0605)	0.880 (1.078)	71.88 (66.39)
[50/157]	0.1025 (0.0995)	0.0631 (0.0608)	1.244 (1.064)	68.75 (67.03)
[60/157]	0.1010 (0.0998)	0.0617 (0.0611)	0.784 (1.065)	78.12 (67.01)
[70/157]	0.1026 (0.1000)	0.0623 (0.0613)	1.022 (1.061)	68.75 (66.59)
[80/157]	0.1012 (0.1001)	0.0617 (0.0614)	1.004 (1.056)	75.00 (67.32)
[90/157]	0.1029 (0.1003)	0.0620 (0.0615)	1.082 (1.062)	65.62 (67.24)
[100/157]	0.1010 (0.1003)	0.0618 (0.0615)	1.049 (1.060)	65.62 (67.33)
[110/157]	0.1013 (0.1004)	0.0614 (0.0616)	1.031 (1.070)	65.62 (66.78)
[120/157]	0.1019 (0.1004)	0.0620 (0.0616)	1.416 (1.077)	50.00 (66.50)
[130/157]	0.1020 (0.1005)	0.0617 (0.0617)	1.146 (1.082)	56.25 (66.22)
[140/157]	0.1013 (0.1005)	0.0616 (0.0617)	0.974 (1.086)	68.75 (66.07)
[150/157]	0.1031 (0.1005)	0.0617 (0.0618)	1.140 (1.091)	56.25 (65.77)
[156/157]	0.0849 (0.1004)	0.0564 (0.0617)	0.723 (1.091)	87.50 (65.88)
 * Train Acc 65.880
 * Val Acc 65.500, Total time 0.60
 * Val loss 1.021, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0480 (0.0480)	0.0089 (0.0089)	1.216 (1.216)	56.25 (56.25)
[10/157]	0.0990 (0.0951)	0.0603 (0.0560)	1.270 (1.056)	56.25 (64.49)
[20/157]	0.0998 (0.0976)	0.0610 (0.0588)	0.997 (1.063)	78.12 (64.43)
[30/157]	0.1019 (0.0989)	0.0628 (0.0601)	0.829 (1.062)	75.00 (64.82)
[40/157]	0.1003 (0.0992)	0.0617 (0.0604)	0.871 (1.064)	75.00 (64.94)
[50/157]	0.1029 (0.0995)	0.0636 (0.0609)	1.111 (1.058)	53.12 (65.69)
[60/157]	0.1023 (0.0999)	0.0631 (0.0612)	1.230 (1.061)	62.50 (66.14)
[70/157]	0.1001 (0.1001)	0.0609 (0.0613)	0.956 (1.059)	71.88 (65.93)
[80/157]	0.1027 (0.1002)	0.0632 (0.0614)	0.927 (1.065)	68.75 (65.78)
[90/157]	0.1007 (0.1003)	0.0614 (0.0615)	0.873 (1.071)	75.00 (65.38)
[100/157]	0.0999 (0.1003)	0.0609 (0.0615)	1.057 (1.077)	65.62 (65.50)
[110/157]	0.1023 (0.1004)	0.0607 (0.0616)	1.052 (1.079)	65.62 (65.79)
[120/157]	0.1007 (0.1004)	0.0620 (0.0616)	1.143 (1.075)	56.25 (65.83)
[130/157]	0.1012 (0.1004)	0.0625 (0.0616)	1.096 (1.077)	65.62 (65.94)
[140/157]	0.1021 (0.1004)	0.0629 (0.0617)	1.251 (1.079)	53.12 (65.85)
[150/157]	0.1001 (0.1004)	0.0604 (0.0617)	1.163 (1.080)	65.62 (65.65)
[156/157]	0.0844 (0.1003)	0.0571 (0.0617)	1.182 (1.079)	62.50 (65.66)
 * Train Acc 65.660
 * Val Acc 66.700, Total time 0.59
 * Val loss 0.977, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0087 (0.0087)	1.371 (1.371)	53.12 (53.12)
[10/157]	0.1006 (0.0949)	0.0618 (0.0571)	0.904 (1.075)	75.00 (63.64)
[20/157]	0.1012 (0.0974)	0.0618 (0.0594)	1.065 (1.047)	62.50 (65.92)
[30/157]	0.1019 (0.0986)	0.0623 (0.0605)	1.028 (1.048)	65.62 (66.63)
[40/157]	0.1002 (0.0991)	0.0610 (0.0608)	1.106 (1.060)	62.50 (66.31)
[50/157]	0.1018 (0.0993)	0.0610 (0.0609)	0.904 (1.052)	78.12 (67.52)
[60/157]	0.1013 (0.0995)	0.0611 (0.0610)	1.288 (1.060)	50.00 (67.21)
[70/157]	0.1001 (0.0997)	0.0607 (0.0611)	0.836 (1.079)	81.25 (66.81)
[80/157]	0.1017 (0.0998)	0.0624 (0.0612)	1.125 (1.079)	62.50 (66.82)
[90/157]	0.1000 (0.1000)	0.0609 (0.0613)	1.079 (1.071)	65.62 (67.00)
[100/157]	0.1016 (0.1001)	0.0627 (0.0614)	0.974 (1.062)	71.88 (67.36)
[110/157]	0.1008 (0.1002)	0.0615 (0.0614)	1.133 (1.052)	62.50 (67.57)
[120/157]	0.1010 (0.1002)	0.0618 (0.0615)	1.001 (1.054)	68.75 (67.54)
[130/157]	0.0997 (0.1003)	0.0600 (0.0616)	1.008 (1.053)	62.50 (67.70)
[140/157]	0.1000 (0.1003)	0.0605 (0.0616)	1.419 (1.055)	50.00 (67.40)
[150/157]	0.1019 (0.1003)	0.0634 (0.0616)	1.190 (1.056)	59.38 (67.22)
[156/157]	0.0842 (0.1002)	0.0563 (0.0616)	1.475 (1.056)	50.00 (67.22)
 * Train Acc 67.220
 * Val Acc 67.100, Total time 0.59
 * Val loss 0.970, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0085 (0.0085)	1.280 (1.280)	65.62 (65.62)
[10/157]	0.1009 (0.0947)	0.0620 (0.0573)	1.195 (1.133)	59.38 (65.06)
[20/157]	0.1003 (0.0979)	0.0613 (0.0598)	0.858 (1.024)	75.00 (68.75)
[30/157]	0.1014 (0.0987)	0.0630 (0.0607)	1.341 (1.037)	62.50 (68.35)
[40/157]	0.1013 (0.0994)	0.0613 (0.0612)	1.096 (1.016)	65.62 (68.90)
[50/157]	0.1016 (0.0996)	0.0627 (0.0614)	0.829 (1.013)	75.00 (68.93)
[60/157]	0.1006 (0.0999)	0.0616 (0.0615)	1.126 (1.036)	65.62 (68.03)
[70/157]	0.1029 (0.1000)	0.0634 (0.0616)	1.106 (1.033)	71.88 (68.09)
[80/157]	0.1002 (0.1002)	0.0604 (0.0616)	1.071 (1.037)	68.75 (67.71)
[90/157]	0.1023 (0.1003)	0.0620 (0.0617)	0.928 (1.029)	59.38 (67.89)
[100/157]	0.0999 (0.1003)	0.0607 (0.0617)	1.086 (1.032)	68.75 (67.98)
[110/157]	0.1015 (0.1004)	0.0618 (0.0618)	0.877 (1.037)	75.00 (67.68)
[120/157]	0.0998 (0.1004)	0.0607 (0.0617)	0.911 (1.039)	68.75 (67.69)
[130/157]	0.1047 (0.1005)	0.0615 (0.0618)	1.145 (1.037)	71.88 (67.96)
[140/157]	0.1022 (0.1005)	0.0621 (0.0618)	1.053 (1.040)	65.62 (67.84)
[150/157]	0.1004 (0.1005)	0.0622 (0.0618)	1.134 (1.041)	65.62 (67.63)
[156/157]	0.0856 (0.1004)	0.0584 (0.0618)	1.384 (1.039)	62.50 (67.60)
 * Train Acc 67.600
 * Val Acc 68.100, Total time 0.60
 * Val loss 0.941, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0090 (0.0090)	0.891 (0.891)	71.88 (71.88)
[10/157]	0.1010 (0.0952)	0.0620 (0.0569)	1.197 (1.089)	68.75 (67.61)
[20/157]	0.1008 (0.0976)	0.0608 (0.0592)	1.014 (1.038)	62.50 (69.35)
[30/157]	0.1028 (0.0986)	0.0635 (0.0603)	1.376 (1.050)	59.38 (67.74)
[40/157]	0.0989 (0.0992)	0.0602 (0.0607)	0.742 (1.049)	75.00 (67.45)
[50/157]	0.1027 (0.0997)	0.0633 (0.0611)	1.025 (1.046)	71.88 (67.77)
[60/157]	0.1015 (0.0998)	0.0617 (0.0612)	1.096 (1.044)	68.75 (68.19)
[70/157]	0.1012 (0.0999)	0.0625 (0.0614)	1.161 (1.044)	68.75 (68.09)
[80/157]	0.1000 (0.1001)	0.0610 (0.0615)	0.807 (1.023)	75.00 (68.71)
[90/157]	0.1001 (0.1001)	0.0614 (0.0615)	0.856 (1.015)	75.00 (68.99)
[100/157]	0.1019 (0.1001)	0.0629 (0.0616)	0.922 (1.026)	71.88 (68.53)
[110/157]	0.1007 (0.1002)	0.0615 (0.0617)	1.175 (1.021)	71.88 (68.64)
[120/157]	0.0995 (0.1002)	0.0605 (0.0617)	0.885 (1.025)	68.75 (68.47)
[130/157]	0.1011 (0.1002)	0.0626 (0.0617)	1.015 (1.020)	65.62 (68.49)
[140/157]	0.0999 (0.1003)	0.0607 (0.0618)	0.875 (1.017)	71.88 (68.66)
[150/157]	0.1001 (0.1003)	0.0612 (0.0618)	1.386 (1.019)	40.62 (68.48)
[156/157]	0.0854 (0.1002)	0.0574 (0.0618)	1.366 (1.026)	62.50 (68.12)
 * Train Acc 68.120
 * Val Acc 69.000, Total time 0.60
 * Val loss 0.949, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0087 (0.0087)	0.760 (0.760)	71.88 (71.88)
[10/157]	0.1013 (0.0951)	0.0621 (0.0567)	1.105 (0.925)	65.62 (69.03)
[20/157]	0.1008 (0.0975)	0.0617 (0.0591)	1.111 (0.991)	56.25 (69.94)
[30/157]	0.1010 (0.0989)	0.0620 (0.0603)	0.813 (0.967)	78.12 (71.07)
[40/157]	0.1016 (0.0992)	0.0619 (0.0607)	1.028 (0.960)	75.00 (71.42)
[50/157]	0.1038 (0.0996)	0.0640 (0.0611)	1.033 (0.989)	71.88 (70.28)
[60/157]	0.1030 (0.0998)	0.0628 (0.0614)	0.856 (0.996)	68.75 (69.83)
[70/157]	0.1020 (0.1000)	0.0615 (0.0615)	0.713 (0.995)	81.25 (69.41)
[80/157]	0.1002 (0.1001)	0.0609 (0.0615)	0.709 (0.996)	81.25 (69.41)
[90/157]	0.0997 (0.1003)	0.0608 (0.0617)	1.292 (1.007)	56.25 (68.89)
[100/157]	0.1021 (0.1003)	0.0626 (0.0617)	0.961 (1.009)	75.00 (69.03)
[110/157]	0.1015 (0.1004)	0.0598 (0.0618)	0.874 (1.008)	75.00 (69.03)
[120/157]	0.1016 (0.1004)	0.0628 (0.0618)	1.391 (1.014)	53.12 (68.75)
[130/157]	0.1000 (0.1005)	0.0609 (0.0619)	1.184 (1.014)	62.50 (68.61)
[140/157]	0.1018 (0.1005)	0.0629 (0.0619)	1.020 (1.012)	71.88 (68.79)
[150/157]	0.1023 (0.1005)	0.0628 (0.0619)	1.125 (1.012)	50.00 (68.63)
[156/157]	0.0831 (0.1004)	0.0565 (0.0619)	1.158 (1.009)	62.50 (68.74)
 * Train Acc 68.740
 * Val Acc 70.100, Total time 0.60
 * Val loss 0.914, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0092 (0.0092)	1.345 (1.345)	53.12 (53.12)
[10/157]	0.1015 (0.0955)	0.0615 (0.0572)	1.294 (1.063)	46.88 (66.19)
[20/157]	0.1017 (0.0984)	0.0615 (0.0599)	1.017 (1.045)	71.88 (67.41)
[30/157]	0.1024 (0.0992)	0.0628 (0.0607)	0.773 (1.013)	78.12 (68.85)
[40/157]	0.1002 (0.0997)	0.0609 (0.0610)	1.368 (1.023)	43.75 (67.91)
[50/157]	0.1025 (0.1001)	0.0630 (0.0614)	1.079 (1.030)	71.88 (67.22)
[60/157]	0.1020 (0.1002)	0.0622 (0.0615)	1.075 (1.030)	68.75 (67.01)
[70/157]	0.1002 (0.1004)	0.0599 (0.0615)	1.325 (1.041)	56.25 (66.95)
[80/157]	0.1008 (0.1005)	0.0606 (0.0616)	1.064 (1.035)	65.62 (67.40)
[90/157]	0.1027 (0.1006)	0.0624 (0.0616)	0.829 (1.031)	71.88 (67.62)
[100/157]	0.1014 (0.1007)	0.0623 (0.0617)	0.966 (1.029)	68.75 (67.79)
[110/157]	0.1016 (0.1008)	0.0618 (0.0617)	0.826 (1.023)	78.12 (68.19)
[120/157]	0.1007 (0.1008)	0.0617 (0.0618)	0.888 (1.020)	71.88 (68.29)
[130/157]	0.1015 (0.1008)	0.0622 (0.0618)	0.763 (1.016)	71.88 (68.44)
[140/157]	0.1012 (0.1009)	0.0616 (0.0619)	1.124 (1.020)	56.25 (67.95)
[150/157]	0.1015 (0.1009)	0.0619 (0.0619)	1.069 (1.018)	68.75 (68.11)
[156/157]	0.0844 (0.1008)	0.0557 (0.0619)	0.717 (1.016)	75.00 (68.14)
 * Train Acc 68.140
 * Val Acc 70.000, Total time 0.60
 * Val loss 0.911, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0090 (0.0090)	1.267 (1.267)	65.62 (65.62)
[10/157]	0.1015 (0.0949)	0.0625 (0.0566)	0.938 (0.945)	71.88 (72.16)
[20/157]	0.1013 (0.0980)	0.0615 (0.0595)	1.187 (0.968)	59.38 (71.28)
[30/157]	0.1013 (0.0989)	0.0617 (0.0602)	1.097 (0.970)	68.75 (70.97)
[40/157]	0.1018 (0.0996)	0.0616 (0.0608)	1.043 (0.969)	65.62 (71.19)
[50/157]	0.1006 (0.1000)	0.0612 (0.0610)	0.803 (0.955)	71.88 (71.88)
[60/157]	0.1018 (0.1001)	0.0626 (0.0612)	1.301 (0.964)	62.50 (70.80)
[70/157]	0.1003 (0.1003)	0.0610 (0.0614)	0.904 (0.968)	71.88 (70.42)
[80/157]	0.1025 (0.1004)	0.0620 (0.0615)	0.915 (0.971)	75.00 (70.06)
[90/157]	0.1026 (0.1005)	0.0629 (0.0616)	0.922 (0.975)	71.88 (69.88)
[100/157]	0.1008 (0.1006)	0.0613 (0.0617)	0.903 (0.986)	81.25 (69.77)
[110/157]	0.1023 (0.1007)	0.0609 (0.0618)	0.917 (0.988)	56.25 (69.40)
[120/157]	0.1027 (0.1007)	0.0632 (0.0618)	0.840 (0.991)	65.62 (69.16)
[130/157]	0.0979 (0.1008)	0.0579 (0.0618)	1.244 (0.996)	56.25 (68.96)
[140/157]	0.1016 (0.1008)	0.0623 (0.0619)	1.281 (0.995)	59.38 (68.84)
[150/157]	0.1007 (0.1008)	0.0615 (0.0619)	0.893 (0.992)	71.88 (69.21)
[156/157]	0.0843 (0.1007)	0.0568 (0.0619)	1.282 (0.991)	50.00 (69.20)
 * Train Acc 69.200
 * Val Acc 70.300, Total time 0.60
 * Val loss 0.898, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0094 (0.0094)	1.196 (1.196)	59.38 (59.38)
[10/157]	0.0972 (0.1001)	0.0577 (0.0608)	0.651 (0.942)	78.12 (70.45)
[20/157]	0.0970 (0.0989)	0.0583 (0.0600)	1.030 (0.952)	65.62 (69.35)
[30/157]	0.0978 (0.0985)	0.0583 (0.0597)	0.900 (0.968)	71.88 (69.05)
[40/157]	0.0991 (0.0982)	0.0594 (0.0596)	0.917 (0.970)	68.75 (69.44)
[50/157]	0.0982 (0.0981)	0.0591 (0.0596)	1.033 (0.961)	75.00 (69.85)
[60/157]	0.1203 (0.0982)	0.0797 (0.0597)	0.890 (0.959)	68.75 (69.67)
[70/157]	0.1067 (0.1000)	0.0667 (0.0614)	1.394 (0.959)	59.38 (69.89)
[80/157]	0.1084 (0.1010)	0.0677 (0.0621)	0.983 (0.972)	71.88 (69.75)
[90/157]	0.0951 (0.1008)	0.0574 (0.0620)	1.164 (0.976)	62.50 (69.47)
[100/157]	0.1090 (0.1010)	0.0697 (0.0623)	1.123 (0.981)	62.50 (69.06)
[110/157]	0.0977 (0.1007)	0.0592 (0.0622)	0.772 (0.979)	78.12 (68.98)
[120/157]	0.0975 (0.1003)	0.0590 (0.0618)	0.827 (0.977)	59.38 (69.01)
[130/157]	0.1023 (0.1004)	0.0634 (0.0619)	1.029 (0.975)	59.38 (69.04)
[140/157]	0.1022 (0.1005)	0.0630 (0.0620)	1.090 (0.975)	68.75 (69.17)
[150/157]	0.1001 (0.1006)	0.0625 (0.0621)	0.865 (0.980)	71.88 (69.10)
[156/157]	0.0851 (0.1005)	0.0581 (0.0621)	0.977 (0.979)	75.00 (69.22)
 * Train Acc 69.220
 * Val Acc 69.600, Total time 0.59
 * Val loss 0.891, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0086 (0.0086)	1.024 (1.024)	71.88 (71.88)
[10/157]	0.1019 (0.0956)	0.0624 (0.0575)	0.831 (0.977)	81.25 (72.16)
[20/157]	0.1019 (0.0986)	0.0626 (0.0602)	1.091 (0.970)	75.00 (70.09)
[30/157]	0.1019 (0.0997)	0.0621 (0.0611)	0.770 (0.948)	78.12 (70.46)
[40/157]	0.1023 (0.1002)	0.0627 (0.0616)	0.784 (0.933)	75.00 (70.27)
[50/157]	0.1014 (0.1006)	0.0619 (0.0619)	0.827 (0.931)	68.75 (70.16)
[60/157]	0.1001 (0.1008)	0.0612 (0.0622)	0.864 (0.938)	71.88 (70.18)
[70/157]	0.1012 (0.1010)	0.0623 (0.0623)	1.106 (0.947)	62.50 (70.20)
[80/157]	0.1030 (0.1011)	0.0629 (0.0624)	0.821 (0.948)	75.00 (70.10)
[90/157]	0.1020 (0.1012)	0.0622 (0.0625)	0.594 (0.943)	84.38 (70.50)
[100/157]	0.1009 (0.1012)	0.0621 (0.0626)	0.782 (0.944)	81.25 (70.82)
[110/157]	0.1013 (0.1013)	0.0616 (0.0627)	0.965 (0.953)	65.62 (70.58)
[120/157]	0.1029 (0.1014)	0.0632 (0.0627)	0.893 (0.949)	68.75 (70.53)
[130/157]	0.1020 (0.1014)	0.0633 (0.0628)	1.026 (0.954)	65.62 (70.32)
[140/157]	0.1009 (0.1015)	0.0619 (0.0628)	0.984 (0.963)	65.62 (70.17)
[150/157]	0.1035 (0.1015)	0.0628 (0.0628)	1.134 (0.968)	65.62 (69.89)
[156/157]	0.0860 (0.1014)	0.0579 (0.0628)	1.316 (0.971)	50.00 (69.74)
 * Train Acc 69.740
 * Val Acc 71.100, Total time 0.60
 * Val loss 0.891, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0094 (0.0094)	0.873 (0.873)	78.12 (78.12)
[10/157]	0.1032 (0.0962)	0.0638 (0.0578)	0.877 (0.941)	68.75 (70.45)
[20/157]	0.1013 (0.0990)	0.0622 (0.0604)	0.853 (0.945)	71.88 (70.54)
[30/157]	0.1018 (0.0999)	0.0625 (0.0612)	0.898 (0.929)	75.00 (70.97)
[40/157]	0.1026 (0.1005)	0.0619 (0.0615)	0.958 (0.932)	71.88 (71.11)
[50/157]	0.1024 (0.1007)	0.0621 (0.0617)	0.741 (0.940)	81.25 (70.83)
[60/157]	0.1032 (0.1009)	0.0635 (0.0619)	1.044 (0.947)	65.62 (70.34)
[70/157]	0.1014 (0.1011)	0.0626 (0.0621)	0.996 (0.943)	68.75 (70.55)
[80/157]	0.1010 (0.1012)	0.0619 (0.0622)	1.002 (0.941)	65.62 (70.91)
[90/157]	0.1028 (0.1013)	0.0630 (0.0624)	1.035 (0.952)	53.12 (70.19)
[100/157]	0.1009 (0.1014)	0.0613 (0.0624)	1.202 (0.950)	59.38 (70.39)
[110/157]	0.1001 (0.1014)	0.0616 (0.0625)	0.942 (0.946)	68.75 (70.47)
[120/157]	0.1008 (0.1014)	0.0617 (0.0626)	0.883 (0.951)	65.62 (70.12)
[130/157]	0.1017 (0.1015)	0.0611 (0.0626)	0.824 (0.946)	75.00 (70.37)
[140/157]	0.1014 (0.1015)	0.0612 (0.0626)	1.435 (0.952)	59.38 (70.32)
[150/157]	0.1016 (0.1016)	0.0616 (0.0626)	0.936 (0.957)	81.25 (70.05)
[156/157]	0.0857 (0.1015)	0.0592 (0.0626)	1.375 (0.957)	62.50 (70.10)
 * Train Acc 70.100
 * Val Acc 70.500, Total time 0.60
 * Val loss 0.880, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0092 (0.0092)	0.710 (0.710)	78.12 (78.12)
[10/157]	0.1015 (0.0957)	0.0627 (0.0576)	0.893 (0.897)	78.12 (69.60)
[20/157]	0.1016 (0.0987)	0.0626 (0.0603)	0.770 (0.881)	78.12 (72.62)
[30/157]	0.1015 (0.0998)	0.0631 (0.0614)	1.096 (0.893)	62.50 (71.57)
[40/157]	0.1023 (0.1004)	0.0627 (0.0618)	0.919 (0.919)	75.00 (71.34)
[50/157]	0.1068 (0.1008)	0.0649 (0.0621)	0.905 (0.927)	68.75 (70.71)
[60/157]	0.1023 (0.1009)	0.0631 (0.0622)	0.974 (0.960)	59.38 (69.47)
[70/157]	0.1037 (0.1011)	0.0639 (0.0623)	0.659 (0.964)	87.50 (69.50)
[80/157]	0.1036 (0.1012)	0.0639 (0.0624)	0.841 (0.958)	65.62 (69.48)
[90/157]	0.1028 (0.1013)	0.0632 (0.0625)	0.715 (0.937)	78.12 (70.19)
[100/157]	0.1028 (0.1014)	0.0630 (0.0625)	0.889 (0.944)	71.88 (69.93)
[110/157]	0.1032 (0.1014)	0.0633 (0.0626)	0.956 (0.939)	65.62 (70.05)
[120/157]	0.1044 (0.1015)	0.0645 (0.0627)	0.890 (0.942)	68.75 (70.04)
[130/157]	0.1030 (0.1015)	0.0635 (0.0627)	0.568 (0.950)	87.50 (69.99)
[140/157]	0.1022 (0.1015)	0.0635 (0.0627)	0.902 (0.951)	71.88 (69.97)
[150/157]	0.1009 (0.1016)	0.0620 (0.0628)	1.088 (0.955)	71.88 (69.76)
[156/157]	0.0861 (0.1015)	0.0570 (0.0627)	1.829 (0.958)	50.00 (69.64)
 * Train Acc 69.640
 * Val Acc 71.000, Total time 0.59
 * Val loss 0.879, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0107 (0.0107)	1.220 (1.220)	65.62 (65.62)
[10/157]	0.1024 (0.0964)	0.0630 (0.0581)	0.776 (0.992)	75.00 (71.59)
[20/157]	0.1016 (0.0991)	0.0621 (0.0605)	0.769 (0.931)	71.88 (73.07)
[30/157]	0.1007 (0.1001)	0.0613 (0.0614)	0.916 (0.950)	62.50 (70.97)
[40/157]	0.0996 (0.1005)	0.0607 (0.0617)	1.039 (0.925)	81.25 (72.10)
[50/157]	0.0987 (0.1008)	0.0592 (0.0618)	1.270 (0.959)	53.12 (70.59)
[60/157]	0.1003 (0.1010)	0.0607 (0.0620)	1.146 (0.950)	68.75 (70.95)
[70/157]	0.1022 (0.1011)	0.0622 (0.0621)	1.063 (0.950)	65.62 (70.99)
[80/157]	0.1017 (0.1013)	0.0620 (0.0622)	1.042 (0.950)	71.88 (71.14)
[90/157]	0.1000 (0.1013)	0.0614 (0.0623)	0.949 (0.936)	75.00 (71.60)
[100/157]	0.1023 (0.1014)	0.0634 (0.0624)	0.672 (0.934)	84.38 (71.50)
[110/157]	0.1021 (0.1015)	0.0635 (0.0625)	0.891 (0.933)	75.00 (71.68)
[120/157]	0.1018 (0.1015)	0.0630 (0.0626)	1.092 (0.937)	71.88 (71.28)
[130/157]	0.1024 (0.1015)	0.0630 (0.0626)	0.794 (0.941)	68.75 (70.92)
[140/157]	0.1019 (0.1016)	0.0633 (0.0627)	0.830 (0.945)	78.12 (70.66)
[150/157]	0.1006 (0.1016)	0.0623 (0.0627)	0.993 (0.947)	62.50 (70.65)
[156/157]	0.0879 (0.1015)	0.0597 (0.0627)	1.806 (0.951)	37.50 (70.52)
 * Train Acc 70.520
 * Val Acc 71.000, Total time 0.59
 * Val loss 0.879, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0092 (0.0092)	1.056 (1.056)	62.50 (62.50)
[10/157]	0.1020 (0.0961)	0.0624 (0.0577)	0.792 (1.001)	75.00 (69.03)
[20/157]	0.1022 (0.0989)	0.0629 (0.0602)	1.363 (1.008)	62.50 (69.20)
[30/157]	0.1013 (0.0999)	0.0621 (0.0613)	1.016 (0.969)	65.62 (70.36)
[40/157]	0.1023 (0.1004)	0.0624 (0.0616)	0.871 (0.952)	81.25 (71.27)
[50/157]	0.1005 (0.1006)	0.0613 (0.0618)	0.874 (0.943)	71.88 (71.26)
[60/157]	0.1028 (0.1009)	0.0621 (0.0620)	0.773 (0.938)	81.25 (71.06)
[70/157]	0.1019 (0.1010)	0.0633 (0.0622)	0.742 (0.926)	81.25 (71.52)
[80/157]	0.1028 (0.1012)	0.0624 (0.0624)	0.998 (0.926)	68.75 (71.76)
[90/157]	0.1020 (0.1012)	0.0627 (0.0624)	1.395 (0.934)	59.38 (71.50)
[100/157]	0.1016 (0.1013)	0.0627 (0.0625)	0.923 (0.937)	65.62 (71.47)
[110/157]	0.1012 (0.1013)	0.0621 (0.0625)	1.081 (0.936)	59.38 (71.09)
[120/157]	0.1025 (0.1014)	0.0623 (0.0626)	0.852 (0.942)	78.12 (70.92)
[130/157]	0.1013 (0.1014)	0.0625 (0.0626)	1.182 (0.940)	65.62 (71.16)
[140/157]	0.1011 (0.1015)	0.0619 (0.0627)	0.870 (0.941)	75.00 (71.14)
[150/157]	0.1007 (0.1015)	0.0614 (0.0627)	1.114 (0.943)	62.50 (71.13)
[156/157]	0.0840 (0.1014)	0.0574 (0.0627)	1.290 (0.942)	87.50 (71.24)
 * Train Acc 71.240
 * Val Acc 71.400, Total time 0.59
 * Val loss 0.864, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0087 (0.0087)	1.239 (1.239)	56.25 (56.25)
[10/157]	0.1018 (0.0957)	0.0630 (0.0578)	0.997 (0.901)	68.75 (73.58)
[20/157]	0.0999 (0.0986)	0.0615 (0.0604)	1.150 (0.924)	65.62 (72.77)
[30/157]	0.1004 (0.0997)	0.0608 (0.0613)	1.174 (0.945)	56.25 (71.98)
[40/157]	0.1004 (0.1002)	0.0611 (0.0617)	1.318 (0.954)	59.38 (71.19)
[50/157]	0.1017 (0.1006)	0.0629 (0.0620)	0.874 (0.945)	68.75 (71.08)
[60/157]	0.1024 (0.1009)	0.0621 (0.0622)	1.165 (0.964)	62.50 (70.49)
[70/157]	0.1019 (0.1010)	0.0628 (0.0622)	0.663 (0.963)	87.50 (70.60)
[80/157]	0.1023 (0.1012)	0.0628 (0.0623)	0.809 (0.949)	78.12 (70.95)
[90/157]	0.1011 (0.1012)	0.0612 (0.0623)	1.427 (0.953)	43.75 (70.57)
[100/157]	0.1014 (0.1013)	0.0621 (0.0624)	0.924 (0.944)	75.00 (70.95)
[110/157]	0.1005 (0.1013)	0.0609 (0.0624)	1.064 (0.944)	62.50 (70.69)
[120/157]	0.1016 (0.1014)	0.0615 (0.0625)	0.658 (0.942)	90.62 (70.82)
[130/157]	0.1012 (0.1014)	0.0614 (0.0625)	1.173 (0.942)	65.62 (70.92)
[140/157]	0.1000 (0.1015)	0.0611 (0.0625)	0.856 (0.943)	71.88 (70.77)
[150/157]	0.1031 (0.1015)	0.0638 (0.0625)	0.949 (0.935)	65.62 (71.07)
[156/157]	0.0852 (0.1013)	0.0579 (0.0625)	0.842 (0.931)	75.00 (71.30)
 * Train Acc 71.300
 * Val Acc 71.600, Total time 0.60
 * Val loss 0.851, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0089 (0.0089)	0.777 (0.777)	71.88 (71.88)
[10/157]	0.1019 (0.0957)	0.0628 (0.0577)	0.907 (0.974)	68.75 (68.75)
[20/157]	0.1019 (0.0986)	0.0621 (0.0601)	0.860 (0.955)	78.12 (68.75)
[30/157]	0.1049 (0.0995)	0.0647 (0.0610)	0.697 (0.943)	84.38 (70.06)
[40/157]	0.1018 (0.0999)	0.0629 (0.0614)	0.819 (0.932)	68.75 (70.43)
[50/157]	0.1015 (0.1003)	0.0623 (0.0617)	0.782 (0.931)	75.00 (70.59)
[60/157]	0.1011 (0.1006)	0.0625 (0.0619)	1.077 (0.925)	68.75 (71.06)
[70/157]	0.1028 (0.1008)	0.0628 (0.0621)	1.279 (0.924)	62.50 (70.99)
[80/157]	0.1008 (0.1009)	0.0618 (0.0622)	1.083 (0.923)	62.50 (71.18)
[90/157]	0.1010 (0.1010)	0.0616 (0.0622)	0.834 (0.916)	71.88 (71.60)
[100/157]	0.1008 (0.1011)	0.0606 (0.0623)	0.901 (0.922)	71.88 (71.35)
[110/157]	0.1022 (0.1012)	0.0626 (0.0624)	0.955 (0.917)	68.75 (71.51)
[120/157]	0.1004 (0.1012)	0.0609 (0.0624)	0.667 (0.917)	84.38 (71.54)
[130/157]	0.1016 (0.1013)	0.0626 (0.0624)	0.835 (0.917)	65.62 (71.37)
[140/157]	0.1013 (0.1014)	0.0625 (0.0625)	1.160 (0.919)	62.50 (71.17)
[150/157]	0.1021 (0.1014)	0.0631 (0.0625)	0.813 (0.914)	81.25 (71.54)
[156/157]	0.0859 (0.1013)	0.0572 (0.0625)	1.352 (0.915)	50.00 (71.50)
 * Train Acc 71.500
 * Val Acc 71.600, Total time 0.61
 * Val loss 0.845, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0454 (0.0454)	0.0094 (0.0094)	0.825 (0.825)	78.12 (78.12)
[10/157]	0.1024 (0.0960)	0.0629 (0.0578)	0.819 (0.944)	65.62 (67.05)
[20/157]	0.1057 (0.0988)	0.0648 (0.0604)	0.791 (0.910)	75.00 (70.39)
[30/157]	0.1015 (0.1000)	0.0618 (0.0613)	1.004 (0.932)	71.88 (70.56)
[40/157]	0.1009 (0.1005)	0.0618 (0.0617)	0.536 (0.913)	84.38 (71.57)
[50/157]	0.1024 (0.1008)	0.0625 (0.0620)	0.925 (0.914)	71.88 (71.94)
[60/157]	0.1022 (0.1010)	0.0629 (0.0622)	0.894 (0.904)	65.62 (72.23)
[70/157]	0.1014 (0.1011)	0.0625 (0.0623)	1.108 (0.915)	71.88 (71.61)
[80/157]	0.1009 (0.1012)	0.0610 (0.0624)	1.115 (0.919)	65.62 (71.26)
[90/157]	0.1010 (0.1013)	0.0612 (0.0625)	1.167 (0.918)	62.50 (71.33)
[100/157]	0.1009 (0.1013)	0.0617 (0.0625)	1.073 (0.920)	56.25 (71.07)
[110/157]	0.1027 (0.1014)	0.0633 (0.0626)	0.944 (0.918)	68.75 (71.26)
[120/157]	0.1037 (0.1014)	0.0640 (0.0627)	1.286 (0.921)	62.50 (71.33)
[130/157]	0.1008 (0.1014)	0.0622 (0.0626)	0.885 (0.916)	68.75 (71.33)
[140/157]	0.1012 (0.1014)	0.0626 (0.0627)	0.939 (0.913)	75.00 (71.50)
[150/157]	0.1013 (0.1014)	0.0622 (0.0627)	0.842 (0.913)	71.88 (71.61)
[156/157]	0.0858 (0.1014)	0.0588 (0.0627)	1.085 (0.914)	75.00 (71.58)
 * Train Acc 71.580
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.874, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0085 (0.0085)	1.248 (1.248)	56.25 (56.25)
[10/157]	0.1009 (0.0955)	0.0622 (0.0576)	1.067 (0.951)	65.62 (68.47)
[20/157]	0.1022 (0.0984)	0.0627 (0.0603)	0.695 (0.917)	78.12 (69.94)
[30/157]	0.1025 (0.0996)	0.0633 (0.0613)	1.194 (0.945)	68.75 (69.76)
[40/157]	0.1022 (0.0999)	0.0629 (0.0615)	0.970 (0.953)	65.62 (69.44)
[50/157]	0.1034 (0.1004)	0.0624 (0.0618)	1.160 (0.961)	53.12 (68.93)
[60/157]	0.1033 (0.1006)	0.0636 (0.0620)	1.038 (0.950)	68.75 (69.52)
[70/157]	0.1034 (0.1008)	0.0631 (0.0622)	1.005 (0.941)	68.75 (69.45)
[80/157]	0.1041 (0.1010)	0.0638 (0.0623)	1.137 (0.931)	56.25 (69.79)
[90/157]	0.1041 (0.1011)	0.0630 (0.0623)	0.883 (0.932)	75.00 (70.05)
[100/157]	0.1036 (0.1012)	0.0628 (0.0624)	1.040 (0.936)	68.75 (70.17)
[110/157]	0.1024 (0.1013)	0.0633 (0.0625)	0.787 (0.933)	81.25 (70.33)
[120/157]	0.1041 (0.1013)	0.0640 (0.0626)	0.829 (0.933)	71.88 (70.45)
[130/157]	0.1020 (0.1013)	0.0638 (0.0626)	0.789 (0.933)	71.88 (70.66)
[140/157]	0.1006 (0.1013)	0.0620 (0.0626)	0.861 (0.937)	68.75 (70.35)
[150/157]	0.0975 (0.1014)	0.0591 (0.0626)	1.131 (0.938)	62.50 (70.18)
[156/157]	0.0889 (0.1013)	0.0593 (0.0626)	1.692 (0.936)	50.00 (70.28)
 * Train Acc 70.280
 * Val Acc 72.500, Total time 0.60
 * Val loss 0.841, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0419 (0.0419)	0.0075 (0.0075)	0.918 (0.918)	68.75 (68.75)
[10/157]	0.1026 (0.0960)	0.0636 (0.0577)	0.793 (0.900)	75.00 (72.73)
[20/157]	0.1038 (0.0988)	0.0641 (0.0602)	1.090 (0.944)	62.50 (71.58)
[30/157]	0.1033 (0.0998)	0.0636 (0.0611)	0.699 (0.938)	81.25 (71.47)
[40/157]	0.1035 (0.1004)	0.0634 (0.0616)	0.963 (0.949)	68.75 (70.73)
[50/157]	0.1032 (0.1006)	0.0642 (0.0620)	0.696 (0.940)	78.12 (70.96)
[60/157]	0.1022 (0.1008)	0.0639 (0.0622)	1.030 (0.925)	62.50 (71.36)
[70/157]	0.1027 (0.1010)	0.0636 (0.0624)	0.574 (0.916)	87.50 (71.83)
[80/157]	0.1023 (0.1010)	0.0632 (0.0625)	1.080 (0.913)	59.38 (71.88)
[90/157]	0.1031 (0.1012)	0.0641 (0.0625)	0.831 (0.912)	71.88 (71.81)
[100/157]	0.1011 (0.1012)	0.0622 (0.0626)	1.237 (0.914)	65.62 (71.81)
[110/157]	0.1043 (0.1013)	0.0630 (0.0626)	0.895 (0.903)	81.25 (72.24)
[120/157]	0.1012 (0.1014)	0.0614 (0.0626)	0.815 (0.899)	75.00 (72.26)
[130/157]	0.1016 (0.1014)	0.0620 (0.0627)	0.957 (0.901)	78.12 (72.04)
[140/157]	0.1025 (0.1015)	0.0626 (0.0627)	0.901 (0.900)	75.00 (72.07)
[150/157]	0.1005 (0.1015)	0.0615 (0.0627)	0.827 (0.904)	75.00 (71.90)
[156/157]	0.0860 (0.1014)	0.0579 (0.0626)	0.693 (0.910)	75.00 (71.70)
 * Train Acc 71.700
 * Val Acc 72.200, Total time 0.60
 * Val loss 0.849, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0102 (0.0102)	0.883 (0.883)	78.12 (78.12)
[10/157]	0.1034 (0.0959)	0.0636 (0.0572)	0.659 (0.860)	84.38 (73.86)
[20/157]	0.1041 (0.0988)	0.0636 (0.0601)	0.821 (0.885)	84.38 (73.81)
[30/157]	0.1027 (0.0997)	0.0631 (0.0609)	0.884 (0.884)	78.12 (74.29)
[40/157]	0.1020 (0.1002)	0.0632 (0.0614)	0.896 (0.908)	75.00 (73.09)
[50/157]	0.1025 (0.1005)	0.0634 (0.0617)	1.158 (0.911)	59.38 (73.10)
[60/157]	0.1017 (0.1008)	0.0587 (0.0618)	1.001 (0.900)	68.75 (73.00)
[70/157]	0.0990 (0.1009)	0.0593 (0.0619)	0.609 (0.898)	81.25 (73.02)
[80/157]	0.1000 (0.1010)	0.0612 (0.0620)	1.027 (0.902)	75.00 (72.88)
[90/157]	0.1026 (0.1011)	0.0629 (0.0621)	0.747 (0.902)	84.38 (72.97)
[100/157]	0.1024 (0.1011)	0.0636 (0.0622)	0.727 (0.904)	75.00 (72.80)
[110/157]	0.1023 (0.1012)	0.0633 (0.0623)	0.631 (0.905)	81.25 (72.66)
[120/157]	0.1010 (0.1012)	0.0622 (0.0623)	0.605 (0.899)	78.12 (72.73)
[130/157]	0.1021 (0.1012)	0.0631 (0.0624)	0.690 (0.899)	78.12 (72.76)
[140/157]	0.1016 (0.1013)	0.0624 (0.0625)	1.024 (0.901)	68.75 (72.83)
[150/157]	0.1013 (0.1013)	0.0619 (0.0625)	0.721 (0.906)	84.38 (72.83)
[156/157]	0.0856 (0.1012)	0.0588 (0.0625)	0.881 (0.903)	62.50 (72.88)
 * Train Acc 72.880
 * Val Acc 72.400, Total time 0.59
 * Val loss 0.825, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0084 (0.0084)	0.791 (0.791)	75.00 (75.00)
[10/157]	0.1011 (0.0959)	0.0616 (0.0578)	0.862 (0.879)	71.88 (75.00)
[20/157]	0.1019 (0.0987)	0.0618 (0.0605)	0.828 (0.879)	75.00 (73.96)
[30/157]	0.1011 (0.0994)	0.0622 (0.0611)	1.179 (0.898)	62.50 (71.57)
[40/157]	0.1010 (0.0999)	0.0624 (0.0616)	0.843 (0.907)	78.12 (72.33)
[50/157]	0.1017 (0.1003)	0.0630 (0.0619)	0.951 (0.923)	68.75 (71.69)
[60/157]	0.1012 (0.1006)	0.0618 (0.0622)	1.249 (0.911)	62.50 (72.44)
[70/157]	0.1018 (0.1007)	0.0626 (0.0623)	1.041 (0.911)	65.62 (72.93)
[80/157]	0.1033 (0.1008)	0.0632 (0.0624)	0.709 (0.906)	78.12 (73.23)
[90/157]	0.1031 (0.1009)	0.0639 (0.0625)	0.844 (0.899)	75.00 (73.25)
[100/157]	0.1007 (0.1009)	0.0618 (0.0625)	0.955 (0.897)	62.50 (73.33)
[110/157]	0.1023 (0.1011)	0.0631 (0.0626)	0.833 (0.895)	71.88 (73.34)
[120/157]	0.1022 (0.1011)	0.0630 (0.0627)	0.710 (0.894)	75.00 (73.63)
[130/157]	0.1006 (0.1012)	0.0618 (0.0627)	1.046 (0.892)	68.75 (73.69)
[140/157]	0.1001 (0.1012)	0.0609 (0.0627)	0.849 (0.892)	71.88 (73.63)
[150/157]	0.1019 (0.1013)	0.0617 (0.0628)	1.186 (0.892)	62.50 (73.49)
[156/157]	0.0866 (0.1012)	0.0589 (0.0628)	1.527 (0.898)	37.50 (73.28)
 * Train Acc 73.280
 * Val Acc 72.100, Total time 0.61
 * Val loss 0.849, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0092 (0.0092)	0.738 (0.738)	78.12 (78.12)
[10/157]	0.1025 (0.0952)	0.0634 (0.0575)	1.237 (0.906)	56.25 (68.47)
[20/157]	0.1021 (0.0982)	0.0620 (0.0601)	1.365 (0.932)	62.50 (69.49)
[30/157]	0.1006 (0.0994)	0.0620 (0.0609)	0.875 (0.928)	75.00 (70.36)
[40/157]	0.0997 (0.1000)	0.0608 (0.0614)	0.869 (0.927)	71.88 (70.35)
[50/157]	0.1009 (0.1003)	0.0618 (0.0617)	0.796 (0.906)	71.88 (71.02)
[60/157]	0.1012 (0.1006)	0.0622 (0.0619)	1.048 (0.910)	62.50 (70.90)
[70/157]	0.1015 (0.1008)	0.0621 (0.0621)	0.678 (0.890)	75.00 (71.70)
[80/157]	0.1024 (0.1009)	0.0628 (0.0622)	0.853 (0.877)	78.12 (72.34)
[90/157]	0.1018 (0.1011)	0.0622 (0.0624)	0.794 (0.883)	78.12 (72.42)
[100/157]	0.1006 (0.1012)	0.0610 (0.0623)	1.061 (0.886)	65.62 (72.56)
[110/157]	0.1013 (0.1012)	0.0615 (0.0624)	1.057 (0.885)	65.62 (72.72)
[120/157]	0.1015 (0.1013)	0.0623 (0.0624)	0.854 (0.886)	68.75 (72.83)
[130/157]	0.1013 (0.1013)	0.0609 (0.0625)	0.816 (0.892)	71.88 (72.61)
[140/157]	0.1041 (0.1014)	0.0615 (0.0625)	0.805 (0.888)	78.12 (72.81)
[150/157]	0.1024 (0.1014)	0.0614 (0.0624)	1.046 (0.887)	65.62 (72.89)
[156/157]	0.0846 (0.1013)	0.0567 (0.0624)	0.608 (0.887)	87.50 (73.04)
 * Train Acc 73.040
 * Val Acc 72.100, Total time 0.61
 * Val loss 0.837, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0095 (0.0095)	0.777 (0.777)	75.00 (75.00)
[10/157]	0.1009 (0.0961)	0.0615 (0.0575)	0.831 (0.894)	75.00 (71.59)
[20/157]	0.1029 (0.0988)	0.0627 (0.0600)	0.643 (0.846)	81.25 (74.26)
[30/157]	0.1041 (0.0997)	0.0640 (0.0609)	1.033 (0.841)	68.75 (75.50)
[40/157]	0.1010 (0.1000)	0.0621 (0.0612)	0.988 (0.857)	65.62 (74.85)
[50/157]	0.1013 (0.1004)	0.0626 (0.0616)	0.903 (0.867)	71.88 (74.82)
[60/157]	0.1009 (0.1006)	0.0618 (0.0619)	0.734 (0.874)	75.00 (74.44)
[70/157]	0.1008 (0.1008)	0.0617 (0.0621)	0.933 (0.874)	68.75 (74.12)
[80/157]	0.1011 (0.1009)	0.0624 (0.0623)	0.659 (0.866)	84.38 (74.23)
[90/157]	0.1029 (0.1010)	0.0634 (0.0624)	0.817 (0.869)	71.88 (73.94)
[100/157]	0.1026 (0.1011)	0.0625 (0.0624)	0.799 (0.869)	71.88 (73.61)
[110/157]	0.1026 (0.1011)	0.0628 (0.0624)	0.703 (0.874)	81.25 (73.62)
[120/157]	0.1019 (0.1012)	0.0630 (0.0624)	1.085 (0.875)	59.38 (73.48)
[130/157]	0.1008 (0.1012)	0.0616 (0.0624)	1.168 (0.879)	53.12 (73.31)
[140/157]	0.1028 (0.1013)	0.0623 (0.0625)	0.929 (0.881)	65.62 (73.03)
[150/157]	0.1009 (0.1013)	0.0621 (0.0625)	0.944 (0.879)	71.88 (72.99)
[156/157]	0.0864 (0.1012)	0.0579 (0.0625)	0.752 (0.882)	75.00 (72.92)
 * Train Acc 72.920
 * Val Acc 72.700, Total time 0.61
 * Val loss 0.829, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0091 (0.0091)	0.874 (0.874)	68.75 (68.75)
[10/157]	0.1020 (0.0965)	0.0617 (0.0579)	1.031 (0.856)	71.88 (74.72)
[20/157]	0.1024 (0.0988)	0.0629 (0.0603)	0.982 (0.895)	71.88 (73.21)
[30/157]	0.1011 (0.0995)	0.0619 (0.0610)	0.861 (0.877)	71.88 (73.49)
[40/157]	0.1023 (0.1001)	0.0619 (0.0614)	0.761 (0.862)	75.00 (74.39)
[50/157]	0.1015 (0.1005)	0.0622 (0.0617)	1.179 (0.858)	68.75 (74.33)
[60/157]	0.1017 (0.1007)	0.0617 (0.0619)	0.825 (0.876)	75.00 (73.87)
[70/157]	0.1022 (0.1009)	0.0623 (0.0620)	0.717 (0.890)	81.25 (73.42)
[80/157]	0.1013 (0.1010)	0.0616 (0.0622)	0.742 (0.894)	68.75 (73.19)
[90/157]	0.1044 (0.1012)	0.0629 (0.0623)	0.972 (0.899)	75.00 (72.73)
[100/157]	0.1003 (0.1012)	0.0616 (0.0623)	1.245 (0.898)	56.25 (72.90)
[110/157]	0.1005 (0.1013)	0.0613 (0.0624)	0.735 (0.902)	81.25 (72.64)
[120/157]	0.1007 (0.1013)	0.0616 (0.0625)	0.699 (0.896)	84.38 (72.91)
[130/157]	0.1031 (0.1013)	0.0628 (0.0626)	0.675 (0.895)	71.88 (72.81)
[140/157]	0.1044 (0.1014)	0.0641 (0.0626)	0.878 (0.899)	68.75 (72.52)
[150/157]	0.1019 (0.1013)	0.0623 (0.0626)	0.851 (0.897)	65.62 (72.41)
[156/157]	0.0860 (0.1013)	0.0578 (0.0626)	0.771 (0.896)	75.00 (72.40)
 * Train Acc 72.400
 * Val Acc 72.500, Total time 0.60
 * Val loss 0.840, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0088 (0.0088)	0.761 (0.761)	78.12 (78.12)
[10/157]	0.1001 (0.0957)	0.0610 (0.0576)	0.843 (0.890)	71.88 (71.88)
[20/157]	0.1015 (0.0986)	0.0611 (0.0602)	0.731 (0.926)	75.00 (71.28)
[30/157]	0.1046 (0.0996)	0.0636 (0.0610)	1.083 (0.916)	65.62 (71.07)
[40/157]	0.1021 (0.1000)	0.0631 (0.0615)	1.136 (0.917)	75.00 (71.72)
[50/157]	0.1015 (0.1003)	0.0626 (0.0618)	0.932 (0.911)	75.00 (72.00)
[60/157]	0.1007 (0.1005)	0.0620 (0.0620)	1.081 (0.901)	62.50 (71.88)
[70/157]	0.1010 (0.1007)	0.0618 (0.0622)	0.998 (0.908)	68.75 (71.35)
[80/157]	0.1019 (0.1008)	0.0628 (0.0623)	0.894 (0.903)	78.12 (71.76)
[90/157]	0.1027 (0.1009)	0.0630 (0.0623)	0.956 (0.898)	78.12 (71.98)
[100/157]	0.1027 (0.1010)	0.0630 (0.0624)	0.932 (0.896)	62.50 (71.97)
[110/157]	0.1011 (0.1010)	0.0617 (0.0624)	0.582 (0.884)	81.25 (72.64)
[120/157]	0.1001 (0.1011)	0.0603 (0.0624)	1.200 (0.894)	75.00 (72.65)
[130/157]	0.1018 (0.1011)	0.0618 (0.0624)	0.839 (0.889)	78.12 (72.59)
[140/157]	0.1025 (0.1011)	0.0626 (0.0624)	0.880 (0.885)	68.75 (72.56)
[150/157]	0.1002 (0.1011)	0.0609 (0.0624)	0.781 (0.883)	81.25 (72.83)
[156/157]	0.0848 (0.1010)	0.0581 (0.0623)	0.811 (0.883)	75.00 (72.82)
 * Train Acc 72.820
 * Val Acc 72.800, Total time 0.60
 * Val loss 0.821, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0083 (0.0083)	0.653 (0.653)	84.38 (84.38)
[10/157]	0.1010 (0.0951)	0.0629 (0.0575)	0.692 (0.804)	81.25 (74.72)
[20/157]	0.0996 (0.0980)	0.0603 (0.0599)	0.944 (0.820)	68.75 (74.40)
[30/157]	0.1034 (0.0992)	0.0631 (0.0609)	0.783 (0.825)	75.00 (74.40)
[40/157]	0.1037 (0.0999)	0.0628 (0.0614)	0.781 (0.864)	84.38 (72.71)
[50/157]	0.1043 (0.1002)	0.0639 (0.0616)	0.731 (0.864)	75.00 (72.73)
[60/157]	0.1012 (0.1003)	0.0618 (0.0617)	0.771 (0.850)	81.25 (73.26)
[70/157]	0.1015 (0.1006)	0.0622 (0.0618)	0.915 (0.858)	71.88 (72.80)
[80/157]	0.1011 (0.1007)	0.0619 (0.0620)	1.158 (0.864)	71.88 (72.72)
[90/157]	0.1008 (0.1008)	0.0614 (0.0620)	0.722 (0.867)	78.12 (72.73)
[100/157]	0.1021 (0.1009)	0.0620 (0.0621)	0.738 (0.870)	78.12 (72.59)
[110/157]	0.1018 (0.1009)	0.0619 (0.0621)	0.595 (0.868)	81.25 (72.58)
[120/157]	0.0998 (0.1009)	0.0610 (0.0621)	0.516 (0.865)	87.50 (72.70)
[130/157]	0.1032 (0.1010)	0.0627 (0.0622)	0.965 (0.863)	81.25 (73.00)
[140/157]	0.1019 (0.1010)	0.0620 (0.0621)	0.693 (0.862)	81.25 (72.85)
[150/157]	0.1033 (0.1011)	0.0636 (0.0622)	0.945 (0.866)	65.62 (72.64)
[156/157]	0.0850 (0.1010)	0.0566 (0.0622)	1.193 (0.866)	62.50 (72.66)
 * Train Acc 72.660
 * Val Acc 73.300, Total time 0.60
 * Val loss 0.815, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0090 (0.0090)	0.904 (0.904)	75.00 (75.00)
[10/157]	0.1022 (0.0955)	0.0625 (0.0571)	0.879 (0.866)	75.00 (73.01)
[20/157]	0.1017 (0.0983)	0.0627 (0.0596)	0.948 (0.841)	68.75 (73.36)
[30/157]	0.1020 (0.0994)	0.0629 (0.0606)	1.120 (0.830)	68.75 (74.50)
[40/157]	0.1009 (0.1000)	0.0609 (0.0611)	0.894 (0.847)	71.88 (73.93)
[50/157]	0.1000 (0.1004)	0.0606 (0.0614)	0.875 (0.851)	68.75 (73.71)
[60/157]	0.1029 (0.1006)	0.0632 (0.0617)	0.779 (0.849)	75.00 (73.72)
[70/157]	0.1030 (0.1007)	0.0626 (0.0619)	0.760 (0.850)	78.12 (73.77)
[80/157]	0.1021 (0.1009)	0.0624 (0.0620)	0.870 (0.849)	68.75 (73.73)
[90/157]	0.1035 (0.1010)	0.0631 (0.0621)	0.876 (0.858)	65.62 (73.04)
[100/157]	0.1022 (0.1010)	0.0624 (0.0621)	0.610 (0.860)	84.38 (72.96)
[110/157]	0.1006 (0.1011)	0.0613 (0.0621)	0.798 (0.857)	75.00 (73.20)
[120/157]	0.1029 (0.1011)	0.0623 (0.0622)	0.977 (0.861)	65.62 (73.22)
[130/157]	0.0960 (0.1011)	0.0577 (0.0622)	0.551 (0.855)	81.25 (73.43)
[140/157]	0.0953 (0.1006)	0.0574 (0.0619)	0.721 (0.858)	81.25 (73.47)
[150/157]	0.0931 (0.1003)	0.0558 (0.0616)	0.815 (0.857)	81.25 (73.53)
[156/157]	0.0800 (0.1000)	0.0537 (0.0614)	1.138 (0.858)	62.50 (73.48)
 * Train Acc 73.480
 * Val Acc 72.200, Total time 0.58
 * Val loss 0.812, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0455 (0.0455)	0.0085 (0.0085)	1.003 (1.003)	71.88 (71.88)
[10/157]	0.1038 (0.0980)	0.0635 (0.0591)	0.738 (0.773)	75.00 (79.26)
[20/157]	0.1033 (0.1010)	0.0641 (0.0619)	0.665 (0.791)	78.12 (76.79)
[30/157]	0.0988 (0.0990)	0.0603 (0.0606)	0.667 (0.777)	78.12 (77.52)
[40/157]	0.0963 (0.0982)	0.0577 (0.0600)	0.748 (0.824)	75.00 (76.07)
[50/157]	0.0986 (0.0976)	0.0579 (0.0597)	0.933 (0.836)	75.00 (75.55)
[60/157]	0.1102 (0.0995)	0.0703 (0.0614)	0.822 (0.843)	68.75 (75.15)
[70/157]	0.0936 (0.1002)	0.0562 (0.0620)	1.041 (0.853)	62.50 (74.43)
[80/157]	0.0964 (0.0997)	0.0585 (0.0616)	0.958 (0.859)	71.88 (74.23)
[90/157]	0.1040 (0.0994)	0.0641 (0.0613)	0.617 (0.861)	84.38 (74.31)
[100/157]	0.1018 (0.0997)	0.0623 (0.0615)	0.792 (0.865)	78.12 (73.98)
[110/157]	0.1017 (0.0999)	0.0626 (0.0616)	0.957 (0.858)	71.88 (74.21)
[120/157]	0.1039 (0.1001)	0.0643 (0.0619)	0.832 (0.856)	84.38 (74.28)
[130/157]	0.1050 (0.1003)	0.0650 (0.0620)	0.703 (0.860)	87.50 (74.19)
[140/157]	0.1013 (0.1005)	0.0624 (0.0621)	0.772 (0.861)	75.00 (74.05)
[150/157]	0.1038 (0.1006)	0.0636 (0.0622)	0.863 (0.861)	75.00 (74.15)
[156/157]	0.0863 (0.1006)	0.0578 (0.0622)	0.932 (0.863)	75.00 (74.06)
 * Train Acc 74.060
 * Val Acc 73.700, Total time 0.60
 * Val loss 0.810, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0085 (0.0085)	0.909 (0.909)	68.75 (68.75)
[10/157]	0.1034 (0.0968)	0.0628 (0.0581)	0.916 (0.936)	75.00 (70.74)
[20/157]	0.1033 (0.0996)	0.0628 (0.0605)	0.896 (0.843)	71.88 (74.55)
[30/157]	0.1044 (0.1006)	0.0647 (0.0615)	0.925 (0.854)	68.75 (73.49)
[40/157]	0.1018 (0.1011)	0.0623 (0.0620)	1.100 (0.848)	62.50 (74.24)
[50/157]	0.1049 (0.1015)	0.0647 (0.0623)	0.742 (0.844)	84.38 (74.45)
[60/157]	0.1018 (0.1016)	0.0614 (0.0625)	0.567 (0.834)	84.38 (74.85)
[70/157]	0.1035 (0.1018)	0.0643 (0.0626)	0.994 (0.847)	75.00 (74.34)
[80/157]	0.1034 (0.1019)	0.0641 (0.0627)	0.965 (0.852)	68.75 (74.15)
[90/157]	0.1023 (0.1020)	0.0621 (0.0627)	0.771 (0.856)	75.00 (73.73)
[100/157]	0.1057 (0.1021)	0.0643 (0.0628)	0.604 (0.848)	81.25 (73.89)
[110/157]	0.1030 (0.1022)	0.0633 (0.0629)	0.729 (0.842)	75.00 (74.04)
[120/157]	0.1023 (0.1022)	0.0623 (0.0630)	0.907 (0.835)	75.00 (74.35)
[130/157]	0.1021 (0.1023)	0.0617 (0.0630)	1.153 (0.842)	71.88 (74.17)
[140/157]	0.1032 (0.1023)	0.0628 (0.0630)	0.982 (0.846)	78.12 (74.09)
[150/157]	0.1035 (0.1023)	0.0632 (0.0630)	1.211 (0.854)	62.50 (73.92)
[156/157]	0.0859 (0.1022)	0.0581 (0.0630)	0.966 (0.852)	75.00 (73.98)
 * Train Acc 73.980
 * Val Acc 73.200, Total time 0.56
 * Val loss 0.806, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0088 (0.0088)	0.768 (0.768)	81.25 (81.25)
[10/157]	0.0941 (0.0923)	0.0567 (0.0542)	1.007 (0.863)	62.50 (74.15)
[20/157]	0.0961 (0.0990)	0.0578 (0.0608)	0.748 (0.882)	71.88 (72.32)
[30/157]	0.1013 (0.0987)	0.0624 (0.0606)	0.924 (0.916)	75.00 (70.97)
[40/157]	0.1007 (0.0991)	0.0619 (0.0609)	0.886 (0.912)	71.88 (71.04)
[50/157]	0.1017 (0.0996)	0.0610 (0.0613)	0.833 (0.897)	68.75 (72.00)
[60/157]	0.1023 (0.0999)	0.0628 (0.0614)	0.877 (0.884)	68.75 (72.49)
[70/157]	0.1014 (0.1000)	0.0623 (0.0615)	1.095 (0.880)	68.75 (72.89)
[80/157]	0.1017 (0.1003)	0.0610 (0.0617)	0.692 (0.874)	81.25 (73.15)
[90/157]	0.1025 (0.1003)	0.0630 (0.0617)	0.829 (0.874)	68.75 (73.11)
[100/157]	0.1004 (0.1004)	0.0608 (0.0618)	0.557 (0.863)	90.62 (73.76)
[110/157]	0.1023 (0.1005)	0.0623 (0.0618)	0.555 (0.864)	84.38 (73.56)
[120/157]	0.1017 (0.1005)	0.0626 (0.0619)	0.933 (0.859)	75.00 (73.81)
[130/157]	0.1006 (0.1006)	0.0610 (0.0619)	0.913 (0.852)	68.75 (74.02)
[140/157]	0.1010 (0.1007)	0.0612 (0.0620)	0.785 (0.859)	68.75 (73.67)
[150/157]	0.1033 (0.1007)	0.0628 (0.0620)	0.796 (0.856)	75.00 (73.86)
[156/157]	0.0870 (0.1006)	0.0571 (0.0620)	0.614 (0.855)	87.50 (73.86)
 * Train Acc 73.860
 * Val Acc 73.200, Total time 0.60
 * Val loss 0.804, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0091 (0.0091)	0.865 (0.865)	75.00 (75.00)
[10/157]	0.1017 (0.0957)	0.0620 (0.0575)	0.712 (0.786)	81.25 (76.99)
[20/157]	0.1026 (0.0983)	0.0630 (0.0600)	0.734 (0.770)	78.12 (77.98)
[30/157]	0.1016 (0.0993)	0.0622 (0.0609)	0.885 (0.805)	75.00 (76.21)
[40/157]	0.1011 (0.0999)	0.0616 (0.0613)	0.550 (0.795)	87.50 (76.22)
[50/157]	0.1018 (0.1002)	0.0623 (0.0616)	1.043 (0.797)	56.25 (75.92)
[60/157]	0.1020 (0.1004)	0.0624 (0.0618)	0.965 (0.805)	68.75 (75.61)
[70/157]	0.1038 (0.1006)	0.0632 (0.0619)	0.687 (0.813)	78.12 (75.44)
[80/157]	0.1003 (0.1006)	0.0606 (0.0620)	1.047 (0.811)	59.38 (75.81)
[90/157]	0.1028 (0.1008)	0.0633 (0.0621)	0.995 (0.824)	68.75 (75.52)
[100/157]	0.1003 (0.1007)	0.0610 (0.0621)	0.891 (0.831)	75.00 (75.19)
[110/157]	0.1028 (0.1008)	0.0626 (0.0622)	0.642 (0.829)	78.12 (75.08)
[120/157]	0.1029 (0.1008)	0.0632 (0.0622)	0.985 (0.840)	71.88 (74.64)
[130/157]	0.1019 (0.1009)	0.0623 (0.0622)	0.886 (0.844)	75.00 (74.26)
[140/157]	0.1013 (0.1009)	0.0619 (0.0622)	0.814 (0.843)	71.88 (74.45)
[150/157]	0.1007 (0.1010)	0.0608 (0.0622)	1.060 (0.842)	75.00 (74.44)
[156/157]	0.0869 (0.1009)	0.0592 (0.0622)	1.102 (0.846)	62.50 (74.30)
 * Train Acc 74.300
 * Val Acc 72.900, Total time 0.60
 * Val loss 0.801, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0091 (0.0091)	0.755 (0.755)	81.25 (81.25)
[10/157]	0.1020 (0.0957)	0.0615 (0.0573)	1.024 (0.990)	71.88 (70.17)
[20/157]	0.1024 (0.0983)	0.0632 (0.0599)	0.821 (0.923)	68.75 (72.17)
[30/157]	0.1004 (0.0992)	0.0613 (0.0607)	0.908 (0.924)	71.88 (71.77)
[40/157]	0.1018 (0.0995)	0.0631 (0.0612)	1.019 (0.893)	71.88 (72.71)
[50/157]	0.1003 (0.0999)	0.0608 (0.0614)	0.961 (0.873)	59.38 (72.79)
[60/157]	0.1000 (0.1002)	0.0614 (0.0617)	0.559 (0.864)	84.38 (73.62)
[70/157]	0.1011 (0.1002)	0.0622 (0.0617)	0.817 (0.862)	65.62 (73.37)
[80/157]	0.1008 (0.1004)	0.0610 (0.0618)	0.897 (0.854)	78.12 (73.88)
[90/157]	0.1037 (0.1005)	0.0628 (0.0619)	0.966 (0.853)	62.50 (74.00)
[100/157]	0.1028 (0.1006)	0.0615 (0.0619)	0.600 (0.848)	81.25 (74.10)
[110/157]	0.0989 (0.1006)	0.0608 (0.0620)	0.968 (0.842)	71.88 (74.41)
[120/157]	0.1020 (0.1007)	0.0617 (0.0621)	0.782 (0.842)	75.00 (74.33)
[130/157]	0.1008 (0.1007)	0.0616 (0.0621)	0.946 (0.841)	71.88 (74.19)
[140/157]	0.1015 (0.1007)	0.0628 (0.0621)	0.851 (0.839)	78.12 (74.45)
[150/157]	0.1022 (0.1008)	0.0631 (0.0621)	0.540 (0.837)	84.38 (74.63)
[156/157]	0.0850 (0.1007)	0.0577 (0.0621)	1.745 (0.840)	37.50 (74.42)
 * Train Acc 74.420
 * Val Acc 72.900, Total time 0.60
 * Val loss 0.801, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0086 (0.0086)	0.806 (0.806)	75.00 (75.00)
[10/157]	0.1006 (0.0956)	0.0617 (0.0573)	0.806 (0.798)	75.00 (76.42)
[20/157]	0.1024 (0.0980)	0.0637 (0.0598)	0.722 (0.824)	81.25 (74.85)
[30/157]	0.0999 (0.0991)	0.0607 (0.0607)	0.527 (0.831)	81.25 (75.00)
[40/157]	0.1019 (0.0997)	0.0620 (0.0613)	0.660 (0.851)	87.50 (74.31)
[50/157]	0.0996 (0.0998)	0.0616 (0.0614)	0.710 (0.849)	75.00 (74.02)
[60/157]	0.1028 (0.1000)	0.0626 (0.0616)	0.711 (0.842)	71.88 (74.03)
[70/157]	0.1014 (0.1001)	0.0627 (0.0617)	0.688 (0.827)	81.25 (74.52)
[80/157]	0.1032 (0.1002)	0.0634 (0.0618)	0.884 (0.833)	75.00 (74.81)
[90/157]	0.1009 (0.1003)	0.0612 (0.0618)	0.750 (0.848)	75.00 (74.07)
[100/157]	0.1022 (0.1004)	0.0616 (0.0619)	0.482 (0.843)	93.75 (74.20)
[110/157]	0.0990 (0.1005)	0.0588 (0.0619)	0.982 (0.849)	68.75 (73.90)
[120/157]	0.0991 (0.1006)	0.0605 (0.0620)	0.844 (0.851)	81.25 (73.89)
[130/157]	0.1008 (0.1007)	0.0613 (0.0620)	0.794 (0.852)	75.00 (73.88)
[140/157]	0.1005 (0.1008)	0.0607 (0.0621)	0.983 (0.858)	75.00 (73.76)
[150/157]	0.1026 (0.1008)	0.0622 (0.0621)	0.752 (0.853)	78.12 (73.92)
[156/157]	0.0839 (0.1007)	0.0561 (0.0621)	0.968 (0.853)	62.50 (73.78)
 * Train Acc 73.780
 * Val Acc 73.500, Total time 0.60
 * Val loss 0.826, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0090 (0.0090)	0.740 (0.740)	78.12 (78.12)
[10/157]	0.1012 (0.0949)	0.0623 (0.0573)	0.703 (0.778)	78.12 (75.85)
[20/157]	0.1010 (0.0981)	0.0618 (0.0600)	1.090 (0.809)	59.38 (74.70)
[30/157]	0.1028 (0.0992)	0.0626 (0.0607)	0.707 (0.811)	78.12 (75.30)
[40/157]	0.1005 (0.0995)	0.0618 (0.0611)	0.828 (0.812)	75.00 (75.23)
[50/157]	0.1036 (0.0999)	0.0637 (0.0614)	0.893 (0.806)	65.62 (75.49)
[60/157]	0.0998 (0.1000)	0.0609 (0.0615)	0.558 (0.816)	87.50 (74.90)
[70/157]	0.1017 (0.1002)	0.0625 (0.0618)	0.902 (0.817)	71.88 (75.18)
[80/157]	0.0983 (0.1002)	0.0591 (0.0618)	0.996 (0.824)	68.75 (75.15)
[90/157]	0.1010 (0.1004)	0.0616 (0.0619)	1.037 (0.823)	68.75 (75.07)
[100/157]	0.1016 (0.1005)	0.0626 (0.0620)	1.142 (0.836)	56.25 (74.78)
[110/157]	0.1001 (0.1005)	0.0615 (0.0620)	0.594 (0.837)	90.62 (74.89)
[120/157]	0.1013 (0.1006)	0.0611 (0.0621)	0.934 (0.845)	65.62 (74.56)
[130/157]	0.1016 (0.1007)	0.0620 (0.0621)	0.677 (0.842)	78.12 (74.59)
[140/157]	0.1013 (0.1007)	0.0611 (0.0621)	0.909 (0.854)	78.12 (73.94)
[150/157]	0.1024 (0.1007)	0.0626 (0.0622)	0.645 (0.850)	84.38 (73.99)
[156/157]	0.0845 (0.1006)	0.0570 (0.0622)	1.831 (0.851)	25.00 (73.86)
 * Train Acc 73.860
 * Val Acc 73.100, Total time 0.60
 * Val loss 0.804, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0457 (0.0457)	0.0093 (0.0093)	0.885 (0.885)	75.00 (75.00)
[10/157]	0.0997 (0.0958)	0.0603 (0.0574)	0.779 (0.766)	81.25 (78.12)
[20/157]	0.1019 (0.0985)	0.0617 (0.0599)	0.685 (0.828)	78.12 (73.96)
[30/157]	0.0996 (0.0991)	0.0609 (0.0604)	0.537 (0.834)	87.50 (73.29)
[40/157]	0.1022 (0.0997)	0.0623 (0.0610)	0.579 (0.835)	78.12 (73.78)
[50/157]	0.1000 (0.0999)	0.0611 (0.0612)	0.783 (0.850)	78.12 (73.65)
[60/157]	0.1010 (0.1002)	0.0615 (0.0614)	0.820 (0.838)	75.00 (74.28)
[70/157]	0.1017 (0.1003)	0.0628 (0.0616)	0.569 (0.828)	78.12 (74.56)
[80/157]	0.1014 (0.1004)	0.0623 (0.0617)	1.034 (0.833)	65.62 (73.88)
[90/157]	0.1030 (0.1005)	0.0632 (0.0619)	0.904 (0.838)	75.00 (73.63)
[100/157]	0.1035 (0.1006)	0.0629 (0.0620)	1.380 (0.837)	65.62 (73.95)
[110/157]	0.1013 (0.1007)	0.0621 (0.0620)	0.886 (0.837)	75.00 (73.85)
[120/157]	0.1005 (0.1007)	0.0614 (0.0621)	0.824 (0.838)	71.88 (73.81)
[130/157]	0.1015 (0.1008)	0.0621 (0.0621)	0.990 (0.836)	71.88 (73.90)
[140/157]	0.1011 (0.1008)	0.0626 (0.0622)	1.148 (0.835)	71.88 (74.05)
[150/157]	0.1010 (0.1009)	0.0618 (0.0622)	0.828 (0.835)	71.88 (74.11)
[156/157]	0.0865 (0.1008)	0.0586 (0.0622)	0.763 (0.834)	75.00 (74.14)
 * Train Acc 74.140
 * Val Acc 73.200, Total time 0.60
 * Val loss 0.803, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0094 (0.0094)	1.084 (1.084)	62.50 (62.50)
[10/157]	0.1015 (0.0953)	0.0622 (0.0573)	0.921 (0.823)	81.25 (76.99)
[20/157]	0.1012 (0.0983)	0.0623 (0.0600)	0.585 (0.775)	84.38 (78.87)
[30/157]	0.1020 (0.0992)	0.0630 (0.0609)	0.979 (0.826)	65.62 (76.41)
[40/157]	0.1011 (0.0997)	0.0616 (0.0612)	0.679 (0.831)	81.25 (75.99)
[50/157]	0.1014 (0.0999)	0.0625 (0.0615)	1.065 (0.827)	68.75 (75.92)
[60/157]	0.1009 (0.1001)	0.0615 (0.0617)	0.717 (0.826)	78.12 (75.67)
[70/157]	0.1020 (0.1003)	0.0619 (0.0618)	0.873 (0.828)	75.00 (75.31)
[80/157]	0.1002 (0.1003)	0.0622 (0.0619)	0.876 (0.830)	68.75 (75.04)
[90/157]	0.1008 (0.1005)	0.0617 (0.0620)	0.787 (0.831)	68.75 (74.76)
[100/157]	0.1019 (0.1006)	0.0624 (0.0621)	0.726 (0.835)	81.25 (74.85)
[110/157]	0.1011 (0.1007)	0.0614 (0.0621)	0.682 (0.827)	75.00 (75.14)
[120/157]	0.1007 (0.1007)	0.0617 (0.0621)	1.046 (0.827)	62.50 (74.85)
[130/157]	0.1018 (0.1008)	0.0629 (0.0622)	0.958 (0.824)	78.12 (74.98)
[140/157]	0.1009 (0.1008)	0.0623 (0.0622)	0.779 (0.825)	78.12 (74.71)
[150/157]	0.1004 (0.1008)	0.0609 (0.0622)	0.620 (0.825)	81.25 (74.81)
[156/157]	0.0861 (0.1007)	0.0594 (0.0622)	1.341 (0.823)	62.50 (74.88)
 * Train Acc 74.880
 * Val Acc 73.200, Total time 0.60
 * Val loss 0.802, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0085 (0.0085)	0.716 (0.716)	84.38 (84.38)
[10/157]	0.1006 (0.0958)	0.0604 (0.0569)	0.794 (0.839)	75.00 (76.14)
[20/157]	0.1002 (0.0986)	0.0612 (0.0594)	1.012 (0.882)	68.75 (73.07)
[30/157]	0.1036 (0.0996)	0.0625 (0.0604)	0.744 (0.843)	81.25 (75.10)
[40/157]	0.1003 (0.0998)	0.0614 (0.0609)	0.823 (0.828)	71.88 (75.23)
[50/157]	0.1008 (0.1002)	0.0619 (0.0612)	0.730 (0.813)	78.12 (75.31)
[60/157]	0.1013 (0.1005)	0.0609 (0.0615)	0.865 (0.806)	78.12 (75.67)
[70/157]	0.1025 (0.1005)	0.0629 (0.0616)	1.182 (0.822)	68.75 (75.26)
[80/157]	0.1028 (0.1007)	0.0621 (0.0616)	0.843 (0.819)	75.00 (75.27)
[90/157]	0.1011 (0.1008)	0.0614 (0.0617)	0.720 (0.833)	78.12 (74.79)
[100/157]	0.1007 (0.1009)	0.0613 (0.0618)	0.774 (0.832)	71.88 (74.85)
[110/157]	0.1019 (0.1008)	0.0630 (0.0618)	1.101 (0.838)	62.50 (74.52)
[120/157]	0.1007 (0.1009)	0.0612 (0.0619)	0.754 (0.834)	71.88 (74.54)
[130/157]	0.1016 (0.1010)	0.0623 (0.0619)	1.086 (0.841)	62.50 (74.31)
[140/157]	0.1018 (0.1010)	0.0624 (0.0619)	0.866 (0.837)	71.88 (74.34)
[150/157]	0.1001 (0.1010)	0.0610 (0.0620)	0.996 (0.835)	71.88 (74.42)
[156/157]	0.0878 (0.1009)	0.0599 (0.0620)	2.127 (0.836)	37.50 (74.36)
 * Train Acc 74.360
 * Val Acc 73.500, Total time 0.60
 * Val loss 0.794, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0091 (0.0091)	0.697 (0.697)	81.25 (81.25)
[10/157]	0.1015 (0.0957)	0.0624 (0.0578)	1.046 (0.832)	62.50 (74.43)
[20/157]	0.1001 (0.0979)	0.0611 (0.0599)	1.120 (0.857)	56.25 (73.66)
[30/157]	0.1018 (0.0989)	0.0629 (0.0610)	0.843 (0.865)	71.88 (72.58)
[40/157]	0.1009 (0.0994)	0.0625 (0.0613)	0.793 (0.859)	71.88 (73.02)
[50/157]	0.1020 (0.0999)	0.0612 (0.0616)	0.974 (0.842)	65.62 (73.41)
[60/157]	0.1014 (0.1000)	0.0625 (0.0617)	0.707 (0.846)	78.12 (73.57)
[70/157]	0.1001 (0.1002)	0.0610 (0.0619)	1.057 (0.855)	75.00 (73.24)
[80/157]	0.1029 (0.1003)	0.0637 (0.0620)	1.006 (0.850)	65.62 (73.50)
[90/157]	0.1024 (0.1004)	0.0620 (0.0620)	0.758 (0.850)	78.12 (73.63)
[100/157]	0.0979 (0.1005)	0.0586 (0.0621)	1.021 (0.851)	62.50 (73.48)
[110/157]	0.1030 (0.1006)	0.0639 (0.0622)	0.732 (0.853)	84.38 (73.42)
[120/157]	0.1011 (0.1006)	0.0615 (0.0622)	0.875 (0.855)	71.88 (73.30)
[130/157]	0.1007 (0.1007)	0.0616 (0.0622)	0.774 (0.858)	71.88 (73.14)
[140/157]	0.1026 (0.1007)	0.0631 (0.0623)	0.871 (0.855)	78.12 (73.40)
[150/157]	0.1016 (0.1008)	0.0625 (0.0623)	1.114 (0.854)	65.62 (73.45)
[156/157]	0.0846 (0.1007)	0.0576 (0.0623)	0.861 (0.851)	75.00 (73.62)
 * Train Acc 73.620
 * Val Acc 73.800, Total time 0.59
 * Val loss 0.794, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0089 (0.0089)	0.857 (0.857)	78.12 (78.12)
[10/157]	0.1006 (0.0957)	0.0617 (0.0579)	1.016 (0.870)	56.25 (73.01)
[20/157]	0.1034 (0.0984)	0.0641 (0.0604)	0.701 (0.872)	71.88 (72.77)
[30/157]	0.1009 (0.0991)	0.0621 (0.0611)	0.793 (0.851)	81.25 (73.99)
[40/157]	0.1010 (0.0998)	0.0619 (0.0616)	0.757 (0.855)	78.12 (74.09)
[50/157]	0.1015 (0.1000)	0.0628 (0.0618)	1.082 (0.857)	71.88 (74.02)
[60/157]	0.1022 (0.1002)	0.0632 (0.0620)	0.805 (0.863)	87.50 (73.82)
[70/157]	0.0997 (0.1004)	0.0608 (0.0621)	1.179 (0.863)	62.50 (73.64)
[80/157]	0.1015 (0.1006)	0.0613 (0.0622)	0.764 (0.880)	71.88 (72.57)
[90/157]	0.1023 (0.1008)	0.0629 (0.0623)	0.937 (0.884)	71.88 (72.32)
[100/157]	0.1009 (0.1008)	0.0624 (0.0623)	0.819 (0.866)	75.00 (73.11)
[110/157]	0.1005 (0.1008)	0.0610 (0.0623)	1.059 (0.862)	71.88 (73.20)
[120/157]	0.1008 (0.1009)	0.0614 (0.0624)	0.506 (0.856)	87.50 (73.45)
[130/157]	0.1012 (0.1009)	0.0618 (0.0623)	0.638 (0.853)	78.12 (73.43)
[140/157]	0.0999 (0.1009)	0.0604 (0.0624)	0.812 (0.854)	71.88 (73.43)
[150/157]	0.1006 (0.1010)	0.0614 (0.0624)	0.810 (0.852)	71.88 (73.41)
[156/157]	0.0860 (0.1008)	0.0586 (0.0624)	0.600 (0.848)	87.50 (73.58)
 * Train Acc 73.580
 * Val Acc 73.500, Total time 0.59
 * Val loss 0.793, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0085 (0.0085)	0.617 (0.617)	78.12 (78.12)
[10/157]	0.1015 (0.0952)	0.0620 (0.0571)	0.590 (0.773)	84.38 (75.00)
[20/157]	0.1014 (0.0983)	0.0620 (0.0598)	0.842 (0.841)	75.00 (73.21)
[30/157]	0.1026 (0.0994)	0.0627 (0.0608)	0.706 (0.810)	78.12 (75.00)
[40/157]	0.1019 (0.0998)	0.0625 (0.0612)	0.832 (0.787)	78.12 (76.30)
[50/157]	0.1005 (0.1000)	0.0614 (0.0614)	0.780 (0.814)	78.12 (75.25)
[60/157]	0.1028 (0.1002)	0.0634 (0.0616)	0.709 (0.831)	81.25 (74.90)
[70/157]	0.1008 (0.1004)	0.0622 (0.0618)	0.937 (0.821)	78.12 (75.53)
[80/157]	0.0962 (0.1000)	0.0587 (0.0615)	0.558 (0.825)	90.62 (75.23)
[90/157]	0.0941 (0.0994)	0.0568 (0.0612)	0.786 (0.830)	71.88 (74.86)
[100/157]	0.0955 (0.0990)	0.0582 (0.0609)	0.948 (0.831)	71.88 (74.85)
[110/157]	0.1187 (0.0989)	0.0784 (0.0609)	0.976 (0.837)	68.75 (74.66)
[120/157]	0.0977 (0.0999)	0.0601 (0.0618)	0.918 (0.838)	75.00 (74.85)
[130/157]	0.1002 (0.0997)	0.0614 (0.0616)	0.991 (0.839)	68.75 (74.86)
[140/157]	0.1002 (0.0997)	0.0602 (0.0615)	0.847 (0.841)	75.00 (74.76)
[150/157]	0.1000 (0.0997)	0.0603 (0.0615)	1.289 (0.840)	62.50 (74.65)
[156/157]	0.0828 (0.0996)	0.0555 (0.0614)	1.032 (0.838)	62.50 (74.64)
 * Train Acc 74.640
 * Val Acc 74.300, Total time 0.59
 * Val loss 0.798, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0090 (0.0090)	1.049 (1.049)	53.12 (53.12)
[10/157]	0.1159 (0.1008)	0.0756 (0.0628)	0.928 (0.867)	65.62 (71.88)
[20/157]	0.0994 (0.0998)	0.0605 (0.0618)	1.110 (0.845)	75.00 (73.81)
[30/157]	0.0999 (0.0994)	0.0607 (0.0613)	0.587 (0.847)	78.12 (74.19)
[40/157]	0.1000 (0.0993)	0.0605 (0.0611)	1.012 (0.840)	62.50 (74.09)
[50/157]	0.1006 (0.0991)	0.0611 (0.0609)	0.817 (0.834)	78.12 (74.33)
[60/157]	0.1063 (0.0995)	0.0653 (0.0611)	0.624 (0.824)	81.25 (75.00)
[70/157]	0.1041 (0.1002)	0.0641 (0.0615)	1.068 (0.834)	71.88 (74.69)
[80/157]	0.1045 (0.1006)	0.0653 (0.0619)	0.672 (0.835)	84.38 (74.69)
[90/157]	0.1048 (0.1010)	0.0650 (0.0623)	0.807 (0.831)	75.00 (74.79)
[100/157]	0.1054 (0.1013)	0.0653 (0.0625)	0.925 (0.845)	75.00 (74.44)
[110/157]	0.1028 (0.1014)	0.0625 (0.0626)	0.876 (0.846)	75.00 (74.35)
[120/157]	0.1039 (0.1016)	0.0641 (0.0628)	0.626 (0.842)	81.25 (74.28)
[130/157]	0.1039 (0.1018)	0.0642 (0.0629)	0.889 (0.844)	68.75 (74.21)
[140/157]	0.1037 (0.1019)	0.0635 (0.0630)	0.721 (0.849)	78.12 (74.14)
[150/157]	0.1053 (0.1020)	0.0642 (0.0631)	0.699 (0.840)	84.38 (74.40)
[156/157]	0.0874 (0.1020)	0.0613 (0.0631)	1.206 (0.845)	50.00 (74.20)
 * Train Acc 74.200
 * Val Acc 73.400, Total time 0.61
 * Val loss 0.799, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0093 (0.0093)	1.086 (1.086)	59.38 (59.38)
[10/157]	0.1055 (0.0978)	0.0657 (0.0591)	1.074 (0.902)	65.62 (71.88)
[20/157]	0.1062 (0.1006)	0.0653 (0.0616)	0.772 (0.859)	78.12 (73.51)
[30/157]	0.0963 (0.1005)	0.0578 (0.0614)	0.934 (0.838)	71.88 (75.50)
[40/157]	0.0943 (0.0992)	0.0563 (0.0606)	0.914 (0.838)	75.00 (74.70)
[50/157]	0.0962 (0.0984)	0.0578 (0.0601)	1.239 (0.841)	65.62 (74.63)
[60/157]	0.0967 (0.0979)	0.0567 (0.0596)	0.719 (0.830)	78.12 (74.95)
[70/157]	0.0961 (0.0975)	0.0574 (0.0594)	0.854 (0.829)	75.00 (75.18)
[80/157]	0.1094 (0.0981)	0.0699 (0.0600)	0.890 (0.828)	65.62 (75.00)
[90/157]	0.0975 (0.0983)	0.0586 (0.0603)	1.206 (0.836)	68.75 (74.90)
[100/157]	0.0970 (0.0982)	0.0585 (0.0601)	0.528 (0.836)	84.38 (74.60)
[110/157]	0.0975 (0.0981)	0.0583 (0.0600)	1.028 (0.833)	71.88 (74.66)
[120/157]	0.0971 (0.0980)	0.0589 (0.0599)	0.876 (0.825)	78.12 (75.05)
[130/157]	0.0988 (0.0979)	0.0576 (0.0598)	0.907 (0.830)	71.88 (74.76)
[140/157]	0.0973 (0.0977)	0.0593 (0.0597)	0.745 (0.826)	81.25 (74.80)
[150/157]	0.1036 (0.0980)	0.0641 (0.0599)	0.700 (0.828)	81.25 (74.73)
[156/157]	0.0869 (0.0981)	0.0600 (0.0601)	0.700 (0.831)	75.00 (74.60)
 * Train Acc 74.600
 * Val Acc 73.600, Total time 0.62
 * Val loss 0.793, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0086 (0.0086)	0.766 (0.766)	75.00 (75.00)
[10/157]	0.1039 (0.0972)	0.0650 (0.0593)	0.899 (0.853)	78.12 (73.86)
[20/157]	0.1037 (0.1003)	0.0642 (0.0619)	0.731 (0.846)	71.88 (73.96)
[30/157]	0.1041 (0.1015)	0.0648 (0.0628)	0.480 (0.824)	90.62 (75.00)
[40/157]	0.1013 (0.1020)	0.0624 (0.0633)	0.616 (0.831)	84.38 (75.23)
[50/157]	0.1044 (0.1023)	0.0644 (0.0636)	0.739 (0.807)	75.00 (75.61)
[60/157]	0.1055 (0.1025)	0.0651 (0.0638)	0.727 (0.819)	78.12 (74.80)
[70/157]	0.1035 (0.1026)	0.0644 (0.0639)	0.587 (0.836)	87.50 (74.12)
[80/157]	0.0957 (0.1019)	0.0577 (0.0634)	0.600 (0.836)	84.38 (73.92)
[90/157]	0.1047 (0.1019)	0.0642 (0.0633)	0.723 (0.833)	75.00 (74.11)
[100/157]	0.1030 (0.1020)	0.0635 (0.0633)	0.952 (0.828)	75.00 (74.13)
[110/157]	0.1038 (0.1022)	0.0641 (0.0634)	1.234 (0.839)	62.50 (73.96)
[120/157]	0.1020 (0.1023)	0.0624 (0.0635)	0.775 (0.834)	68.75 (74.15)
[130/157]	0.0963 (0.1019)	0.0580 (0.0632)	0.770 (0.847)	81.25 (74.02)
[140/157]	0.0946 (0.1015)	0.0567 (0.0629)	0.959 (0.845)	68.75 (73.98)
[150/157]	0.0959 (0.1010)	0.0577 (0.0626)	0.672 (0.839)	78.12 (74.28)
[156/157]	0.0782 (0.1007)	0.0524 (0.0623)	0.803 (0.837)	62.50 (74.36)
 * Train Acc 74.360
 * Val Acc 73.700, Total time 0.62
 * Val loss 0.791, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0471 (0.0471)	0.0097 (0.0097)	0.915 (0.915)	71.88 (71.88)
[10/157]	0.0967 (0.0993)	0.0587 (0.0608)	0.939 (0.848)	65.62 (74.15)
[20/157]	0.0964 (0.0981)	0.0585 (0.0598)	0.907 (0.860)	65.62 (73.66)
[30/157]	0.0969 (0.0979)	0.0583 (0.0596)	0.941 (0.825)	75.00 (75.20)
[40/157]	0.0967 (0.0977)	0.0578 (0.0595)	0.830 (0.805)	68.75 (75.30)
[50/157]	0.0967 (0.0975)	0.0587 (0.0594)	0.733 (0.821)	81.25 (74.94)
[60/157]	0.0976 (0.0974)	0.0589 (0.0594)	1.108 (0.817)	62.50 (75.20)
[70/157]	0.0972 (0.0973)	0.0575 (0.0593)	0.967 (0.816)	65.62 (75.18)
[80/157]	0.1098 (0.0996)	0.0707 (0.0613)	1.220 (0.816)	59.38 (75.31)
[90/157]	0.0963 (0.0993)	0.0584 (0.0610)	0.736 (0.816)	78.12 (75.41)
[100/157]	0.0982 (0.0991)	0.0579 (0.0608)	0.976 (0.823)	71.88 (75.22)
[110/157]	0.0962 (0.0989)	0.0578 (0.0606)	0.641 (0.821)	78.12 (75.14)
[120/157]	0.0971 (0.0987)	0.0583 (0.0604)	1.021 (0.824)	68.75 (75.08)
[130/157]	0.0990 (0.0993)	0.0593 (0.0610)	0.640 (0.826)	75.00 (74.88)
[140/157]	0.0983 (0.0992)	0.0598 (0.0609)	0.703 (0.824)	84.38 (75.00)
[150/157]	0.0985 (0.0992)	0.0596 (0.0608)	0.970 (0.825)	65.62 (74.88)
[156/157]	0.0837 (0.0991)	0.0553 (0.0607)	1.537 (0.827)	62.50 (74.84)
 * Train Acc 74.840
 * Val Acc 74.400, Total time 0.58
 * Val loss 0.780, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0093 (0.0093)	1.068 (1.068)	62.50 (62.50)
[10/157]	0.0977 (0.0927)	0.0589 (0.0545)	0.659 (0.882)	78.12 (72.73)
[20/157]	0.0995 (0.0955)	0.0601 (0.0573)	0.718 (0.871)	78.12 (73.36)
[30/157]	0.1160 (0.0972)	0.0756 (0.0585)	0.970 (0.850)	75.00 (74.40)
[40/157]	0.1015 (0.0994)	0.0626 (0.0607)	0.897 (0.849)	71.88 (74.16)
[50/157]	0.1016 (0.0998)	0.0623 (0.0611)	0.882 (0.849)	65.62 (73.90)
[60/157]	0.1010 (0.1001)	0.0619 (0.0614)	0.591 (0.842)	90.62 (74.39)
[70/157]	0.1021 (0.1003)	0.0625 (0.0616)	0.727 (0.838)	78.12 (74.87)
[80/157]	0.1014 (0.1004)	0.0627 (0.0617)	0.850 (0.834)	84.38 (75.00)
[90/157]	0.1009 (0.1005)	0.0620 (0.0619)	0.637 (0.826)	93.75 (75.31)
[100/157]	0.1010 (0.1006)	0.0621 (0.0621)	0.827 (0.828)	81.25 (75.22)
[110/157]	0.1030 (0.1008)	0.0626 (0.0622)	0.620 (0.830)	78.12 (75.06)
[120/157]	0.1018 (0.1009)	0.0623 (0.0623)	0.838 (0.835)	71.88 (75.10)
[130/157]	0.1019 (0.1009)	0.0621 (0.0623)	0.746 (0.830)	75.00 (75.10)
[140/157]	0.1018 (0.1010)	0.0621 (0.0623)	0.944 (0.828)	65.62 (74.93)
[150/157]	0.1026 (0.1010)	0.0633 (0.0624)	0.937 (0.828)	65.62 (75.08)
[156/157]	0.0854 (0.1009)	0.0578 (0.0623)	1.699 (0.826)	50.00 (75.06)
 * Train Acc 75.060
 * Val Acc 73.700, Total time 0.60
 * Val loss 0.789, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0084 (0.0084)	0.865 (0.865)	78.12 (78.12)
[10/157]	0.1010 (0.0954)	0.0623 (0.0576)	0.731 (0.845)	78.12 (73.58)
[20/157]	0.1018 (0.0985)	0.0624 (0.0603)	0.739 (0.792)	81.25 (76.19)
[30/157]	0.1015 (0.0996)	0.0621 (0.0610)	0.759 (0.789)	81.25 (76.51)
[40/157]	0.1004 (0.1001)	0.0612 (0.0613)	0.546 (0.788)	87.50 (75.91)
[50/157]	0.0987 (0.1004)	0.0588 (0.0617)	0.508 (0.777)	84.38 (76.53)
[60/157]	0.1046 (0.1006)	0.0643 (0.0618)	0.695 (0.786)	81.25 (76.33)
[70/157]	0.1044 (0.1008)	0.0643 (0.0620)	0.840 (0.783)	71.88 (76.50)
[80/157]	0.1027 (0.1008)	0.0626 (0.0621)	0.919 (0.793)	65.62 (76.04)
[90/157]	0.1022 (0.1010)	0.0630 (0.0621)	0.675 (0.791)	81.25 (76.10)
[100/157]	0.1010 (0.1010)	0.0617 (0.0621)	0.811 (0.787)	81.25 (76.14)
[110/157]	0.1021 (0.1011)	0.0625 (0.0622)	1.016 (0.796)	68.75 (75.87)
[120/157]	0.1023 (0.1011)	0.0622 (0.0623)	0.828 (0.797)	78.12 (76.01)
[130/157]	0.1044 (0.1012)	0.0624 (0.0623)	0.692 (0.802)	78.12 (75.79)
[140/157]	0.0999 (0.1013)	0.0612 (0.0624)	1.203 (0.813)	68.75 (75.44)
[150/157]	0.1027 (0.1013)	0.0632 (0.0624)	0.738 (0.809)	71.88 (75.54)
[156/157]	0.0928 (0.1012)	0.0597 (0.0624)	0.826 (0.813)	75.00 (75.34)
 * Train Acc 75.340
 * Val Acc 73.100, Total time 0.59
 * Val loss 0.791, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0090 (0.0090)	0.804 (0.804)	78.12 (78.12)
[10/157]	0.1005 (0.0949)	0.0617 (0.0572)	1.085 (0.828)	59.38 (73.01)
[20/157]	0.1023 (0.0978)	0.0632 (0.0598)	0.939 (0.812)	75.00 (74.40)
[30/157]	0.1010 (0.0989)	0.0620 (0.0607)	0.965 (0.816)	65.62 (75.30)
[40/157]	0.1006 (0.0996)	0.0618 (0.0612)	0.923 (0.819)	75.00 (75.15)
[50/157]	0.1013 (0.0998)	0.0625 (0.0615)	0.936 (0.817)	75.00 (74.94)
[60/157]	0.1006 (0.1000)	0.0615 (0.0617)	1.061 (0.832)	68.75 (74.54)
[70/157]	0.1015 (0.1003)	0.0625 (0.0619)	0.556 (0.824)	84.38 (75.09)
[80/157]	0.1032 (0.1004)	0.0631 (0.0621)	0.745 (0.829)	78.12 (74.46)
[90/157]	0.1017 (0.1006)	0.0628 (0.0622)	0.666 (0.821)	81.25 (74.73)
[100/157]	0.1069 (0.1002)	0.0669 (0.0619)	0.470 (0.820)	87.50 (74.78)
[110/157]	0.0986 (0.1006)	0.0592 (0.0623)	0.876 (0.824)	81.25 (74.58)
[120/157]	0.0985 (0.1004)	0.0597 (0.0621)	1.023 (0.824)	75.00 (74.56)
[130/157]	0.0990 (0.1002)	0.0604 (0.0619)	0.887 (0.832)	68.75 (74.26)
[140/157]	0.0986 (0.1000)	0.0593 (0.0617)	0.845 (0.838)	71.88 (74.14)
[150/157]	0.0988 (0.0999)	0.0597 (0.0616)	0.750 (0.836)	75.00 (74.19)
[156/157]	0.0807 (0.0996)	0.0547 (0.0615)	0.893 (0.839)	62.50 (74.12)
 * Train Acc 74.120
 * Val Acc 74.600, Total time 0.60
 * Val loss 0.791, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0090 (0.0090)	0.982 (0.982)	62.50 (62.50)
[10/157]	0.0997 (0.0926)	0.0592 (0.0547)	0.859 (0.840)	68.75 (75.00)
[20/157]	0.0960 (0.0949)	0.0579 (0.0568)	1.035 (0.833)	56.25 (74.26)
[30/157]	0.0966 (0.0959)	0.0586 (0.0575)	0.902 (0.831)	68.75 (74.60)
[40/157]	0.0968 (0.0963)	0.0589 (0.0579)	0.634 (0.826)	81.25 (75.08)
[50/157]	0.0981 (0.0966)	0.0597 (0.0582)	0.795 (0.811)	75.00 (75.55)
[60/157]	0.0975 (0.0968)	0.0593 (0.0584)	0.778 (0.819)	71.88 (74.80)
[70/157]	0.0988 (0.0969)	0.0596 (0.0586)	0.677 (0.824)	81.25 (74.34)
[80/157]	0.0979 (0.0970)	0.0586 (0.0587)	1.006 (0.825)	71.88 (74.73)
[90/157]	0.0952 (0.0976)	0.0573 (0.0593)	0.896 (0.825)	75.00 (74.76)
[100/157]	0.0969 (0.0974)	0.0581 (0.0591)	0.787 (0.832)	84.38 (74.57)
[110/157]	0.1021 (0.0975)	0.0626 (0.0592)	0.730 (0.830)	75.00 (74.69)
[120/157]	0.1007 (0.0978)	0.0605 (0.0594)	0.584 (0.824)	84.38 (74.90)
[130/157]	0.1019 (0.0981)	0.0622 (0.0596)	0.927 (0.828)	68.75 (74.67)
[140/157]	0.1030 (0.0983)	0.0634 (0.0598)	1.247 (0.825)	68.75 (74.82)
[150/157]	0.1001 (0.0985)	0.0612 (0.0600)	0.506 (0.825)	84.38 (74.81)
[156/157]	0.0850 (0.0985)	0.0573 (0.0601)	0.520 (0.826)	87.50 (74.78)
 * Train Acc 74.780
 * Val Acc 74.100, Total time 0.61
 * Val loss 0.787, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0091 (0.0091)	0.770 (0.770)	71.88 (71.88)
[10/157]	0.1004 (0.0953)	0.0614 (0.0572)	0.582 (0.901)	87.50 (71.02)
[20/157]	0.1024 (0.0980)	0.0633 (0.0597)	1.055 (0.876)	65.62 (71.73)
[30/157]	0.0984 (0.0990)	0.0596 (0.0605)	0.879 (0.856)	71.88 (72.58)
[40/157]	0.0989 (0.0997)	0.0609 (0.0611)	0.819 (0.848)	68.75 (72.48)
[50/157]	0.1024 (0.1000)	0.0630 (0.0613)	0.704 (0.831)	81.25 (73.47)
[60/157]	0.0983 (0.1002)	0.0596 (0.0614)	0.756 (0.839)	81.25 (73.77)
[70/157]	0.1010 (0.1004)	0.0617 (0.0616)	0.694 (0.834)	81.25 (74.12)
[80/157]	0.1008 (0.1005)	0.0616 (0.0618)	0.698 (0.826)	84.38 (74.23)
[90/157]	0.1030 (0.1006)	0.0624 (0.0618)	0.665 (0.815)	81.25 (74.62)
[100/157]	0.1003 (0.1006)	0.0613 (0.0619)	0.663 (0.821)	78.12 (74.50)
[110/157]	0.1029 (0.1007)	0.0636 (0.0620)	0.609 (0.821)	71.88 (74.44)
[120/157]	0.0999 (0.1007)	0.0606 (0.0621)	0.796 (0.821)	75.00 (74.66)
[130/157]	0.1026 (0.1007)	0.0636 (0.0621)	0.706 (0.824)	81.25 (74.69)
[140/157]	0.1012 (0.1007)	0.0623 (0.0621)	0.979 (0.822)	68.75 (74.91)
[150/157]	0.1008 (0.1008)	0.0618 (0.0622)	0.916 (0.825)	71.88 (74.67)
[156/157]	0.0842 (0.1007)	0.0570 (0.0621)	0.975 (0.824)	50.00 (74.68)
 * Train Acc 74.680
 * Val Acc 73.300, Total time 0.60
 * Val loss 0.792, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0090 (0.0090)	0.806 (0.806)	71.88 (71.88)
[10/157]	0.1026 (0.0952)	0.0633 (0.0574)	1.002 (0.838)	68.75 (75.00)
[20/157]	0.1011 (0.0981)	0.0623 (0.0600)	1.225 (0.850)	65.62 (75.60)
[30/157]	0.1011 (0.0992)	0.0624 (0.0610)	0.690 (0.846)	75.00 (74.70)
[40/157]	0.1008 (0.0999)	0.0618 (0.0616)	0.712 (0.844)	71.88 (74.47)
[50/157]	0.1022 (0.1002)	0.0628 (0.0619)	0.612 (0.844)	84.38 (74.57)
[60/157]	0.1017 (0.1003)	0.0630 (0.0620)	0.938 (0.838)	68.75 (74.28)
[70/157]	0.1014 (0.1005)	0.0621 (0.0621)	1.031 (0.834)	75.00 (74.56)
[80/157]	0.1005 (0.1007)	0.0615 (0.0623)	0.981 (0.832)	68.75 (74.58)
[90/157]	0.1027 (0.1008)	0.0625 (0.0623)	0.678 (0.824)	81.25 (74.93)
[100/157]	0.0995 (0.1007)	0.0611 (0.0623)	0.677 (0.823)	75.00 (74.72)
[110/157]	0.1022 (0.1008)	0.0629 (0.0624)	0.875 (0.829)	68.75 (74.38)
[120/157]	0.1014 (0.1008)	0.0623 (0.0624)	1.020 (0.828)	68.75 (74.43)
[130/157]	0.1011 (0.1009)	0.0618 (0.0625)	0.671 (0.819)	87.50 (74.69)
[140/157]	0.1004 (0.1009)	0.0611 (0.0625)	0.827 (0.814)	81.25 (75.02)
[150/157]	0.1011 (0.1009)	0.0623 (0.0625)	1.119 (0.820)	59.38 (74.67)
[156/157]	0.0845 (0.1008)	0.0575 (0.0625)	1.854 (0.827)	50.00 (74.36)
 * Train Acc 74.360
 * Val Acc 74.600, Total time 0.60
 * Val loss 0.789, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0089 (0.0089)	0.828 (0.828)	84.38 (84.38)
[10/157]	0.1016 (0.0951)	0.0622 (0.0571)	0.676 (0.779)	81.25 (78.12)
[20/157]	0.0996 (0.0982)	0.0609 (0.0599)	0.658 (0.799)	78.12 (76.64)
[30/157]	0.1029 (0.0993)	0.0636 (0.0610)	0.392 (0.780)	93.75 (76.61)
[40/157]	0.1011 (0.1000)	0.0619 (0.0615)	0.801 (0.800)	78.12 (76.14)
[50/157]	0.1024 (0.1002)	0.0632 (0.0617)	0.946 (0.810)	71.88 (75.67)
[60/157]	0.1005 (0.1003)	0.0612 (0.0618)	0.571 (0.814)	90.62 (76.13)
[70/157]	0.1031 (0.1005)	0.0632 (0.0620)	0.780 (0.808)	75.00 (76.32)
[80/157]	0.1009 (0.1005)	0.0622 (0.0620)	0.956 (0.814)	65.62 (76.12)
[90/157]	0.1005 (0.1006)	0.0611 (0.0620)	0.811 (0.814)	81.25 (76.00)
[100/157]	0.1028 (0.1007)	0.0633 (0.0620)	1.008 (0.809)	62.50 (76.02)
[110/157]	0.1011 (0.1007)	0.0621 (0.0620)	0.587 (0.803)	78.12 (76.24)
[120/157]	0.1026 (0.1008)	0.0632 (0.0621)	0.752 (0.802)	81.25 (76.34)
[130/157]	0.1017 (0.1008)	0.0629 (0.0622)	0.489 (0.804)	93.75 (76.19)
[140/157]	0.1009 (0.1008)	0.0618 (0.0622)	0.987 (0.804)	53.12 (76.24)
[150/157]	0.1027 (0.1009)	0.0625 (0.0623)	1.191 (0.806)	68.75 (76.12)
[156/157]	0.0861 (0.1008)	0.0584 (0.0622)	1.655 (0.808)	37.50 (76.02)
 * Train Acc 76.020
 * Val Acc 73.400, Total time 0.60
 * Val loss 0.796, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0082 (0.0082)	0.976 (0.976)	75.00 (75.00)
[10/157]	0.1016 (0.0954)	0.0623 (0.0575)	1.141 (0.887)	59.38 (75.85)
[20/157]	0.1029 (0.0986)	0.0625 (0.0600)	1.074 (0.852)	68.75 (75.15)
[30/157]	0.1024 (0.0997)	0.0615 (0.0607)	0.892 (0.843)	68.75 (74.80)
[40/157]	0.1012 (0.1001)	0.0618 (0.0611)	0.981 (0.834)	65.62 (74.24)
[50/157]	0.1011 (0.1004)	0.0617 (0.0614)	0.954 (0.835)	78.12 (73.96)
[60/157]	0.1031 (0.1006)	0.0630 (0.0615)	0.948 (0.848)	75.00 (73.46)
[70/157]	0.1028 (0.1007)	0.0633 (0.0617)	0.661 (0.849)	81.25 (73.46)
[80/157]	0.1023 (0.1008)	0.0625 (0.0617)	0.667 (0.845)	81.25 (73.73)
[90/157]	0.1008 (0.1009)	0.0618 (0.0618)	0.969 (0.841)	78.12 (73.73)
[100/157]	0.1011 (0.1010)	0.0617 (0.0620)	0.641 (0.830)	81.25 (73.89)
[110/157]	0.1034 (0.1010)	0.0633 (0.0620)	0.962 (0.834)	65.62 (73.70)
[120/157]	0.1023 (0.1010)	0.0620 (0.0620)	0.601 (0.833)	90.62 (73.92)
[130/157]	0.1004 (0.1011)	0.0616 (0.0620)	0.756 (0.826)	71.88 (74.38)
[140/157]	0.1010 (0.1011)	0.0612 (0.0620)	0.518 (0.821)	93.75 (74.78)
[150/157]	0.1008 (0.1012)	0.0620 (0.0621)	0.874 (0.821)	68.75 (74.69)
[156/157]	0.0858 (0.1011)	0.0587 (0.0621)	0.669 (0.822)	75.00 (74.66)
 * Train Acc 74.660
 * Val Acc 73.800, Total time 0.58
 * Val loss 0.775, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0091 (0.0091)	0.790 (0.790)	78.12 (78.12)
[10/157]	0.0954 (0.0984)	0.0576 (0.0606)	0.519 (0.727)	87.50 (79.55)
[20/157]	0.0976 (0.0970)	0.0584 (0.0597)	0.642 (0.739)	81.25 (78.27)
[30/157]	0.0960 (0.0965)	0.0587 (0.0592)	0.859 (0.775)	65.62 (75.81)
[40/157]	0.0969 (0.0965)	0.0588 (0.0591)	0.871 (0.798)	75.00 (75.30)
[50/157]	0.1207 (0.0977)	0.0803 (0.0602)	1.021 (0.808)	71.88 (74.69)
[60/157]	0.0979 (0.0983)	0.0584 (0.0608)	0.879 (0.806)	71.88 (74.64)
[70/157]	0.0967 (0.0981)	0.0588 (0.0605)	0.675 (0.803)	78.12 (74.91)
[80/157]	0.0977 (0.0980)	0.0583 (0.0603)	0.968 (0.805)	59.38 (74.69)
[90/157]	0.0959 (0.0978)	0.0579 (0.0602)	0.734 (0.804)	78.12 (74.79)
[100/157]	0.0982 (0.0978)	0.0587 (0.0601)	0.746 (0.816)	71.88 (74.66)
[110/157]	0.1089 (0.0987)	0.0678 (0.0609)	0.739 (0.813)	81.25 (74.86)
[120/157]	0.1077 (0.0993)	0.0681 (0.0615)	0.867 (0.812)	81.25 (75.10)
[130/157]	0.0950 (0.0997)	0.0571 (0.0617)	0.743 (0.814)	81.25 (75.02)
[140/157]	0.1208 (0.0998)	0.0799 (0.0618)	0.678 (0.816)	81.25 (75.13)
[150/157]	0.0962 (0.1001)	0.0583 (0.0622)	0.902 (0.819)	75.00 (75.06)
[156/157]	0.0777 (0.0998)	0.0516 (0.0620)	1.027 (0.819)	75.00 (75.10)
 * Train Acc 75.100
 * Val Acc 73.800, Total time 0.62
 * Val loss 0.781, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0089 (0.0089)	1.021 (1.021)	71.88 (71.88)
[10/157]	0.1034 (0.0982)	0.0632 (0.0589)	0.802 (0.796)	78.12 (76.14)
[20/157]	0.1044 (0.1012)	0.0642 (0.0619)	0.679 (0.789)	84.38 (76.34)
[30/157]	0.1052 (0.1022)	0.0657 (0.0630)	1.054 (0.788)	65.62 (75.91)
[40/157]	0.1059 (0.1027)	0.0657 (0.0636)	0.921 (0.795)	71.88 (75.46)
[50/157]	0.1024 (0.1018)	0.0626 (0.0630)	0.734 (0.818)	84.38 (74.88)
[60/157]	0.1057 (0.1025)	0.0659 (0.0636)	0.900 (0.826)	71.88 (74.59)
[70/157]	0.0962 (0.1019)	0.0575 (0.0632)	0.935 (0.825)	68.75 (74.87)
[80/157]	0.0948 (0.1011)	0.0555 (0.0626)	0.916 (0.828)	78.12 (74.61)
[90/157]	0.1013 (0.1009)	0.0617 (0.0623)	1.337 (0.831)	59.38 (74.69)
[100/157]	0.1009 (0.1010)	0.0617 (0.0624)	0.655 (0.825)	81.25 (75.03)
[110/157]	0.1010 (0.1011)	0.0627 (0.0625)	0.832 (0.821)	75.00 (75.31)
[120/157]	0.1008 (0.1011)	0.0614 (0.0625)	0.995 (0.826)	59.38 (74.87)
[130/157]	0.1000 (0.1012)	0.0611 (0.0625)	0.811 (0.818)	75.00 (74.98)
[140/157]	0.1011 (0.1012)	0.0612 (0.0625)	0.611 (0.818)	81.25 (75.02)
[150/157]	0.1023 (0.1012)	0.0630 (0.0625)	0.690 (0.821)	81.25 (74.90)
[156/157]	0.0833 (0.1011)	0.0544 (0.0625)	0.496 (0.820)	87.50 (74.80)
 * Train Acc 74.800
 * Val Acc 74.400, Total time 0.60
 * Val loss 0.772, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0465 (0.0465)	0.0124 (0.0124)	0.946 (0.946)	68.75 (68.75)
[10/157]	0.1026 (0.0961)	0.0620 (0.0576)	0.912 (0.886)	71.88 (73.01)
[20/157]	0.1014 (0.0984)	0.0625 (0.0598)	0.873 (0.851)	75.00 (73.36)
[30/157]	0.0995 (0.0996)	0.0591 (0.0607)	0.748 (0.830)	81.25 (75.40)
[40/157]	0.0996 (0.1001)	0.0607 (0.0612)	0.745 (0.833)	71.88 (74.77)
[50/157]	0.1018 (0.1004)	0.0628 (0.0616)	0.890 (0.843)	75.00 (74.51)
[60/157]	0.1022 (0.1007)	0.0623 (0.0618)	0.924 (0.836)	68.75 (74.69)
[70/157]	0.1042 (0.1008)	0.0642 (0.0619)	0.838 (0.828)	75.00 (75.26)
[80/157]	0.1014 (0.1008)	0.0624 (0.0620)	1.035 (0.822)	75.00 (75.39)
[90/157]	0.0935 (0.1004)	0.0566 (0.0617)	0.867 (0.819)	68.75 (75.41)
[100/157]	0.0948 (0.0999)	0.0576 (0.0614)	0.712 (0.819)	84.38 (75.43)
[110/157]	0.1054 (0.1004)	0.0658 (0.0620)	0.702 (0.818)	84.38 (75.68)
[120/157]	0.1051 (0.1008)	0.0651 (0.0623)	1.204 (0.827)	68.75 (75.41)
[130/157]	0.1056 (0.1011)	0.0663 (0.0625)	0.667 (0.823)	78.12 (75.62)
[140/157]	0.1053 (0.1013)	0.0629 (0.0627)	0.934 (0.822)	75.00 (75.60)
[150/157]	0.1070 (0.1016)	0.0666 (0.0629)	0.878 (0.818)	75.00 (75.72)
[156/157]	0.0783 (0.1012)	0.0531 (0.0627)	1.170 (0.819)	75.00 (75.72)
 * Train Acc 75.720
 * Val Acc 74.300, Total time 0.58
 * Val loss 0.770, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0089 (0.0089)	0.686 (0.686)	78.12 (78.12)
[10/157]	0.0950 (0.0915)	0.0561 (0.0538)	0.872 (0.794)	65.62 (75.85)
[20/157]	0.0990 (0.0936)	0.0575 (0.0556)	0.803 (0.783)	78.12 (76.64)
[30/157]	0.1080 (0.0978)	0.0680 (0.0594)	0.978 (0.804)	65.62 (75.91)
[40/157]	0.0953 (0.0975)	0.0580 (0.0594)	0.615 (0.792)	84.38 (75.69)
[50/157]	0.1042 (0.0983)	0.0649 (0.0602)	0.493 (0.788)	84.38 (75.67)
[60/157]	0.1034 (0.0993)	0.0645 (0.0610)	0.709 (0.803)	75.00 (74.95)
[70/157]	0.1052 (0.0999)	0.0649 (0.0616)	1.115 (0.814)	71.88 (74.96)
[80/157]	0.1053 (0.1004)	0.0650 (0.0620)	0.821 (0.805)	78.12 (75.31)
[90/157]	0.0972 (0.0999)	0.0584 (0.0617)	0.619 (0.800)	81.25 (75.31)
[100/157]	0.0951 (0.0995)	0.0579 (0.0614)	1.174 (0.798)	71.88 (75.34)
[110/157]	0.1186 (0.1001)	0.0786 (0.0620)	1.063 (0.809)	59.38 (75.17)
[120/157]	0.1022 (0.1003)	0.0630 (0.0621)	0.522 (0.812)	87.50 (75.05)
[130/157]	0.1008 (0.1004)	0.0617 (0.0622)	0.866 (0.817)	71.88 (74.90)
[140/157]	0.1016 (0.1005)	0.0624 (0.0623)	0.674 (0.817)	78.12 (74.98)
[150/157]	0.1010 (0.1006)	0.0620 (0.0623)	0.807 (0.817)	78.12 (74.94)
[156/157]	0.0866 (0.1006)	0.0592 (0.0623)	0.596 (0.814)	87.50 (75.00)
 * Train Acc 75.000
 * Val Acc 74.600, Total time 0.60
 * Val loss 0.788, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0087 (0.0087)	0.672 (0.672)	84.38 (84.38)
[10/157]	0.1014 (0.0959)	0.0625 (0.0580)	0.669 (0.833)	81.25 (74.72)
[20/157]	0.1019 (0.0985)	0.0622 (0.0605)	0.759 (0.831)	75.00 (73.51)
[30/157]	0.1011 (0.0993)	0.0624 (0.0612)	0.902 (0.805)	71.88 (74.40)
[40/157]	0.1011 (0.0998)	0.0624 (0.0616)	1.148 (0.820)	53.12 (73.70)
[50/157]	0.1002 (0.1002)	0.0615 (0.0620)	0.645 (0.811)	81.25 (74.63)
[60/157]	0.1018 (0.1003)	0.0627 (0.0621)	0.848 (0.809)	75.00 (74.69)
[70/157]	0.1007 (0.1005)	0.0616 (0.0622)	0.585 (0.810)	81.25 (74.21)
[80/157]	0.1007 (0.1007)	0.0623 (0.0623)	0.907 (0.801)	68.75 (74.96)
[90/157]	0.1006 (0.1008)	0.0602 (0.0624)	0.933 (0.812)	59.38 (74.62)
[100/157]	0.1030 (0.1009)	0.0635 (0.0625)	0.632 (0.811)	78.12 (74.85)
[110/157]	0.1036 (0.1009)	0.0632 (0.0626)	0.757 (0.821)	81.25 (74.69)
[120/157]	0.1025 (0.1010)	0.0632 (0.0626)	0.782 (0.821)	81.25 (74.72)
[130/157]	0.1017 (0.1010)	0.0623 (0.0626)	0.963 (0.815)	68.75 (74.98)
[140/157]	0.1022 (0.1011)	0.0631 (0.0626)	0.678 (0.807)	87.50 (75.47)
[150/157]	0.1018 (0.1011)	0.0621 (0.0626)	0.674 (0.804)	81.25 (75.52)
[156/157]	0.0852 (0.1011)	0.0569 (0.0626)	0.360 (0.802)	87.50 (75.48)
 * Train Acc 75.480
 * Val Acc 74.700, Total time 0.60
 * Val loss 0.776, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0089 (0.0089)	1.029 (1.029)	71.88 (71.88)
[10/157]	0.1017 (0.0953)	0.0619 (0.0567)	0.769 (0.853)	84.38 (75.28)
[20/157]	0.1015 (0.0984)	0.0628 (0.0596)	0.984 (0.838)	68.75 (75.89)
[30/157]	0.0998 (0.0995)	0.0611 (0.0605)	0.557 (0.823)	84.38 (76.01)
[40/157]	0.1011 (0.1001)	0.0613 (0.0612)	0.512 (0.803)	87.50 (76.07)
[50/157]	0.1003 (0.1004)	0.0622 (0.0616)	0.804 (0.793)	81.25 (76.47)
[60/157]	0.1042 (0.1006)	0.0639 (0.0618)	0.934 (0.808)	71.88 (75.87)
[70/157]	0.1026 (0.1007)	0.0631 (0.0620)	0.810 (0.809)	68.75 (75.57)
[80/157]	0.1006 (0.1008)	0.0618 (0.0621)	0.842 (0.807)	71.88 (75.46)
[90/157]	0.1002 (0.1009)	0.0613 (0.0623)	0.986 (0.811)	75.00 (75.52)
[100/157]	0.1019 (0.1010)	0.0618 (0.0623)	0.833 (0.806)	81.25 (75.65)
[110/157]	0.1003 (0.1011)	0.0614 (0.0624)	0.737 (0.803)	78.12 (75.68)
[120/157]	0.1028 (0.1011)	0.0631 (0.0625)	0.862 (0.799)	81.25 (75.72)
[130/157]	0.1022 (0.1011)	0.0633 (0.0625)	0.849 (0.802)	65.62 (75.72)
[140/157]	0.1019 (0.1011)	0.0630 (0.0625)	0.619 (0.802)	81.25 (75.49)
[150/157]	0.1007 (0.1012)	0.0619 (0.0626)	1.081 (0.805)	68.75 (75.25)
[156/157]	0.0858 (0.1011)	0.0586 (0.0626)	0.997 (0.805)	62.50 (75.26)
 * Train Acc 75.260
 * Val Acc 74.200, Total time 0.60
 * Val loss 0.770, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0089 (0.0089)	0.975 (0.975)	71.88 (71.88)
[10/157]	0.1013 (0.0962)	0.0619 (0.0571)	0.727 (0.765)	81.25 (78.12)
[20/157]	0.1014 (0.0988)	0.0617 (0.0599)	0.770 (0.774)	87.50 (77.53)
[30/157]	0.1053 (0.0997)	0.0627 (0.0608)	0.632 (0.789)	78.12 (76.31)
[40/157]	0.1042 (0.1001)	0.0642 (0.0611)	0.662 (0.796)	84.38 (75.76)
[50/157]	0.1044 (0.1004)	0.0621 (0.0614)	0.577 (0.796)	78.12 (75.55)
[60/157]	0.1012 (0.1006)	0.0620 (0.0616)	1.005 (0.797)	68.75 (75.26)
[70/157]	0.1010 (0.1008)	0.0618 (0.0618)	0.955 (0.799)	75.00 (75.13)
[80/157]	0.1003 (0.1009)	0.0613 (0.0619)	1.119 (0.808)	65.62 (75.04)
[90/157]	0.1026 (0.1010)	0.0633 (0.0620)	0.771 (0.808)	68.75 (74.93)
[100/157]	0.1032 (0.1010)	0.0636 (0.0621)	1.230 (0.813)	62.50 (74.78)
[110/157]	0.1011 (0.1010)	0.0621 (0.0621)	0.813 (0.814)	75.00 (74.89)
[120/157]	0.1005 (0.1011)	0.0614 (0.0622)	0.590 (0.810)	81.25 (75.13)
[130/157]	0.1018 (0.1012)	0.0614 (0.0623)	0.963 (0.800)	65.62 (75.48)
[140/157]	0.1009 (0.1012)	0.0605 (0.0623)	0.850 (0.801)	81.25 (75.71)
[150/157]	0.1009 (0.1013)	0.0621 (0.0624)	0.929 (0.802)	62.50 (75.64)
[156/157]	0.0857 (0.1012)	0.0584 (0.0623)	0.571 (0.802)	87.50 (75.70)
 * Train Acc 75.700
 * Val Acc 74.500, Total time 0.60
 * Val loss 0.769, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0090 (0.0090)	0.925 (0.925)	78.12 (78.12)
[10/157]	0.1008 (0.0957)	0.0620 (0.0571)	0.747 (0.907)	75.00 (73.58)
[20/157]	0.1010 (0.0987)	0.0620 (0.0602)	0.972 (0.816)	65.62 (75.45)
[30/157]	0.1018 (0.0997)	0.0628 (0.0612)	0.846 (0.838)	78.12 (73.99)
[40/157]	0.1021 (0.1002)	0.0631 (0.0616)	0.845 (0.837)	81.25 (74.09)
[50/157]	0.1059 (0.1006)	0.0638 (0.0619)	0.601 (0.825)	87.50 (74.69)
[60/157]	0.1034 (0.1007)	0.0637 (0.0620)	0.756 (0.823)	78.12 (74.95)
[70/157]	0.1029 (0.1009)	0.0630 (0.0622)	0.909 (0.826)	78.12 (74.87)
[80/157]	0.1029 (0.1010)	0.0632 (0.0623)	0.903 (0.822)	71.88 (74.77)
[90/157]	0.1025 (0.1012)	0.0627 (0.0623)	0.686 (0.823)	84.38 (74.52)
[100/157]	0.0961 (0.1006)	0.0566 (0.0619)	0.572 (0.815)	84.38 (74.81)
[110/157]	0.0979 (0.1003)	0.0592 (0.0617)	0.900 (0.814)	71.88 (74.80)
[120/157]	0.0983 (0.1001)	0.0592 (0.0615)	0.724 (0.813)	75.00 (74.97)
[130/157]	0.0983 (0.0999)	0.0594 (0.0613)	0.603 (0.811)	87.50 (74.98)
[140/157]	0.0955 (0.0997)	0.0571 (0.0611)	0.588 (0.807)	84.38 (75.09)
[150/157]	0.1179 (0.1005)	0.0769 (0.0619)	0.552 (0.802)	90.62 (75.35)
[156/157]	0.0850 (0.1004)	0.0578 (0.0619)	0.559 (0.807)	87.50 (75.14)
 * Train Acc 75.140
 * Val Acc 74.500, Total time 0.59
 * Val loss 0.764, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0080 (0.0080)	0.945 (0.945)	65.62 (65.62)
[10/157]	0.0972 (0.0947)	0.0581 (0.0563)	0.762 (0.840)	81.25 (76.14)
[20/157]	0.1004 (0.0972)	0.0614 (0.0588)	0.838 (0.799)	75.00 (76.34)
[30/157]	0.0999 (0.0981)	0.0608 (0.0596)	0.713 (0.803)	84.38 (77.42)
[40/157]	0.1008 (0.0986)	0.0615 (0.0600)	0.672 (0.818)	78.12 (76.60)
[50/157]	0.1006 (0.0988)	0.0614 (0.0603)	0.941 (0.816)	71.88 (76.65)
[60/157]	0.1003 (0.0990)	0.0615 (0.0606)	0.758 (0.815)	84.38 (76.33)
[70/157]	0.0999 (0.0991)	0.0604 (0.0607)	0.949 (0.813)	71.88 (76.06)
[80/157]	0.1005 (0.0993)	0.0611 (0.0608)	0.785 (0.823)	87.50 (75.77)
[90/157]	0.1010 (0.0994)	0.0615 (0.0609)	0.844 (0.816)	68.75 (75.82)
[100/157]	0.1005 (0.0996)	0.0619 (0.0611)	0.724 (0.821)	81.25 (75.56)
[110/157]	0.0995 (0.0996)	0.0601 (0.0611)	0.799 (0.822)	78.12 (75.56)
[120/157]	0.1011 (0.0997)	0.0616 (0.0611)	0.872 (0.820)	78.12 (75.65)
[130/157]	0.1008 (0.0998)	0.0615 (0.0612)	0.803 (0.819)	68.75 (75.62)
[140/157]	0.1006 (0.0998)	0.0608 (0.0612)	0.745 (0.816)	78.12 (75.66)
[150/157]	0.1004 (0.0999)	0.0610 (0.0613)	0.971 (0.815)	68.75 (75.62)
[156/157]	0.0865 (0.0998)	0.0577 (0.0613)	0.847 (0.815)	75.00 (75.58)
 * Train Acc 75.580
 * Val Acc 74.300, Total time 0.60
 * Val loss 0.777, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0090 (0.0090)	0.913 (0.913)	71.88 (71.88)
[10/157]	0.1012 (0.0950)	0.0622 (0.0575)	0.836 (0.788)	81.25 (76.99)
[20/157]	0.1001 (0.0977)	0.0608 (0.0596)	0.725 (0.766)	78.12 (76.79)
[30/157]	0.1001 (0.0985)	0.0608 (0.0602)	0.890 (0.801)	65.62 (76.21)
[40/157]	0.1008 (0.0988)	0.0614 (0.0605)	0.659 (0.802)	78.12 (76.22)
[50/157]	0.1004 (0.0991)	0.0609 (0.0607)	0.654 (0.790)	87.50 (77.02)
[60/157]	0.1002 (0.0992)	0.0612 (0.0609)	0.608 (0.786)	87.50 (77.10)
[70/157]	0.1003 (0.0993)	0.0612 (0.0610)	0.619 (0.790)	81.25 (76.63)
[80/157]	0.1004 (0.0994)	0.0611 (0.0610)	0.769 (0.796)	78.12 (76.12)
[90/157]	0.1001 (0.0995)	0.0608 (0.0611)	0.719 (0.793)	78.12 (76.03)
[100/157]	0.1007 (0.0996)	0.0625 (0.0612)	0.562 (0.793)	90.62 (76.24)
[110/157]	0.1006 (0.0996)	0.0619 (0.0613)	1.099 (0.801)	56.25 (76.07)
[120/157]	0.1002 (0.0997)	0.0613 (0.0614)	0.952 (0.795)	71.88 (76.32)
[130/157]	0.1003 (0.0998)	0.0615 (0.0614)	0.844 (0.800)	71.88 (76.07)
[140/157]	0.0999 (0.0998)	0.0609 (0.0615)	0.929 (0.803)	68.75 (75.89)
[150/157]	0.0995 (0.0998)	0.0605 (0.0615)	0.691 (0.798)	81.25 (76.12)
[156/157]	0.0830 (0.0997)	0.0562 (0.0614)	1.136 (0.796)	62.50 (76.22)
 * Train Acc 76.220
 * Val Acc 74.600, Total time 0.61
 * Val loss 0.764, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0086 (0.0086)	0.867 (0.867)	75.00 (75.00)
[10/157]	0.0986 (0.0927)	0.0596 (0.0554)	0.901 (0.879)	71.88 (74.15)
[20/157]	0.0990 (0.0954)	0.0607 (0.0576)	0.787 (0.837)	78.12 (75.30)
[30/157]	0.0992 (0.0964)	0.0593 (0.0583)	0.595 (0.851)	87.50 (74.90)
[40/157]	0.0963 (0.0995)	0.0582 (0.0614)	0.565 (0.833)	84.38 (75.30)
[50/157]	0.1206 (0.0997)	0.0802 (0.0616)	0.812 (0.818)	71.88 (75.49)
[60/157]	0.1057 (0.1009)	0.0661 (0.0627)	0.527 (0.801)	81.25 (75.87)
[70/157]	0.1030 (0.1014)	0.0636 (0.0631)	0.883 (0.802)	65.62 (75.40)
[80/157]	0.1044 (0.1018)	0.0651 (0.0634)	0.584 (0.809)	84.38 (75.39)
[90/157]	0.1060 (0.1021)	0.0661 (0.0637)	0.780 (0.807)	71.88 (75.27)
[100/157]	0.0953 (0.1022)	0.0579 (0.0638)	0.510 (0.805)	90.62 (75.50)
[110/157]	0.1195 (0.1018)	0.0789 (0.0635)	0.774 (0.806)	78.12 (75.70)
[120/157]	0.0982 (0.1021)	0.0591 (0.0638)	0.794 (0.796)	75.00 (76.08)
[130/157]	0.0968 (0.1017)	0.0590 (0.0634)	0.926 (0.801)	71.88 (75.88)
[140/157]	0.0974 (0.1013)	0.0590 (0.0631)	0.827 (0.800)	78.12 (75.91)
[150/157]	0.1205 (0.1016)	0.0802 (0.0633)	0.707 (0.801)	84.38 (75.93)
[156/157]	0.0859 (0.1020)	0.0590 (0.0637)	0.715 (0.797)	75.00 (76.02)
 * Train Acc 76.020
 * Val Acc 74.300, Total time 0.59
 * Val loss 0.774, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0090 (0.0090)	0.689 (0.689)	81.25 (81.25)
[10/157]	0.1000 (0.0934)	0.0612 (0.0557)	0.596 (0.796)	84.38 (75.85)
[20/157]	0.0985 (0.0961)	0.0612 (0.0582)	0.803 (0.774)	68.75 (75.30)
[30/157]	0.0996 (0.0973)	0.0605 (0.0592)	0.620 (0.787)	78.12 (75.50)
[40/157]	0.1005 (0.0978)	0.0604 (0.0595)	0.658 (0.799)	78.12 (75.53)
[50/157]	0.1005 (0.0982)	0.0598 (0.0596)	0.602 (0.794)	93.75 (76.29)
[60/157]	0.0996 (0.0984)	0.0611 (0.0598)	0.796 (0.820)	68.75 (75.10)
[70/157]	0.0991 (0.0985)	0.0603 (0.0600)	0.637 (0.821)	75.00 (74.74)
[80/157]	0.0997 (0.0987)	0.0607 (0.0602)	1.097 (0.826)	65.62 (74.65)
[90/157]	0.0988 (0.0987)	0.0606 (0.0602)	1.064 (0.819)	59.38 (74.97)
[100/157]	0.0990 (0.0987)	0.0606 (0.0603)	0.829 (0.823)	65.62 (74.60)
[110/157]	0.0992 (0.0988)	0.0605 (0.0604)	0.982 (0.822)	65.62 (74.44)
[120/157]	0.0996 (0.0988)	0.0603 (0.0604)	0.690 (0.815)	81.25 (74.85)
[130/157]	0.0990 (0.0988)	0.0597 (0.0604)	0.787 (0.808)	71.88 (75.10)
[140/157]	0.0971 (0.0989)	0.0588 (0.0604)	0.898 (0.811)	68.75 (75.09)
[150/157]	0.0992 (0.0989)	0.0603 (0.0605)	0.701 (0.808)	81.25 (75.21)
[156/157]	0.0840 (0.0987)	0.0583 (0.0604)	0.915 (0.806)	62.50 (75.18)
 * Train Acc 75.180
 * Val Acc 74.400, Total time 0.62
 * Val loss 0.776, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0086 (0.0086)	0.961 (0.961)	71.88 (71.88)
[10/157]	0.1041 (0.0990)	0.0649 (0.0604)	0.788 (0.799)	71.88 (74.43)
[20/157]	0.1060 (0.1020)	0.0670 (0.0633)	0.814 (0.799)	78.12 (75.89)
[30/157]	0.1066 (0.1030)	0.0664 (0.0642)	0.767 (0.806)	75.00 (75.81)
[40/157]	0.1056 (0.1037)	0.0647 (0.0648)	0.808 (0.808)	71.88 (75.38)
[50/157]	0.1040 (0.1041)	0.0644 (0.0651)	0.906 (0.810)	65.62 (75.37)
[60/157]	0.0951 (0.1035)	0.0573 (0.0646)	0.889 (0.801)	71.88 (75.56)
[70/157]	0.0951 (0.1024)	0.0573 (0.0637)	0.772 (0.813)	78.12 (75.22)
[80/157]	0.0981 (0.1016)	0.0598 (0.0631)	0.932 (0.815)	65.62 (75.12)
[90/157]	0.0996 (0.1013)	0.0601 (0.0628)	0.751 (0.818)	78.12 (74.76)
[100/157]	0.0996 (0.1010)	0.0605 (0.0625)	0.849 (0.825)	75.00 (74.75)
[110/157]	0.1001 (0.1009)	0.0596 (0.0623)	0.872 (0.818)	68.75 (75.00)
[120/157]	0.0994 (0.1007)	0.0597 (0.0621)	0.729 (0.812)	78.12 (75.46)
[130/157]	0.0995 (0.1005)	0.0607 (0.0620)	0.833 (0.812)	71.88 (75.43)
[140/157]	0.0980 (0.1004)	0.0592 (0.0618)	0.888 (0.801)	75.00 (75.84)
[150/157]	0.0999 (0.1003)	0.0596 (0.0617)	0.615 (0.801)	84.38 (75.89)
[156/157]	0.0810 (0.1001)	0.0537 (0.0616)	1.203 (0.801)	62.50 (75.82)
 * Train Acc 75.820
 * Val Acc 75.100, Total time 0.60
 * Val loss 0.771, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0087 (0.0087)	0.988 (0.988)	75.00 (75.00)
[10/157]	0.0995 (0.0930)	0.0609 (0.0553)	0.604 (0.731)	87.50 (78.69)
[20/157]	0.0987 (0.0955)	0.0607 (0.0576)	0.624 (0.704)	90.62 (79.76)
[30/157]	0.0984 (0.0964)	0.0604 (0.0585)	0.553 (0.740)	87.50 (78.53)
[40/157]	0.0974 (0.0969)	0.0586 (0.0589)	0.595 (0.746)	81.25 (77.74)
[50/157]	0.1029 (0.0981)	0.0633 (0.0600)	0.662 (0.748)	78.12 (77.51)
[60/157]	0.1028 (0.0990)	0.0635 (0.0608)	1.011 (0.762)	75.00 (77.31)
[70/157]	0.1035 (0.0997)	0.0640 (0.0614)	0.724 (0.770)	75.00 (76.67)
[80/157]	0.1031 (0.1003)	0.0628 (0.0618)	1.141 (0.779)	71.88 (76.00)
[90/157]	0.0993 (0.1006)	0.0604 (0.0620)	0.580 (0.784)	87.50 (75.96)
[100/157]	0.0940 (0.1001)	0.0564 (0.0616)	0.836 (0.778)	71.88 (76.14)
[110/157]	0.0971 (0.0997)	0.0585 (0.0613)	0.808 (0.783)	75.00 (75.90)
[120/157]	0.0962 (0.0993)	0.0584 (0.0611)	0.780 (0.792)	75.00 (75.80)
[130/157]	0.0970 (0.0992)	0.0583 (0.0610)	1.097 (0.798)	62.50 (75.41)
[140/157]	0.1006 (0.0998)	0.0616 (0.0616)	1.001 (0.791)	71.88 (75.66)
[150/157]	0.1001 (0.0998)	0.0611 (0.0616)	1.057 (0.794)	71.88 (75.64)
[156/157]	0.0841 (0.0997)	0.0571 (0.0616)	1.286 (0.793)	62.50 (75.66)
 * Train Acc 75.660
 * Val Acc 74.200, Total time 0.60
 * Val loss 0.776, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0085 (0.0085)	0.675 (0.675)	84.38 (84.38)
[10/157]	0.1002 (0.0943)	0.0615 (0.0569)	1.169 (0.808)	65.62 (75.57)
[20/157]	0.1008 (0.0972)	0.0617 (0.0595)	0.712 (0.797)	81.25 (76.34)
[30/157]	0.1003 (0.0984)	0.0610 (0.0603)	0.736 (0.789)	75.00 (76.41)
[40/157]	0.0999 (0.0988)	0.0607 (0.0605)	0.712 (0.790)	68.75 (76.30)
[50/157]	0.0999 (0.0990)	0.0607 (0.0607)	0.767 (0.778)	71.88 (76.96)
[60/157]	0.0995 (0.0992)	0.0601 (0.0608)	1.012 (0.786)	75.00 (76.38)
[70/157]	0.1003 (0.0993)	0.0608 (0.0608)	1.004 (0.793)	68.75 (75.92)
[80/157]	0.0985 (0.0994)	0.0595 (0.0609)	0.688 (0.786)	81.25 (76.23)
[90/157]	0.1004 (0.0995)	0.0611 (0.0609)	0.781 (0.792)	71.88 (75.82)
[100/157]	0.0997 (0.0995)	0.0597 (0.0609)	0.766 (0.790)	78.12 (75.96)
[110/157]	0.1003 (0.0996)	0.0612 (0.0610)	0.725 (0.784)	71.88 (76.10)
[120/157]	0.1003 (0.0997)	0.0602 (0.0611)	0.992 (0.782)	75.00 (76.29)
[130/157]	0.1008 (0.0998)	0.0613 (0.0611)	1.118 (0.791)	59.38 (75.79)
[140/157]	0.1015 (0.0998)	0.0594 (0.0611)	0.724 (0.793)	71.88 (75.62)
[150/157]	0.0997 (0.0998)	0.0607 (0.0611)	0.927 (0.799)	78.12 (75.54)
[156/157]	0.0851 (0.0997)	0.0558 (0.0611)	0.725 (0.801)	75.00 (75.46)
 * Train Acc 75.460
 * Val Acc 73.500, Total time 0.60
 * Val loss 0.773, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0459 (0.0459)	0.0089 (0.0089)	0.836 (0.836)	75.00 (75.00)
[10/157]	0.1007 (0.0951)	0.0616 (0.0567)	0.957 (0.898)	71.88 (71.31)
[20/157]	0.1000 (0.0974)	0.0604 (0.0589)	0.541 (0.847)	90.62 (73.51)
[30/157]	0.1000 (0.0982)	0.0611 (0.0597)	0.778 (0.826)	71.88 (74.70)
[40/157]	0.0999 (0.0986)	0.0608 (0.0600)	0.504 (0.815)	90.62 (75.08)
[50/157]	0.1002 (0.0989)	0.0607 (0.0603)	0.628 (0.800)	78.12 (75.37)
[60/157]	0.0998 (0.0991)	0.0603 (0.0604)	1.080 (0.815)	75.00 (75.15)
[70/157]	0.1003 (0.0992)	0.0608 (0.0605)	0.754 (0.816)	71.88 (74.87)
[80/157]	0.0999 (0.0993)	0.0607 (0.0606)	0.971 (0.828)	71.88 (74.96)
[90/157]	0.1005 (0.0994)	0.0613 (0.0607)	0.784 (0.832)	78.12 (74.86)
[100/157]	0.1001 (0.0994)	0.0609 (0.0608)	0.769 (0.834)	81.25 (74.72)
[110/157]	0.1000 (0.0995)	0.0610 (0.0608)	1.213 (0.836)	65.62 (74.47)
[120/157]	0.1004 (0.0995)	0.0607 (0.0609)	0.773 (0.832)	78.12 (74.69)
[130/157]	0.1006 (0.0996)	0.0615 (0.0609)	0.686 (0.825)	87.50 (75.05)
[140/157]	0.1012 (0.0996)	0.0616 (0.0609)	0.675 (0.817)	78.12 (75.18)
[150/157]	0.1013 (0.0997)	0.0624 (0.0610)	1.023 (0.808)	65.62 (75.58)
[156/157]	0.0831 (0.0996)	0.0550 (0.0610)	0.965 (0.804)	87.50 (75.78)
 * Train Acc 75.780
 * Val Acc 74.900, Total time 0.60
 * Val loss 0.772, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0422 (0.0422)	0.0087 (0.0087)	0.872 (0.872)	68.75 (68.75)
[10/157]	0.0990 (0.0945)	0.0601 (0.0566)	0.691 (0.807)	78.12 (75.28)
[20/157]	0.1013 (0.0974)	0.0613 (0.0590)	0.695 (0.849)	78.12 (73.96)
[30/157]	0.0998 (0.0984)	0.0599 (0.0598)	0.865 (0.846)	68.75 (73.29)
[40/157]	0.1004 (0.0988)	0.0608 (0.0601)	0.598 (0.831)	78.12 (74.47)
[50/157]	0.0993 (0.0990)	0.0602 (0.0602)	0.586 (0.825)	81.25 (74.75)
[60/157]	0.1008 (0.0992)	0.0614 (0.0604)	0.716 (0.815)	78.12 (74.80)
[70/157]	0.1000 (0.0994)	0.0611 (0.0606)	0.705 (0.818)	75.00 (74.91)
[80/157]	0.1029 (0.0996)	0.0619 (0.0608)	1.004 (0.826)	65.62 (74.96)
[90/157]	0.1001 (0.0997)	0.0604 (0.0608)	1.203 (0.825)	53.12 (74.97)
[100/157]	0.0989 (0.0997)	0.0593 (0.0608)	0.774 (0.830)	71.88 (74.72)
[110/157]	0.1005 (0.0997)	0.0607 (0.0608)	0.908 (0.818)	71.88 (75.14)
[120/157]	0.1001 (0.0997)	0.0605 (0.0608)	0.778 (0.814)	78.12 (75.28)
[130/157]	0.0986 (0.0998)	0.0588 (0.0608)	0.783 (0.813)	81.25 (75.31)
[140/157]	0.1003 (0.0998)	0.0606 (0.0608)	0.559 (0.808)	81.25 (75.53)
[150/157]	0.0986 (0.0998)	0.0591 (0.0608)	0.880 (0.805)	68.75 (75.75)
[156/157]	0.0836 (0.0996)	0.0568 (0.0607)	0.915 (0.801)	75.00 (75.92)
 * Train Acc 75.920
 * Val Acc 74.600, Total time 0.59
 * Val loss 0.777, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0086 (0.0086)	0.889 (0.889)	81.25 (81.25)
[10/157]	0.1003 (0.0942)	0.0615 (0.0568)	0.984 (0.818)	75.00 (76.99)
[20/157]	0.1010 (0.0972)	0.0617 (0.0594)	0.839 (0.808)	71.88 (76.04)
[30/157]	0.0997 (0.0984)	0.0601 (0.0601)	1.065 (0.788)	65.62 (76.21)
[40/157]	0.1010 (0.0988)	0.0615 (0.0603)	0.571 (0.792)	81.25 (75.76)
[50/157]	0.1008 (0.0991)	0.0613 (0.0605)	0.933 (0.786)	75.00 (76.16)
[60/157]	0.1001 (0.0992)	0.0607 (0.0605)	0.777 (0.790)	78.12 (76.33)
[70/157]	0.1004 (0.0993)	0.0610 (0.0606)	0.810 (0.781)	71.88 (76.58)
[80/157]	0.0993 (0.0994)	0.0600 (0.0607)	0.676 (0.794)	81.25 (76.27)
[90/157]	0.1001 (0.0994)	0.0603 (0.0607)	0.954 (0.794)	68.75 (76.06)
[100/157]	0.0993 (0.0995)	0.0606 (0.0608)	0.398 (0.793)	90.62 (76.05)
[110/157]	0.1001 (0.0995)	0.0609 (0.0608)	0.774 (0.788)	71.88 (76.01)
[120/157]	0.1000 (0.0996)	0.0609 (0.0608)	0.591 (0.785)	84.38 (76.19)
[130/157]	0.0998 (0.0996)	0.0602 (0.0608)	0.784 (0.790)	75.00 (76.10)
[140/157]	0.1004 (0.0996)	0.0589 (0.0608)	0.626 (0.787)	75.00 (76.40)
[150/157]	0.0989 (0.0996)	0.0588 (0.0608)	1.042 (0.792)	62.50 (76.12)
[156/157]	0.0830 (0.0995)	0.0545 (0.0608)	1.628 (0.790)	75.00 (76.20)
 * Train Acc 76.200
 * Val Acc 74.800, Total time 0.60
 * Val loss 0.764, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0093 (0.0093)	0.874 (0.874)	75.00 (75.00)
[10/157]	0.1023 (0.0950)	0.0611 (0.0562)	0.918 (0.839)	68.75 (72.44)
[20/157]	0.1001 (0.0974)	0.0611 (0.0585)	0.894 (0.828)	65.62 (73.21)
[30/157]	0.1009 (0.0982)	0.0620 (0.0593)	0.878 (0.819)	78.12 (73.89)
[40/157]	0.1005 (0.0986)	0.0613 (0.0598)	0.828 (0.806)	71.88 (74.77)
[50/157]	0.1004 (0.0989)	0.0610 (0.0600)	0.857 (0.823)	75.00 (74.45)
[60/157]	0.0999 (0.0991)	0.0605 (0.0603)	0.768 (0.813)	65.62 (74.90)
[70/157]	0.0998 (0.0992)	0.0606 (0.0604)	1.263 (0.802)	62.50 (75.53)
[80/157]	0.1000 (0.0993)	0.0607 (0.0605)	0.544 (0.795)	84.38 (75.58)
[90/157]	0.1002 (0.0994)	0.0614 (0.0606)	0.603 (0.790)	87.50 (76.00)
[100/157]	0.0996 (0.0994)	0.0602 (0.0607)	0.996 (0.794)	68.75 (75.74)
[110/157]	0.1006 (0.0995)	0.0610 (0.0607)	0.518 (0.790)	90.62 (75.90)
[120/157]	0.1002 (0.0995)	0.0607 (0.0607)	0.671 (0.791)	81.25 (75.83)
[130/157]	0.1013 (0.0996)	0.0622 (0.0608)	0.624 (0.786)	84.38 (76.05)
[140/157]	0.0998 (0.0997)	0.0605 (0.0608)	0.717 (0.783)	75.00 (76.31)
[150/157]	0.1006 (0.0998)	0.0607 (0.0609)	0.634 (0.790)	78.12 (75.91)
[156/157]	0.0840 (0.0996)	0.0569 (0.0608)	0.836 (0.793)	75.00 (75.78)
 * Train Acc 75.780
 * Val Acc 74.500, Total time 0.59
 * Val loss 0.763, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0084 (0.0084)	0.656 (0.656)	81.25 (81.25)
[10/157]	0.1011 (0.0939)	0.0613 (0.0561)	0.826 (0.821)	75.00 (73.30)
[20/157]	0.0999 (0.0968)	0.0605 (0.0585)	0.589 (0.751)	75.00 (76.93)
[30/157]	0.1000 (0.0978)	0.0607 (0.0593)	0.767 (0.790)	75.00 (75.81)
[40/157]	0.0998 (0.0983)	0.0607 (0.0597)	0.621 (0.784)	87.50 (76.22)
[50/157]	0.1013 (0.0986)	0.0616 (0.0600)	0.750 (0.781)	71.88 (76.35)
[60/157]	0.1011 (0.0989)	0.0616 (0.0602)	0.866 (0.780)	78.12 (76.49)
[70/157]	0.0998 (0.0990)	0.0606 (0.0603)	0.684 (0.784)	84.38 (76.63)
[80/157]	0.1008 (0.0992)	0.0599 (0.0604)	0.850 (0.787)	78.12 (76.54)
[90/157]	0.1002 (0.0992)	0.0603 (0.0604)	0.908 (0.789)	62.50 (76.17)
[100/157]	0.0991 (0.0993)	0.0601 (0.0605)	0.992 (0.788)	68.75 (76.36)
[110/157]	0.1001 (0.0994)	0.0601 (0.0605)	1.148 (0.790)	65.62 (76.49)
[120/157]	0.1001 (0.0994)	0.0606 (0.0606)	0.830 (0.790)	78.12 (76.37)
[130/157]	0.1002 (0.0995)	0.0610 (0.0606)	0.948 (0.790)	71.88 (76.43)
[140/157]	0.1005 (0.0995)	0.0614 (0.0606)	0.768 (0.794)	81.25 (76.29)
[150/157]	0.1009 (0.0995)	0.0602 (0.0607)	0.865 (0.793)	68.75 (76.22)
[156/157]	0.0840 (0.0994)	0.0560 (0.0606)	1.008 (0.794)	62.50 (76.10)
 * Train Acc 76.100
 * Val Acc 74.500, Total time 0.60
 * Val loss 0.768, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0089 (0.0089)	0.811 (0.811)	87.50 (87.50)
[10/157]	0.1008 (0.0945)	0.0611 (0.0564)	1.271 (0.778)	68.75 (77.27)
[20/157]	0.0997 (0.0970)	0.0610 (0.0589)	0.768 (0.758)	75.00 (78.12)
[30/157]	0.1014 (0.0980)	0.0621 (0.0599)	0.597 (0.763)	81.25 (77.52)
[40/157]	0.0992 (0.0986)	0.0605 (0.0604)	0.593 (0.755)	87.50 (78.28)
[50/157]	0.0999 (0.0988)	0.0610 (0.0606)	0.655 (0.765)	81.25 (78.43)
[60/157]	0.1026 (0.0990)	0.0632 (0.0608)	0.701 (0.771)	71.88 (77.87)
[70/157]	0.0996 (0.0992)	0.0604 (0.0608)	0.749 (0.780)	75.00 (77.42)
[80/157]	0.1008 (0.0994)	0.0605 (0.0608)	0.780 (0.779)	75.00 (77.31)
[90/157]	0.1004 (0.0994)	0.0609 (0.0608)	0.801 (0.787)	68.75 (76.75)
[100/157]	0.1006 (0.0995)	0.0612 (0.0609)	0.767 (0.791)	75.00 (76.24)
[110/157]	0.1013 (0.0995)	0.0613 (0.0609)	0.750 (0.792)	75.00 (76.10)
[120/157]	0.0998 (0.0995)	0.0603 (0.0609)	0.865 (0.792)	71.88 (76.11)
[130/157]	0.1011 (0.0996)	0.0609 (0.0609)	0.764 (0.794)	68.75 (75.98)
[140/157]	0.1001 (0.0996)	0.0602 (0.0609)	0.541 (0.792)	81.25 (76.17)
[150/157]	0.1001 (0.0996)	0.0615 (0.0610)	0.732 (0.793)	71.88 (76.03)
[156/157]	0.0815 (0.0995)	0.0554 (0.0609)	1.023 (0.793)	62.50 (75.96)
 * Train Acc 75.960
 * Val Acc 74.200, Total time 0.60
 * Val loss 0.772, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0422 (0.0422)	0.0076 (0.0076)	1.296 (1.296)	71.88 (71.88)
[10/157]	0.1000 (0.0939)	0.0605 (0.0561)	0.565 (0.836)	84.38 (74.15)
[20/157]	0.1004 (0.0967)	0.0609 (0.0584)	0.835 (0.846)	65.62 (74.26)
[30/157]	0.0999 (0.0978)	0.0603 (0.0592)	0.741 (0.865)	78.12 (74.29)
[40/157]	0.0951 (0.0983)	0.0554 (0.0595)	0.975 (0.855)	71.88 (74.16)
[50/157]	0.1007 (0.0987)	0.0603 (0.0597)	0.632 (0.844)	81.25 (74.26)
[60/157]	0.1026 (0.0989)	0.0598 (0.0598)	0.457 (0.826)	90.62 (74.90)
[70/157]	0.1007 (0.0991)	0.0602 (0.0599)	0.768 (0.822)	75.00 (74.82)
[80/157]	0.0994 (0.0991)	0.0595 (0.0600)	0.852 (0.823)	71.88 (74.46)
[90/157]	0.0996 (0.0992)	0.0601 (0.0601)	0.534 (0.827)	84.38 (74.35)
[100/157]	0.0997 (0.0993)	0.0601 (0.0602)	0.708 (0.822)	78.12 (74.47)
[110/157]	0.1004 (0.0994)	0.0611 (0.0603)	0.595 (0.814)	81.25 (74.89)
[120/157]	0.0998 (0.0994)	0.0606 (0.0603)	0.695 (0.818)	75.00 (74.59)
[130/157]	0.1005 (0.0994)	0.0614 (0.0604)	0.603 (0.814)	84.38 (74.88)
[140/157]	0.1002 (0.0995)	0.0610 (0.0604)	0.864 (0.813)	68.75 (74.96)
[150/157]	0.1004 (0.0995)	0.0609 (0.0605)	0.579 (0.812)	81.25 (75.12)
[156/157]	0.0823 (0.0994)	0.0551 (0.0605)	0.795 (0.813)	62.50 (74.96)
 * Train Acc 74.960
 * Val Acc 75.200, Total time 0.59
 * Val loss 0.769, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0084 (0.0084)	0.865 (0.865)	62.50 (62.50)
[10/157]	0.1003 (0.0943)	0.0614 (0.0563)	0.678 (0.824)	78.12 (74.15)
[20/157]	0.1003 (0.0970)	0.0609 (0.0587)	0.801 (0.816)	65.62 (73.96)
[30/157]	0.0999 (0.0980)	0.0608 (0.0596)	0.532 (0.798)	81.25 (75.10)
[40/157]	0.0991 (0.0985)	0.0589 (0.0599)	0.744 (0.797)	75.00 (75.69)
[50/157]	0.0997 (0.0988)	0.0599 (0.0602)	1.084 (0.803)	59.38 (74.94)
[60/157]	0.0992 (0.0989)	0.0599 (0.0603)	0.989 (0.796)	65.62 (75.31)
[70/157]	0.0994 (0.0991)	0.0594 (0.0603)	0.637 (0.796)	75.00 (74.91)
[80/157]	0.1006 (0.0992)	0.0616 (0.0604)	0.675 (0.793)	78.12 (75.12)
[90/157]	0.1007 (0.0993)	0.0610 (0.0605)	1.100 (0.804)	65.62 (75.00)
[100/157]	0.0989 (0.0994)	0.0598 (0.0605)	0.740 (0.802)	75.00 (75.28)
[110/157]	0.1001 (0.0994)	0.0609 (0.0606)	0.758 (0.803)	78.12 (75.20)
[120/157]	0.0967 (0.0995)	0.0576 (0.0606)	0.868 (0.797)	71.88 (75.39)
[130/157]	0.0998 (0.0995)	0.0603 (0.0607)	0.675 (0.800)	87.50 (75.45)
[140/157]	0.1007 (0.0995)	0.0603 (0.0606)	1.161 (0.796)	56.25 (75.49)
[150/157]	0.1002 (0.0996)	0.0609 (0.0607)	0.878 (0.795)	68.75 (75.50)
[156/157]	0.0834 (0.0994)	0.0551 (0.0606)	0.977 (0.795)	87.50 (75.50)
 * Train Acc 75.500
 * Val Acc 74.200, Total time 0.59
 * Val loss 0.772, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0088 (0.0088)	1.041 (1.041)	81.25 (81.25)
[10/157]	0.1017 (0.0946)	0.0622 (0.0565)	0.996 (0.769)	75.00 (76.99)
[20/157]	0.1000 (0.0975)	0.0589 (0.0586)	0.877 (0.784)	78.12 (75.74)
[30/157]	0.1006 (0.0983)	0.0600 (0.0592)	0.560 (0.772)	90.62 (76.51)
[40/157]	0.0985 (0.0987)	0.0592 (0.0596)	0.680 (0.777)	81.25 (76.30)
[50/157]	0.0985 (0.0990)	0.0596 (0.0599)	0.811 (0.786)	78.12 (76.23)
[60/157]	0.1007 (0.0991)	0.0612 (0.0601)	0.909 (0.784)	78.12 (76.08)
[70/157]	0.1001 (0.0992)	0.0606 (0.0603)	0.764 (0.793)	81.25 (75.79)
[80/157]	0.0995 (0.0993)	0.0602 (0.0604)	0.796 (0.790)	81.25 (76.08)
[90/157]	0.0998 (0.0994)	0.0607 (0.0605)	0.679 (0.785)	90.62 (76.34)
[100/157]	0.1004 (0.0994)	0.0612 (0.0606)	0.785 (0.796)	75.00 (75.87)
[110/157]	0.1004 (0.0995)	0.0612 (0.0606)	1.050 (0.799)	65.62 (75.68)
[120/157]	0.1002 (0.0995)	0.0605 (0.0607)	0.611 (0.795)	81.25 (75.83)
[130/157]	0.1007 (0.0996)	0.0613 (0.0607)	0.747 (0.796)	78.12 (75.62)
[140/157]	0.1001 (0.0996)	0.0607 (0.0607)	0.672 (0.792)	84.38 (75.91)
[150/157]	0.1000 (0.0996)	0.0603 (0.0607)	0.701 (0.791)	78.12 (76.01)
[156/157]	0.0831 (0.0995)	0.0561 (0.0607)	1.100 (0.794)	62.50 (75.90)
 * Train Acc 75.900
 * Val Acc 74.400, Total time 0.60
 * Val loss 0.765, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 5/27; cond: 435919.46875, radio:3.5767388908425346e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0042,  0.1073, -0.1580],
          [-0.1381, -0.0746,  0.0501],
          [ 0.0051,  0.1573, -0.0150]],

         [[ 0.0548, -0.0551, -0.0376],
          [-0.1811, -0.1278, -0.0789],
          [ 0.0137,  0.0788,  0.1180]],

         [[-0.1300, -0.0833,  0.0677],
          [ 0.1596, -0.0409,  0.1445],
          [-0.0281,  0.0217,  0.1758]]],


        [[[-0.1679, -0.1050, -0.0260],
          [-0.0666,  0.1735, -0.1107],
          [-0.0792, -0.1350, -0.1715]],

         [[-0.1062,  0.1791,  0.1032],
          [ 0.0969,  0.0175, -0.0889],
          [ 0.0345, -0.1830, -0.1367]],

         [[-0.0979,  0.1279,  0.1213],
          [-0.0829, -0.0013,  0.1282],
          [ 0.1913,  0.0733,  0.0267]]],


        [[[ 0.1332, -0.1135,  0.0364],
          [-0.1476, -0.1349, -0.0989],
          [ 0.0869,  0.0754, -0.1128]],

         [[ 0.0591,  0.1011, -0.0280],
          [ 0.0059,  0.0401,  0.1156],
          [ 0.1802, -0.1534, -0.0733]],

         [[ 0.0775,  0.1566,  0.1640],
          [ 0.1684,  0.0345, -0.1717],
          [ 0.0118, -0.1272, -0.1840]]],


        ...,


        [[[ 0.0683, -0.0763, -0.1207],
          [ 0.0761,  0.0336, -0.1409],
          [-0.1322,  0.0944,  0.1445]],

         [[ 0.0183, -0.0976,  0.1003],
          [ 0.0361, -0.1679, -0.1243],
          [ 0.0342,  0.1231, -0.0629]],

         [[ 0.0806, -0.1306,  0.1822],
          [ 0.1275,  0.0268,  0.0758],
          [ 0.0453, -0.1182, -0.0129]]],


        [[[ 0.0626, -0.0405,  0.1924],
          [-0.0631, -0.1656, -0.0866],
          [-0.1705,  0.1684, -0.0840]],

         [[-0.1291, -0.0212, -0.1254],
          [-0.0925,  0.1033, -0.1426],
          [ 0.0977,  0.1513,  0.0007]],

         [[ 0.0848, -0.1124,  0.1497],
          [ 0.1868,  0.0453,  0.1723],
          [-0.1953, -0.0193,  0.0317]]],


        [[[-0.1668,  0.0944, -0.0725],
          [-0.0709, -0.1545,  0.0821],
          [-0.1406, -0.0457,  0.0852]],

         [[ 0.1452,  0.0467, -0.0711],
          [-0.0835,  0.1102,  0.0390],
          [ 0.1596,  0.0672,  0.1517]],

         [[-0.1560,  0.1530, -0.0939],
          [-0.0553, -0.0041, -0.0432],
          [ 0.0336,  0.1605, -0.1260]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.6356e+06, 2.4700e+05, 2.4550e+05, 9.2915e+04, 9.0359e+04, 7.5705e+04,
        1.3692e+04, 9.6284e+03, 3.0444e+03, 2.8960e+03, 2.8639e+03, 2.5690e+03,
        1.0538e+03, 5.7362e+02, 5.5406e+02, 4.9682e+02, 4.5359e+02, 2.9392e+02,
        1.1775e+02, 1.0728e+02, 8.8673e+01, 6.4177e+01, 5.8564e+01, 2.1588e+01,
        1.9943e+01, 1.6389e+01, 6.0462e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 5]) 

NULL SPACE BASIS :  tensor([[-2.0226e-01,  1.2869e-01,  1.2314e-01,  1.2683e-01,  8.2125e-02],
        [-9.0874e-03, -2.2821e-01,  6.5514e-03, -2.3405e-01, -1.4923e-01],
        [ 2.1318e-01,  1.2278e-01, -1.3066e-01,  1.2475e-01,  8.2985e-02],
        [ 3.8838e-01, -4.7508e-04, -2.2750e-01, -2.3258e-01, -1.4694e-01],
        [-6.1967e-04, -1.0733e-02, -3.1609e-03,  4.2555e-01,  2.6526e-01],
        [-3.8926e-01,  1.0911e-02,  2.3097e-01, -2.2842e-01, -1.4848e-01],
        [-2.1574e-01, -1.2754e-01,  1.2813e-01,  1.2682e-01,  8.2332e-02],
        [ 1.0372e-02,  2.3904e-01, -3.3171e-03, -2.3209e-01, -1.4984e-01],
        [ 2.0498e-01, -1.3452e-01, -1.2419e-01,  1.2440e-01,  8.3000e-02],
        [ 1.8125e-02, -2.4968e-01, -2.3863e-01, -1.3400e-02, -1.5184e-01],
        [-6.8846e-04,  4.4077e-01, -1.0432e-02,  1.9813e-02,  2.7581e-01],
        [-1.9809e-02, -2.3687e-01,  2.5079e-01, -1.0620e-02, -1.5251e-01],
        [-2.8765e-02,  1.0887e-02,  4.4098e-01,  1.4977e-02,  2.7051e-01],
        [ 8.0693e-04,  2.2157e-03, -8.4955e-04, -1.5298e-02, -4.8889e-01],
        [ 3.0773e-02, -1.1749e-02, -4.4066e-01,  6.8770e-03,  2.7240e-01],
        [ 1.1125e-02,  2.3694e-01, -2.5017e-01, -4.0248e-03, -1.5053e-01],
        [-8.3548e-04, -4.4256e-01,  1.2247e-02,  2.5296e-04,  2.7436e-01],
        [-1.0676e-02,  2.4995e-01,  2.3665e-01,  1.8359e-03, -1.5164e-01],
        [ 2.0831e-01,  1.4352e-01,  1.3685e-01, -1.2899e-01,  8.2541e-02],
        [ 1.1140e-02, -2.5237e-01,  4.7155e-03,  2.4323e-01, -1.4996e-01],
        [-2.1884e-01,  1.3563e-01, -1.4250e-01, -1.2998e-01,  8.2418e-02],
        [-4.0461e-01, -1.2052e-02, -2.5276e-01,  2.4619e-01, -1.4633e-01],
        [-8.6195e-04,  9.8140e-03,  4.4535e-03, -4.6339e-01,  2.6505e-01],
        [ 4.0404e-01,  8.0925e-04,  2.4863e-01,  2.5098e-01, -1.4698e-01],
        [ 2.3049e-01, -1.3000e-01,  1.4418e-01, -1.3894e-01,  8.0731e-02],
        [-1.0100e-02,  2.4187e-01, -1.0225e-02,  2.6228e-01, -1.4753e-01],
        [-2.1958e-01, -1.3692e-01, -1.3327e-01, -1.4305e-01,  8.1410e-02]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0427, -0.0307, -0.0093, -0.0663,  0.0331,  0.0301,  0.0294, -0.0060,
         -0.0230, -0.0355,  0.0361, -0.0042,  0.0383, -0.0188, -0.0173, -0.0069,
         -0.0146,  0.0229, -0.0073, -0.0070,  0.0154,  0.0306, -0.0157, -0.0140,
         -0.0253,  0.0237, -0.0005],
        [-0.0307,  0.0578, -0.0324,  0.0320, -0.0612,  0.0350, -0.0045,  0.0098,
         -0.0060,  0.0362, -0.0655,  0.0363, -0.0193,  0.0340, -0.0191, -0.0145,
          0.0269, -0.0148, -0.0071,  0.0103, -0.0053, -0.0139,  0.0298, -0.0175,
          0.0219, -0.0423,  0.0240],
        [-0.0093, -0.0324,  0.0447,  0.0319,  0.0331, -0.0683, -0.0249, -0.0042,
          0.0294, -0.0044,  0.0361, -0.0358, -0.0170, -0.0187,  0.0385,  0.0229,
         -0.0149, -0.0066,  0.0156, -0.0051, -0.0093, -0.0162, -0.0158,  0.0326,
          0.0017,  0.0221, -0.0256],
        [-0.0663,  0.0320,  0.0319,  0.1244, -0.0615, -0.0576, -0.0691,  0.0361,
          0.0299,  0.0389, -0.0193, -0.0178, -0.0692,  0.0339,  0.0316,  0.0376,
         -0.0194, -0.0162,  0.0302, -0.0139, -0.0154, -0.0605,  0.0302,  0.0284,
          0.0345, -0.0184, -0.0150],
        [ 0.0331, -0.0612,  0.0331, -0.0615,  0.1125, -0.0614,  0.0344, -0.0631,
          0.0343, -0.0190,  0.0344, -0.0193,  0.0343, -0.0609,  0.0343, -0.0194,
          0.0347, -0.0192, -0.0157,  0.0297, -0.0153,  0.0300, -0.0568,  0.0299,
         -0.0165,  0.0313, -0.0167],
        [ 0.0301,  0.0350, -0.0683, -0.0576, -0.0614,  0.1249,  0.0317,  0.0327,
         -0.0674, -0.0176, -0.0191,  0.0394,  0.0311,  0.0338, -0.0697, -0.0162,
         -0.0190,  0.0374, -0.0137, -0.0176,  0.0318,  0.0288,  0.0304, -0.0605,
         -0.0170, -0.0151,  0.0330],
        [ 0.0294, -0.0045, -0.0249, -0.0691,  0.0344,  0.0317,  0.0457, -0.0335,
         -0.0091, -0.0075, -0.0144,  0.0236,  0.0382, -0.0191, -0.0171, -0.0347,
          0.0361, -0.0051, -0.0247,  0.0219,  0.0009,  0.0338, -0.0167, -0.0160,
         -0.0115, -0.0040,  0.0162],
        [-0.0060,  0.0098, -0.0042,  0.0361, -0.0631,  0.0327, -0.0335,  0.0597,
         -0.0317, -0.0147,  0.0266, -0.0145, -0.0193,  0.0346, -0.0194,  0.0363,
         -0.0657,  0.0363,  0.0240, -0.0421,  0.0217, -0.0185,  0.0314, -0.0146,
         -0.0040,  0.0085, -0.0061],
        [-0.0230, -0.0060,  0.0294,  0.0299,  0.0343, -0.0674, -0.0091, -0.0317,
          0.0438,  0.0236, -0.0147, -0.0077, -0.0169, -0.0190,  0.0385, -0.0052,
          0.0361, -0.0347, -0.0013,  0.0239, -0.0245, -0.0141, -0.0169,  0.0317,
          0.0162, -0.0058, -0.0094],
        [-0.0355,  0.0362, -0.0044,  0.0389, -0.0190, -0.0176, -0.0075, -0.0147,
          0.0236,  0.0639, -0.0670,  0.0099, -0.0670,  0.0331,  0.0300,  0.0106,
          0.0295, -0.0430, -0.0338,  0.0365, -0.0065,  0.0335, -0.0168, -0.0149,
         -0.0037, -0.0176,  0.0231],
        [ 0.0361, -0.0655,  0.0361, -0.0193,  0.0344, -0.0191, -0.0144,  0.0266,
         -0.0147, -0.0670,  0.1211, -0.0668,  0.0336, -0.0600,  0.0334,  0.0293,
         -0.0535,  0.0295,  0.0366, -0.0661,  0.0365, -0.0169,  0.0305, -0.0170,
         -0.0176,  0.0319, -0.0175],
        [-0.0042,  0.0363, -0.0358, -0.0178, -0.0193,  0.0394,  0.0236, -0.0145,
         -0.0077,  0.0099, -0.0668,  0.0638,  0.0300,  0.0331, -0.0671, -0.0430,
          0.0295,  0.0105, -0.0067,  0.0362, -0.0334, -0.0147, -0.0164,  0.0331,
          0.0231, -0.0179, -0.0034],
        [ 0.0383, -0.0193, -0.0170, -0.0692,  0.0343,  0.0311,  0.0382, -0.0193,
         -0.0169, -0.0670,  0.0336,  0.0300,  0.1202, -0.0594, -0.0544, -0.0666,
          0.0335,  0.0297,  0.0341, -0.0170, -0.0155, -0.0608,  0.0299,  0.0277,
          0.0337, -0.0168, -0.0152],
        [-0.0188,  0.0340, -0.0187,  0.0339, -0.0609,  0.0338, -0.0191,  0.0346,
         -0.0190,  0.0331, -0.0600,  0.0331, -0.0594,  0.1070, -0.0594,  0.0333,
         -0.0604,  0.0333, -0.0170,  0.0309, -0.0170,  0.0302, -0.0548,  0.0305,
         -0.0168,  0.0307, -0.0170],
        [-0.0173, -0.0191,  0.0385,  0.0316,  0.0343, -0.0697, -0.0171, -0.0194,
          0.0385,  0.0300,  0.0334, -0.0671, -0.0544, -0.0594,  0.1205,  0.0299,
          0.0333, -0.0666, -0.0152, -0.0170,  0.0340,  0.0272,  0.0299, -0.0606,
         -0.0151, -0.0166,  0.0334],
        [-0.0069, -0.0145,  0.0229,  0.0376, -0.0194, -0.0162, -0.0347,  0.0363,
         -0.0052,  0.0106,  0.0293, -0.0430, -0.0666,  0.0333,  0.0299,  0.0633,
         -0.0667,  0.0102, -0.0044, -0.0176,  0.0239,  0.0344, -0.0165, -0.0163,
         -0.0339,  0.0362, -0.0059],
        [-0.0146,  0.0269, -0.0149, -0.0194,  0.0347, -0.0190,  0.0361, -0.0657,
          0.0361,  0.0295, -0.0535,  0.0295,  0.0335, -0.0604,  0.0333, -0.0667,
          0.1213, -0.0668, -0.0176,  0.0316, -0.0174, -0.0168,  0.0306, -0.0170,
          0.0363, -0.0660,  0.0364],
        [ 0.0229, -0.0148, -0.0066, -0.0162, -0.0192,  0.0374, -0.0051,  0.0363,
         -0.0347, -0.0430,  0.0295,  0.0105,  0.0297,  0.0333, -0.0666,  0.0102,
         -0.0668,  0.0633,  0.0238, -0.0174, -0.0046, -0.0160, -0.0168,  0.0346,
         -0.0060,  0.0362, -0.0340],
        [-0.0073, -0.0071,  0.0156,  0.0302, -0.0157, -0.0137, -0.0247,  0.0240,
         -0.0013, -0.0338,  0.0366, -0.0067,  0.0341, -0.0170, -0.0152, -0.0044,
         -0.0176,  0.0238,  0.0475, -0.0344, -0.0099, -0.0735,  0.0373,  0.0330,
          0.0329, -0.0066, -0.0261],
        [-0.0070,  0.0103, -0.0051, -0.0139,  0.0297, -0.0176,  0.0219, -0.0421,
          0.0239,  0.0365, -0.0661,  0.0362, -0.0170,  0.0309, -0.0170, -0.0176,
          0.0316, -0.0174, -0.0344,  0.0651, -0.0364,  0.0354, -0.0693,  0.0396,
         -0.0044,  0.0111, -0.0069],
        [ 0.0154, -0.0053, -0.0093, -0.0154, -0.0153,  0.0318,  0.0009,  0.0217,
         -0.0245, -0.0065,  0.0365, -0.0334, -0.0155, -0.0170,  0.0340,  0.0239,
         -0.0174, -0.0046, -0.0099, -0.0364,  0.0493,  0.0353,  0.0371, -0.0753,
         -0.0286, -0.0044,  0.0330],
        [ 0.0306, -0.0139, -0.0162, -0.0605,  0.0300,  0.0288,  0.0338, -0.0185,
         -0.0141,  0.0335, -0.0169, -0.0147, -0.0608,  0.0302,  0.0272,  0.0344,
         -0.0168, -0.0160, -0.0735,  0.0354,  0.0353,  0.1385, -0.0688, -0.0640,
         -0.0779,  0.0402,  0.0345],
        [-0.0157,  0.0298, -0.0158,  0.0302, -0.0568,  0.0304, -0.0167,  0.0314,
         -0.0169, -0.0168,  0.0305, -0.0164,  0.0299, -0.0548,  0.0299, -0.0165,
          0.0306, -0.0168,  0.0373, -0.0693,  0.0371, -0.0688,  0.1275, -0.0691,
          0.0380, -0.0708,  0.0385],
        [-0.0140, -0.0175,  0.0326,  0.0284,  0.0299, -0.0605, -0.0160, -0.0146,
          0.0317, -0.0149, -0.0170,  0.0331,  0.0277,  0.0305, -0.0606, -0.0163,
         -0.0170,  0.0346,  0.0330,  0.0396, -0.0753, -0.0640, -0.0691,  0.1385,
          0.0367,  0.0363, -0.0760],
        [-0.0253,  0.0219,  0.0017,  0.0345, -0.0165, -0.0170, -0.0115, -0.0040,
          0.0162, -0.0037, -0.0176,  0.0231,  0.0337, -0.0168, -0.0151, -0.0339,
          0.0363, -0.0060,  0.0329, -0.0044, -0.0286, -0.0779,  0.0380,  0.0367,
          0.0522, -0.0374, -0.0114],
        [ 0.0237, -0.0423,  0.0221, -0.0184,  0.0313, -0.0151, -0.0040,  0.0085,
         -0.0058, -0.0176,  0.0319, -0.0179, -0.0168,  0.0307, -0.0166,  0.0362,
         -0.0660,  0.0362, -0.0066,  0.0111, -0.0044,  0.0402, -0.0708,  0.0363,
         -0.0374,  0.0668, -0.0354],
        [-0.0005,  0.0240, -0.0256, -0.0150, -0.0167,  0.0330,  0.0162, -0.0061,
         -0.0094,  0.0231, -0.0175, -0.0034, -0.0152, -0.0170,  0.0334, -0.0059,
          0.0364, -0.0340, -0.0261, -0.0069,  0.0330,  0.0345,  0.0385, -0.0760,
         -0.0114, -0.0354,  0.0500]], device='cuda:0') 

reserving basis 76/576; cond: 9786888.0, radio:3.589055995689705e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1442e-02, -3.9699e-02, -7.3239e-03],
          [ 3.8360e-02,  2.6495e-02,  3.1708e-02],
          [ 2.2145e-02, -1.7721e-03,  8.7654e-03]],

         [[-1.4047e-02,  3.7914e-02,  1.8429e-02],
          [-3.0050e-02,  3.6756e-02, -3.0560e-03],
          [-6.9214e-03, -1.7013e-02, -2.0454e-02]],

         [[-3.0945e-03, -2.0751e-02, -3.6768e-02],
          [-2.9161e-02, -1.0355e-02,  3.3282e-02],
          [ 2.0518e-02, -3.1933e-02, -9.4233e-03]],

         ...,

         [[-2.5484e-02,  1.1627e-02,  2.1990e-05],
          [ 1.4895e-02, -6.5112e-03, -9.4555e-03],
          [-2.0675e-03,  8.6916e-03, -3.1102e-02]],

         [[-4.1523e-02,  1.3746e-02, -3.4936e-02],
          [-6.7166e-03,  1.2074e-02,  1.1154e-03],
          [ 2.0808e-02,  1.0435e-04,  4.0977e-03]],

         [[-1.5954e-02,  1.9744e-03,  1.1387e-02],
          [ 2.3127e-02, -3.9674e-02, -1.6122e-02],
          [-4.1571e-02, -1.1388e-02,  2.9610e-02]]],


        [[[-4.2295e-02, -4.7123e-02,  1.4101e-02],
          [-2.3749e-02,  2.0021e-02,  2.8244e-03],
          [ 8.1642e-03,  2.3253e-02, -4.2239e-02]],

         [[ 3.2926e-02,  7.9987e-03, -3.0276e-02],
          [ 1.1830e-02, -1.1395e-02, -1.7360e-02],
          [ 3.4426e-03, -1.1702e-02,  4.0707e-02]],

         [[ 2.6483e-02, -1.6386e-02, -4.4874e-02],
          [-1.7273e-02, -1.1009e-02,  1.0072e-02],
          [-1.2268e-02, -2.3757e-03, -6.4153e-03]],

         ...,

         [[-5.4202e-03, -3.8348e-03,  8.7000e-03],
          [ 1.5725e-02, -3.4017e-02,  1.7224e-02],
          [ 3.3129e-02,  7.4635e-03,  3.0870e-02]],

         [[-2.1748e-02,  6.7416e-03, -4.5239e-04],
          [-1.1094e-02,  2.3796e-02, -4.6477e-02],
          [-1.0510e-03, -1.4405e-02, -2.0106e-02]],

         [[-1.4744e-02, -3.2102e-02,  4.2609e-03],
          [-6.8714e-03, -3.1466e-02, -8.1357e-03],
          [-8.7181e-03,  1.5952e-02, -4.1645e-02]]],


        [[[-3.0706e-03,  1.8454e-02,  2.0589e-02],
          [ 8.0253e-03,  1.3019e-02,  4.1732e-02],
          [-1.3451e-02,  2.5660e-02,  3.2367e-02]],

         [[-3.1226e-02,  2.0000e-02,  2.9018e-02],
          [-2.4375e-03,  3.2330e-02,  6.1856e-03],
          [ 3.5409e-02,  6.7337e-03, -2.0952e-02]],

         [[ 3.8734e-02,  1.5243e-02,  3.4345e-02],
          [ 1.8316e-02,  2.5616e-02,  2.0296e-02],
          [ 1.6754e-02, -5.9270e-03,  9.4100e-03]],

         ...,

         [[ 2.1091e-02, -1.6885e-02,  4.4115e-02],
          [-3.2290e-03,  3.8407e-02, -3.4348e-03],
          [ 2.4417e-02,  3.3255e-02, -3.6191e-02]],

         [[-2.0650e-02, -3.6382e-02,  3.6766e-02],
          [ 4.5200e-02, -1.3468e-02, -1.8889e-02],
          [ 1.1999e-02,  3.0961e-02,  1.3907e-02]],

         [[-1.4891e-02, -2.3227e-02,  3.0091e-02],
          [-2.9750e-02, -4.1757e-02, -2.1326e-02],
          [ 3.6296e-03,  5.9261e-03,  6.1331e-03]]],


        ...,


        [[[-4.4297e-02, -1.8221e-02,  9.4923e-03],
          [ 2.4370e-02, -1.7215e-02, -8.6940e-03],
          [-4.2859e-03,  2.1019e-02, -3.4605e-02]],

         [[-4.1661e-02,  2.7714e-02,  5.7683e-03],
          [-1.3590e-02, -1.1886e-02,  8.7876e-03],
          [-1.9311e-02,  2.8396e-02,  1.2227e-02]],

         [[ 2.4057e-02,  3.1997e-02,  9.8119e-04],
          [ 4.7689e-02, -7.7435e-03, -3.3968e-02],
          [ 3.8651e-02, -1.4952e-02, -3.0443e-03]],

         ...,

         [[-2.5741e-02, -5.3301e-02,  2.3093e-02],
          [-2.0503e-02,  2.2681e-02, -4.7223e-02],
          [ 1.6430e-02,  4.0668e-02,  2.7448e-02]],

         [[-4.6399e-02, -5.5646e-02,  1.6252e-02],
          [-1.3056e-02, -1.4861e-02, -6.4318e-03],
          [-2.0938e-02,  9.8233e-03, -9.3547e-03]],

         [[ 3.6319e-02, -2.2392e-02,  4.1185e-02],
          [ 9.5202e-03, -5.7886e-03,  4.7849e-02],
          [ 3.1237e-02,  2.7950e-02,  4.8107e-02]]],


        [[[ 2.5291e-02,  1.4054e-02, -1.3517e-02],
          [-1.3465e-02, -3.1993e-02, -2.0700e-02],
          [ 1.5919e-02, -5.8666e-02, -4.1980e-02]],

         [[-8.1838e-03,  2.0829e-02,  8.9185e-03],
          [ 9.9018e-03,  1.0416e-03, -4.0523e-02],
          [-9.4016e-03,  3.7837e-02,  1.2263e-02]],

         [[-9.3012e-03,  8.4122e-03,  4.0419e-02],
          [ 2.5701e-04,  5.5516e-03,  3.0350e-02],
          [-3.1405e-02,  2.5731e-02,  3.2507e-02]],

         ...,

         [[ 4.0160e-02,  1.9613e-02,  3.8750e-02],
          [-3.5736e-02,  9.9988e-03,  7.7954e-04],
          [-1.2074e-03, -4.1721e-02, -3.9181e-02]],

         [[ 1.6753e-02,  2.3247e-02, -9.1604e-03],
          [ 2.6064e-02,  8.0691e-03,  1.8205e-02],
          [-4.8339e-02, -2.4930e-02, -5.2931e-03]],

         [[ 2.7847e-02,  2.5206e-02, -3.1823e-02],
          [-3.3118e-02, -2.8869e-02, -1.2036e-02],
          [-8.0713e-03, -3.9838e-02,  2.2247e-02]]],


        [[[ 2.7648e-02,  1.0131e-02, -1.6798e-02],
          [-6.0850e-03,  6.5487e-03, -1.2578e-02],
          [-1.8354e-03, -3.5150e-02, -7.6565e-03]],

         [[-4.3863e-02, -2.8570e-02, -2.9446e-02],
          [ 1.0712e-02, -5.6367e-02, -1.2280e-03],
          [ 2.6747e-02, -3.5984e-02, -6.8771e-03]],

         [[-1.7091e-02,  1.3640e-02,  2.2168e-02],
          [ 1.5381e-02, -1.3497e-02,  1.0447e-02],
          [-2.2057e-02,  3.6026e-02,  3.6931e-03]],

         ...,

         [[-2.1289e-02, -9.0968e-03,  1.0026e-02],
          [ 3.9826e-03,  3.7258e-02, -4.2773e-02],
          [-2.2903e-02,  1.5708e-02,  2.4650e-02]],

         [[-3.1402e-02, -3.8686e-02,  5.5387e-03],
          [-1.9615e-02, -1.0675e-02, -4.8775e-02],
          [-2.8331e-02, -5.1245e-02, -3.9130e-02]],

         [[-4.0257e-03, -1.0372e-02,  3.1064e-02],
          [ 3.3933e-02,  4.4959e-03,  3.5803e-02],
          [-2.8812e-02,  4.5877e-02, -3.2614e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.6853e+07, 2.4888e+06, 2.0184e+06, 1.8966e+06, 1.4359e+06, 1.0354e+06,
        9.7028e+05, 9.0275e+05, 7.7074e+05, 5.9977e+05, 5.2505e+05, 4.6920e+05,
        3.9719e+05, 2.9357e+05, 1.8061e+05, 1.5357e+05, 1.4904e+05, 1.3253e+05,
        1.1286e+05, 1.0295e+05, 9.1090e+04, 8.3114e+04, 6.8274e+04, 6.0422e+04,
        5.6890e+04, 5.6069e+04, 5.0474e+04, 4.8713e+04, 4.6861e+04, 4.2841e+04,
        4.1594e+04, 3.5960e+04, 3.4128e+04, 3.2289e+04, 3.0846e+04, 2.8616e+04,
        2.6398e+04, 2.4440e+04, 2.3046e+04, 2.1253e+04, 1.9915e+04, 1.9360e+04,
        1.8398e+04, 1.7963e+04, 1.7187e+04, 1.6276e+04, 1.4827e+04, 1.4512e+04,
        1.3658e+04, 1.3263e+04, 1.2693e+04, 1.2372e+04, 1.1988e+04, 1.1834e+04,
        1.1359e+04, 1.0586e+04, 1.0073e+04, 9.8783e+03, 9.5479e+03, 9.4261e+03,
        9.0761e+03, 8.7412e+03, 8.5562e+03, 8.2251e+03, 8.0888e+03, 8.0245e+03,
        7.8046e+03, 7.6636e+03, 7.4670e+03, 7.1778e+03, 7.0551e+03, 6.7627e+03,
        6.5751e+03, 6.4560e+03, 6.3388e+03, 6.1503e+03, 5.9961e+03, 5.7235e+03,
        5.6580e+03, 5.3996e+03, 5.2947e+03, 5.1394e+03, 5.1004e+03, 5.0772e+03,
        4.8569e+03, 4.8171e+03, 4.6220e+03, 4.4540e+03, 4.4047e+03, 4.3307e+03,
        4.1979e+03, 4.0789e+03, 3.9443e+03, 3.9197e+03, 3.7725e+03, 3.7020e+03,
        3.6228e+03, 3.4878e+03, 3.3978e+03, 3.3269e+03, 3.2754e+03, 3.1880e+03,
        3.1251e+03, 3.0434e+03, 3.0077e+03, 2.8641e+03, 2.8328e+03, 2.8055e+03,
        2.7393e+03, 2.7137e+03, 2.6579e+03, 2.6010e+03, 2.5302e+03, 2.4809e+03,
        2.4542e+03, 2.4204e+03, 2.3794e+03, 2.3208e+03, 2.2587e+03, 2.2191e+03,
        2.2091e+03, 2.1797e+03, 2.1474e+03, 2.0780e+03, 2.0442e+03, 2.0186e+03,
        1.9854e+03, 1.9020e+03, 1.8865e+03, 1.8697e+03, 1.8341e+03, 1.7879e+03,
        1.7764e+03, 1.7624e+03, 1.7493e+03, 1.7195e+03, 1.6705e+03, 1.6480e+03,
        1.6341e+03, 1.6114e+03, 1.5725e+03, 1.5585e+03, 1.5376e+03, 1.5239e+03,
        1.5044e+03, 1.4913e+03, 1.4368e+03, 1.4318e+03, 1.4081e+03, 1.3859e+03,
        1.3791e+03, 1.3496e+03, 1.3393e+03, 1.3199e+03, 1.2964e+03, 1.2700e+03,
        1.2605e+03, 1.2457e+03, 1.2428e+03, 1.2181e+03, 1.1928e+03, 1.1815e+03,
        1.1678e+03, 1.1553e+03, 1.1435e+03, 1.1038e+03, 1.0957e+03, 1.0770e+03,
        1.0688e+03, 1.0623e+03, 1.0314e+03, 1.0194e+03, 1.0000e+03, 9.8627e+02,
        9.8380e+02, 9.7815e+02, 9.5823e+02, 9.4195e+02, 9.3165e+02, 9.2377e+02,
        9.1503e+02, 9.1020e+02, 8.8276e+02, 8.7824e+02, 8.6626e+02, 8.5330e+02,
        8.3285e+02, 8.3270e+02, 8.2281e+02, 8.0973e+02, 7.9769e+02, 7.9092e+02,
        7.8914e+02, 7.7799e+02, 7.6593e+02, 7.6297e+02, 7.5537e+02, 7.4533e+02,
        7.3961e+02, 7.2823e+02, 7.2478e+02, 7.1397e+02, 7.0140e+02, 6.9269e+02,
        6.8591e+02, 6.7926e+02, 6.7247e+02, 6.6711e+02, 6.5831e+02, 6.5702e+02,
        6.4648e+02, 6.3661e+02, 6.3427e+02, 6.3250e+02, 6.1741e+02, 6.1110e+02,
        6.0410e+02, 5.9848e+02, 5.9004e+02, 5.8543e+02, 5.8048e+02, 5.7385e+02,
        5.6773e+02, 5.6125e+02, 5.5780e+02, 5.4896e+02, 5.4572e+02, 5.4082e+02,
        5.3353e+02, 5.2005e+02, 5.1684e+02, 5.1520e+02, 5.1055e+02, 5.0584e+02,
        5.0086e+02, 4.9486e+02, 4.9406e+02, 4.8551e+02, 4.8055e+02, 4.7297e+02,
        4.6995e+02, 4.6751e+02, 4.6305e+02, 4.6049e+02, 4.5394e+02, 4.4805e+02,
        4.4489e+02, 4.4100e+02, 4.3688e+02, 4.3125e+02, 4.2741e+02, 4.2328e+02,
        4.2043e+02, 4.1812e+02, 4.1199e+02, 4.0710e+02, 4.0553e+02, 4.0285e+02,
        3.9798e+02, 3.9603e+02, 3.9420e+02, 3.8563e+02, 3.8251e+02, 3.8094e+02,
        3.7826e+02, 3.7359e+02, 3.6803e+02, 3.6433e+02, 3.6095e+02, 3.5674e+02,
        3.5251e+02, 3.5210e+02, 3.4882e+02, 3.4612e+02, 3.4380e+02, 3.3899e+02,
        3.3831e+02, 3.3704e+02, 3.3158e+02, 3.2594e+02, 3.2519e+02, 3.2063e+02,
        3.1818e+02, 3.1720e+02, 3.1409e+02, 3.1123e+02, 3.0934e+02, 3.0620e+02,
        3.0294e+02, 3.0069e+02, 2.9971e+02, 2.9520e+02, 2.9438e+02, 2.9152e+02,
        2.8863e+02, 2.8535e+02, 2.8478e+02, 2.8139e+02, 2.8014e+02, 2.7788e+02,
        2.7419e+02, 2.7181e+02, 2.6943e+02, 2.6700e+02, 2.6474e+02, 2.6345e+02,
        2.6065e+02, 2.5997e+02, 2.5715e+02, 2.5449e+02, 2.5281e+02, 2.4953e+02,
        2.4893e+02, 2.4757e+02, 2.4382e+02, 2.4250e+02, 2.4171e+02, 2.4037e+02,
        2.3702e+02, 2.3494e+02, 2.3278e+02, 2.3049e+02, 2.2744e+02, 2.2618e+02,
        2.2499e+02, 2.2341e+02, 2.2099e+02, 2.1963e+02, 2.1797e+02, 2.1756e+02,
        2.1518e+02, 2.1362e+02, 2.1293e+02, 2.0961e+02, 2.0846e+02, 2.0640e+02,
        2.0538e+02, 2.0375e+02, 2.0213e+02, 1.9964e+02, 1.9770e+02, 1.9640e+02,
        1.9459e+02, 1.9294e+02, 1.9223e+02, 1.9134e+02, 1.8979e+02, 1.8730e+02,
        1.8534e+02, 1.8460e+02, 1.8369e+02, 1.8271e+02, 1.8174e+02, 1.8100e+02,
        1.7862e+02, 1.7771e+02, 1.7693e+02, 1.7530e+02, 1.7307e+02, 1.7270e+02,
        1.7167e+02, 1.7087e+02, 1.6807e+02, 1.6753e+02, 1.6595e+02, 1.6465e+02,
        1.6289e+02, 1.6248e+02, 1.6100e+02, 1.5958e+02, 1.5882e+02, 1.5835e+02,
        1.5688e+02, 1.5483e+02, 1.5424e+02, 1.5277e+02, 1.5203e+02, 1.4999e+02,
        1.4987e+02, 1.4897e+02, 1.4835e+02, 1.4705e+02, 1.4529e+02, 1.4450e+02,
        1.4405e+02, 1.4256e+02, 1.4234e+02, 1.4141e+02, 1.4024e+02, 1.3967e+02,
        1.3875e+02, 1.3751e+02, 1.3612e+02, 1.3486e+02, 1.3272e+02, 1.3271e+02,
        1.3187e+02, 1.3128e+02, 1.2981e+02, 1.2938e+02, 1.2833e+02, 1.2726e+02,
        1.2687e+02, 1.2552e+02, 1.2525e+02, 1.2434e+02, 1.2293e+02, 1.2211e+02,
        1.2172e+02, 1.2123e+02, 1.1986e+02, 1.1908e+02, 1.1835e+02, 1.1781e+02,
        1.1645e+02, 1.1506e+02, 1.1475e+02, 1.1438e+02, 1.1358e+02, 1.1307e+02,
        1.1261e+02, 1.1128e+02, 1.1044e+02, 1.0960e+02, 1.0916e+02, 1.0738e+02,
        1.0715e+02, 1.0603e+02, 1.0514e+02, 1.0439e+02, 1.0418e+02, 1.0346e+02,
        1.0226e+02, 1.0189e+02, 9.9903e+01, 9.9783e+01, 9.8632e+01, 9.7435e+01,
        9.6730e+01, 9.6535e+01, 9.6045e+01, 9.5363e+01, 9.4422e+01, 9.3939e+01,
        9.3288e+01, 9.2628e+01, 9.1631e+01, 9.1224e+01, 9.0198e+01, 8.9904e+01,
        8.9306e+01, 8.8155e+01, 8.8012e+01, 8.7143e+01, 8.5872e+01, 8.5112e+01,
        8.4688e+01, 8.3585e+01, 8.3207e+01, 8.2272e+01, 8.1969e+01, 8.1554e+01,
        8.1479e+01, 8.0204e+01, 8.0036e+01, 7.9553e+01, 7.8544e+01, 7.7312e+01,
        7.6964e+01, 7.6122e+01, 7.5794e+01, 7.5089e+01, 7.4234e+01, 7.3716e+01,
        7.3057e+01, 7.2522e+01, 7.1755e+01, 7.0942e+01, 7.0800e+01, 6.9918e+01,
        6.9208e+01, 6.8239e+01, 6.7997e+01, 6.7275e+01, 6.6378e+01, 6.5964e+01,
        6.5710e+01, 6.5190e+01, 6.4479e+01, 6.3985e+01, 6.3405e+01, 6.2925e+01,
        6.1740e+01, 6.1626e+01, 6.0222e+01, 6.0102e+01, 5.9242e+01, 5.9200e+01,
        5.8385e+01, 5.8091e+01, 5.7864e+01, 5.7134e+01, 5.6350e+01, 5.6000e+01,
        5.5659e+01, 5.5032e+01, 5.4251e+01, 5.4217e+01, 5.3304e+01, 5.3019e+01,
        5.2590e+01, 5.1868e+01, 5.1508e+01, 5.0598e+01, 4.9684e+01, 4.9457e+01,
        4.9093e+01, 4.8310e+01, 4.7392e+01, 4.6922e+01, 4.6309e+01, 4.6163e+01,
        4.5283e+01, 4.4959e+01, 4.4103e+01, 4.3620e+01, 4.3137e+01, 4.2777e+01,
        4.1311e+01, 4.0913e+01, 4.0539e+01, 3.9048e+01, 3.8320e+01, 3.8041e+01,
        3.7297e+01, 3.6566e+01, 3.6438e+01, 3.5812e+01, 3.5508e+01, 3.4854e+01,
        3.4553e+01, 3.3755e+01, 3.3032e+01, 3.1995e+01, 3.1774e+01, 3.1418e+01,
        3.0533e+01, 3.0018e+01, 2.9507e+01, 2.8391e+01, 2.7677e+01, 2.6727e+01,
        2.6050e+01, 2.5137e+01, 2.4209e+01, 2.2733e+01, 2.2320e+01, 2.1107e+01,
        1.9896e+01, 1.9610e+01, 1.9398e+01, 1.8426e+01, 1.8009e+01, 1.7355e+01,
        1.6897e+01, 1.6747e+01, 1.5285e+01, 1.4629e+01, 1.3203e+01, 1.2353e+01,
        1.1923e+01, 9.3271e+00, 8.4176e+00, 7.9214e+00, 7.2054e+00, 5.8091e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 76]) 

NULL SPACE BASIS :  tensor([[ 0.0069, -0.0307,  0.0697,  ...,  0.0128, -0.0058,  0.0057],
        [ 0.0072,  0.0245, -0.0485,  ..., -0.0039,  0.0040, -0.0057],
        [ 0.0253,  0.0386, -0.0294,  ..., -0.0038, -0.0017,  0.0020],
        ...,
        [-0.0193,  0.0163, -0.0013,  ...,  0.0037, -0.0004, -0.0022],
        [-0.0079,  0.0194,  0.0094,  ..., -0.0018,  0.0008,  0.0017],
        [-0.0168, -0.0221,  0.0095,  ..., -0.0043, -0.0006,  0.0029]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.6247e-03, -6.6816e-03,  9.7376e-04,  ..., -3.8730e-04,
          7.5833e-04, -3.5676e-04],
        [-6.6816e-03,  1.4272e-02, -8.0597e-03,  ...,  1.2437e-03,
         -7.2212e-05, -4.2444e-04],
        [ 9.7376e-04, -8.0597e-03,  8.6335e-03,  ..., -9.8397e-04,
         -1.5042e-04,  6.2052e-04],
        ...,
        [-3.8730e-04,  1.2437e-03, -9.8397e-04,  ...,  1.9803e-03,
         -1.3052e-03, -1.2228e-05],
        [ 7.5833e-04, -7.2212e-05, -1.5042e-04,  ..., -1.3052e-03,
          2.6834e-03, -1.0177e-03],
        [-3.5676e-04, -4.2444e-04,  6.2052e-04,  ..., -1.2228e-05,
         -1.0177e-03,  1.4001e-03]], device='cuda:0') 

reserving basis 161/576; cond: 1116380.25, radio:0.000637961900793016
PARAMETER       :  Parameter containing:
tensor([[[[-0.0142,  0.0038,  0.0140],
          [ 0.0237,  0.0077,  0.0079],
          [-0.0523, -0.0492, -0.0114]],

         [[ 0.0031, -0.0117, -0.0064],
          [ 0.0270, -0.0293,  0.0263],
          [-0.0625, -0.0075, -0.0568]],

         [[-0.0004,  0.0194,  0.0253],
          [-0.0142, -0.0342,  0.0466],
          [-0.0165, -0.0083,  0.0517]],

         ...,

         [[-0.0399,  0.0275,  0.0216],
          [-0.0109,  0.0077, -0.0069],
          [-0.0351,  0.0126,  0.0237]],

         [[-0.0175,  0.0130,  0.0016],
          [-0.0340,  0.0198, -0.0050],
          [ 0.0176, -0.0389,  0.0345]],

         [[ 0.0213, -0.0013,  0.0239],
          [ 0.0104,  0.0111,  0.0408],
          [-0.0298,  0.0204, -0.0060]]],


        [[[ 0.0324,  0.0113, -0.0229],
          [-0.0091,  0.0006,  0.0105],
          [ 0.0037, -0.0466,  0.0090]],

         [[ 0.0345,  0.0247,  0.0224],
          [-0.0203, -0.0088, -0.0240],
          [ 0.0082, -0.0296,  0.0155]],

         [[-0.0308, -0.0145, -0.0278],
          [-0.0038,  0.0133,  0.0100],
          [ 0.0349,  0.0010, -0.0427]],

         ...,

         [[ 0.0167,  0.0328,  0.0061],
          [ 0.0362,  0.0392, -0.0080],
          [-0.0082, -0.0296,  0.0360]],

         [[-0.0053,  0.0127, -0.0180],
          [-0.0053, -0.0206,  0.0152],
          [-0.0405, -0.0232, -0.0345]],

         [[ 0.0349, -0.0180,  0.0097],
          [ 0.0105,  0.0151,  0.0038],
          [ 0.0236, -0.0137,  0.0233]]],


        [[[ 0.0307, -0.0055, -0.0452],
          [ 0.0316,  0.0373,  0.0301],
          [-0.0148, -0.0072,  0.0389]],

         [[ 0.0143,  0.0035,  0.0092],
          [ 0.0190, -0.0289, -0.0028],
          [ 0.0299,  0.0280, -0.0159]],

         [[ 0.0417,  0.0023,  0.0096],
          [ 0.0142, -0.0183,  0.0389],
          [-0.0337, -0.0383,  0.0080]],

         ...,

         [[-0.0004,  0.0307, -0.0124],
          [-0.0161, -0.0449,  0.0061],
          [-0.0102, -0.0253, -0.0253]],

         [[-0.0084,  0.0243, -0.0458],
          [ 0.0329,  0.0238, -0.0224],
          [-0.0272, -0.0245,  0.0114]],

         [[ 0.0379, -0.0035, -0.0089],
          [-0.0256, -0.0260,  0.0094],
          [ 0.0028,  0.0419,  0.0345]]],


        ...,


        [[[ 0.0370, -0.0316, -0.0340],
          [ 0.0363, -0.0353, -0.0469],
          [-0.0150, -0.0218,  0.0141]],

         [[ 0.0383,  0.0163,  0.0284],
          [-0.0416, -0.0429,  0.0035],
          [-0.0478,  0.0247, -0.0390]],

         [[ 0.0126,  0.0452,  0.0053],
          [-0.0448, -0.0421,  0.0210],
          [-0.0214,  0.0177,  0.0275]],

         ...,

         [[-0.0250, -0.0244,  0.0182],
          [ 0.0025,  0.0296,  0.0367],
          [ 0.0343, -0.0486,  0.0120]],

         [[ 0.0007, -0.0231,  0.0334],
          [-0.0380, -0.0060, -0.0028],
          [-0.0081, -0.0446,  0.0063]],

         [[-0.0037, -0.0197, -0.0072],
          [ 0.0308,  0.0480, -0.0027],
          [-0.0026,  0.0312,  0.0076]]],


        [[[-0.0005, -0.0416,  0.0147],
          [-0.0409,  0.0211,  0.0333],
          [ 0.0133,  0.0281, -0.0119]],

         [[ 0.0130, -0.0208, -0.0189],
          [ 0.0070, -0.0446,  0.0018],
          [-0.0218, -0.0203, -0.0052]],

         [[ 0.0268,  0.0253,  0.0152],
          [-0.0049,  0.0059, -0.0201],
          [ 0.0332, -0.0353, -0.0011]],

         ...,

         [[-0.0216,  0.0429,  0.0049],
          [ 0.0250,  0.0238, -0.0308],
          [-0.0042, -0.0389,  0.0145]],

         [[-0.0233, -0.0143, -0.0358],
          [-0.0037,  0.0211,  0.0028],
          [ 0.0098, -0.0312,  0.0331]],

         [[ 0.0104,  0.0434,  0.0124],
          [-0.0029,  0.0224,  0.0180],
          [-0.0127,  0.0050,  0.0226]]],


        [[[ 0.0084, -0.0092, -0.0075],
          [-0.0159,  0.0164, -0.0081],
          [-0.0395, -0.0432,  0.0427]],

         [[-0.0251,  0.0046, -0.0184],
          [-0.0319, -0.0281,  0.0006],
          [-0.0333,  0.0117,  0.0107]],

         [[ 0.0005,  0.0141, -0.0149],
          [ 0.0068, -0.0374, -0.0085],
          [-0.0234,  0.0278, -0.0470]],

         ...,

         [[ 0.0403, -0.0363, -0.0335],
          [-0.0111,  0.0118,  0.0402],
          [-0.0144,  0.0309, -0.0371]],

         [[-0.0124, -0.0310, -0.0107],
          [ 0.0161,  0.0211, -0.0295],
          [ 0.0316, -0.0316,  0.0295]],

         [[-0.0031, -0.0312, -0.0277],
          [-0.0212,  0.0329,  0.0004],
          [-0.0159,  0.0395,  0.0057]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([4.9637e+07, 1.9097e+06, 1.5931e+06, 1.4926e+06, 1.4530e+06, 9.5484e+05,
        7.5611e+05, 6.8752e+05, 6.2153e+05, 4.8766e+05, 4.2229e+05, 4.1534e+05,
        3.7413e+05, 3.3541e+05, 2.7007e+05, 1.9059e+05, 1.7634e+05, 1.6563e+05,
        1.4865e+05, 1.2859e+05, 1.2188e+05, 9.5561e+04, 9.1666e+04, 9.1584e+04,
        8.6723e+04, 7.6419e+04, 7.4261e+04, 6.8063e+04, 6.4984e+04, 5.6508e+04,
        5.3200e+04, 5.0303e+04, 4.9729e+04, 4.5527e+04, 4.2188e+04, 4.0100e+04,
        3.9496e+04, 3.7062e+04, 3.2545e+04, 3.1981e+04, 2.9927e+04, 2.7830e+04,
        2.6735e+04, 2.6188e+04, 2.4329e+04, 2.3844e+04, 2.2769e+04, 2.1979e+04,
        2.0759e+04, 2.0103e+04, 1.9661e+04, 1.9172e+04, 1.8797e+04, 1.8374e+04,
        1.7620e+04, 1.6745e+04, 1.6518e+04, 1.6343e+04, 1.5336e+04, 1.4809e+04,
        1.4392e+04, 1.4294e+04, 1.3776e+04, 1.3353e+04, 1.3047e+04, 1.2615e+04,
        1.2248e+04, 1.1766e+04, 1.1602e+04, 1.1293e+04, 1.1186e+04, 1.0879e+04,
        1.0766e+04, 1.0650e+04, 1.0505e+04, 1.0167e+04, 9.8764e+03, 9.6908e+03,
        9.4830e+03, 9.3019e+03, 9.1759e+03, 8.9246e+03, 8.8333e+03, 8.6153e+03,
        8.3422e+03, 8.3024e+03, 8.1738e+03, 7.9163e+03, 7.8457e+03, 7.7208e+03,
        7.4953e+03, 7.4046e+03, 7.3546e+03, 7.1360e+03, 7.1035e+03, 6.9723e+03,
        6.8099e+03, 6.7250e+03, 6.6733e+03, 6.5559e+03, 6.4854e+03, 6.3363e+03,
        6.2414e+03, 6.1600e+03, 6.0890e+03, 5.9041e+03, 5.8789e+03, 5.7600e+03,
        5.7367e+03, 5.6273e+03, 5.5185e+03, 5.4928e+03, 5.4314e+03, 5.3223e+03,
        5.3043e+03, 5.2152e+03, 5.1227e+03, 5.0249e+03, 4.9384e+03, 4.8617e+03,
        4.8332e+03, 4.7593e+03, 4.6802e+03, 4.6520e+03, 4.5871e+03, 4.5296e+03,
        4.4544e+03, 4.4146e+03, 4.3311e+03, 4.3204e+03, 4.2611e+03, 4.1529e+03,
        4.1333e+03, 4.0853e+03, 4.0110e+03, 3.9860e+03, 3.8876e+03, 3.8678e+03,
        3.8370e+03, 3.7587e+03, 3.7375e+03, 3.6751e+03, 3.6397e+03, 3.6162e+03,
        3.5681e+03, 3.4821e+03, 3.4112e+03, 3.3922e+03, 3.3413e+03, 3.3179e+03,
        3.2966e+03, 3.2523e+03, 3.2287e+03, 3.1539e+03, 3.1431e+03, 3.1093e+03,
        3.0923e+03, 3.0206e+03, 3.0034e+03, 2.9492e+03, 2.9273e+03, 2.9123e+03,
        2.9005e+03, 2.8420e+03, 2.7979e+03, 2.7746e+03, 2.7667e+03, 2.7231e+03,
        2.7007e+03, 2.6664e+03, 2.6423e+03, 2.6181e+03, 2.5993e+03, 2.5770e+03,
        2.5605e+03, 2.5439e+03, 2.5353e+03, 2.4890e+03, 2.4432e+03, 2.4252e+03,
        2.3970e+03, 2.3818e+03, 2.3613e+03, 2.3503e+03, 2.3020e+03, 2.2882e+03,
        2.2727e+03, 2.2431e+03, 2.2370e+03, 2.2105e+03, 2.1983e+03, 2.1789e+03,
        2.1399e+03, 2.1195e+03, 2.1066e+03, 2.0742e+03, 2.0670e+03, 2.0561e+03,
        2.0506e+03, 2.0219e+03, 2.0096e+03, 1.9807e+03, 1.9775e+03, 1.9304e+03,
        1.9086e+03, 1.9064e+03, 1.8824e+03, 1.8690e+03, 1.8618e+03, 1.8527e+03,
        1.8442e+03, 1.8127e+03, 1.8008e+03, 1.7910e+03, 1.7751e+03, 1.7460e+03,
        1.7230e+03, 1.7180e+03, 1.7149e+03, 1.6968e+03, 1.6786e+03, 1.6658e+03,
        1.6584e+03, 1.6382e+03, 1.6295e+03, 1.6137e+03, 1.6052e+03, 1.5919e+03,
        1.5706e+03, 1.5500e+03, 1.5425e+03, 1.5200e+03, 1.5160e+03, 1.5059e+03,
        1.5004e+03, 1.4922e+03, 1.4752e+03, 1.4714e+03, 1.4510e+03, 1.4403e+03,
        1.4295e+03, 1.4204e+03, 1.4099e+03, 1.3973e+03, 1.3942e+03, 1.3645e+03,
        1.3596e+03, 1.3491e+03, 1.3338e+03, 1.3251e+03, 1.3226e+03, 1.3201e+03,
        1.2979e+03, 1.2928e+03, 1.2857e+03, 1.2771e+03, 1.2658e+03, 1.2590e+03,
        1.2541e+03, 1.2419e+03, 1.2274e+03, 1.2199e+03, 1.2068e+03, 1.2021e+03,
        1.1975e+03, 1.1891e+03, 1.1753e+03, 1.1706e+03, 1.1485e+03, 1.1440e+03,
        1.1429e+03, 1.1354e+03, 1.1246e+03, 1.1201e+03, 1.1023e+03, 1.0987e+03,
        1.0926e+03, 1.0846e+03, 1.0762e+03, 1.0680e+03, 1.0546e+03, 1.0513e+03,
        1.0450e+03, 1.0358e+03, 1.0338e+03, 1.0251e+03, 1.0222e+03, 1.0150e+03,
        1.0078e+03, 1.0049e+03, 9.9335e+02, 9.7897e+02, 9.6974e+02, 9.6841e+02,
        9.6346e+02, 9.5693e+02, 9.5388e+02, 9.3744e+02, 9.3614e+02, 9.2698e+02,
        9.2449e+02, 9.1765e+02, 9.1413e+02, 9.0579e+02, 9.0004e+02, 8.9557e+02,
        8.9127e+02, 8.8813e+02, 8.7663e+02, 8.7399e+02, 8.6889e+02, 8.6193e+02,
        8.5642e+02, 8.5142e+02, 8.4417e+02, 8.3822e+02, 8.3049e+02, 8.2346e+02,
        8.1986e+02, 8.1573e+02, 8.1224e+02, 8.0225e+02, 8.0126e+02, 7.9774e+02,
        7.9427e+02, 7.9092e+02, 7.8267e+02, 7.7761e+02, 7.7343e+02, 7.6713e+02,
        7.6533e+02, 7.5323e+02, 7.4987e+02, 7.4769e+02, 7.4444e+02, 7.4245e+02,
        7.3340e+02, 7.2845e+02, 7.2516e+02, 7.1832e+02, 7.1303e+02, 7.0940e+02,
        7.0542e+02, 6.9858e+02, 6.9501e+02, 6.8954e+02, 6.8761e+02, 6.8608e+02,
        6.8149e+02, 6.7594e+02, 6.6990e+02, 6.6669e+02, 6.6390e+02, 6.6141e+02,
        6.5744e+02, 6.5247e+02, 6.4746e+02, 6.4424e+02, 6.3466e+02, 6.2956e+02,
        6.2664e+02, 6.2443e+02, 6.2093e+02, 6.1969e+02, 6.1442e+02, 6.1069e+02,
        6.0840e+02, 6.0557e+02, 6.0020e+02, 5.9519e+02, 5.9330e+02, 5.8869e+02,
        5.8602e+02, 5.8177e+02, 5.7944e+02, 5.7935e+02, 5.7632e+02, 5.7113e+02,
        5.6925e+02, 5.6775e+02, 5.6035e+02, 5.5911e+02, 5.5193e+02, 5.4961e+02,
        5.4644e+02, 5.4138e+02, 5.4057e+02, 5.3551e+02, 5.3318e+02, 5.2696e+02,
        5.2429e+02, 5.1944e+02, 5.1619e+02, 5.1109e+02, 5.0982e+02, 5.0278e+02,
        5.0068e+02, 4.9965e+02, 4.9641e+02, 4.9184e+02, 4.9161e+02, 4.8705e+02,
        4.8460e+02, 4.7989e+02, 4.7912e+02, 4.7538e+02, 4.7235e+02, 4.6839e+02,
        4.6591e+02, 4.6214e+02, 4.6001e+02, 4.5743e+02, 4.5163e+02, 4.5047e+02,
        4.4740e+02, 4.4446e+02, 4.4375e+02, 4.4164e+02, 4.3976e+02, 4.3704e+02,
        4.3350e+02, 4.3337e+02, 4.2882e+02, 4.2590e+02, 4.2473e+02, 4.2260e+02,
        4.1864e+02, 4.1668e+02, 4.1307e+02, 4.1082e+02, 4.0678e+02, 4.0415e+02,
        4.0071e+02, 3.9979e+02, 3.9389e+02, 3.9294e+02, 3.9045e+02, 3.8880e+02,
        3.8232e+02, 3.8052e+02, 3.7842e+02, 3.7705e+02, 3.7587e+02, 3.7309e+02,
        3.7019e+02, 3.6674e+02, 3.6428e+02, 3.6263e+02, 3.5882e+02, 3.5781e+02,
        3.5469e+02, 3.5376e+02, 3.5201e+02, 3.4932e+02, 3.4703e+02, 3.4568e+02,
        3.4516e+02, 3.4050e+02, 3.4023e+02, 3.3578e+02, 3.2955e+02, 3.2904e+02,
        3.2539e+02, 3.2377e+02, 3.2092e+02, 3.2024e+02, 3.1878e+02, 3.1634e+02,
        3.1443e+02, 3.1325e+02, 3.1054e+02, 3.0815e+02, 3.0677e+02, 3.0420e+02,
        2.9992e+02, 2.9910e+02, 2.9708e+02, 2.9506e+02, 2.9450e+02, 2.9262e+02,
        2.9134e+02, 2.8877e+02, 2.8740e+02, 2.8307e+02, 2.8096e+02, 2.7950e+02,
        2.7552e+02, 2.7405e+02, 2.7231e+02, 2.7075e+02, 2.6952e+02, 2.6660e+02,
        2.6480e+02, 2.6034e+02, 2.5847e+02, 2.5766e+02, 2.5613e+02, 2.5322e+02,
        2.4925e+02, 2.4647e+02, 2.4542e+02, 2.4428e+02, 2.4014e+02, 2.3734e+02,
        2.3484e+02, 2.3336e+02, 2.3080e+02, 2.3045e+02, 2.2839e+02, 2.2535e+02,
        2.2439e+02, 2.2231e+02, 2.1941e+02, 2.1864e+02, 2.1848e+02, 2.1708e+02,
        2.1250e+02, 2.0849e+02, 2.0785e+02, 2.0532e+02, 2.0349e+02, 2.0085e+02,
        1.9803e+02, 1.9686e+02, 1.9446e+02, 1.9284e+02, 1.9080e+02, 1.8763e+02,
        1.8712e+02, 1.8493e+02, 1.8353e+02, 1.7747e+02, 1.7493e+02, 1.7275e+02,
        1.7235e+02, 1.6891e+02, 1.6842e+02, 1.6493e+02, 1.6373e+02, 1.6350e+02,
        1.5878e+02, 1.5609e+02, 1.5435e+02, 1.5277e+02, 1.5020e+02, 1.4865e+02,
        1.4592e+02, 1.4395e+02, 1.3981e+02, 1.3865e+02, 1.3726e+02, 1.3448e+02,
        1.3228e+02, 1.2959e+02, 1.2874e+02, 1.2571e+02, 1.2272e+02, 1.2119e+02,
        1.1866e+02, 1.1368e+02, 1.1102e+02, 1.1031e+02, 1.0697e+02, 1.0266e+02,
        9.6547e+01, 9.2666e+01, 9.0500e+01, 8.6528e+01, 8.2889e+01, 8.1413e+01,
        8.0976e+01, 7.4715e+01, 7.2001e+01, 6.9078e+01, 5.8270e+01, 4.4463e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 161]) 

NULL SPACE BASIS :  tensor([[ 0.0060, -0.0270,  0.0553,  ..., -0.0066,  0.0056, -0.0155],
        [ 0.0045,  0.0434, -0.0047,  ..., -0.0032,  0.0033,  0.0296],
        [-0.0464, -0.0199,  0.0156,  ...,  0.0108,  0.0028, -0.0176],
        ...,
        [-0.0429, -0.0597, -0.0047,  ..., -0.0227, -0.2024, -0.0452],
        [-0.0620, -0.0485, -0.0493,  ..., -0.0190,  0.3289,  0.0499],
        [-0.0370, -0.0067, -0.0284,  ...,  0.0366, -0.1517, -0.0193]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0360, -0.0200, -0.0067,  ...,  0.0010,  0.0004,  0.0001],
        [-0.0200,  0.0426, -0.0199,  ..., -0.0003, -0.0001,  0.0002],
        [-0.0067, -0.0199,  0.0347,  ..., -0.0008,  0.0004,  0.0004],
        ...,
        [ 0.0010, -0.0003, -0.0008,  ...,  0.0467, -0.0211, -0.0095],
        [ 0.0004, -0.0001,  0.0004,  ..., -0.0211,  0.0513, -0.0196],
        [ 0.0001,  0.0002,  0.0004,  ..., -0.0095, -0.0196,  0.0446]],
       device='cuda:0') 

reserving basis 229/576; cond: 1044592.1875, radio:0.0008702647173777223
PARAMETER       :  Parameter containing:
tensor([[[[ 1.9033e-02,  3.9921e-02,  3.1765e-02],
          [ 3.4677e-03, -2.7198e-03, -8.8225e-03],
          [-3.4885e-02,  2.8626e-02,  1.6459e-02]],

         [[-4.2707e-02, -1.9440e-02, -2.2048e-02],
          [-4.7611e-02, -4.6118e-02, -2.7455e-02],
          [-2.3504e-02,  5.1583e-03, -5.9178e-03]],

         [[ 1.7413e-02,  2.3994e-02, -5.0555e-02],
          [-1.4601e-02, -2.9966e-02, -1.8841e-02],
          [ 1.1212e-02,  1.0248e-02,  1.9116e-02]],

         ...,

         [[-5.5245e-02,  3.1972e-02,  4.5004e-02],
          [-2.9809e-02, -7.0129e-03, -1.5286e-02],
          [ 2.3749e-02, -2.3224e-02, -3.3525e-02]],

         [[ 1.0871e-02, -2.8662e-02,  3.4875e-02],
          [ 4.0451e-02, -2.5943e-02, -3.3943e-03],
          [-1.8954e-02, -3.2919e-02,  1.9852e-02]],

         [[-2.6731e-02, -2.0638e-02,  5.9575e-03],
          [ 1.6771e-02,  1.4554e-02, -2.3939e-02],
          [-2.5585e-02,  3.7166e-03,  3.9671e-03]]],


        [[[-3.0227e-02, -2.0211e-02, -1.1569e-02],
          [ 3.6540e-02,  2.0870e-02,  2.9213e-02],
          [-1.3929e-02,  4.6035e-02,  1.5373e-03]],

         [[-3.8620e-02, -3.2836e-02, -3.1150e-02],
          [-7.9381e-05, -4.0463e-02, -2.4290e-02],
          [-1.4525e-02,  2.0147e-02,  2.2005e-02]],

         [[-8.9853e-03,  1.5414e-02, -2.3295e-02],
          [-3.5343e-02, -3.0371e-02,  1.4177e-02],
          [ 3.0213e-02,  1.1727e-02, -1.8797e-02]],

         ...,

         [[ 2.4618e-02, -4.5958e-02,  1.7386e-02],
          [-1.2366e-02,  1.2145e-02, -2.0411e-02],
          [ 2.6859e-02,  2.5334e-02, -7.1179e-03]],

         [[ 1.1955e-02, -4.0453e-02, -7.6009e-03],
          [-1.3566e-02, -1.8839e-02, -4.1032e-02],
          [-4.0557e-03,  5.3573e-03, -4.7472e-04]],

         [[-1.0977e-03, -2.7181e-02, -1.0826e-02],
          [-3.1033e-02, -2.7866e-02, -1.5821e-02],
          [ 3.4259e-02,  2.1335e-02, -1.7886e-02]]],


        [[[-4.0501e-02, -8.9662e-03, -2.1044e-03],
          [-5.7665e-02, -4.2825e-02, -3.4594e-02],
          [-1.3440e-03,  3.5098e-02,  3.8340e-02]],

         [[-1.8503e-02, -9.3823e-03, -3.9783e-03],
          [-2.3481e-02, -4.0522e-02, -3.9058e-02],
          [-1.7375e-02, -1.7779e-02, -9.7153e-03]],

         [[ 8.1277e-03, -1.4545e-02,  5.0697e-03],
          [-3.5708e-02,  5.0520e-02, -8.5814e-03],
          [-5.8539e-03,  2.6690e-02,  3.7825e-02]],

         ...,

         [[-4.5108e-02,  2.3032e-02,  3.6758e-02],
          [-1.8671e-02, -4.0402e-02,  3.3559e-02],
          [-1.1653e-02, -2.9648e-02,  1.3162e-02]],

         [[ 2.6934e-02,  1.8387e-02,  4.0169e-02],
          [ 2.3167e-02, -3.0135e-02, -2.2086e-02],
          [-3.4302e-02, -4.9862e-02,  1.9723e-02]],

         [[-2.9118e-02,  5.0130e-02,  3.2271e-02],
          [ 2.0163e-02,  8.5773e-03, -1.1762e-02],
          [-1.5162e-02, -7.5275e-04, -3.7755e-02]]],


        ...,


        [[[ 3.4644e-02,  5.3747e-03, -6.4272e-03],
          [ 3.9219e-02,  4.4826e-02,  1.7079e-03],
          [-1.5317e-02,  1.2164e-02, -3.7992e-02]],

         [[-3.1174e-02,  1.6849e-03, -2.8545e-02],
          [-1.0276e-02,  3.4148e-02,  1.4081e-02],
          [-4.6548e-03, -1.7615e-02,  2.0995e-02]],

         [[-4.2059e-02, -4.1017e-02, -3.6233e-02],
          [-2.7423e-02,  3.5792e-02,  4.2819e-02],
          [-1.1149e-02, -3.4135e-02, -2.7437e-02]],

         ...,

         [[ 3.3882e-02, -6.3907e-03,  3.3727e-02],
          [-1.1729e-02,  4.3104e-02, -1.2377e-02],
          [-3.9271e-02,  2.6076e-02, -2.3786e-02]],

         [[-1.8337e-02, -4.2287e-02, -1.4326e-02],
          [-1.1453e-02,  1.8700e-02, -3.5098e-02],
          [ 2.4366e-02,  2.2526e-02,  1.3419e-02]],

         [[ 2.1733e-02, -1.5779e-02,  1.0258e-02],
          [ 3.3001e-02, -3.9058e-03, -2.6507e-02],
          [-4.0509e-02,  1.4374e-02, -1.1496e-02]]],


        [[[-3.7977e-02,  2.4256e-02, -4.7702e-02],
          [ 3.5033e-03, -4.1978e-02,  2.0255e-02],
          [-1.6538e-02, -7.0169e-03,  3.6896e-02]],

         [[ 1.8338e-02, -3.0480e-02,  7.2049e-03],
          [-7.8614e-03,  2.7391e-02,  5.6840e-03],
          [ 2.4096e-02, -1.1738e-02,  1.6696e-02]],

         [[-3.4391e-02, -4.1529e-02, -2.4321e-02],
          [-2.4189e-02, -1.7355e-02,  1.7950e-02],
          [ 7.8008e-03, -3.3608e-02, -3.7288e-02]],

         ...,

         [[-9.1215e-03, -1.6405e-02, -3.0646e-02],
          [-2.0532e-02,  2.8520e-02, -2.3138e-02],
          [-4.0658e-02,  3.4322e-02,  3.1481e-02]],

         [[-3.0096e-02, -1.0690e-02, -2.9172e-02],
          [ 3.6852e-02,  2.5742e-03, -1.6434e-02],
          [ 2.9757e-02,  3.2318e-02, -2.0585e-02]],

         [[ 1.4528e-02, -2.6609e-02, -7.1984e-03],
          [-4.0115e-02, -3.4508e-02,  3.8693e-02],
          [ 3.8741e-02,  2.2520e-02,  3.8717e-02]]],


        [[[-3.8609e-02,  3.2151e-02, -4.2423e-03],
          [-3.3767e-02,  3.4913e-02, -7.6448e-03],
          [-3.7926e-02, -2.2937e-02,  3.0225e-02]],

         [[-3.4416e-03, -2.5166e-02,  8.8817e-03],
          [ 1.8175e-02,  2.0508e-03, -9.4386e-03],
          [-1.4611e-02, -2.5456e-03,  3.1417e-02]],

         [[-2.2426e-02, -3.7395e-02, -6.0483e-03],
          [-1.4674e-03, -1.3775e-03,  2.6762e-03],
          [ 3.5015e-02, -2.6613e-02,  2.7634e-03]],

         ...,

         [[ 1.2518e-02, -1.6590e-02, -1.9661e-02],
          [-2.0260e-02,  3.0780e-02, -4.4386e-02],
          [-2.7139e-05,  1.5564e-02, -2.9919e-02]],

         [[ 3.5640e-02, -6.5380e-03,  3.1672e-02],
          [ 4.9749e-03, -2.0872e-02, -3.8418e-02],
          [-2.0558e-02,  8.4023e-03, -3.6308e-02]],

         [[ 1.0209e-02,  4.1290e-02, -1.7725e-02],
          [-3.1672e-02,  4.1831e-02,  3.6279e-02],
          [ 2.6939e-02,  1.5439e-02,  7.3830e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.1236e+07, 1.9466e+06, 1.7079e+06, 1.4177e+06, 1.1776e+06, 8.2997e+05,
        7.6363e+05, 6.1465e+05, 5.6692e+05, 4.4962e+05, 4.1179e+05, 3.7500e+05,
        2.9873e+05, 2.3663e+05, 1.2677e+05, 1.1210e+05, 1.0594e+05, 9.8909e+04,
        9.6872e+04, 8.6044e+04, 8.2945e+04, 7.2864e+04, 6.7644e+04, 6.2942e+04,
        5.9649e+04, 5.3700e+04, 5.2127e+04, 4.8651e+04, 4.7782e+04, 4.6140e+04,
        4.2183e+04, 3.8818e+04, 3.8474e+04, 3.6861e+04, 3.3820e+04, 3.1133e+04,
        3.0018e+04, 2.8869e+04, 2.8403e+04, 2.6975e+04, 2.6617e+04, 2.5631e+04,
        2.5257e+04, 2.3931e+04, 2.3324e+04, 2.1815e+04, 2.1179e+04, 2.0512e+04,
        1.9808e+04, 1.9244e+04, 1.7937e+04, 1.7419e+04, 1.6709e+04, 1.5788e+04,
        1.5550e+04, 1.4629e+04, 1.4156e+04, 1.3885e+04, 1.3340e+04, 1.3072e+04,
        1.2189e+04, 1.1854e+04, 1.1802e+04, 1.1537e+04, 1.1258e+04, 1.1086e+04,
        1.0617e+04, 1.0522e+04, 1.0300e+04, 1.0259e+04, 9.9972e+03, 9.4722e+03,
        9.2758e+03, 9.0399e+03, 8.7397e+03, 8.3531e+03, 8.2503e+03, 8.2002e+03,
        8.0658e+03, 7.8027e+03, 7.7609e+03, 7.6177e+03, 7.4510e+03, 7.3367e+03,
        7.0256e+03, 6.9436e+03, 6.8756e+03, 6.6569e+03, 6.6322e+03, 6.5321e+03,
        6.4193e+03, 6.3626e+03, 6.2195e+03, 6.1653e+03, 5.8852e+03, 5.7637e+03,
        5.5997e+03, 5.5850e+03, 5.4598e+03, 5.4033e+03, 5.3032e+03, 5.1964e+03,
        5.0772e+03, 4.9956e+03, 4.9584e+03, 4.9077e+03, 4.7580e+03, 4.6931e+03,
        4.6045e+03, 4.5219e+03, 4.4399e+03, 4.4041e+03, 4.3007e+03, 4.2416e+03,
        4.1916e+03, 4.1291e+03, 4.0973e+03, 3.9943e+03, 3.9296e+03, 3.8495e+03,
        3.8113e+03, 3.7527e+03, 3.6726e+03, 3.6321e+03, 3.5917e+03, 3.5498e+03,
        3.5051e+03, 3.4746e+03, 3.4123e+03, 3.3838e+03, 3.2780e+03, 3.2314e+03,
        3.1992e+03, 3.1321e+03, 3.0700e+03, 3.0590e+03, 3.0220e+03, 2.9257e+03,
        2.8992e+03, 2.8793e+03, 2.8275e+03, 2.7917e+03, 2.7424e+03, 2.7279e+03,
        2.6976e+03, 2.6703e+03, 2.6594e+03, 2.5999e+03, 2.5892e+03, 2.5695e+03,
        2.5026e+03, 2.4652e+03, 2.4316e+03, 2.4052e+03, 2.3983e+03, 2.3525e+03,
        2.3389e+03, 2.3085e+03, 2.2805e+03, 2.2594e+03, 2.2309e+03, 2.1896e+03,
        2.1829e+03, 2.1769e+03, 2.1646e+03, 2.1433e+03, 2.1412e+03, 2.0890e+03,
        2.0671e+03, 2.0384e+03, 2.0250e+03, 2.0147e+03, 1.9859e+03, 1.9729e+03,
        1.9421e+03, 1.9244e+03, 1.9085e+03, 1.8935e+03, 1.8707e+03, 1.8483e+03,
        1.8301e+03, 1.8102e+03, 1.7851e+03, 1.7721e+03, 1.7426e+03, 1.7243e+03,
        1.7119e+03, 1.7089e+03, 1.6993e+03, 1.6734e+03, 1.6620e+03, 1.6530e+03,
        1.6234e+03, 1.6162e+03, 1.5787e+03, 1.5591e+03, 1.5507e+03, 1.5279e+03,
        1.5156e+03, 1.4982e+03, 1.4903e+03, 1.4643e+03, 1.4568e+03, 1.4366e+03,
        1.4237e+03, 1.4128e+03, 1.4050e+03, 1.3948e+03, 1.3865e+03, 1.3778e+03,
        1.3569e+03, 1.3455e+03, 1.3335e+03, 1.3277e+03, 1.3154e+03, 1.2975e+03,
        1.2942e+03, 1.2919e+03, 1.2765e+03, 1.2640e+03, 1.2512e+03, 1.2430e+03,
        1.2333e+03, 1.2280e+03, 1.1999e+03, 1.1917e+03, 1.1801e+03, 1.1741e+03,
        1.1676e+03, 1.1646e+03, 1.1485e+03, 1.1407e+03, 1.1316e+03, 1.1252e+03,
        1.1135e+03, 1.1052e+03, 1.0952e+03, 1.0897e+03, 1.0788e+03, 1.0757e+03,
        1.0656e+03, 1.0601e+03, 1.0381e+03, 1.0297e+03, 1.0287e+03, 1.0121e+03,
        1.0095e+03, 1.0010e+03, 9.8696e+02, 9.8356e+02, 9.6909e+02, 9.6326e+02,
        9.6264e+02, 9.5021e+02, 9.4427e+02, 9.4147e+02, 9.3490e+02, 9.1650e+02,
        9.1557e+02, 9.0353e+02, 9.0137e+02, 8.9670e+02, 8.8573e+02, 8.8325e+02,
        8.7249e+02, 8.6865e+02, 8.5524e+02, 8.5284e+02, 8.4782e+02, 8.4544e+02,
        8.3600e+02, 8.3107e+02, 8.2165e+02, 8.1697e+02, 8.1452e+02, 8.0586e+02,
        8.0176e+02, 7.9213e+02, 7.8713e+02, 7.7804e+02, 7.7412e+02, 7.7248e+02,
        7.6865e+02, 7.6391e+02, 7.6234e+02, 7.5388e+02, 7.4658e+02, 7.4373e+02,
        7.3792e+02, 7.3226e+02, 7.2840e+02, 7.2513e+02, 7.1643e+02, 7.0838e+02,
        7.0483e+02, 7.0306e+02, 6.9804e+02, 6.9249e+02, 6.8713e+02, 6.8002e+02,
        6.7659e+02, 6.7222e+02, 6.6830e+02, 6.6227e+02, 6.5589e+02, 6.5365e+02,
        6.5059e+02, 6.4293e+02, 6.3673e+02, 6.3366e+02, 6.3031e+02, 6.2124e+02,
        6.1750e+02, 6.1591e+02, 6.1556e+02, 6.0958e+02, 6.0493e+02, 5.9790e+02,
        5.9784e+02, 5.9253e+02, 5.8393e+02, 5.7852e+02, 5.7355e+02, 5.7102e+02,
        5.6380e+02, 5.5735e+02, 5.5619e+02, 5.5526e+02, 5.4902e+02, 5.4848e+02,
        5.4397e+02, 5.3972e+02, 5.3836e+02, 5.3280e+02, 5.2968e+02, 5.2615e+02,
        5.2299e+02, 5.1936e+02, 5.1458e+02, 5.0811e+02, 5.0679e+02, 5.0373e+02,
        4.9993e+02, 4.9719e+02, 4.9514e+02, 4.9327e+02, 4.9281e+02, 4.8483e+02,
        4.8000e+02, 4.7753e+02, 4.7567e+02, 4.7333e+02, 4.7052e+02, 4.6882e+02,
        4.6450e+02, 4.6172e+02, 4.5993e+02, 4.5759e+02, 4.5548e+02, 4.5248e+02,
        4.4603e+02, 4.4349e+02, 4.4101e+02, 4.3850e+02, 4.3691e+02, 4.3364e+02,
        4.2832e+02, 4.2603e+02, 4.2072e+02, 4.1704e+02, 4.1459e+02, 4.1187e+02,
        4.1098e+02, 4.1032e+02, 4.0420e+02, 4.0269e+02, 4.0048e+02, 3.9778e+02,
        3.9558e+02, 3.9461e+02, 3.9309e+02, 3.8961e+02, 3.8627e+02, 3.8525e+02,
        3.7964e+02, 3.7880e+02, 3.7566e+02, 3.7379e+02, 3.7029e+02, 3.6859e+02,
        3.6593e+02, 3.6267e+02, 3.6137e+02, 3.5935e+02, 3.5764e+02, 3.5456e+02,
        3.5168e+02, 3.5096e+02, 3.4620e+02, 3.4511e+02, 3.4424e+02, 3.4282e+02,
        3.4006e+02, 3.3937e+02, 3.3644e+02, 3.3376e+02, 3.3106e+02, 3.2935e+02,
        3.2871e+02, 3.2629e+02, 3.2393e+02, 3.2111e+02, 3.2062e+02, 3.1911e+02,
        3.1751e+02, 3.1472e+02, 3.1369e+02, 3.1150e+02, 3.0821e+02, 3.0561e+02,
        3.0389e+02, 2.9983e+02, 2.9728e+02, 2.9508e+02, 2.9378e+02, 2.9137e+02,
        2.9072e+02, 2.8893e+02, 2.8789e+02, 2.8644e+02, 2.8504e+02, 2.8475e+02,
        2.8127e+02, 2.7933e+02, 2.7860e+02, 2.7593e+02, 2.7485e+02, 2.7203e+02,
        2.7032e+02, 2.6749e+02, 2.6734e+02, 2.6467e+02, 2.6413e+02, 2.6135e+02,
        2.6055e+02, 2.5937e+02, 2.5516e+02, 2.5407e+02, 2.5346e+02, 2.5174e+02,
        2.5131e+02, 2.5092e+02, 2.4871e+02, 2.4652e+02, 2.4488e+02, 2.4236e+02,
        2.4114e+02, 2.4023e+02, 2.3758e+02, 2.3533e+02, 2.3367e+02, 2.3312e+02,
        2.2988e+02, 2.2943e+02, 2.2666e+02, 2.2398e+02, 2.2293e+02, 2.2014e+02,
        2.1972e+02, 2.1832e+02, 2.1671e+02, 2.1536e+02, 2.1275e+02, 2.1155e+02,
        2.1108e+02, 2.0973e+02, 2.0795e+02, 2.0689e+02, 2.0477e+02, 2.0326e+02,
        2.0047e+02, 1.9981e+02, 1.9846e+02, 1.9748e+02, 1.9651e+02, 1.9399e+02,
        1.9298e+02, 1.9174e+02, 1.9078e+02, 1.8907e+02, 1.8757e+02, 1.8686e+02,
        1.8350e+02, 1.8281e+02, 1.8184e+02, 1.8088e+02, 1.7937e+02, 1.7724e+02,
        1.7570e+02, 1.7458e+02, 1.7424e+02, 1.7123e+02, 1.7003e+02, 1.6875e+02,
        1.6731e+02, 1.6620e+02, 1.6535e+02, 1.6261e+02, 1.6050e+02, 1.5895e+02,
        1.5826e+02, 1.5731e+02, 1.5545e+02, 1.5467e+02, 1.5357e+02, 1.5267e+02,
        1.5060e+02, 1.4905e+02, 1.4840e+02, 1.4702e+02, 1.4527e+02, 1.4382e+02,
        1.4137e+02, 1.4025e+02, 1.3873e+02, 1.3756e+02, 1.3712e+02, 1.3640e+02,
        1.3606e+02, 1.3481e+02, 1.3297e+02, 1.3141e+02, 1.3046e+02, 1.2945e+02,
        1.2694e+02, 1.2567e+02, 1.2397e+02, 1.2250e+02, 1.2082e+02, 1.2006e+02,
        1.1938e+02, 1.1797e+02, 1.1606e+02, 1.1577e+02, 1.1316e+02, 1.1166e+02,
        1.1148e+02, 1.1101e+02, 1.0979e+02, 1.0900e+02, 1.0780e+02, 1.0625e+02,
        1.0449e+02, 1.0347e+02, 1.0186e+02, 1.0084e+02, 9.8960e+01, 9.7694e+01,
        9.6172e+01, 9.3366e+01, 9.2481e+01, 8.9902e+01, 8.6635e+01, 8.4899e+01,
        8.4575e+01, 8.2122e+01, 7.9002e+01, 7.6653e+01, 7.4430e+01, 7.1912e+01,
        6.9707e+01, 6.8126e+01, 6.6419e+01, 6.3865e+01, 5.0972e+01, 4.9049e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 229]) 

NULL SPACE BASIS :  tensor([[-0.0133, -0.0510, -0.0301,  ...,  0.0014,  0.0166, -0.0124],
        [ 0.0432,  0.0369,  0.0053,  ..., -0.0029, -0.0235,  0.0167],
        [-0.0242, -0.0809,  0.0288,  ...,  0.0026,  0.0073, -0.0062],
        ...,
        [-0.0304,  0.0115,  0.0068,  ...,  0.0004, -0.0086, -0.0126],
        [-0.0490, -0.0174, -0.0585,  ..., -0.0203,  0.0082,  0.0154],
        [-0.0365, -0.0942, -0.0328,  ...,  0.0221, -0.0105, -0.0077]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0223, -0.0167, -0.0011,  ...,  0.0002,  0.0003,  0.0006],
        [-0.0167,  0.0331, -0.0136,  ..., -0.0015,  0.0005, -0.0018],
        [-0.0011, -0.0136,  0.0189,  ...,  0.0007, -0.0012,  0.0006],
        ...,
        [ 0.0002, -0.0015,  0.0007,  ...,  0.0196, -0.0098, -0.0023],
        [ 0.0003,  0.0005, -0.0012,  ..., -0.0098,  0.0259, -0.0101],
        [ 0.0006, -0.0018,  0.0006,  ..., -0.0023, -0.0101,  0.0220]],
       device='cuda:0') 

reserving basis 317/576; cond: 269095.5, radio:0.004356339108198881
PARAMETER       :  Parameter containing:
tensor([[[[-2.7411e-02, -4.4628e-03,  1.9013e-02],
          [-2.4019e-02, -2.7392e-02, -1.2921e-02],
          [ 1.0832e-02,  1.1779e-02,  8.1988e-03]],

         [[ 4.4939e-04,  1.5277e-02,  5.3900e-03],
          [-4.6515e-03,  1.3007e-03,  1.8379e-02],
          [-1.9377e-02, -1.6356e-02, -4.7613e-02]],

         [[ 1.8957e-03,  4.2979e-02,  4.3463e-02],
          [-3.4639e-03, -3.7437e-02, -4.3806e-03],
          [-3.5145e-02, -4.8050e-03, -2.5127e-02]],

         ...,

         [[-1.8258e-02,  4.4323e-02,  3.3440e-03],
          [-2.7330e-02, -9.4700e-03,  4.1360e-02],
          [-1.4109e-02, -3.8762e-02, -1.6865e-02]],

         [[ 2.9103e-02,  1.6191e-02, -4.4864e-02],
          [ 2.8185e-02,  6.1809e-03,  4.6320e-02],
          [-3.3194e-02,  2.1508e-02, -2.1077e-02]],

         [[ 3.5037e-02,  2.2096e-02,  2.1679e-02],
          [-3.6161e-02, -9.4341e-03,  2.5703e-02],
          [ 3.0412e-02, -1.5499e-02, -2.6707e-02]]],


        [[[-2.7290e-02, -2.3716e-02, -4.6378e-02],
          [ 9.1210e-03, -9.5873e-03,  8.3866e-03],
          [-5.2157e-02,  7.1940e-03,  1.1586e-03]],

         [[-3.2590e-02,  1.9034e-02,  5.1873e-02],
          [ 1.8205e-03,  2.2291e-02, -1.2808e-02],
          [-1.3455e-02,  2.9644e-02,  4.6099e-02]],

         [[-1.8535e-03, -1.6932e-02,  3.0475e-02],
          [-1.2691e-03, -3.1385e-02,  3.6265e-02],
          [ 3.4243e-02,  3.3233e-02, -2.8610e-03]],

         ...,

         [[-7.1694e-03,  2.1001e-02, -2.2315e-02],
          [-2.7674e-02,  2.2314e-02,  1.4318e-02],
          [-3.9074e-02,  2.8091e-02, -4.3904e-02]],

         [[-9.0837e-03,  2.1905e-02, -2.1700e-02],
          [-1.9145e-02, -2.9892e-02, -3.6172e-02],
          [-4.5107e-02, -1.0412e-02, -3.8485e-02]],

         [[ 2.2824e-02, -3.7906e-02, -3.5030e-02],
          [-2.0624e-02, -3.4316e-02, -1.6361e-02],
          [-2.3364e-02,  1.1597e-03,  9.7535e-03]]],


        [[[-5.7373e-03, -5.6744e-03, -1.6684e-02],
          [-1.8898e-04, -4.4180e-02, -3.8945e-02],
          [-4.1230e-02, -1.8348e-03, -1.3703e-02]],

         [[-2.5178e-02, -2.2526e-02, -1.9577e-02],
          [ 5.6527e-05, -2.7243e-02, -1.2531e-02],
          [ 1.6384e-02, -2.5977e-02,  1.0326e-03]],

         [[ 4.7991e-02,  4.1414e-02,  2.5954e-03],
          [ 2.8826e-02,  4.1722e-03, -3.9617e-02],
          [ 1.1643e-02,  8.0554e-03, -1.0899e-03]],

         ...,

         [[ 1.3642e-02, -1.3527e-02, -2.7174e-03],
          [ 1.5230e-02,  1.7076e-02, -4.0413e-02],
          [-3.4958e-02,  9.8294e-03, -1.2781e-02]],

         [[ 1.1528e-02,  5.9817e-03, -2.5523e-02],
          [-2.7884e-02,  9.3198e-03,  2.5907e-02],
          [ 4.6422e-03, -2.1872e-02,  8.8684e-04]],

         [[ 1.8198e-02, -1.1941e-02, -3.1753e-02],
          [ 1.1371e-02, -3.3693e-02, -3.3664e-02],
          [ 3.1380e-02, -1.5502e-02,  9.6022e-04]]],


        ...,


        [[[ 1.2804e-02,  3.2383e-02,  1.4937e-02],
          [-1.3077e-03, -1.6023e-02, -1.8770e-02],
          [ 4.9140e-02,  2.3313e-02, -5.4439e-02]],

         [[ 4.3248e-03, -2.6245e-02,  2.9878e-03],
          [-3.1040e-02,  2.6048e-02, -2.0677e-02],
          [ 1.7465e-02, -2.4907e-02,  4.4053e-02]],

         [[ 2.5087e-02, -2.0067e-02,  4.3115e-02],
          [ 3.2611e-02, -3.9192e-02, -3.9397e-02],
          [ 1.5875e-02, -1.2306e-02, -3.7874e-03]],

         ...,

         [[-1.6268e-02, -1.4001e-02,  3.3155e-03],
          [ 1.3513e-02,  8.8425e-03,  1.3642e-02],
          [ 2.5972e-03, -1.0780e-02,  3.1333e-03]],

         [[ 1.4632e-02,  2.1648e-02, -4.6750e-02],
          [-3.0931e-02, -1.9901e-02, -3.2023e-03],
          [ 9.5188e-03,  8.9424e-03,  1.4870e-02]],

         [[ 3.0729e-02,  6.1785e-03,  2.9550e-02],
          [ 1.7090e-02,  4.3513e-02,  2.6548e-03],
          [ 3.9297e-02, -1.0513e-02,  1.9866e-02]]],


        [[[-1.2149e-02,  1.9886e-02,  3.5044e-02],
          [ 1.6109e-02,  2.4272e-02,  7.5036e-03],
          [-3.6220e-03,  2.7998e-02, -5.1563e-02]],

         [[ 3.5048e-02, -1.4504e-02, -2.6832e-02],
          [-3.7375e-02,  6.0022e-03, -3.5405e-02],
          [-3.9465e-02, -4.0590e-03,  4.0596e-02]],

         [[ 1.9898e-02, -4.2177e-02, -2.3736e-02],
          [-3.5070e-02,  2.7850e-02, -1.6506e-02],
          [-3.5401e-02,  1.4344e-02, -4.3011e-03]],

         ...,

         [[-1.7549e-02, -2.8282e-02,  1.4640e-02],
          [ 1.2887e-02,  3.7775e-02,  8.9215e-03],
          [-2.0551e-02, -3.4172e-02, -5.8641e-03]],

         [[-2.0609e-02,  3.7531e-02, -2.4493e-02],
          [ 6.0518e-03, -1.5344e-02,  3.8538e-02],
          [ 4.6002e-02, -1.7134e-03, -1.0341e-02]],

         [[-2.2731e-02,  5.9724e-03,  2.9239e-02],
          [-3.2833e-02, -2.2465e-02,  9.9156e-03],
          [ 1.0969e-02,  1.0394e-02, -3.1725e-03]]],


        [[[ 3.8404e-02,  4.3074e-02,  4.8460e-04],
          [ 2.0495e-02, -1.2166e-02,  2.4868e-02],
          [ 6.4394e-03, -3.5337e-02, -5.0101e-02]],

         [[ 4.5839e-02, -5.0422e-03,  3.2256e-02],
          [ 1.9461e-02,  3.4560e-02,  9.1595e-03],
          [-3.1724e-04, -8.3642e-03,  3.7600e-02]],

         [[-2.3502e-02,  8.3000e-04, -4.1950e-02],
          [-2.8397e-03, -8.6597e-03,  4.0093e-02],
          [ 2.1220e-02, -1.7941e-02,  2.7312e-02]],

         ...,

         [[-8.0662e-03,  3.8296e-02, -3.3634e-02],
          [-9.0112e-03, -3.7654e-02,  2.7767e-02],
          [-3.7778e-02,  6.5787e-03,  4.3284e-04]],

         [[ 3.2347e-03, -3.5656e-02, -1.6029e-02],
          [-3.6413e-02,  2.6083e-02, -2.1496e-02],
          [-2.3614e-02, -3.9805e-02,  7.2714e-03]],

         [[ 4.1792e-02, -5.3387e-03,  3.5140e-02],
          [ 3.4361e-02, -1.4445e-02,  2.4215e-02],
          [ 5.0838e-04,  2.5768e-02,  2.3955e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([6.0683e+07, 1.6518e+06, 1.4283e+06, 1.2453e+06, 1.0971e+06, 7.5179e+05,
        7.2821e+05, 6.1662e+05, 5.6310e+05, 4.9921e+05, 4.5146e+05, 4.1141e+05,
        3.6534e+05, 2.8187e+05, 2.2866e+05, 1.9003e+05, 1.7052e+05, 1.4346e+05,
        1.3223e+05, 1.2283e+05, 1.1710e+05, 1.1206e+05, 1.0872e+05, 9.7120e+04,
        9.1939e+04, 8.7178e+04, 8.2177e+04, 7.7684e+04, 7.3898e+04, 6.7028e+04,
        6.3917e+04, 6.1932e+04, 5.7764e+04, 5.5639e+04, 4.9554e+04, 4.7865e+04,
        4.6114e+04, 4.4045e+04, 4.1638e+04, 4.0874e+04, 4.0189e+04, 3.7892e+04,
        3.7160e+04, 3.3808e+04, 3.3132e+04, 3.1883e+04, 3.0430e+04, 2.9884e+04,
        2.9139e+04, 2.8327e+04, 2.7545e+04, 2.6594e+04, 2.4704e+04, 2.4441e+04,
        2.3362e+04, 2.3155e+04, 2.3099e+04, 2.2540e+04, 2.1888e+04, 2.1061e+04,
        2.0944e+04, 2.0446e+04, 1.9927e+04, 1.8735e+04, 1.8408e+04, 1.8031e+04,
        1.7363e+04, 1.7269e+04, 1.7034e+04, 1.6507e+04, 1.6357e+04, 1.5889e+04,
        1.5670e+04, 1.5590e+04, 1.4925e+04, 1.4825e+04, 1.4648e+04, 1.4330e+04,
        1.3974e+04, 1.3626e+04, 1.3446e+04, 1.3213e+04, 1.2886e+04, 1.2702e+04,
        1.2584e+04, 1.2379e+04, 1.1989e+04, 1.1851e+04, 1.1571e+04, 1.1531e+04,
        1.1335e+04, 1.1010e+04, 1.0847e+04, 1.0681e+04, 1.0262e+04, 1.0146e+04,
        1.0122e+04, 1.0021e+04, 9.6630e+03, 9.6252e+03, 9.3972e+03, 9.3277e+03,
        9.2238e+03, 9.1752e+03, 9.0741e+03, 9.0092e+03, 8.7834e+03, 8.7562e+03,
        8.5821e+03, 8.5420e+03, 8.2158e+03, 8.1457e+03, 8.0051e+03, 7.8640e+03,
        7.7994e+03, 7.7275e+03, 7.6124e+03, 7.5385e+03, 7.5127e+03, 7.4506e+03,
        7.3951e+03, 7.2632e+03, 7.2169e+03, 7.1260e+03, 7.0238e+03, 6.9662e+03,
        6.8705e+03, 6.7791e+03, 6.7073e+03, 6.6229e+03, 6.4452e+03, 6.4208e+03,
        6.3760e+03, 6.2807e+03, 6.2557e+03, 6.2030e+03, 6.1068e+03, 5.9789e+03,
        5.9594e+03, 5.9122e+03, 5.8381e+03, 5.7638e+03, 5.7373e+03, 5.7111e+03,
        5.6092e+03, 5.5456e+03, 5.4860e+03, 5.4516e+03, 5.3764e+03, 5.3336e+03,
        5.3041e+03, 5.1942e+03, 5.1469e+03, 5.0714e+03, 5.0440e+03, 5.0120e+03,
        4.9423e+03, 4.8730e+03, 4.8386e+03, 4.7966e+03, 4.7329e+03, 4.7083e+03,
        4.6778e+03, 4.6515e+03, 4.6089e+03, 4.5560e+03, 4.5272e+03, 4.5040e+03,
        4.4625e+03, 4.4094e+03, 4.3869e+03, 4.3601e+03, 4.3071e+03, 4.2952e+03,
        4.2552e+03, 4.2315e+03, 4.1996e+03, 4.1737e+03, 4.1270e+03, 4.0988e+03,
        4.0832e+03, 4.0209e+03, 4.0062e+03, 3.9697e+03, 3.9542e+03, 3.9216e+03,
        3.8536e+03, 3.8478e+03, 3.8081e+03, 3.7828e+03, 3.7299e+03, 3.7025e+03,
        3.6653e+03, 3.6298e+03, 3.6251e+03, 3.6005e+03, 3.5356e+03, 3.5129e+03,
        3.4891e+03, 3.4762e+03, 3.4389e+03, 3.4128e+03, 3.3875e+03, 3.3867e+03,
        3.3564e+03, 3.3501e+03, 3.3008e+03, 3.2780e+03, 3.2650e+03, 3.2101e+03,
        3.2008e+03, 3.1623e+03, 3.1281e+03, 3.1055e+03, 3.0803e+03, 3.0498e+03,
        3.0364e+03, 3.0191e+03, 3.0085e+03, 2.9922e+03, 2.9567e+03, 2.9285e+03,
        2.9088e+03, 2.8757e+03, 2.8665e+03, 2.8476e+03, 2.8264e+03, 2.8078e+03,
        2.7746e+03, 2.7574e+03, 2.7422e+03, 2.7309e+03, 2.7073e+03, 2.6812e+03,
        2.6776e+03, 2.6474e+03, 2.6362e+03, 2.6135e+03, 2.5963e+03, 2.5839e+03,
        2.5657e+03, 2.5519e+03, 2.5432e+03, 2.5263e+03, 2.5046e+03, 2.4709e+03,
        2.4609e+03, 2.4553e+03, 2.4403e+03, 2.4229e+03, 2.4060e+03, 2.3802e+03,
        2.3662e+03, 2.3553e+03, 2.3539e+03, 2.3420e+03, 2.3132e+03, 2.2913e+03,
        2.2733e+03, 2.2537e+03, 2.2494e+03, 2.2349e+03, 2.2156e+03, 2.2142e+03,
        2.2010e+03, 2.1841e+03, 2.1625e+03, 2.1486e+03, 2.1445e+03, 2.1281e+03,
        2.1040e+03, 2.0868e+03, 2.0761e+03, 2.0660e+03, 2.0591e+03, 2.0470e+03,
        2.0312e+03, 2.0278e+03, 2.0168e+03, 1.9934e+03, 1.9843e+03, 1.9703e+03,
        1.9545e+03, 1.9434e+03, 1.9309e+03, 1.9223e+03, 1.9075e+03, 1.9050e+03,
        1.8891e+03, 1.8860e+03, 1.8672e+03, 1.8545e+03, 1.8484e+03, 1.8415e+03,
        1.8292e+03, 1.8233e+03, 1.8053e+03, 1.8023e+03, 1.7861e+03, 1.7702e+03,
        1.7598e+03, 1.7481e+03, 1.7328e+03, 1.7251e+03, 1.7162e+03, 1.7051e+03,
        1.6986e+03, 1.6921e+03, 1.6820e+03, 1.6654e+03, 1.6569e+03, 1.6444e+03,
        1.6398e+03, 1.6371e+03, 1.6154e+03, 1.6054e+03, 1.6004e+03, 1.5935e+03,
        1.5891e+03, 1.5727e+03, 1.5647e+03, 1.5602e+03, 1.5514e+03, 1.5461e+03,
        1.5322e+03, 1.5264e+03, 1.5228e+03, 1.5130e+03, 1.5026e+03, 1.4969e+03,
        1.4884e+03, 1.4879e+03, 1.4674e+03, 1.4647e+03, 1.4539e+03, 1.4455e+03,
        1.4349e+03, 1.4298e+03, 1.4163e+03, 1.4090e+03, 1.4030e+03, 1.3995e+03,
        1.3867e+03, 1.3846e+03, 1.3829e+03, 1.3751e+03, 1.3705e+03, 1.3539e+03,
        1.3462e+03, 1.3401e+03, 1.3351e+03, 1.3330e+03, 1.3198e+03, 1.3178e+03,
        1.3112e+03, 1.3039e+03, 1.2946e+03, 1.2877e+03, 1.2813e+03, 1.2762e+03,
        1.2649e+03, 1.2568e+03, 1.2501e+03, 1.2425e+03, 1.2377e+03, 1.2303e+03,
        1.2235e+03, 1.2169e+03, 1.2106e+03, 1.2036e+03, 1.1989e+03, 1.1917e+03,
        1.1842e+03, 1.1784e+03, 1.1741e+03, 1.1700e+03, 1.1638e+03, 1.1578e+03,
        1.1503e+03, 1.1467e+03, 1.1393e+03, 1.1330e+03, 1.1263e+03, 1.1174e+03,
        1.1146e+03, 1.1092e+03, 1.1035e+03, 1.1001e+03, 1.0964e+03, 1.0897e+03,
        1.0818e+03, 1.0709e+03, 1.0685e+03, 1.0619e+03, 1.0581e+03, 1.0488e+03,
        1.0415e+03, 1.0342e+03, 1.0329e+03, 1.0308e+03, 1.0257e+03, 1.0216e+03,
        1.0204e+03, 1.0146e+03, 1.0049e+03, 1.0028e+03, 9.9656e+02, 9.8710e+02,
        9.8096e+02, 9.7595e+02, 9.7354e+02, 9.6683e+02, 9.6482e+02, 9.5454e+02,
        9.5064e+02, 9.4638e+02, 9.4393e+02, 9.3615e+02, 9.3501e+02, 9.2781e+02,
        9.1936e+02, 9.1798e+02, 9.0676e+02, 9.0354e+02, 8.9734e+02, 8.9336e+02,
        8.8781e+02, 8.8472e+02, 8.8116e+02, 8.7832e+02, 8.7680e+02, 8.7350e+02,
        8.6184e+02, 8.5659e+02, 8.5175e+02, 8.4806e+02, 8.4417e+02, 8.4166e+02,
        8.3913e+02, 8.2701e+02, 8.2533e+02, 8.2153e+02, 8.1458e+02, 8.0920e+02,
        8.0279e+02, 7.9809e+02, 7.9586e+02, 7.9080e+02, 7.8762e+02, 7.8294e+02,
        7.7523e+02, 7.7281e+02, 7.6965e+02, 7.6613e+02, 7.6468e+02, 7.5990e+02,
        7.5545e+02, 7.5399e+02, 7.4982e+02, 7.4235e+02, 7.3759e+02, 7.3374e+02,
        7.2873e+02, 7.2360e+02, 7.2088e+02, 7.1392e+02, 7.1249e+02, 7.0930e+02,
        7.0321e+02, 6.9807e+02, 6.9454e+02, 6.9165e+02, 6.8624e+02, 6.8240e+02,
        6.7500e+02, 6.7240e+02, 6.7034e+02, 6.6662e+02, 6.6113e+02, 6.5825e+02,
        6.5487e+02, 6.5214e+02, 6.4634e+02, 6.4455e+02, 6.3663e+02, 6.3362e+02,
        6.2914e+02, 6.2121e+02, 6.1983e+02, 6.1879e+02, 6.1170e+02, 6.0835e+02,
        6.0013e+02, 5.9504e+02, 5.9347e+02, 5.9211e+02, 5.8826e+02, 5.8457e+02,
        5.7554e+02, 5.7311e+02, 5.6825e+02, 5.6760e+02, 5.6362e+02, 5.6122e+02,
        5.5800e+02, 5.5368e+02, 5.5056e+02, 5.4703e+02, 5.4215e+02, 5.3536e+02,
        5.3305e+02, 5.3093e+02, 5.2420e+02, 5.1969e+02, 5.1476e+02, 5.1401e+02,
        5.0926e+02, 5.0891e+02, 5.0069e+02, 4.9508e+02, 4.9276e+02, 4.8692e+02,
        4.8300e+02, 4.7794e+02, 4.7331e+02, 4.6823e+02, 4.6658e+02, 4.6109e+02,
        4.5485e+02, 4.5344e+02, 4.4972e+02, 4.4489e+02, 4.4361e+02, 4.3898e+02,
        4.3709e+02, 4.3299e+02, 4.2720e+02, 4.2437e+02, 4.2365e+02, 4.1497e+02,
        4.1229e+02, 4.0944e+02, 4.0543e+02, 4.0315e+02, 3.9609e+02, 3.9531e+02,
        3.9026e+02, 3.8924e+02, 3.8439e+02, 3.8083e+02, 3.7324e+02, 3.6657e+02,
        3.6315e+02, 3.5664e+02, 3.5344e+02, 3.4902e+02, 3.4580e+02, 3.4195e+02,
        3.3403e+02, 3.2958e+02, 3.2553e+02, 3.2413e+02, 3.2067e+02, 3.1223e+02,
        3.0586e+02, 3.0300e+02, 2.9433e+02, 2.8494e+02, 2.7955e+02, 2.7416e+02,
        2.6724e+02, 2.5951e+02, 2.5478e+02, 2.4322e+02, 2.2932e+02, 2.2551e+02],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 317]) 

NULL SPACE BASIS :  tensor([[ 0.0956, -0.0505,  0.0202,  ..., -0.0103,  0.0105,  0.0101],
        [ 0.0415,  0.0718,  0.0025,  ...,  0.0119, -0.0314, -0.0084],
        [-0.0433,  0.0976, -0.0147,  ..., -0.0093,  0.0196,  0.0035],
        ...,
        [ 0.0484,  0.0498,  0.0447,  ...,  0.0161, -0.0318,  0.0131],
        [-0.0315,  0.0560,  0.0279,  ..., -0.0111,  0.0416, -0.0383],
        [ 0.0478,  0.0182,  0.0657,  ...,  0.0021, -0.0094,  0.0179]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.7883e-02, -8.9582e-03, -2.7244e-03,  ...,  1.1588e-03,
         -3.4525e-04,  2.4366e-04],
        [-8.9582e-03,  3.2050e-02, -9.1802e-03,  ..., -8.8013e-04,
         -3.3870e-04,  3.9006e-05],
        [-2.7244e-03, -9.1802e-03,  2.3727e-02,  ...,  3.8157e-04,
         -7.1965e-04, -1.2872e-03],
        ...,
        [ 1.1588e-03, -8.8013e-04,  3.8157e-04,  ...,  3.2645e-02,
         -8.2648e-03, -3.7490e-03],
        [-3.4525e-04, -3.3870e-04, -7.1965e-04,  ..., -8.2648e-03,
          3.7189e-02, -7.6364e-03],
        [ 2.4366e-04,  3.9006e-05, -1.2872e-03,  ..., -3.7490e-03,
         -7.6364e-03,  3.0361e-02]], device='cuda:0') 

reserving basis 278/576; cond: 500027.21875, radio:0.0021881565917283297
PARAMETER       :  Parameter containing:
tensor([[[[-2.7859e-02, -1.2210e-02, -2.4671e-02],
          [ 3.3162e-02, -1.6388e-02, -2.4148e-02],
          [ 1.6420e-02, -6.6275e-03,  4.5280e-02]],

         [[ 3.5201e-02, -7.7829e-03, -2.6631e-02],
          [ 3.2572e-02,  6.6000e-03,  3.6145e-02],
          [ 8.3485e-03, -2.0796e-02,  3.0192e-03]],

         [[-7.5437e-03, -2.2211e-02, -1.7875e-02],
          [ 2.5353e-02, -4.6249e-02,  2.0481e-02],
          [-3.2398e-02,  3.3166e-03, -1.0975e-02]],

         ...,

         [[-1.7351e-02,  1.5358e-02,  2.5321e-02],
          [ 1.4740e-02, -1.3570e-02, -1.0209e-02],
          [ 1.5087e-02, -1.4562e-02, -3.2854e-02]],

         [[ 2.7130e-03,  3.1956e-02,  4.2685e-02],
          [ 3.7881e-03,  2.4610e-02,  2.4015e-02],
          [ 6.1070e-03,  1.0044e-02,  3.4152e-02]],

         [[-4.0268e-02, -2.2834e-02, -1.3327e-02],
          [-2.1470e-02, -2.2509e-02,  2.3429e-02],
          [-3.5842e-02, -2.3115e-03, -3.7702e-02]]],


        [[[ 5.5344e-03, -1.7546e-02, -5.0241e-02],
          [-1.1739e-02, -2.3487e-02, -4.5093e-02],
          [-5.7099e-03, -1.8456e-02,  1.2178e-02]],

         [[-1.4013e-02, -5.4110e-03,  9.3526e-03],
          [ 1.9952e-02,  4.0003e-02,  2.5699e-02],
          [-1.2606e-02, -3.8740e-03,  7.9317e-03]],

         [[-2.1016e-02,  1.0519e-02, -1.7463e-02],
          [ 2.8092e-02, -3.3990e-02,  3.8399e-02],
          [-1.2414e-02,  7.1135e-03, -9.7178e-04]],

         ...,

         [[ 4.0335e-03,  4.2339e-02, -1.3801e-02],
          [-1.5349e-03, -7.8859e-04, -2.0573e-02],
          [-2.5964e-02, -1.0459e-02, -1.2165e-02]],

         [[-5.1203e-02,  3.3391e-03,  1.4870e-02],
          [ 1.3741e-02, -1.6775e-03, -1.0970e-02],
          [ 4.7979e-02, -9.0390e-03,  3.4724e-04]],

         [[-4.2070e-02,  8.4189e-03,  1.4814e-02],
          [-1.4192e-02,  2.7290e-02, -9.6598e-03],
          [ 2.3167e-02,  3.5919e-02,  3.7459e-02]]],


        [[[-3.8706e-02,  2.2729e-02,  1.3583e-02],
          [ 3.0510e-02,  1.7473e-02,  3.0496e-02],
          [ 3.3173e-02,  1.5749e-02,  2.2929e-02]],

         [[-6.0795e-03,  3.6676e-03, -7.9247e-03],
          [ 2.4571e-02, -2.1313e-02,  2.0763e-02],
          [-8.3434e-03, -3.7340e-02, -2.6348e-02]],

         [[ 4.2727e-02, -1.5341e-02, -1.0047e-02],
          [ 1.0722e-02,  3.6555e-02, -5.6712e-03],
          [-2.2170e-02,  1.2887e-02,  3.8220e-02]],

         ...,

         [[ 1.9455e-02, -2.4549e-02,  2.3672e-02],
          [ 4.3069e-02,  1.2389e-02, -2.0179e-02],
          [ 3.4326e-02,  2.5017e-02, -1.3722e-02]],

         [[ 2.0404e-02, -4.0134e-02, -1.7871e-02],
          [-2.9824e-02,  2.7610e-02,  4.8830e-03],
          [ 3.1546e-03,  2.9653e-02,  2.1387e-02]],

         [[-2.3893e-02,  7.9305e-03,  7.5218e-03],
          [-2.4572e-02,  3.8636e-03, -3.3901e-02],
          [ 3.8801e-03,  1.6807e-02,  4.8535e-03]]],


        ...,


        [[[-1.5206e-02, -2.8057e-02, -3.5050e-02],
          [-8.5067e-04,  1.1929e-02, -4.9272e-02],
          [-1.0265e-02, -1.0006e-02, -3.8480e-02]],

         [[-2.1346e-02,  2.6254e-02,  3.8558e-02],
          [-1.5156e-02, -2.9832e-02, -3.2739e-02],
          [-1.2336e-02,  3.8251e-02,  4.2736e-02]],

         [[-2.4134e-03, -1.8527e-02,  4.0284e-02],
          [-5.5992e-03,  3.0038e-02,  3.9423e-02],
          [ 1.5040e-03,  4.2130e-02,  1.8024e-02]],

         ...,

         [[ 3.0666e-02, -4.3447e-02, -3.5813e-02],
          [ 1.6000e-02,  2.1196e-02, -9.5467e-03],
          [ 1.7019e-03, -4.6934e-03,  8.7309e-03]],

         [[ 2.2473e-02,  9.2749e-06,  9.0984e-03],
          [ 2.6860e-02,  9.4863e-03, -4.0362e-02],
          [ 4.5153e-02,  3.7055e-02, -3.8777e-02]],

         [[ 2.7882e-03,  4.2513e-02, -3.8839e-04],
          [ 3.0617e-02, -3.4279e-02, -6.8069e-03],
          [ 3.1812e-03, -3.3171e-02,  1.8405e-02]]],


        [[[ 3.6540e-02, -1.1815e-02,  4.5604e-02],
          [-4.7938e-02,  1.9978e-02, -1.3968e-02],
          [ 3.0630e-02, -1.3635e-02, -2.0748e-02]],

         [[ 2.6569e-02, -2.3445e-02, -1.2538e-02],
          [ 2.4012e-02,  8.5566e-03, -2.2560e-02],
          [ 5.1890e-02, -2.3666e-02,  4.6153e-02]],

         [[-3.0447e-02, -3.5765e-02,  2.7502e-02],
          [ 1.7170e-02, -3.6559e-02, -5.3488e-02],
          [-3.8927e-02, -7.0238e-04, -1.2344e-02]],

         ...,

         [[-1.9888e-02,  1.9197e-02,  3.1988e-02],
          [-3.5276e-02,  9.5271e-03, -9.0982e-03],
          [ 3.1348e-02, -3.3838e-02,  3.6209e-02]],

         [[-4.9153e-03, -1.7457e-02, -3.2185e-02],
          [ 2.3817e-02, -1.0599e-02,  1.2714e-02],
          [-2.6371e-04,  3.4321e-02,  1.3087e-02]],

         [[-3.0680e-02, -1.4641e-02, -4.1828e-02],
          [ 2.6975e-02, -4.2090e-02, -2.9494e-02],
          [-3.6312e-02,  4.3775e-02,  1.6672e-02]]],


        [[[-1.4981e-03, -4.6557e-03,  1.5433e-02],
          [-3.8705e-02, -5.8281e-03, -7.9163e-03],
          [ 2.5041e-02,  4.4432e-03,  3.6936e-02]],

         [[-2.3359e-02,  2.8299e-02,  1.5313e-02],
          [ 3.8844e-02,  2.6041e-02,  1.8919e-02],
          [-4.9095e-03,  2.3613e-02,  1.2319e-02]],

         [[-1.4572e-02, -2.6778e-02,  3.5154e-02],
          [-3.7369e-02,  1.9766e-02,  1.7001e-02],
          [ 3.2413e-03, -2.7073e-02, -3.2824e-02]],

         ...,

         [[ 5.1477e-02, -2.0128e-02,  2.7520e-02],
          [ 3.3307e-02, -3.9033e-02,  2.7467e-02],
          [-8.1189e-03, -4.1801e-02, -1.9757e-02]],

         [[-1.6146e-02, -7.4947e-04, -1.8757e-02],
          [-3.0740e-02, -4.8442e-02, -3.8817e-02],
          [ 2.2101e-02,  1.8996e-02, -1.4381e-02]],

         [[-8.6475e-03, -1.3376e-02,  4.7559e-02],
          [ 2.3293e-02,  1.7963e-02,  2.2865e-03],
          [ 1.1829e-02,  3.0597e-02, -9.5890e-04]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.1892e+07, 5.8519e+05, 5.3453e+05, 3.3868e+05, 2.4558e+05, 1.4286e+05,
        1.3283e+05, 8.9014e+04, 7.2739e+04, 6.2755e+04, 4.0513e+04, 3.6968e+04,
        3.2898e+04, 3.0370e+04, 2.7288e+04, 2.5388e+04, 2.4415e+04, 2.2736e+04,
        2.1369e+04, 1.9434e+04, 1.9272e+04, 1.7002e+04, 1.6204e+04, 1.4861e+04,
        1.4351e+04, 1.3693e+04, 1.2764e+04, 1.2218e+04, 1.1305e+04, 1.0784e+04,
        1.0213e+04, 9.8878e+03, 9.4307e+03, 8.7799e+03, 8.1343e+03, 8.0753e+03,
        7.3435e+03, 6.9375e+03, 6.5060e+03, 6.2587e+03, 5.6384e+03, 5.5620e+03,
        5.4589e+03, 5.2769e+03, 5.2376e+03, 5.0301e+03, 4.8349e+03, 4.5549e+03,
        4.3880e+03, 4.3307e+03, 4.2279e+03, 3.9420e+03, 3.7462e+03, 3.6799e+03,
        3.5834e+03, 3.5222e+03, 3.3658e+03, 3.3100e+03, 3.2574e+03, 3.1491e+03,
        3.0881e+03, 3.0511e+03, 3.0181e+03, 2.9837e+03, 2.8878e+03, 2.7646e+03,
        2.7538e+03, 2.6570e+03, 2.6344e+03, 2.5419e+03, 2.4468e+03, 2.4273e+03,
        2.3733e+03, 2.3187e+03, 2.2685e+03, 2.2207e+03, 2.2046e+03, 2.1322e+03,
        2.0788e+03, 2.0383e+03, 2.0320e+03, 1.9858e+03, 1.9531e+03, 1.8525e+03,
        1.8303e+03, 1.8191e+03, 1.7556e+03, 1.7427e+03, 1.7082e+03, 1.6600e+03,
        1.6580e+03, 1.6437e+03, 1.6184e+03, 1.5980e+03, 1.5914e+03, 1.5387e+03,
        1.4972e+03, 1.4758e+03, 1.4284e+03, 1.4191e+03, 1.4064e+03, 1.4022e+03,
        1.3393e+03, 1.3260e+03, 1.3116e+03, 1.2906e+03, 1.2746e+03, 1.2523e+03,
        1.2414e+03, 1.2311e+03, 1.2212e+03, 1.2049e+03, 1.1713e+03, 1.1473e+03,
        1.1432e+03, 1.1229e+03, 1.1130e+03, 1.0855e+03, 1.0777e+03, 1.0587e+03,
        1.0471e+03, 1.0406e+03, 1.0335e+03, 1.0054e+03, 9.9245e+02, 9.8750e+02,
        9.7230e+02, 9.6302e+02, 9.5650e+02, 9.4913e+02, 9.2963e+02, 9.2889e+02,
        9.1233e+02, 9.0834e+02, 8.9800e+02, 8.9466e+02, 8.7925e+02, 8.6762e+02,
        8.5730e+02, 8.4546e+02, 8.3722e+02, 8.2836e+02, 8.1266e+02, 8.0123e+02,
        7.9581e+02, 7.9186e+02, 7.7455e+02, 7.7394e+02, 7.6647e+02, 7.6011e+02,
        7.4592e+02, 7.3882e+02, 7.3033e+02, 7.1845e+02, 7.1527e+02, 7.0793e+02,
        6.9837e+02, 6.9677e+02, 6.8606e+02, 6.8269e+02, 6.7235e+02, 6.7072e+02,
        6.5873e+02, 6.5490e+02, 6.5191e+02, 6.4111e+02, 6.3181e+02, 6.2963e+02,
        6.2092e+02, 6.1815e+02, 6.1435e+02, 6.1058e+02, 6.0547e+02, 5.9852e+02,
        5.9100e+02, 5.8457e+02, 5.8127e+02, 5.7696e+02, 5.6792e+02, 5.6552e+02,
        5.6302e+02, 5.5596e+02, 5.4996e+02, 5.4389e+02, 5.3835e+02, 5.3202e+02,
        5.3000e+02, 5.2547e+02, 5.2020e+02, 5.1525e+02, 5.1153e+02, 5.0927e+02,
        5.0287e+02, 4.9393e+02, 4.9215e+02, 4.9028e+02, 4.8544e+02, 4.7790e+02,
        4.7537e+02, 4.7202e+02, 4.6821e+02, 4.6413e+02, 4.6174e+02, 4.5626e+02,
        4.5407e+02, 4.5260e+02, 4.5017e+02, 4.4580e+02, 4.4130e+02, 4.3798e+02,
        4.2944e+02, 4.2800e+02, 4.2716e+02, 4.2395e+02, 4.2190e+02, 4.1995e+02,
        4.1390e+02, 4.1219e+02, 4.0914e+02, 4.0855e+02, 4.0527e+02, 4.0355e+02,
        4.0105e+02, 3.9749e+02, 3.9539e+02, 3.9323e+02, 3.9177e+02, 3.8326e+02,
        3.7995e+02, 3.7750e+02, 3.7398e+02, 3.7177e+02, 3.6937e+02, 3.6777e+02,
        3.6173e+02, 3.6052e+02, 3.5985e+02, 3.5777e+02, 3.5521e+02, 3.5251e+02,
        3.4847e+02, 3.4757e+02, 3.4500e+02, 3.4260e+02, 3.3807e+02, 3.3499e+02,
        3.3331e+02, 3.3255e+02, 3.3115e+02, 3.2677e+02, 3.2335e+02, 3.2012e+02,
        3.1852e+02, 3.1728e+02, 3.1540e+02, 3.1383e+02, 3.1105e+02, 3.0919e+02,
        3.0552e+02, 3.0163e+02, 3.0078e+02, 2.9953e+02, 2.9765e+02, 2.9588e+02,
        2.9511e+02, 2.8950e+02, 2.8917e+02, 2.8682e+02, 2.8581e+02, 2.8534e+02,
        2.8470e+02, 2.8403e+02, 2.7986e+02, 2.7706e+02, 2.7674e+02, 2.7508e+02,
        2.7331e+02, 2.7114e+02, 2.6970e+02, 2.6760e+02, 2.6619e+02, 2.6492e+02,
        2.6286e+02, 2.6088e+02, 2.5908e+02, 2.5829e+02, 2.5644e+02, 2.5421e+02,
        2.5326e+02, 2.5132e+02, 2.5123e+02, 2.4982e+02, 2.4743e+02, 2.4720e+02,
        2.4421e+02, 2.4359e+02, 2.4055e+02, 2.3865e+02, 2.3768e+02, 2.3676e+02,
        2.3563e+02, 2.3407e+02, 2.3211e+02, 2.3162e+02, 2.3048e+02, 2.2804e+02,
        2.2700e+02, 2.2511e+02, 2.2480e+02, 2.2340e+02, 2.2192e+02, 2.2092e+02,
        2.1981e+02, 2.1818e+02, 2.1594e+02, 2.1559e+02, 2.1368e+02, 2.1280e+02,
        2.1129e+02, 2.1118e+02, 2.0980e+02, 2.0873e+02, 2.0810e+02, 2.0694e+02,
        2.0617e+02, 2.0438e+02, 2.0303e+02, 2.0074e+02, 1.9911e+02, 1.9758e+02,
        1.9714e+02, 1.9603e+02, 1.9586e+02, 1.9431e+02, 1.9297e+02, 1.9132e+02,
        1.9116e+02, 1.9001e+02, 1.8836e+02, 1.8805e+02, 1.8677e+02, 1.8612e+02,
        1.8286e+02, 1.8260e+02, 1.8182e+02, 1.7974e+02, 1.7829e+02, 1.7812e+02,
        1.7653e+02, 1.7587e+02, 1.7538e+02, 1.7401e+02, 1.7254e+02, 1.7063e+02,
        1.7034e+02, 1.6908e+02, 1.6816e+02, 1.6782e+02, 1.6738e+02, 1.6629e+02,
        1.6564e+02, 1.6487e+02, 1.6456e+02, 1.6323e+02, 1.6254e+02, 1.6130e+02,
        1.6000e+02, 1.5966e+02, 1.5880e+02, 1.5799e+02, 1.5657e+02, 1.5605e+02,
        1.5546e+02, 1.5425e+02, 1.5326e+02, 1.5244e+02, 1.5078e+02, 1.4988e+02,
        1.4967e+02, 1.4843e+02, 1.4825e+02, 1.4794e+02, 1.4626e+02, 1.4600e+02,
        1.4540e+02, 1.4484e+02, 1.4389e+02, 1.4383e+02, 1.4233e+02, 1.4180e+02,
        1.4135e+02, 1.3989e+02, 1.3869e+02, 1.3837e+02, 1.3764e+02, 1.3699e+02,
        1.3662e+02, 1.3527e+02, 1.3479e+02, 1.3467e+02, 1.3339e+02, 1.3282e+02,
        1.3120e+02, 1.3101e+02, 1.3003e+02, 1.2899e+02, 1.2809e+02, 1.2774e+02,
        1.2734e+02, 1.2584e+02, 1.2534e+02, 1.2488e+02, 1.2417e+02, 1.2392e+02,
        1.2342e+02, 1.2258e+02, 1.2244e+02, 1.2176e+02, 1.2152e+02, 1.1996e+02,
        1.1950e+02, 1.1913e+02, 1.1808e+02, 1.1762e+02, 1.1713e+02, 1.1648e+02,
        1.1563e+02, 1.1507e+02, 1.1418e+02, 1.1387e+02, 1.1368e+02, 1.1174e+02,
        1.1152e+02, 1.1090e+02, 1.1042e+02, 1.0983e+02, 1.0962e+02, 1.0924e+02,
        1.0818e+02, 1.0812e+02, 1.0746e+02, 1.0691e+02, 1.0676e+02, 1.0588e+02,
        1.0528e+02, 1.0484e+02, 1.0412e+02, 1.0375e+02, 1.0323e+02, 1.0186e+02,
        1.0158e+02, 1.0111e+02, 1.0049e+02, 1.0016e+02, 9.8685e+01, 9.8464e+01,
        9.7920e+01, 9.6874e+01, 9.6460e+01, 9.6104e+01, 9.5883e+01, 9.4692e+01,
        9.4477e+01, 9.3818e+01, 9.3409e+01, 9.2749e+01, 9.2016e+01, 9.1918e+01,
        9.1463e+01, 9.0399e+01, 9.0183e+01, 8.9914e+01, 8.8901e+01, 8.8507e+01,
        8.8033e+01, 8.6742e+01, 8.6581e+01, 8.6060e+01, 8.5830e+01, 8.5487e+01,
        8.4665e+01, 8.3975e+01, 8.3405e+01, 8.2453e+01, 8.2186e+01, 8.1451e+01,
        8.1076e+01, 8.0373e+01, 7.9473e+01, 7.8740e+01, 7.8527e+01, 7.8021e+01,
        7.7186e+01, 7.6399e+01, 7.6266e+01, 7.5843e+01, 7.5217e+01, 7.4277e+01,
        7.3857e+01, 7.3449e+01, 7.2998e+01, 7.2607e+01, 7.1802e+01, 7.1578e+01,
        7.0966e+01, 7.0466e+01, 7.0059e+01, 6.9445e+01, 6.8849e+01, 6.8278e+01,
        6.7860e+01, 6.7655e+01, 6.7083e+01, 6.6470e+01, 6.5676e+01, 6.5415e+01,
        6.5241e+01, 6.4341e+01, 6.3789e+01, 6.3564e+01, 6.2789e+01, 6.2587e+01,
        6.2343e+01, 6.1855e+01, 6.1207e+01, 6.0898e+01, 6.0133e+01, 5.9754e+01,
        5.9405e+01, 5.8615e+01, 5.7998e+01, 5.7811e+01, 5.7396e+01, 5.6906e+01,
        5.6801e+01, 5.6403e+01, 5.5858e+01, 5.4795e+01, 5.4481e+01, 5.4245e+01,
        5.3558e+01, 5.2811e+01, 5.2577e+01, 5.2011e+01, 5.0987e+01, 5.0617e+01,
        5.0192e+01, 4.9774e+01, 4.8822e+01, 4.8748e+01, 4.8581e+01, 4.6894e+01,
        4.6519e+01, 4.6085e+01, 4.5739e+01, 4.4038e+01, 4.3627e+01, 4.3195e+01,
        4.3063e+01, 4.1979e+01, 4.0600e+01, 4.0475e+01, 3.8887e+01, 3.8016e+01,
        3.7909e+01, 3.6559e+01, 3.5932e+01, 3.5766e+01, 3.5041e+01, 3.3811e+01,
        3.2897e+01, 3.2426e+01, 3.1374e+01, 3.0571e+01, 2.7471e+01, 2.3782e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 278]) 

NULL SPACE BASIS :  tensor([[ 1.3998e-02, -4.1047e-02,  9.2975e-03,  ...,  2.2070e-03,
         -4.9744e-05,  1.2272e-02],
        [-1.8037e-02, -4.2558e-02, -3.0769e-02,  ..., -4.6341e-03,
          3.5403e-03, -1.3281e-02],
        [-2.0410e-02,  3.2217e-02,  5.4869e-03,  ...,  5.1315e-03,
         -6.7858e-03,  2.7841e-03],
        ...,
        [ 6.4695e-03,  2.6069e-02, -1.6580e-02,  ...,  5.8460e-03,
          3.9240e-03,  2.2010e-03],
        [-7.0456e-03, -7.9357e-02, -6.2926e-02,  ..., -8.2226e-04,
         -2.7497e-04,  1.4614e-02],
        [ 1.0568e-02,  3.1593e-02, -5.6261e-02,  ...,  2.2634e-03,
          1.5812e-03, -1.2872e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0198, -0.0167,  0.0023,  ..., -0.0015,  0.0015, -0.0002],
        [-0.0167,  0.0312, -0.0146,  ..., -0.0001, -0.0002, -0.0004],
        [ 0.0023, -0.0146,  0.0167,  ...,  0.0008, -0.0003,  0.0003],
        ...,
        [-0.0015, -0.0001,  0.0008,  ...,  0.0131, -0.0075, -0.0009],
        [ 0.0015, -0.0002, -0.0003,  ..., -0.0075,  0.0173, -0.0064],
        [-0.0002, -0.0004,  0.0003,  ..., -0.0009, -0.0064,  0.0127]],
       device='cuda:0') 

reserving basis 764/1152; cond: 329127.71875, radio:0.008465275168418884
PARAMETER       :  Parameter containing:
tensor([[[[ 1.4870e-02,  3.0715e-02, -1.7655e-02],
          [-5.6544e-04, -1.6358e-02,  9.7845e-03],
          [ 1.8392e-02,  1.8796e-02, -3.4418e-03]],

         [[-1.9141e-02, -2.3302e-02, -2.6594e-02],
          [-2.2895e-02, -2.2660e-02,  6.5328e-03],
          [-1.5139e-02,  9.1636e-03, -2.8665e-02]],

         [[ 9.4304e-04, -1.3839e-02,  4.3069e-03],
          [ 7.9082e-03,  1.5117e-02, -1.5569e-02],
          [-1.4903e-02, -2.5407e-02,  2.6984e-02]],

         ...,

         [[-1.3066e-02,  1.5810e-03,  5.4086e-03],
          [-3.0329e-02, -2.1002e-02, -1.5387e-03],
          [ 6.9691e-03,  1.5423e-02,  2.8935e-02]],

         [[ 9.2666e-03, -3.3894e-02, -1.8551e-02],
          [ 2.3039e-02,  1.5446e-02,  3.2979e-04],
          [ 1.5758e-02, -1.5001e-02,  3.4675e-02]],

         [[ 1.8897e-02,  6.1059e-03,  1.5418e-02],
          [-2.2051e-02, -3.3480e-02, -2.5600e-02],
          [-2.8721e-02,  1.9190e-02, -1.5035e-02]]],


        [[[ 2.5083e-02,  1.0712e-03,  8.2907e-03],
          [-1.4921e-02, -1.2236e-02,  2.2581e-02],
          [-2.6389e-02, -2.8992e-02, -1.4730e-02]],

         [[ 8.0916e-03, -2.3936e-02, -2.2822e-02],
          [ 9.8123e-03, -1.2187e-02,  1.5294e-03],
          [-2.5895e-02,  6.7015e-03, -1.2669e-02]],

         [[ 5.9423e-04, -1.7849e-02, -1.9340e-02],
          [-4.7696e-03,  1.3591e-02, -2.9879e-02],
          [ 2.5560e-02,  1.7845e-02,  2.0956e-03]],

         ...,

         [[ 4.3775e-03,  1.3425e-02, -4.2261e-03],
          [-2.1275e-02,  2.5749e-02,  2.6588e-02],
          [ 1.4883e-02,  2.0589e-02,  2.1466e-02]],

         [[ 2.4669e-02,  1.9154e-03,  7.0320e-03],
          [-8.2071e-03, -2.1692e-02, -2.7322e-02],
          [ 4.1639e-02, -1.9496e-02, -3.0180e-02]],

         [[-1.4472e-02, -1.7660e-02, -3.1979e-02],
          [-2.2404e-02, -1.2954e-02, -7.7414e-03],
          [ 8.0119e-03, -1.0539e-02, -1.2527e-03]]],


        [[[-1.7132e-02,  1.8521e-02, -2.8837e-02],
          [-4.8750e-03,  1.7878e-02, -2.7392e-03],
          [-2.0160e-05,  2.8010e-02,  1.5965e-02]],

         [[ 4.5851e-03, -1.2386e-02,  2.3286e-02],
          [-4.3109e-03, -9.4775e-03,  4.5649e-03],
          [-2.1580e-02,  4.6691e-03,  8.6413e-03]],

         [[-1.7445e-02, -1.5266e-02, -1.2898e-02],
          [-2.5637e-02, -2.6995e-02, -3.1099e-02],
          [-1.2630e-02, -1.7887e-02,  1.6543e-02]],

         ...,

         [[ 2.6246e-02,  8.4690e-03, -1.7514e-02],
          [-3.0733e-02,  1.5222e-02,  4.3003e-03],
          [ 2.0090e-02, -1.6176e-02, -3.2485e-02]],

         [[-4.0605e-02,  2.2904e-03, -1.1481e-02],
          [-1.6824e-02,  1.7339e-02,  8.2713e-03],
          [-1.4189e-02,  7.9208e-03, -1.4641e-02]],

         [[-1.3738e-02, -1.9269e-02,  1.0722e-02],
          [-4.1170e-03, -9.9993e-03,  5.5741e-03],
          [ 1.0713e-02, -1.7666e-02,  3.0991e-02]]],


        ...,


        [[[ 2.7320e-02,  5.2873e-04,  3.1301e-02],
          [ 2.4080e-02,  1.7313e-02, -3.0109e-02],
          [ 4.3434e-03,  6.0461e-04,  2.8272e-02]],

         [[ 9.0701e-03, -7.7809e-03,  1.2702e-02],
          [ 5.2724e-03, -2.4959e-02,  5.1104e-03],
          [-1.5343e-02, -7.4098e-03,  1.6318e-02]],

         [[ 1.6536e-02,  1.2274e-02, -1.0271e-02],
          [-1.3752e-02,  2.7364e-02,  5.5714e-03],
          [ 6.3785e-03, -6.7834e-03, -1.3675e-02]],

         ...,

         [[-2.9111e-02,  1.4445e-02, -1.4405e-02],
          [ 7.0273e-03, -2.8148e-02, -1.5458e-02],
          [-1.9165e-02,  3.0672e-03,  8.0932e-03]],

         [[ 1.9412e-02,  1.8003e-02,  2.8178e-02],
          [-2.1671e-02,  7.6415e-04,  1.6180e-02],
          [-5.5350e-03,  1.0371e-02,  4.8560e-03]],

         [[-1.4680e-02, -3.0341e-03,  2.3134e-02],
          [ 1.5640e-02,  8.9829e-03,  3.0555e-02],
          [ 6.4621e-03,  8.8315e-03,  1.9057e-02]]],


        [[[-3.8095e-03, -1.1931e-02,  4.7467e-03],
          [-2.8248e-02, -8.5109e-03,  1.9560e-02],
          [ 1.0890e-02, -2.7315e-02, -2.2548e-02]],

         [[-1.2588e-02, -1.5462e-02,  2.4336e-02],
          [-1.9338e-02, -3.5177e-02, -4.7126e-03],
          [-2.3978e-02, -8.7793e-05, -2.1879e-02]],

         [[-1.6021e-02, -2.4072e-03, -2.1289e-02],
          [-1.1652e-02,  1.4546e-02,  2.2025e-02],
          [ 8.4307e-03, -2.0119e-02,  1.9386e-02]],

         ...,

         [[-1.4093e-02,  1.1230e-02,  4.6644e-03],
          [-2.9264e-02, -2.1234e-02, -1.3911e-02],
          [-2.3049e-02, -4.9850e-02, -1.7308e-02]],

         [[-2.8277e-03, -6.8079e-03, -1.2457e-02],
          [-3.0732e-02,  3.0774e-02,  3.0071e-02],
          [-3.6638e-02,  2.2601e-02, -7.6781e-03]],

         [[ 2.0831e-02, -2.3087e-02, -8.8255e-03],
          [ 2.9226e-02, -1.6493e-02,  3.1958e-02],
          [-5.6363e-03,  1.7678e-02,  8.6129e-04]]],


        [[[ 4.1629e-03, -2.6965e-02,  2.8915e-02],
          [-2.4589e-02,  4.5228e-03, -3.6134e-02],
          [-1.9897e-03, -2.8387e-02, -1.8316e-02]],

         [[ 1.6068e-02,  1.5029e-02, -1.5460e-02],
          [ 4.3506e-03,  1.3611e-02,  9.0749e-03],
          [-1.6644e-02, -2.7309e-02,  2.2857e-02]],

         [[-8.4178e-03, -8.5465e-03,  7.2391e-03],
          [-1.8349e-02,  1.4526e-03,  2.6336e-02],
          [-1.6539e-02,  2.4725e-02, -4.3329e-03]],

         ...,

         [[ 2.7342e-02,  1.0609e-02, -7.6709e-03],
          [-1.6770e-02,  3.0882e-02,  1.4927e-02],
          [ 2.8223e-02, -1.0639e-02, -9.0348e-04]],

         [[-1.5526e-02,  1.0101e-02, -4.4797e-03],
          [-2.3075e-02,  3.1755e-02,  2.7956e-02],
          [ 1.6233e-02,  1.2740e-02,  7.5976e-03]],

         [[ 1.1124e-02,  3.8596e-02, -1.8333e-02],
          [-1.4353e-02,  1.2442e-02,  3.1692e-02],
          [ 5.0369e-03, -2.5380e-02, -1.8400e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.3040e+07, 8.8687e+05, 8.4572e+05,  ..., 9.3419e+01, 8.2754e+01,
        7.0004e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 764]) 

NULL SPACE BASIS :  tensor([[-0.0037,  0.0243,  0.0389,  ..., -0.0245, -0.0164,  0.0091],
        [-0.0397, -0.0042,  0.0019,  ...,  0.0161,  0.0196, -0.0022],
        [-0.0110,  0.0126,  0.0004,  ...,  0.0087, -0.0048, -0.0024],
        ...,
        [-0.0130,  0.0118,  0.0086,  ..., -0.0031,  0.0045, -0.0057],
        [-0.0068, -0.0280, -0.0141,  ..., -0.0062, -0.0164, -0.0078],
        [ 0.0348,  0.0380,  0.0211,  ...,  0.0237,  0.0150,  0.0110]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.9311e-02, -1.0553e-03, -6.4908e-04,  ..., -3.2734e-04,
          4.7196e-04,  5.7508e-06],
        [-1.0553e-03,  3.0123e-02, -8.1977e-04,  ...,  1.9386e-04,
          2.5551e-04,  1.1955e-04],
        [-6.4908e-04, -8.1977e-04,  2.9675e-02,  ..., -1.1225e-04,
          2.0892e-04,  7.9400e-05],
        ...,
        [-3.2734e-04,  1.9386e-04, -1.1225e-04,  ...,  2.2903e-02,
         -2.7328e-03, -1.1135e-03],
        [ 4.7196e-04,  2.5551e-04,  2.0892e-04,  ..., -2.7328e-03,
          2.3552e-02, -2.9410e-03],
        [ 5.7508e-06,  1.1955e-04,  7.9400e-05,  ..., -1.1135e-03,
         -2.9410e-03,  2.4881e-02]], device='cuda:0') 

reserving basis 44/64; cond: 7202.95703125, radio:0.017640594393014908
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0139]],

         [[ 0.0948]],

         [[ 0.0557]],

         ...,

         [[-0.0838]],

         [[-0.0935]],

         [[-0.0471]]],


        [[[ 0.0271]],

         [[-0.0274]],

         [[-0.0941]],

         ...,

         [[-0.0437]],

         [[ 0.0194]],

         [[ 0.0259]]],


        [[[ 0.0250]],

         [[-0.0640]],

         [[-0.0075]],

         ...,

         [[-0.1410]],

         [[ 0.0904]],

         [[ 0.0214]]],


        ...,


        [[[ 0.0326]],

         [[-0.0812]],

         [[-0.1010]],

         ...,

         [[-0.0261]],

         [[ 0.0552]],

         [[-0.0156]]],


        [[[ 0.0149]],

         [[-0.0607]],

         [[ 0.1204]],

         ...,

         [[-0.0023]],

         [[ 0.0337]],

         [[ 0.0646]]],


        [[[ 0.0032]],

         [[-0.0011]],

         [[ 0.0836]],

         ...,

         [[-0.0030]],

         [[ 0.0595]],

         [[ 0.0878]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.4941e+06, 8.1082e+04, 6.8516e+04, 3.1346e+04, 2.1843e+04, 1.7669e+04,
        1.2390e+04, 9.6102e+03, 6.5866e+03, 5.8947e+03, 5.5133e+03, 5.0102e+03,
        4.2248e+03, 3.7961e+03, 3.4318e+03, 3.3193e+03, 2.9309e+03, 2.8617e+03,
        2.5098e+03, 2.3690e+03, 1.9986e+03, 1.8284e+03, 1.6148e+03, 1.5151e+03,
        1.4539e+03, 1.4436e+03, 1.3459e+03, 1.1705e+03, 1.1317e+03, 1.0487e+03,
        9.7372e+02, 9.2392e+02, 9.0516e+02, 8.6104e+02, 8.1574e+02, 7.7371e+02,
        7.5036e+02, 7.2509e+02, 6.9520e+02, 6.4574e+02, 6.4391e+02, 6.0416e+02,
        5.6487e+02, 5.4632e+02, 5.0083e+02, 4.9804e+02, 4.6323e+02, 4.3073e+02,
        4.2187e+02, 4.1693e+02, 4.0516e+02, 3.9802e+02, 3.7638e+02, 3.5870e+02,
        3.4276e+02, 3.2379e+02, 3.1922e+02, 2.9759e+02, 2.8900e+02, 2.8078e+02,
        2.6121e+02, 2.4363e+02, 2.3899e+02, 2.0743e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 44]) 

NULL SPACE BASIS :  tensor([[-0.2119,  0.1082,  0.1930,  ...,  0.0079, -0.1505, -0.1036],
        [-0.0453,  0.0875, -0.1081,  ...,  0.0437,  0.0272, -0.0428],
        [-0.0416,  0.1450, -0.0092,  ..., -0.2557,  0.1865,  0.0392],
        ...,
        [-0.1151, -0.1312, -0.0108,  ..., -0.0180,  0.0153,  0.0221],
        [ 0.0599, -0.0901,  0.0658,  ..., -0.0084, -0.0402, -0.0328],
        [ 0.2323, -0.0056,  0.0214,  ..., -0.0134, -0.0014,  0.0267]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0984,  0.0025,  0.0174,  ...,  0.0010, -0.0040, -0.0129],
        [ 0.0025,  0.1223, -0.0113,  ...,  0.0111, -0.0097, -0.0031],
        [ 0.0174, -0.0113,  0.0992,  ..., -0.0062,  0.0016,  0.0048],
        ...,
        [ 0.0010,  0.0111, -0.0062,  ...,  0.0367, -0.0328, -0.0029],
        [-0.0040, -0.0097,  0.0016,  ..., -0.0328,  0.1134,  0.0037],
        [-0.0129, -0.0031,  0.0048,  ..., -0.0029,  0.0037,  0.0838]],
       device='cuda:0') 

reserving basis 779/1152; cond: 304827.5, radio:0.009179330430924892
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1696e-02, -3.2170e-03,  9.9465e-03],
          [-3.0597e-03,  1.6576e-02,  1.1385e-02],
          [-1.0499e-02, -1.2327e-02, -1.3256e-02]],

         [[ 4.8255e-03, -1.7214e-02, -2.2510e-02],
          [ 2.6916e-02, -1.9141e-03,  5.6518e-04],
          [ 7.2877e-03,  1.9456e-02, -1.6474e-02]],

         [[ 6.4464e-03,  4.2303e-05,  6.1995e-03],
          [ 1.6554e-02,  1.7493e-02,  1.6326e-02],
          [-2.3852e-02, -1.3850e-02,  1.2907e-02]],

         ...,

         [[ 1.6497e-02,  6.2768e-03, -6.5850e-04],
          [-1.1751e-02,  2.6835e-02,  5.7334e-03],
          [ 1.5214e-02, -1.7594e-03, -7.1075e-03]],

         [[ 8.3742e-03,  1.1820e-02,  6.7925e-03],
          [ 2.4044e-03,  2.2686e-02,  2.0989e-02],
          [ 2.4053e-02,  2.8056e-02, -5.4889e-03]],

         [[ 9.4957e-04,  2.6526e-02,  4.6925e-02],
          [-1.8504e-02,  1.3958e-02,  2.0100e-02],
          [ 6.3228e-03, -2.0387e-02,  1.6921e-02]]],


        [[[-1.0681e-02,  7.5392e-03, -2.1405e-02],
          [ 1.3400e-02, -7.5352e-03,  4.8588e-03],
          [ 2.5251e-03, -2.4189e-02,  1.9538e-02]],

         [[-2.3595e-03,  1.7885e-02,  2.5586e-02],
          [ 2.0876e-02,  1.6609e-02, -4.0364e-05],
          [-9.4193e-03,  1.6523e-02,  1.8253e-02]],

         [[ 1.8855e-02,  1.7154e-02,  9.4133e-03],
          [ 2.5988e-02, -5.7332e-03,  2.1366e-02],
          [-1.6773e-02,  1.3495e-03,  9.2045e-03]],

         ...,

         [[-3.2535e-03,  1.6252e-02, -1.7710e-02],
          [-2.7215e-02, -2.2471e-02, -3.1057e-02],
          [-2.1885e-02, -2.2191e-02, -2.3943e-02]],

         [[-2.2468e-02,  1.7341e-03,  3.1205e-02],
          [-1.5314e-02, -1.7629e-02, -2.4135e-03],
          [ 1.4614e-02, -1.0605e-02, -1.8167e-02]],

         [[ 2.2806e-02, -2.3833e-02,  1.3267e-02],
          [-7.4411e-03, -1.3754e-02, -2.8446e-02],
          [ 2.3426e-03,  1.6074e-02, -3.2001e-02]]],


        [[[ 1.8657e-02,  2.0482e-02,  1.4507e-02],
          [-5.9165e-03, -2.9326e-02,  2.8315e-02],
          [-1.1442e-02, -5.0109e-04, -2.4364e-02]],

         [[-1.0165e-02,  6.1481e-03, -1.5083e-02],
          [ 3.2534e-02,  6.9621e-03, -4.6563e-03],
          [-6.8528e-03, -1.8678e-02,  1.8787e-02]],

         [[-3.1007e-02, -1.5899e-02, -2.2884e-02],
          [-1.5502e-02, -1.4666e-02,  2.2695e-03],
          [ 2.4194e-02, -1.3169e-02, -7.4089e-03]],

         ...,

         [[-3.5022e-02, -6.9892e-03, -2.1200e-02],
          [-6.7407e-03, -3.4707e-02, -2.6557e-02],
          [-3.4172e-03, -2.7809e-02, -8.2347e-04]],

         [[ 2.4794e-02, -1.3229e-02,  5.9161e-03],
          [ 1.9391e-02, -1.7596e-02, -1.9997e-02],
          [-3.0451e-03, -3.0226e-02,  1.0297e-02]],

         [[ 1.0962e-02,  1.4039e-02, -1.6837e-04],
          [ 6.0221e-03,  1.0519e-02, -1.0598e-02],
          [ 4.1644e-03,  2.3974e-02, -1.1763e-02]]],


        ...,


        [[[-1.4991e-02, -2.4123e-02, -3.4843e-02],
          [-1.0181e-02, -1.5385e-02,  1.8386e-03],
          [ 1.5844e-02,  7.3487e-03,  2.3889e-02]],

         [[-3.1630e-02, -7.4632e-03, -2.5669e-02],
          [-1.2835e-02,  1.5231e-02,  1.1085e-02],
          [-1.2624e-02,  3.1509e-02,  2.8416e-02]],

         [[ 2.9952e-02,  6.3921e-03,  7.8735e-04],
          [-1.5974e-02,  2.8401e-03, -2.7843e-02],
          [ 3.1571e-02,  2.3317e-02,  8.6069e-03]],

         ...,

         [[-2.1747e-02, -2.7175e-02,  7.3205e-03],
          [ 1.6367e-02, -1.4322e-02,  1.6443e-02],
          [-1.1212e-02,  3.4318e-02, -5.6772e-03]],

         [[ 1.1410e-02,  3.5534e-02,  1.1477e-02],
          [-1.0643e-02, -1.8542e-02,  2.5101e-02],
          [ 1.7125e-02, -1.6414e-03, -4.2709e-03]],

         [[ 3.7777e-02,  7.9433e-03,  1.7313e-02],
          [ 2.0651e-02, -1.5099e-02,  1.6170e-02],
          [-1.1306e-02, -3.3883e-02, -3.7749e-02]]],


        [[[-2.7503e-02, -1.2820e-03,  1.4410e-02],
          [ 2.1362e-02,  1.4883e-02, -2.0387e-02],
          [ 2.9223e-02,  2.5629e-02, -1.8344e-03]],

         [[ 8.1653e-03, -4.0457e-03,  1.7530e-03],
          [ 2.0509e-02,  3.8619e-02,  1.8745e-02],
          [ 2.2979e-02,  3.8042e-02, -1.1238e-02]],

         [[-1.9963e-02, -1.6715e-02, -2.7843e-02],
          [-1.7135e-02, -1.2359e-03, -9.1499e-03],
          [-1.6374e-03,  5.6141e-03, -2.7799e-02]],

         ...,

         [[-2.6593e-02,  1.6251e-02, -2.5194e-02],
          [ 1.6311e-02, -1.1490e-02, -6.6026e-03],
          [-1.0624e-02,  2.7465e-02,  1.2252e-02]],

         [[-2.7859e-02,  2.3328e-03,  1.4736e-02],
          [-2.7773e-03, -4.9970e-03,  3.4240e-03],
          [ 2.0977e-02,  2.3330e-02,  2.4031e-03]],

         [[-2.8407e-02, -7.3382e-03,  9.5364e-03],
          [-2.6524e-02,  1.2528e-02,  2.7793e-02],
          [-2.9547e-02,  1.0101e-03,  1.4778e-02]]],


        [[[-1.5711e-02,  1.5066e-02, -2.8599e-03],
          [-3.6037e-03,  1.1934e-02,  4.8619e-03],
          [-4.9210e-03, -1.5773e-03,  1.9529e-02]],

         [[-1.1317e-03, -2.1599e-02, -1.8661e-02],
          [-2.4421e-02, -1.2783e-02, -3.8559e-02],
          [ 3.8516e-02,  1.5678e-02,  3.0541e-03]],

         [[ 2.9126e-02,  7.0871e-04, -9.1895e-03],
          [-1.7375e-02, -1.3669e-02, -1.5555e-02],
          [ 2.6006e-02, -7.5679e-03, -2.1097e-02]],

         ...,

         [[ 9.8071e-03, -5.1495e-03,  9.6719e-03],
          [-2.0840e-02, -1.1444e-02,  5.3620e-03],
          [ 1.3636e-02, -1.0158e-04, -1.2906e-02]],

         [[-1.6274e-02, -1.0198e-02,  2.7424e-03],
          [ 1.7203e-02,  1.0447e-02, -3.1689e-02],
          [-1.6655e-02, -6.5730e-03, -9.2878e-03]],

         [[-2.7234e-03, -2.9994e-02, -1.5873e-02],
          [ 3.0273e-03,  2.3400e-02, -2.7314e-02],
          [ 2.5624e-02,  1.2178e-02,  2.1776e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.2350e+07, 8.7134e+05, 8.2900e+05,  ..., 9.2335e+01, 8.9383e+01,
        7.3322e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 779]) 

NULL SPACE BASIS :  tensor([[-0.0354,  0.0315,  0.0091,  ...,  0.0151, -0.0035, -0.0179],
        [-0.0655, -0.0236, -0.0493,  ..., -0.0294,  0.0073,  0.0160],
        [-0.0233, -0.0027,  0.0067,  ...,  0.0063, -0.0087, -0.0063],
        ...,
        [ 0.0450,  0.0040, -0.0021,  ..., -0.0079,  0.0006,  0.0028],
        [ 0.0139, -0.0226, -0.0142,  ...,  0.0024,  0.0043, -0.0047],
        [-0.0326, -0.0159,  0.0169,  ...,  0.0024, -0.0045,  0.0004]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2090e-02, -5.6585e-03, -2.5566e-03,  ..., -3.7057e-04,
         -5.4874e-04,  3.4668e-04],
        [-5.6585e-03,  2.3053e-02, -5.3207e-03,  ..., -1.9675e-05,
          1.1602e-04, -2.3095e-04],
        [-2.5566e-03, -5.3207e-03,  2.1689e-02,  ..., -8.0486e-05,
         -5.6343e-05,  1.4335e-04],
        ...,
        [-3.7057e-04, -1.9675e-05, -8.0486e-05,  ...,  1.9520e-02,
         -6.4937e-03, -2.7879e-03],
        [-5.4874e-04,  1.1602e-04, -5.6343e-05,  ..., -6.4937e-03,
          2.0188e-02, -6.5644e-03],
        [ 3.4668e-04, -2.3095e-04,  1.4335e-04,  ..., -2.7879e-03,
         -6.5644e-03,  1.9717e-02]], device='cuda:0') 

reserving basis 453/1152; cond: 617043.1875, radio:0.003490856382995844
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0018, -0.0132, -0.0071],
          [ 0.0178,  0.0179,  0.0374],
          [-0.0112, -0.0057,  0.0162]],

         [[-0.0165,  0.0196, -0.0095],
          [-0.0121, -0.0234, -0.0255],
          [-0.0005, -0.0164, -0.0037]],

         [[-0.0198, -0.0137,  0.0316],
          [ 0.0376,  0.0242,  0.0006],
          [ 0.0056,  0.0067, -0.0062]],

         ...,

         [[ 0.0378,  0.0126, -0.0063],
          [ 0.0400,  0.0157,  0.0160],
          [-0.0159,  0.0012, -0.0153]],

         [[ 0.0431,  0.0050,  0.0289],
          [ 0.0308,  0.0130,  0.0196],
          [-0.0222, -0.0037,  0.0190]],

         [[ 0.0111, -0.0134, -0.0140],
          [ 0.0204,  0.0184,  0.0376],
          [-0.0145,  0.0149,  0.0006]]],


        [[[ 0.0199,  0.0362, -0.0261],
          [ 0.0242, -0.0036, -0.0274],
          [ 0.0092, -0.0252, -0.0185]],

         [[-0.0315, -0.0344, -0.0423],
          [-0.0196, -0.0232, -0.0072],
          [ 0.0181, -0.0138,  0.0056]],

         [[ 0.0116,  0.0197,  0.0287],
          [-0.0043,  0.0142, -0.0061],
          [ 0.0122, -0.0098, -0.0306]],

         ...,

         [[-0.0107,  0.0305, -0.0167],
          [ 0.0317, -0.0082,  0.0118],
          [ 0.0092,  0.0185, -0.0208]],

         [[-0.0250,  0.0190, -0.0183],
          [-0.0161, -0.0030, -0.0200],
          [ 0.0284, -0.0099,  0.0211]],

         [[ 0.0131, -0.0261,  0.0340],
          [-0.0194,  0.0228,  0.0015],
          [ 0.0160, -0.0003, -0.0071]]],


        [[[-0.0018, -0.0009,  0.0094],
          [-0.0206,  0.0061, -0.0174],
          [-0.0162,  0.0018, -0.0055]],

         [[ 0.0141, -0.0172, -0.0151],
          [ 0.0259, -0.0003,  0.0310],
          [ 0.0338,  0.0201, -0.0144]],

         [[-0.0152,  0.0104, -0.0249],
          [ 0.0068,  0.0177,  0.0026],
          [-0.0003, -0.0066,  0.0309]],

         ...,

         [[ 0.0189,  0.0235,  0.0163],
          [-0.0164, -0.0253,  0.0042],
          [ 0.0030,  0.0174, -0.0102]],

         [[ 0.0156,  0.0164,  0.0086],
          [ 0.0080, -0.0095, -0.0203],
          [ 0.0041,  0.0104,  0.0318]],

         [[-0.0127, -0.0148, -0.0044],
          [-0.0210,  0.0020,  0.0005],
          [-0.0026,  0.0064,  0.0405]]],


        ...,


        [[[-0.0099, -0.0246,  0.0117],
          [ 0.0192, -0.0072,  0.0167],
          [ 0.0031, -0.0156,  0.0243]],

         [[-0.0225, -0.0237, -0.0079],
          [-0.0194, -0.0182,  0.0018],
          [-0.0327,  0.0231, -0.0060]],

         [[-0.0049,  0.0040, -0.0071],
          [-0.0054,  0.0198,  0.0027],
          [ 0.0313,  0.0103,  0.0180]],

         ...,

         [[ 0.0005, -0.0121,  0.0391],
          [-0.0019,  0.0206,  0.0208],
          [ 0.0143,  0.0103,  0.0093]],

         [[-0.0211,  0.0018, -0.0056],
          [-0.0174,  0.0130,  0.0054],
          [ 0.0118, -0.0012,  0.0203]],

         [[-0.0044, -0.0062, -0.0096],
          [ 0.0055,  0.0229, -0.0177],
          [-0.0014, -0.0132, -0.0059]]],


        [[[-0.0085,  0.0387,  0.0111],
          [-0.0024, -0.0008, -0.0009],
          [-0.0238,  0.0237,  0.0229]],

         [[ 0.0160, -0.0088, -0.0004],
          [-0.0033,  0.0326,  0.0119],
          [ 0.0052, -0.0078, -0.0246]],

         [[-0.0094, -0.0146,  0.0064],
          [-0.0141, -0.0324, -0.0043],
          [ 0.0148, -0.0147,  0.0265]],

         ...,

         [[-0.0019,  0.0118, -0.0203],
          [ 0.0159,  0.0006,  0.0173],
          [ 0.0301,  0.0127,  0.0240]],

         [[ 0.0097, -0.0277,  0.0034],
          [-0.0173,  0.0011, -0.0200],
          [ 0.0180,  0.0199,  0.0246]],

         [[-0.0044, -0.0298, -0.0195],
          [ 0.0125,  0.0110,  0.0033],
          [ 0.0158,  0.0056, -0.0281]]],


        [[[ 0.0222, -0.0256,  0.0007],
          [ 0.0110,  0.0072, -0.0284],
          [ 0.0210,  0.0245,  0.0115]],

         [[-0.0415, -0.0171, -0.0308],
          [ 0.0002,  0.0129, -0.0017],
          [ 0.0041, -0.0263,  0.0106]],

         [[-0.0106,  0.0005, -0.0080],
          [-0.0250, -0.0161, -0.0220],
          [ 0.0023, -0.0108,  0.0141]],

         ...,

         [[-0.0147, -0.0279, -0.0284],
          [ 0.0111,  0.0016,  0.0010],
          [ 0.0190, -0.0157, -0.0153]],

         [[-0.0030, -0.0170, -0.0069],
          [ 0.0023,  0.0043,  0.0187],
          [ 0.0012,  0.0019,  0.0033]],

         [[ 0.0206,  0.0127,  0.0008],
          [ 0.0208, -0.0071,  0.0202],
          [-0.0143, -0.0146,  0.0151]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.4369e+07, 9.1775e+05, 8.1362e+05,  ..., 5.6451e+01, 4.4149e+01,
        3.9494e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 453]) 

NULL SPACE BASIS :  tensor([[ 0.0050,  0.0552,  0.0278,  ...,  0.0053,  0.0009, -0.0007],
        [ 0.0369, -0.0135, -0.0167,  ..., -0.0003, -0.0030, -0.0021],
        [ 0.0095,  0.0061, -0.0504,  ..., -0.0024, -0.0003,  0.0030],
        ...,
        [ 0.0204,  0.0125,  0.0007,  ..., -0.0132, -0.0016, -0.0011],
        [ 0.0221, -0.0095,  0.0036,  ...,  0.0085, -0.0027,  0.0028],
        [-0.0129,  0.0190, -0.0348,  ..., -0.0011, -0.0012, -0.0033]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0046, -0.0016, -0.0004,  ...,  0.0006, -0.0003,  0.0005],
        [-0.0016,  0.0064, -0.0017,  ..., -0.0001,  0.0003, -0.0007],
        [-0.0004, -0.0017,  0.0047,  ..., -0.0003, -0.0004, -0.0002],
        ...,
        [ 0.0006, -0.0001, -0.0003,  ...,  0.0308, -0.0075, -0.0026],
        [-0.0003,  0.0003, -0.0004,  ..., -0.0075,  0.0306, -0.0067],
        [ 0.0005, -0.0007, -0.0002,  ..., -0.0026, -0.0067,  0.0299]],
       device='cuda:0') 

reserving basis 768/1152; cond: 346275.78125, radio:0.008154201321303844
PARAMETER       :  Parameter containing:
tensor([[[[-1.9748e-02, -1.4368e-02,  4.2094e-02],
          [ 2.1797e-02,  8.4566e-03, -2.3302e-03],
          [ 1.6020e-02,  2.1027e-02, -2.7737e-02]],

         [[ 6.9373e-03,  1.1308e-03,  3.3261e-02],
          [-2.6344e-03,  1.6974e-02,  2.9794e-02],
          [ 2.7290e-02, -2.4033e-02, -6.9759e-03]],

         [[ 1.8040e-02,  5.7773e-03,  8.0654e-03],
          [-5.9097e-03, -8.4339e-03,  2.5514e-02],
          [ 1.0950e-02,  2.0081e-02,  2.6119e-02]],

         ...,

         [[ 7.2412e-03,  1.6059e-02,  2.6707e-02],
          [ 3.3247e-02, -2.0288e-02,  2.3811e-02],
          [-1.6537e-04, -1.4035e-02, -1.3198e-02]],

         [[-7.7260e-03, -7.1038e-03, -1.8920e-02],
          [-2.3224e-03,  3.8169e-03,  5.1777e-03],
          [ 1.2903e-03, -8.7150e-04, -7.6549e-03]],

         [[-3.7832e-02,  8.2264e-03, -1.8061e-02],
          [ 1.0349e-02,  2.4090e-03,  1.7467e-02],
          [ 2.6296e-02, -4.5330e-03,  2.3700e-02]]],


        [[[ 5.3664e-03, -3.2505e-03,  1.5392e-02],
          [ 2.0475e-02,  1.2627e-02, -1.7803e-03],
          [-3.1023e-03, -1.0289e-02, -3.1351e-03]],

         [[ 3.1485e-03,  1.1916e-02,  1.6786e-02],
          [-2.5469e-02, -2.5815e-03, -2.5241e-02],
          [-1.3642e-02,  1.9053e-02, -2.4720e-03]],

         [[-1.8186e-02, -6.4649e-03,  2.7265e-02],
          [ 3.3650e-03, -1.1620e-02, -1.1860e-02],
          [ 5.4605e-04,  1.3035e-02, -1.3490e-02]],

         ...,

         [[-2.0446e-02, -1.6321e-02, -2.1727e-02],
          [-2.6252e-02,  7.6007e-03, -2.0902e-02],
          [ 3.2572e-02,  1.9375e-03, -3.4973e-02]],

         [[ 1.7756e-02,  2.6553e-02,  3.4458e-02],
          [-1.4308e-02,  1.9365e-02, -9.8392e-03],
          [ 2.2165e-02,  1.9288e-02,  4.0430e-02]],

         [[ 2.6528e-04,  3.7091e-03,  3.9129e-04],
          [-4.2496e-03, -8.7920e-03,  2.0827e-02],
          [ 5.7562e-03,  4.6707e-02,  4.1781e-02]]],


        [[[-2.1024e-02, -2.1066e-02,  3.9962e-03],
          [-2.2946e-02,  4.0445e-03,  9.0166e-04],
          [ 5.8070e-03,  3.1171e-03,  4.7970e-03]],

         [[-2.3759e-03,  7.9103e-03,  9.2341e-03],
          [-3.4723e-04,  2.2601e-02,  6.8285e-03],
          [ 2.4628e-03,  3.4268e-02, -1.9018e-02]],

         [[-4.7416e-03, -1.8725e-02, -2.2706e-02],
          [-2.2445e-02, -2.9967e-02, -6.1511e-03],
          [-7.3542e-03, -2.7820e-02, -1.0740e-02]],

         ...,

         [[ 1.2595e-02,  1.0740e-02,  4.1701e-02],
          [-2.2184e-02,  8.0448e-03, -1.6457e-02],
          [-1.8882e-02,  2.8671e-02, -2.3118e-02]],

         [[-3.6387e-02, -1.0902e-02, -2.1579e-03],
          [ 6.2446e-03, -1.1163e-02,  7.8755e-03],
          [-9.5537e-03,  1.7264e-02, -2.6682e-02]],

         [[ 2.5817e-03, -8.7408e-03, -1.5051e-02],
          [-2.9740e-02,  3.6655e-03,  2.3886e-02],
          [-2.1397e-02,  1.0609e-02,  1.8869e-02]]],


        ...,


        [[[-1.2861e-03,  1.0717e-02, -2.0448e-02],
          [-1.9163e-02, -1.3798e-02, -2.3614e-02],
          [-4.3988e-03, -1.5878e-02, -2.3693e-02]],

         [[ 3.1114e-02,  2.9893e-03,  7.3808e-03],
          [-8.0740e-03,  5.9819e-03, -2.3357e-02],
          [-3.5158e-02, -3.9027e-02, -1.6427e-02]],

         [[ 8.8989e-03,  1.1667e-02, -2.1260e-03],
          [-5.4435e-03, -1.6637e-02,  3.4209e-03],
          [-4.1726e-03,  3.6126e-02,  2.3181e-02]],

         ...,

         [[ 2.0134e-02, -7.3417e-03,  8.8767e-03],
          [-1.2940e-02, -2.3117e-02,  2.0401e-02],
          [ 1.2081e-02, -2.5823e-02, -1.7320e-02]],

         [[-7.8558e-03, -1.2010e-02,  2.9407e-02],
          [-3.5452e-02,  2.5655e-02,  1.5916e-02],
          [-1.8411e-02,  4.5873e-03,  1.5765e-02]],

         [[-2.0743e-02, -2.0232e-02,  4.1964e-03],
          [-2.4115e-03, -1.8741e-02, -2.3940e-02],
          [-5.0127e-03,  1.3986e-02, -1.8427e-02]]],


        [[[-3.0502e-02,  8.1453e-03,  1.0108e-02],
          [-8.7286e-03, -1.5550e-02, -2.5123e-02],
          [-2.4870e-02, -9.9713e-03,  9.7740e-05]],

         [[ 1.0170e-02,  1.8187e-02,  1.2255e-02],
          [-1.8211e-02, -2.1601e-02, -2.3039e-02],
          [-8.9783e-03, -8.2683e-03, -1.7726e-02]],

         [[-2.7752e-02, -2.0118e-02, -1.8662e-02],
          [ 1.8797e-02,  2.2004e-02,  3.0340e-02],
          [ 1.7342e-02, -1.6715e-02, -1.7170e-02]],

         ...,

         [[-3.6454e-02,  1.4795e-02,  1.0689e-02],
          [-3.1914e-02,  1.1924e-03, -5.7243e-03],
          [-2.3335e-03,  1.3858e-03,  1.1319e-02]],

         [[ 2.0595e-03, -1.2753e-02, -3.5690e-04],
          [ 4.0682e-03, -2.9961e-02, -1.2397e-03],
          [ 5.8531e-03, -4.7920e-03,  1.0041e-02]],

         [[-3.6132e-03,  6.2106e-03,  4.0198e-03],
          [-1.0994e-02,  8.4349e-04, -2.1718e-02],
          [ 5.7305e-03,  9.8352e-03, -1.7989e-02]]],


        [[[ 6.8353e-03, -7.3422e-03,  1.8056e-02],
          [ 1.2817e-02, -2.0081e-02,  9.2999e-03],
          [ 7.2215e-03,  2.0657e-02,  4.5574e-03]],

         [[ 3.1552e-02,  6.0479e-03,  9.7507e-03],
          [-9.6388e-03,  3.1225e-02,  1.8020e-02],
          [ 8.9042e-03,  1.6309e-03,  1.4294e-02]],

         [[ 6.6343e-03, -1.2919e-02,  4.4014e-03],
          [-7.1136e-03, -3.5672e-02, -1.9040e-02],
          [-3.6393e-02, -1.2233e-02,  4.3914e-03]],

         ...,

         [[ 8.8151e-03, -3.2212e-02,  1.3340e-02],
          [-3.6543e-02,  1.7636e-02,  2.0316e-03],
          [-1.5878e-02,  5.3753e-03, -1.2339e-02]],

         [[ 1.6066e-02, -2.3157e-03,  1.3255e-03],
          [ 2.7240e-03,  2.7749e-02,  2.8108e-02],
          [ 9.1692e-03,  1.5014e-02,  2.4555e-02]],

         [[ 2.1055e-02,  3.0599e-02,  2.2398e-02],
          [-6.0440e-03,  7.0541e-03, -5.4927e-03],
          [-2.1455e-02,  8.1420e-03, -1.5576e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.9860e+06, 3.5219e+05, 3.3458e+05,  ..., 2.2127e+01, 2.0199e+01,
        1.7287e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 768]) 

NULL SPACE BASIS :  tensor([[-1.0101e-02,  6.1366e-03,  7.4573e-03,  ..., -3.8745e-04,
          1.7788e-02, -7.1219e-03],
        [-3.2481e-02, -3.5740e-03,  6.5796e-03,  ...,  1.0971e-03,
         -1.0933e-02,  5.0877e-03],
        [ 3.2231e-02,  2.9937e-02, -1.3620e-02,  ..., -2.0618e-03,
          3.3572e-03,  1.8518e-03],
        ...,
        [ 4.4141e-02,  2.5287e-02,  5.4002e-02,  ..., -9.7203e-03,
          8.3331e-03,  2.3672e-03],
        [ 2.8449e-02, -1.5008e-03,  4.2056e-02,  ..., -6.1303e-04,
         -2.2712e-02, -6.7002e-05],
        [ 7.4879e-03,  1.5228e-02, -2.6647e-02,  ..., -6.0354e-04,
          3.7762e-03, -7.4324e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.4283e-02, -4.5354e-03, -1.6269e-03,  ..., -2.7463e-04,
         -1.7050e-04,  2.2598e-04],
        [-4.5354e-03,  2.4114e-02, -4.9708e-03,  ..., -2.1206e-04,
         -2.6709e-04,  3.8970e-05],
        [-1.6269e-03, -4.9708e-03,  2.3809e-02,  ..., -4.6831e-04,
          1.8433e-07, -2.0350e-04],
        ...,
        [-2.7463e-04, -2.1206e-04, -4.6831e-04,  ...,  2.3540e-02,
         -6.6506e-03, -2.7014e-03],
        [-1.7050e-04, -2.6709e-04,  1.8433e-07,  ..., -6.6506e-03,
          2.2807e-02, -6.8717e-03],
        [ 2.2598e-04,  3.8970e-05, -2.0350e-04,  ..., -2.7014e-03,
         -6.8717e-03,  2.1487e-02]], device='cuda:0') 

reserving basis 1514/2304; cond: 639964.8125, radio:0.008137203752994537
PARAMETER       :  Parameter containing:
tensor([[[[ 3.5762e-03, -4.8517e-03, -1.5287e-04],
          [-2.4166e-02, -7.3836e-03,  1.3906e-02],
          [-2.5098e-02, -6.7010e-03, -1.7273e-02]],

         [[ 6.7734e-03,  2.1512e-02,  2.5223e-03],
          [-6.2080e-03, -1.8740e-02,  2.0248e-02],
          [-1.1470e-02, -2.1431e-03,  1.3035e-02]],

         [[-1.7237e-02, -1.6206e-02, -1.1122e-02],
          [-1.2453e-03, -1.4644e-02, -1.9968e-02],
          [ 6.5805e-03,  8.6540e-03, -1.1605e-02]],

         ...,

         [[ 1.4318e-02,  1.1083e-02,  1.5135e-02],
          [-1.6668e-02, -1.4093e-02,  9.9643e-03],
          [ 2.6415e-03, -5.1031e-04,  1.1178e-03]],

         [[ 2.1517e-02,  7.2337e-04,  1.4813e-02],
          [-1.3957e-02,  5.7584e-04,  2.3825e-02],
          [ 2.0667e-02, -1.9450e-03,  8.6210e-03]],

         [[ 7.9485e-03,  8.5145e-03, -2.0830e-02],
          [ 6.1312e-03, -3.6807e-03,  2.6446e-03],
          [ 2.3978e-02,  5.6767e-03,  1.1704e-02]]],


        [[[ 8.9760e-03, -4.9043e-03, -1.9979e-03],
          [ 5.1724e-03,  7.5352e-03, -9.4565e-03],
          [ 2.3630e-02, -9.7470e-03, -1.3752e-02]],

         [[-1.0738e-02,  1.1391e-02, -1.5427e-03],
          [-1.8917e-02,  2.1975e-02,  1.2235e-02],
          [ 9.7902e-04, -5.3305e-03,  1.1315e-04]],

         [[-1.6073e-02, -1.4449e-03,  1.1779e-02],
          [ 2.4454e-02,  4.6802e-03,  6.8373e-03],
          [-7.0060e-03,  2.2750e-02, -1.2584e-02]],

         ...,

         [[-1.2274e-02,  2.2235e-02,  1.6393e-03],
          [ 2.1347e-02, -1.3693e-02,  1.5768e-02],
          [ 2.9419e-03, -1.9419e-02, -1.2439e-02]],

         [[-1.5286e-02, -2.6278e-02, -5.7504e-03],
          [-8.4377e-03,  3.7890e-03, -5.5643e-03],
          [-1.5260e-02,  1.6639e-02,  1.8549e-02]],

         [[ 7.8701e-03, -5.1402e-03, -1.9785e-02],
          [-7.5223e-03, -1.3660e-02,  6.9598e-03],
          [-1.9279e-02,  9.7335e-03, -6.2927e-03]]],


        [[[ 1.6429e-02, -2.2018e-02, -1.2135e-02],
          [ 1.6412e-02,  1.8650e-02, -2.4966e-03],
          [ 1.7214e-02,  2.1586e-02, -2.4973e-02]],

         [[-1.0936e-03,  1.2591e-02,  2.1585e-03],
          [-1.0388e-02, -1.3417e-02, -1.9881e-02],
          [-8.0198e-03,  2.3205e-02,  1.5698e-03]],

         [[-4.4018e-03, -1.1179e-02,  3.5465e-04],
          [-5.2430e-03, -4.9166e-03,  9.9028e-03],
          [ 9.1188e-06, -1.3318e-02,  9.6984e-03]],

         ...,

         [[-1.6279e-02,  2.2222e-02, -7.7078e-03],
          [ 1.7199e-02, -1.5987e-02, -3.5953e-03],
          [-1.7122e-02, -1.3989e-02, -2.1728e-02]],

         [[-3.2506e-03,  1.8776e-02, -1.1602e-02],
          [-1.7365e-02, -2.1127e-02, -2.0197e-02],
          [ 1.8344e-02, -1.8491e-02, -9.9116e-03]],

         [[ 8.3649e-03,  3.1313e-03, -1.9553e-02],
          [ 2.5645e-03, -4.5579e-03,  1.1056e-02],
          [-9.6400e-03,  9.4775e-05, -8.8426e-03]]],


        ...,


        [[[ 7.6089e-03,  1.8267e-02,  7.6321e-03],
          [ 1.9722e-02, -1.5490e-02, -1.5521e-02],
          [-4.6469e-03, -1.3529e-02, -1.1446e-02]],

         [[-1.5686e-02, -1.0864e-02,  2.9335e-02],
          [-3.9903e-04,  8.9071e-03, -3.8439e-03],
          [ 2.4045e-02, -1.9801e-02,  1.0884e-02]],

         [[-1.6479e-02, -1.1777e-04, -1.3696e-02],
          [ 5.4001e-03,  2.6644e-02, -2.4097e-02],
          [ 1.0221e-02,  3.2143e-02,  1.3399e-02]],

         ...,

         [[-6.6085e-03,  1.1169e-02,  1.9074e-03],
          [-1.3221e-02, -1.1608e-02,  6.2319e-03],
          [ 5.7540e-04,  1.1070e-03, -2.9069e-02]],

         [[-1.2042e-02,  2.3002e-03, -1.1430e-02],
          [-1.6390e-02, -1.0434e-02, -1.7938e-03],
          [-8.4206e-03,  3.9640e-03,  2.0335e-02]],

         [[-2.5770e-02, -4.1677e-02, -2.7759e-02],
          [ 1.0335e-02, -2.7244e-02, -1.9481e-02],
          [ 1.2424e-02, -1.0992e-02,  1.0344e-02]]],


        [[[-8.8883e-03,  1.6466e-02,  1.1908e-03],
          [-1.2464e-02, -1.9723e-02,  3.9758e-03],
          [ 1.0881e-02, -3.4806e-03, -1.3759e-02]],

         [[-3.3491e-03,  5.9232e-03, -3.9953e-03],
          [ 5.4741e-03, -1.4839e-02,  1.4941e-02],
          [ 9.0742e-03, -1.3457e-02, -1.5654e-02]],

         [[-2.1737e-02, -2.9062e-02, -2.0888e-02],
          [ 2.4862e-02,  1.7012e-02, -7.3046e-04],
          [-3.4661e-03,  8.6496e-03, -5.3839e-03]],

         ...,

         [[ 7.7761e-03, -1.4211e-02, -3.0676e-02],
          [ 1.5678e-02,  1.4171e-02, -2.3822e-02],
          [-1.5291e-02, -2.3676e-02,  1.2528e-02]],

         [[ 4.9917e-03,  1.1976e-03,  1.0921e-02],
          [-1.4163e-02, -8.2254e-03, -1.0293e-02],
          [-8.2141e-03,  8.3476e-03,  4.6737e-03]],

         [[-2.4010e-02,  1.6509e-02,  1.7018e-02],
          [-2.6713e-02, -9.7462e-03,  2.5943e-02],
          [-1.5456e-02,  3.6675e-02,  4.0145e-02]]],


        [[[-2.0672e-03, -3.0791e-02,  9.3520e-03],
          [-1.5800e-02, -8.2875e-03,  2.7402e-02],
          [-2.0382e-02, -9.0560e-03,  9.0254e-03]],

         [[-1.9342e-02, -1.6728e-02,  1.3680e-02],
          [-1.0273e-02,  7.2386e-03, -6.6802e-03],
          [ 5.1146e-03,  1.1990e-02,  1.0770e-03]],

         [[ 9.8553e-03, -6.1967e-03, -5.7903e-03],
          [-1.1662e-02,  2.3062e-02, -3.1822e-03],
          [-2.1467e-02,  1.1184e-03, -5.3425e-03]],

         ...,

         [[-2.8544e-03, -2.8720e-02, -1.3146e-03],
          [-2.3659e-02, -2.6135e-02,  7.8224e-03],
          [ 1.1774e-02, -1.4244e-02,  1.9054e-02]],

         [[ 9.9592e-03,  3.0845e-03,  1.7816e-02],
          [ 6.8440e-03, -1.2200e-02, -5.8837e-04],
          [-2.2538e-02,  1.4357e-02, -2.6200e-02]],

         [[-2.4811e-02,  8.8733e-03, -3.4134e-02],
          [ 1.0363e-02, -1.3700e-02, -5.8340e-03],
          [ 1.3038e-02, -2.4976e-02, -1.9338e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([8.7253e+06, 5.2842e+05, 4.9548e+05,  ..., 1.4891e+01, 1.4097e+01,
        1.3634e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1514]) 

NULL SPACE BASIS :  tensor([[-0.0508, -0.0299,  0.0417,  ..., -0.0052,  0.0043, -0.0014],
        [-0.0175,  0.0009, -0.0027,  ..., -0.0039,  0.0012, -0.0032],
        [ 0.0175,  0.0115, -0.0295,  ...,  0.0059,  0.0023,  0.0027],
        ...,
        [ 0.0124, -0.0204,  0.0169,  ...,  0.0064, -0.0035,  0.0134],
        [ 0.0006, -0.0242, -0.0037,  ..., -0.0056, -0.0007, -0.0098],
        [-0.0061, -0.0103, -0.0051,  ..., -0.0061,  0.0002,  0.0072]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.7048e-02, -5.7223e-04, -4.2905e-04,  ..., -1.1477e-05,
         -5.6091e-05,  1.4450e-04],
        [-5.7223e-04,  1.6645e-02, -7.7236e-04,  ...,  1.9974e-05,
         -4.7979e-05, -4.5216e-05],
        [-4.2905e-04, -7.7236e-04,  1.7097e-02,  ...,  1.1708e-04,
          4.2237e-05, -6.0621e-05],
        ...,
        [-1.1477e-05,  1.9974e-05,  1.1708e-04,  ...,  2.0309e-02,
         -1.8143e-04,  7.4819e-06],
        [-5.6091e-05, -4.7979e-05,  4.2237e-05,  ..., -1.8143e-04,
          1.9937e-02, -2.4301e-04],
        [ 1.4450e-04, -4.5216e-05, -6.0621e-05,  ...,  7.4819e-06,
         -2.4301e-04,  2.0580e-02]], device='cuda:0') 

reserving basis 98/128; cond: 12703.240234375, radio:0.021180521696805954
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0198]],

         [[-0.0354]],

         [[-0.0298]],

         ...,

         [[-0.0372]],

         [[-0.0703]],

         [[ 0.0819]]],


        [[[-0.0455]],

         [[ 0.0609]],

         [[ 0.0744]],

         ...,

         [[-0.0508]],

         [[ 0.0017]],

         [[-0.0181]]],


        [[[ 0.0587]],

         [[-0.0232]],

         [[-0.0302]],

         ...,

         [[ 0.0556]],

         [[ 0.0150]],

         [[-0.0226]]],


        ...,


        [[[-0.0670]],

         [[ 0.0258]],

         [[ 0.0779]],

         ...,

         [[-0.0775]],

         [[-0.0544]],

         [[ 0.0309]]],


        [[[-0.0279]],

         [[-0.0716]],

         [[ 0.0053]],

         ...,

         [[ 0.0317]],

         [[ 0.0485]],

         [[ 0.0250]]],


        [[[ 0.0409]],

         [[-0.0363]],

         [[-0.0647]],

         ...,

         [[-0.0502]],

         [[ 0.0641]],

         [[-0.0344]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([8.4094e+05, 4.4668e+04, 3.6974e+04, 1.5021e+04, 9.2156e+03, 7.7659e+03,
        6.7721e+03, 4.9685e+03, 4.1292e+03, 4.0491e+03, 3.7426e+03, 2.7635e+03,
        2.4975e+03, 2.1363e+03, 1.8005e+03, 1.7516e+03, 1.4797e+03, 1.4231e+03,
        1.2996e+03, 1.1776e+03, 1.1422e+03, 1.0739e+03, 9.8787e+02, 9.5464e+02,
        8.8671e+02, 8.6641e+02, 7.7067e+02, 7.5647e+02, 6.9197e+02, 6.8152e+02,
        6.1397e+02, 6.0565e+02, 5.6987e+02, 5.5458e+02, 5.2624e+02, 5.1694e+02,
        4.8648e+02, 4.5986e+02, 4.3963e+02, 4.3458e+02, 4.2243e+02, 4.0593e+02,
        4.0367e+02, 3.9054e+02, 3.7450e+02, 3.5614e+02, 3.4926e+02, 3.4501e+02,
        3.3247e+02, 3.2528e+02, 3.1329e+02, 3.0848e+02, 2.9291e+02, 2.8740e+02,
        2.8489e+02, 2.7938e+02, 2.6698e+02, 2.6162e+02, 2.5636e+02, 2.5364e+02,
        2.4713e+02, 2.4129e+02, 2.3747e+02, 2.3167e+02, 2.2833e+02, 2.2645e+02,
        2.2209e+02, 2.1690e+02, 2.1211e+02, 2.0809e+02, 2.0592e+02, 2.0196e+02,
        1.9605e+02, 1.9317e+02, 1.9249e+02, 1.8501e+02, 1.8151e+02, 1.7828e+02,
        1.7459e+02, 1.7281e+02, 1.7009e+02, 1.6780e+02, 1.6524e+02, 1.6324e+02,
        1.5998e+02, 1.5733e+02, 1.5634e+02, 1.5191e+02, 1.5157e+02, 1.4789e+02,
        1.4724e+02, 1.4491e+02, 1.4194e+02, 1.4000e+02, 1.3904e+02, 1.3734e+02,
        1.3424e+02, 1.3375e+02, 1.3178e+02, 1.3077e+02, 1.2733e+02, 1.2561e+02,
        1.2305e+02, 1.2249e+02, 1.2125e+02, 1.1876e+02, 1.1669e+02, 1.1583e+02,
        1.1491e+02, 1.1254e+02, 1.0905e+02, 1.0774e+02, 1.0485e+02, 1.0392e+02,
        9.9844e+01, 9.7254e+01, 9.6904e+01, 9.5647e+01, 9.4921e+01, 9.4027e+01,
        9.1718e+01, 8.8989e+01, 8.8587e+01, 8.6881e+01, 8.5247e+01, 8.2044e+01,
        7.6250e+01, 6.6199e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 98]) 

NULL SPACE BASIS :  tensor([[ 0.1157,  0.1703, -0.0515,  ...,  0.0601, -0.0337,  0.0545],
        [-0.0172, -0.1350,  0.0363,  ..., -0.1578,  0.0250, -0.0242],
        [-0.2191, -0.0479,  0.0712,  ..., -0.0760, -0.1917, -0.1052],
        ...,
        [-0.0827, -0.0160, -0.0744,  ..., -0.0192,  0.1093, -0.0160],
        [ 0.0868,  0.0336, -0.0966,  ...,  0.0743, -0.0128,  0.0485],
        [-0.0683, -0.0417,  0.0185,  ...,  0.0708, -0.0337,  0.0782]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0786,  0.0009,  0.0005,  ..., -0.0046,  0.0028, -0.0088],
        [ 0.0009,  0.0658,  0.0012,  ..., -0.0017, -0.0048,  0.0080],
        [ 0.0005,  0.0012,  0.0868,  ...,  0.0005, -0.0012, -0.0033],
        ...,
        [-0.0046, -0.0017,  0.0005,  ...,  0.0765,  0.0010, -0.0069],
        [ 0.0028, -0.0048, -0.0012,  ...,  0.0010,  0.0785, -0.0013],
        [-0.0088,  0.0080, -0.0033,  ..., -0.0069, -0.0013,  0.0702]],
       device='cuda:0') 

reserving basis 1380/2304; cond: 688766.9375, radio:0.007258001249283552
PARAMETER       :  Parameter containing:
tensor([[[[ 5.2354e-03,  1.3873e-03, -1.0522e-02],
          [ 5.1090e-03, -2.0080e-02,  6.4617e-04],
          [-6.3800e-03,  2.1046e-02, -5.6722e-03]],

         [[-1.9924e-03,  1.2569e-02,  2.2565e-03],
          [ 2.6433e-02, -6.3119e-03, -2.1034e-02],
          [-3.2274e-03,  5.9921e-03,  1.1009e-02]],

         [[ 1.5015e-02,  2.6999e-02, -1.3454e-02],
          [ 1.4080e-02,  1.0282e-02,  1.2285e-02],
          [-4.2020e-03,  9.3041e-03,  5.2095e-03]],

         ...,

         [[-5.1999e-04,  2.0564e-02,  2.7549e-03],
          [-2.7657e-02, -5.5517e-03, -3.1239e-02],
          [-7.3423e-04, -1.6429e-02, -2.6691e-03]],

         [[-2.3921e-02,  1.3989e-02,  9.8156e-03],
          [ 4.5108e-03, -2.5899e-02, -1.8742e-02],
          [-1.9246e-02, -1.4963e-02, -1.5381e-02]],

         [[ 1.2657e-03, -1.4302e-03, -2.1768e-02],
          [ 7.6168e-03,  1.1200e-02,  4.9518e-03],
          [ 2.2755e-02,  2.0089e-02, -1.3646e-02]]],


        [[[-5.2890e-03,  2.3027e-02,  3.0530e-02],
          [ 2.0371e-03, -1.9009e-02,  1.5928e-02],
          [ 6.6274e-03,  1.7770e-02, -2.5180e-02]],

         [[-1.3868e-02, -2.0288e-03, -3.9153e-02],
          [-3.4263e-04, -5.8340e-03,  8.4903e-03],
          [ 1.6819e-03,  1.8685e-02,  2.5502e-02]],

         [[-7.6767e-03, -2.2989e-02,  1.5377e-02],
          [-1.0602e-02,  5.0165e-04, -1.4404e-02],
          [ 6.8436e-03,  7.5422e-03,  1.9599e-02]],

         ...,

         [[-1.2072e-02, -2.6294e-02, -1.0994e-02],
          [-8.7882e-03,  6.0774e-03,  1.7199e-02],
          [ 3.5581e-03,  1.3837e-02, -8.2949e-03]],

         [[ 1.7453e-03, -1.7076e-02, -9.8641e-04],
          [ 1.1035e-02,  1.7854e-02,  2.2287e-02],
          [-1.6207e-02, -1.9130e-02,  1.9463e-02]],

         [[ 6.3440e-03, -1.2259e-02, -2.8955e-02],
          [-7.1497e-03,  5.4806e-03,  3.8536e-03],
          [-1.3127e-02, -2.5580e-02, -1.3327e-02]]],


        [[[-1.3567e-02,  2.5094e-04,  3.4109e-03],
          [ 1.2299e-02, -2.1002e-02, -1.7018e-02],
          [-1.0035e-02,  2.1227e-02,  3.6145e-03]],

         [[-1.6524e-02,  7.9007e-03,  3.8732e-03],
          [-1.0754e-02, -3.8613e-03,  1.9081e-02],
          [-1.7866e-02,  1.6308e-02,  2.0907e-03]],

         [[ 2.0840e-02, -9.4343e-03,  1.9046e-02],
          [-1.5295e-02, -3.7547e-03,  2.5456e-02],
          [-1.7626e-02, -1.2584e-02,  2.2005e-02]],

         ...,

         [[ 1.6374e-02, -6.0909e-03, -1.7328e-02],
          [ 1.1478e-02, -2.9006e-02, -3.3160e-03],
          [ 8.7819e-03, -2.8873e-02, -3.2960e-02]],

         [[-8.5994e-03,  1.0070e-03, -1.3963e-02],
          [ 1.3445e-02,  6.3347e-03,  4.1228e-03],
          [ 2.5167e-02, -3.1414e-02, -8.6207e-03]],

         [[-2.6540e-03, -1.2103e-02, -1.1891e-02],
          [-1.0509e-02, -1.5061e-02,  2.8174e-03],
          [ 1.6727e-02,  3.0233e-02,  1.8021e-02]]],


        ...,


        [[[-2.0061e-02,  1.1510e-02, -2.7020e-02],
          [ 4.5169e-03, -1.1237e-02, -2.9681e-02],
          [ 1.0409e-02,  5.9675e-04, -5.6523e-03]],

         [[-1.0395e-02, -2.4242e-02, -2.8493e-03],
          [ 1.3785e-02,  1.6037e-02,  6.6446e-03],
          [-1.7014e-02, -2.5421e-02, -1.2771e-02]],

         [[ 4.3643e-03, -1.7160e-02,  1.2535e-02],
          [-2.0016e-02,  2.9810e-04,  2.1257e-02],
          [ 1.6130e-02,  5.2282e-03, -1.6721e-02]],

         ...,

         [[ 1.5567e-02, -3.7393e-02,  1.2374e-02],
          [-1.3041e-02, -2.0477e-03,  6.0496e-03],
          [-1.3466e-02, -2.3733e-02,  3.4818e-03]],

         [[ 1.5390e-02, -1.3208e-03,  2.6881e-03],
          [-1.4890e-02,  1.1599e-02,  9.1062e-04],
          [-2.1505e-02,  1.9836e-03,  3.5625e-03]],

         [[-2.0704e-02, -1.2567e-02, -2.4195e-02],
          [ 8.2500e-03,  1.2392e-02, -4.9699e-03],
          [-2.2446e-02,  2.5047e-02,  3.3055e-03]]],


        [[[ 7.9398e-03, -4.8663e-03,  3.6095e-02],
          [ 6.1731e-03,  1.9146e-02,  1.1646e-02],
          [ 2.6300e-02, -5.2252e-03,  3.4517e-02]],

         [[ 4.6015e-03,  2.0816e-02, -1.7058e-02],
          [-1.3496e-02,  3.7994e-03,  2.2307e-02],
          [ 3.9283e-03,  1.0172e-02,  2.7408e-02]],

         [[-1.2241e-02,  2.8331e-03,  1.4770e-02],
          [-1.4005e-02,  8.0783e-03, -2.2476e-02],
          [ 2.7058e-03,  1.8186e-02,  1.0198e-02]],

         ...,

         [[ 1.0259e-02,  1.2833e-02, -1.3834e-02],
          [ 1.3784e-02,  1.7741e-02, -1.7855e-02],
          [ 5.6005e-04,  1.8383e-02,  3.1694e-02]],

         [[ 4.0756e-03, -1.0076e-02,  1.9105e-02],
          [ 1.0376e-02,  1.6128e-02,  2.0843e-02],
          [-1.3096e-02,  7.9744e-03,  1.4221e-02]],

         [[-1.3393e-02, -9.9582e-03,  4.2139e-03],
          [ 6.8380e-03, -4.5058e-03,  2.7330e-03],
          [-1.6663e-02, -3.2379e-03,  1.8488e-02]]],


        [[[ 2.8890e-02,  4.4465e-03, -6.3902e-03],
          [ 8.9818e-03, -4.6817e-03,  8.1531e-03],
          [-2.2926e-03, -1.2494e-02, -7.9111e-03]],

         [[-1.0104e-02, -6.5502e-03,  1.4513e-02],
          [ 1.7577e-02, -1.8581e-03,  2.4392e-02],
          [-2.3361e-03,  2.5489e-02,  1.0423e-02]],

         [[-1.2044e-02, -2.1153e-02, -3.5410e-03],
          [-1.3661e-02, -2.6953e-02, -1.3692e-02],
          [ 1.6188e-03,  9.1127e-04,  4.6710e-03]],

         ...,

         [[ 9.7775e-03,  7.7148e-03,  1.7395e-02],
          [ 1.5573e-02,  1.3810e-02, -1.2356e-02],
          [-1.0433e-03, -1.3747e-02, -1.5452e-02]],

         [[ 1.3067e-02, -1.0757e-02, -1.3990e-02],
          [-9.5056e-04, -2.3368e-02, -1.4820e-02],
          [-5.6713e-05, -2.1224e-02, -2.0220e-02]],

         [[ 4.6003e-03, -3.5381e-03, -1.1853e-02],
          [-1.3839e-03, -8.9571e-03, -1.6066e-02],
          [ 2.0866e-02,  2.4041e-03,  1.3731e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([7.5061e+06, 5.0727e+05, 4.8917e+05,  ..., 1.3262e+01, 1.2880e+01,
        1.0898e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1380]) 

NULL SPACE BASIS :  tensor([[-0.0002, -0.0289,  0.0130,  ..., -0.0035,  0.0036, -0.0030],
        [-0.0153, -0.0264, -0.0331,  ...,  0.0057,  0.0007,  0.0056],
        [ 0.0276, -0.0243,  0.0217,  ..., -0.0059,  0.0008, -0.0021],
        ...,
        [ 0.0004,  0.0100,  0.0096,  ...,  0.0238, -0.0214, -0.0013],
        [ 0.0154,  0.0389,  0.0156,  ..., -0.0163,  0.0247, -0.0005],
        [-0.0057,  0.0015, -0.0018,  ..., -0.0024, -0.0068, -0.0009]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.2312e-02, -4.3380e-03, -5.8892e-04,  ...,  1.3495e-04,
         -2.2796e-04,  1.3980e-04],
        [-4.3380e-03,  1.2962e-02, -4.3995e-03,  ..., -4.1177e-06,
          2.9597e-04, -1.2888e-04],
        [-5.8892e-04, -4.3995e-03,  1.2095e-02,  ...,  1.1936e-04,
          1.0198e-04,  2.5187e-04],
        ...,
        [ 1.3495e-04, -4.1177e-06,  1.1936e-04,  ...,  2.1125e-02,
         -6.4301e-04, -5.0128e-04],
        [-2.2796e-04,  2.9597e-04,  1.0198e-04,  ..., -6.4301e-04,
          2.0669e-02, -5.9233e-04],
        [ 1.3980e-04, -1.2888e-04,  2.5187e-04,  ..., -5.0128e-04,
         -5.9233e-04,  2.0469e-02]], device='cuda:0') 

reserving basis 1053/2304; cond: 1080024.125, radio:0.0035183229483664036
PARAMETER       :  Parameter containing:
tensor([[[[-1.3348e-02, -6.5004e-03, -1.5396e-02],
          [-4.1496e-03, -2.9867e-02,  6.1648e-03],
          [-1.5156e-02, -1.3179e-02, -4.4289e-03]],

         [[ 2.3481e-02,  2.6974e-02,  2.9146e-02],
          [-1.0704e-02,  1.2246e-02, -1.9073e-02],
          [-8.7324e-03,  2.4071e-02,  2.6017e-02]],

         [[-4.3954e-03, -1.1487e-02, -2.7606e-02],
          [-1.4070e-02, -1.7409e-02, -3.2604e-03],
          [ 1.4913e-02, -2.1482e-02, -1.5676e-02]],

         ...,

         [[-1.8052e-02,  2.2340e-02, -1.9031e-02],
          [ 4.8352e-03,  2.5601e-03, -2.2865e-02],
          [ 3.8005e-04,  2.2663e-03,  9.1409e-03]],

         [[ 2.0312e-02,  1.9740e-03, -2.0384e-02],
          [-1.2417e-02,  2.8230e-03, -8.7390e-03],
          [-2.6774e-02,  1.5414e-02,  1.0017e-02]],

         [[ 4.0745e-03,  2.5447e-02,  7.4719e-03],
          [-1.0559e-02,  1.4555e-02, -1.7940e-02],
          [ 9.1683e-03, -5.0337e-03,  1.5853e-02]]],


        [[[ 1.2074e-03,  2.0687e-02,  3.3207e-02],
          [-2.4118e-02,  1.9511e-02,  8.0952e-03],
          [ 3.8099e-03, -4.2450e-03,  7.7222e-03]],

         [[ 2.0608e-02,  1.6524e-02, -2.0399e-02],
          [ 1.2253e-02,  3.4492e-03, -3.5462e-03],
          [-2.9224e-02, -1.1409e-02, -1.7233e-02]],

         [[ 2.4715e-03, -2.5619e-03,  1.0008e-02],
          [ 4.2048e-03, -2.6351e-02, -1.6592e-02],
          [ 1.5218e-02, -8.7903e-03,  1.6241e-02]],

         ...,

         [[-1.8550e-02, -3.5247e-03, -1.4384e-02],
          [ 7.7620e-03,  1.2539e-02, -1.8462e-04],
          [-1.8523e-03,  1.4081e-02, -9.3886e-03]],

         [[ 2.7376e-03, -7.1033e-03,  2.0892e-02],
          [-8.8533e-04, -4.3231e-03,  2.0547e-02],
          [ 6.3855e-03,  1.0025e-02,  1.4598e-02]],

         [[-2.3164e-02, -1.5965e-02, -2.2390e-02],
          [-1.8326e-02, -1.5365e-02, -1.7574e-02],
          [-1.9141e-02, -1.6926e-02,  2.7847e-03]]],


        [[[-2.7984e-02, -2.0026e-02, -1.6157e-02],
          [ 1.7476e-03,  1.7046e-02,  8.2502e-03],
          [ 1.0914e-02, -8.8491e-03, -1.5929e-02]],

         [[-1.4698e-02,  1.5855e-02,  4.2996e-03],
          [-8.7453e-03,  3.9061e-03,  3.2929e-03],
          [-3.4811e-03, -1.4116e-02,  1.8182e-02]],

         [[-7.1153e-03,  8.0300e-03,  1.3585e-03],
          [-4.0344e-04, -1.6640e-02, -2.4387e-02],
          [ 9.5826e-03, -5.2199e-03,  2.4163e-03]],

         ...,

         [[-5.3677e-03,  2.7577e-02, -9.0178e-03],
          [-1.8046e-02,  1.6935e-03, -1.5768e-02],
          [-2.0654e-02, -2.3664e-02,  7.6976e-03]],

         [[ 3.8326e-05,  2.3548e-03, -4.7767e-04],
          [-3.0870e-03,  1.3127e-02,  9.2100e-03],
          [-1.8610e-02, -1.6946e-02,  1.0448e-03]],

         [[-2.0130e-02, -1.0618e-02, -3.4704e-04],
          [ 1.0643e-02, -8.4378e-03,  1.9312e-02],
          [-6.2952e-03, -1.8936e-03, -1.2912e-02]]],


        ...,


        [[[-1.4154e-02, -1.6408e-02,  2.4563e-03],
          [-1.9394e-03,  1.1034e-02,  1.1994e-02],
          [-1.2603e-02, -1.1701e-02,  4.9772e-03]],

         [[-3.9412e-04,  2.0396e-02, -3.4856e-03],
          [ 1.3923e-02, -3.2864e-03,  2.8019e-02],
          [-2.9576e-03, -1.4013e-02, -1.1404e-02]],

         [[-1.7940e-03, -1.9018e-02,  7.2211e-03],
          [-6.9528e-04,  1.9503e-02, -3.8215e-03],
          [-3.6030e-02, -6.1084e-03,  2.3570e-02]],

         ...,

         [[ 2.3886e-02,  4.4657e-03,  1.3846e-02],
          [ 9.2326e-03,  1.7379e-02,  1.6344e-02],
          [-1.9841e-03, -1.0526e-02,  6.8001e-03]],

         [[ 3.5428e-03, -1.4355e-02, -2.3533e-02],
          [-4.9022e-03,  1.8958e-02, -5.9345e-03],
          [ 1.1305e-02, -1.2744e-02,  1.6321e-02]],

         [[ 1.2135e-02,  1.3841e-02, -6.8475e-03],
          [-1.1905e-02, -2.0517e-03,  1.8216e-02],
          [ 1.2773e-02, -2.0125e-03,  6.5473e-03]]],


        [[[ 8.5281e-03, -1.7699e-02,  7.0200e-03],
          [ 2.6566e-02,  4.7503e-03, -1.0146e-02],
          [-3.2845e-03,  1.9775e-03, -1.7755e-02]],

         [[ 3.3755e-03, -8.4611e-03,  1.4625e-02],
          [ 1.8173e-02,  1.9717e-02,  3.5267e-02],
          [ 3.0170e-02,  2.0863e-02,  3.3771e-02]],

         [[-1.3966e-02,  1.6235e-02, -7.3327e-03],
          [ 2.6718e-02,  2.9403e-02,  1.1250e-02],
          [-1.7469e-02,  3.3939e-04,  2.3982e-02]],

         ...,

         [[-1.3882e-02,  6.3902e-03,  2.0404e-02],
          [ 1.7238e-02,  9.5846e-03,  1.1741e-02],
          [-1.3960e-02,  9.3964e-03, -1.5097e-02]],

         [[-2.1484e-02, -2.0421e-02,  8.7071e-03],
          [-1.0853e-02,  9.2508e-03, -5.8063e-04],
          [ 1.6787e-02, -1.4733e-02, -8.1645e-03]],

         [[-1.8681e-02,  2.1817e-03,  2.2382e-02],
          [ 1.3478e-02,  2.0701e-02, -1.1479e-02],
          [ 1.2921e-02,  1.2133e-02, -7.2486e-03]]],


        [[[ 1.5276e-02, -1.1736e-02, -2.1002e-03],
          [-6.5848e-03, -1.0937e-02, -2.5694e-03],
          [-2.1584e-02, -1.8495e-02, -1.1189e-03]],

         [[ 4.2866e-03, -6.7026e-03, -1.3540e-02],
          [-6.3844e-03, -2.2284e-03, -1.7485e-02],
          [ 4.9403e-04, -2.0494e-02, -9.6744e-03]],

         [[ 3.0958e-02,  2.1274e-03,  2.0460e-03],
          [ 2.5438e-02,  5.0854e-03,  9.1980e-03],
          [-2.0474e-03, -5.7812e-03, -1.3457e-02]],

         ...,

         [[ 1.4861e-02,  1.2848e-02,  1.9588e-03],
          [ 1.9828e-02, -7.2319e-03,  5.7252e-03],
          [-6.3050e-03, -3.7417e-03,  1.8202e-04]],

         [[-6.7404e-03,  3.9653e-03, -1.9847e-02],
          [ 2.0894e-02,  1.8662e-02,  1.2614e-02],
          [ 7.0217e-04,  3.8343e-02,  3.3284e-02]],

         [[ 2.0720e-02, -7.3506e-04, -1.1406e-02],
          [ 7.9075e-03,  7.7667e-03, -1.2670e-02],
          [-3.0872e-02, -3.9836e-03, -1.7403e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([6.7647e+06, 4.6229e+05, 4.4316e+05,  ..., 7.4508e+00, 7.1923e+00,
        6.2635e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1053]) 

NULL SPACE BASIS :  tensor([[-9.8950e-03, -3.7813e-02,  1.7492e-02,  ..., -5.4110e-03,
          1.8060e-03,  6.3297e-03],
        [-1.0677e-03, -2.2897e-02,  4.4693e-03,  ...,  5.8900e-03,
         -4.3356e-03, -2.0634e-03],
        [-7.9738e-03,  1.9169e-02,  9.1535e-03,  ...,  2.2148e-03,
         -6.5860e-04, -3.4130e-03],
        ...,
        [-2.1172e-02, -8.6570e-03,  3.0824e-02,  ...,  2.4314e-03,
          5.8855e-03, -9.1393e-05],
        [ 6.3823e-03,  8.8204e-03, -1.0915e-02,  ...,  1.4108e-03,
         -3.4701e-03,  9.9263e-04],
        [ 4.9968e-02, -4.8147e-03, -1.1075e-02,  ..., -4.1488e-04,
         -6.6506e-05, -4.2647e-04]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.7704e-02, -4.5476e-03, -8.2524e-04,  ...,  3.3774e-04,
         -7.5068e-05,  3.4628e-04],
        [-4.5476e-03,  1.7673e-02, -4.6096e-03,  ...,  2.1928e-04,
          4.5666e-05, -2.9276e-04],
        [-8.2524e-04, -4.6096e-03,  2.0100e-02,  ...,  1.9154e-04,
          2.0257e-04, -1.7792e-05],
        ...,
        [ 3.3774e-04,  2.1928e-04,  1.9154e-04,  ...,  4.9023e-03,
         -1.8465e-03, -6.7896e-05],
        [-7.5068e-05,  4.5666e-05,  2.0257e-04,  ..., -1.8465e-03,
          4.9410e-03, -1.3409e-03],
        [ 3.4628e-04, -2.9276e-04, -1.7792e-05,  ..., -6.7896e-05,
         -1.3409e-03,  4.5647e-03]], device='cuda:0') 

reserving basis 1221/2304; cond: 1085371.125, radio:0.003985840827226639
PARAMETER       :  Parameter containing:
tensor([[[[-0.0064, -0.0092, -0.0069],
          [-0.0082,  0.0154,  0.0131],
          [-0.0237, -0.0045, -0.0027]],

         [[ 0.0021,  0.0021, -0.0110],
          [-0.0075,  0.0115, -0.0050],
          [ 0.0186, -0.0245,  0.0062]],

         [[ 0.0155, -0.0046,  0.0090],
          [ 0.0130, -0.0104,  0.0019],
          [-0.0180, -0.0119, -0.0075]],

         ...,

         [[ 0.0153, -0.0155,  0.0017],
          [ 0.0105,  0.0105,  0.0138],
          [-0.0012, -0.0259,  0.0129]],

         [[ 0.0203,  0.0114, -0.0015],
          [-0.0105,  0.0025, -0.0107],
          [-0.0025,  0.0181, -0.0170]],

         [[-0.0066, -0.0049,  0.0076],
          [-0.0311,  0.0025, -0.0003],
          [ 0.0058, -0.0108,  0.0204]]],


        [[[-0.0119,  0.0081,  0.0077],
          [ 0.0107, -0.0071,  0.0153],
          [ 0.0156, -0.0063, -0.0009]],

         [[ 0.0191, -0.0014, -0.0189],
          [ 0.0019, -0.0042,  0.0089],
          [ 0.0144, -0.0209, -0.0148]],

         [[ 0.0160, -0.0250,  0.0032],
          [-0.0228,  0.0004,  0.0086],
          [-0.0109, -0.0180,  0.0041]],

         ...,

         [[-0.0217, -0.0182, -0.0079],
          [ 0.0004, -0.0123, -0.0260],
          [ 0.0040, -0.0052, -0.0002]],

         [[-0.0091,  0.0058, -0.0062],
          [ 0.0084,  0.0241, -0.0101],
          [-0.0153,  0.0119, -0.0089]],

         [[ 0.0110,  0.0209,  0.0044],
          [ 0.0012,  0.0066, -0.0089],
          [-0.0074,  0.0266, -0.0025]]],


        [[[ 0.0032, -0.0247, -0.0044],
          [-0.0031,  0.0108, -0.0050],
          [ 0.0069,  0.0104,  0.0032]],

         [[ 0.0215,  0.0182,  0.0201],
          [ 0.0205,  0.0096,  0.0056],
          [ 0.0087, -0.0042,  0.0087]],

         [[-0.0174, -0.0142,  0.0026],
          [ 0.0067, -0.0195, -0.0061],
          [-0.0134, -0.0089, -0.0315]],

         ...,

         [[ 0.0017, -0.0041, -0.0318],
          [-0.0052,  0.0032, -0.0084],
          [ 0.0186,  0.0010, -0.0033]],

         [[ 0.0127,  0.0049, -0.0048],
          [-0.0057, -0.0018,  0.0065],
          [-0.0221, -0.0223, -0.0149]],

         [[ 0.0006,  0.0053, -0.0067],
          [ 0.0013,  0.0169,  0.0003],
          [ 0.0114,  0.0112, -0.0094]]],


        ...,


        [[[-0.0001,  0.0010, -0.0140],
          [-0.0142,  0.0025,  0.0005],
          [ 0.0079, -0.0107, -0.0081]],

         [[ 0.0173, -0.0036, -0.0116],
          [ 0.0216,  0.0153, -0.0154],
          [-0.0070,  0.0029, -0.0156]],

         [[-0.0211, -0.0017,  0.0081],
          [-0.0070, -0.0151,  0.0061],
          [ 0.0047,  0.0036, -0.0253]],

         ...,

         [[ 0.0083, -0.0134,  0.0075],
          [ 0.0013,  0.0045, -0.0055],
          [-0.0060, -0.0077, -0.0296]],

         [[-0.0156,  0.0064, -0.0029],
          [ 0.0077, -0.0234, -0.0215],
          [-0.0077, -0.0134, -0.0115]],

         [[ 0.0101,  0.0184,  0.0071],
          [-0.0105,  0.0117, -0.0104],
          [-0.0023,  0.0093, -0.0057]]],


        [[[-0.0190, -0.0040, -0.0013],
          [-0.0102,  0.0025,  0.0075],
          [ 0.0015,  0.0116,  0.0079]],

         [[-0.0124,  0.0213,  0.0106],
          [-0.0040,  0.0025,  0.0112],
          [ 0.0075, -0.0108,  0.0116]],

         [[-0.0085,  0.0041,  0.0181],
          [ 0.0131, -0.0053,  0.0145],
          [-0.0056, -0.0110, -0.0120]],

         ...,

         [[ 0.0113, -0.0186, -0.0022],
          [-0.0029, -0.0151,  0.0106],
          [-0.0156, -0.0121,  0.0038]],

         [[-0.0028,  0.0069, -0.0264],
          [-0.0156, -0.0059, -0.0338],
          [-0.0148, -0.0201, -0.0059]],

         [[ 0.0114,  0.0177,  0.0275],
          [ 0.0141,  0.0025,  0.0031],
          [ 0.0073, -0.0017,  0.0122]]],


        [[[ 0.0022,  0.0080,  0.0231],
          [-0.0037, -0.0029, -0.0167],
          [ 0.0063,  0.0050, -0.0082]],

         [[ 0.0005, -0.0041, -0.0108],
          [-0.0097, -0.0104, -0.0169],
          [ 0.0064, -0.0265,  0.0029]],

         [[ 0.0016, -0.0176,  0.0082],
          [-0.0131,  0.0096,  0.0045],
          [-0.0204,  0.0006, -0.0094]],

         ...,

         [[-0.0149, -0.0163,  0.0199],
          [-0.0125, -0.0126, -0.0230],
          [ 0.0085, -0.0139,  0.0187]],

         [[ 0.0213,  0.0086, -0.0083],
          [ 0.0138,  0.0113,  0.0162],
          [-0.0098, -0.0217, -0.0010]],

         [[-0.0095, -0.0096,  0.0022],
          [-0.0173,  0.0282,  0.0159],
          [-0.0202, -0.0041, -0.0173]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.0877e+06, 2.3385e+05, 2.2511e+05,  ..., 2.1362e+00, 1.9959e+00,
        1.9235e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1221]) 

NULL SPACE BASIS :  tensor([[-0.0029, -0.0057, -0.0169,  ...,  0.0093,  0.0072, -0.0057],
        [-0.0270,  0.0070,  0.0152,  ..., -0.0011, -0.0056,  0.0005],
        [ 0.0219, -0.0119, -0.0171,  ..., -0.0040,  0.0022, -0.0057],
        ...,
        [-0.0066, -0.0194, -0.0306,  ..., -0.0206,  0.0057,  0.0013],
        [ 0.0259, -0.0052, -0.0052,  ...,  0.0375,  0.0069,  0.0107],
        [ 0.0005, -0.0240, -0.0068,  ..., -0.0143, -0.0170, -0.0035]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.5722e-03, -3.1169e-03,  2.7044e-04,  ...,  2.3309e-04,
         -6.5647e-05, -1.3831e-04],
        [-3.1169e-03,  7.3096e-03, -3.1058e-03,  ..., -1.2374e-04,
          7.8879e-05,  7.4653e-05],
        [ 2.7044e-04, -3.1058e-03,  5.1840e-03,  ...,  3.2230e-04,
          1.9393e-04, -2.9536e-05],
        ...,
        [ 2.3309e-04, -1.2374e-04,  3.2230e-04,  ...,  2.1254e-02,
         -2.5807e-03, -8.8036e-04],
        [-6.5647e-05,  7.8879e-05,  1.9393e-04,  ..., -2.5807e-03,
          2.0178e-02, -3.2488e-03],
        [-1.3831e-04,  7.4653e-05, -2.9536e-05,  ..., -8.8036e-04,
         -3.2488e-03,  1.9290e-02]], device='cuda:0') 

reserving basis 1465/4608; cond: 5656459.0, radio:0.000814124068710953
PARAMETER       :  Parameter containing:
tensor([[[[ 4.5442e-03,  3.9264e-03,  1.3013e-02],
          [-7.3062e-03, -8.5413e-03,  1.2693e-02],
          [-4.1320e-03, -4.8217e-03,  9.8200e-03]],

         [[ 4.4204e-03, -6.1586e-03,  1.8006e-03],
          [ 5.6837e-03,  9.8323e-03,  2.0623e-03],
          [ 1.7539e-02, -7.2082e-03,  8.6905e-03]],

         [[-2.4959e-03, -1.4087e-02, -2.3955e-04],
          [-5.2277e-03, -6.5901e-03, -2.3158e-02],
          [-1.1412e-02,  9.6234e-03, -2.1140e-02]],

         ...,

         [[-5.1408e-03,  2.2512e-03,  3.9646e-03],
          [-1.8296e-03, -1.4706e-03, -1.7317e-02],
          [-1.0444e-02, -1.0061e-03, -2.5519e-02]],

         [[-4.7398e-03,  1.0767e-02,  1.3925e-02],
          [ 2.9192e-03,  1.6446e-02,  3.6132e-03],
          [ 2.0202e-04,  5.0607e-03,  4.9440e-03]],

         [[-1.3894e-02, -3.1402e-02, -2.8306e-02],
          [-6.3775e-03, -1.7463e-04, -1.9686e-02],
          [ 7.5167e-03,  1.1401e-03, -7.7673e-04]]],


        [[[ 8.3531e-03, -1.6260e-02,  8.7061e-03],
          [-3.8688e-03,  5.4628e-03,  6.5313e-03],
          [ 5.0989e-03, -1.3024e-02, -1.1647e-02]],

         [[-1.4806e-02,  7.8896e-03, -9.2840e-03],
          [-3.2710e-03, -2.5731e-03, -3.4978e-03],
          [ 7.1008e-03,  4.5585e-03, -1.0680e-02]],

         [[-1.2946e-03,  5.9329e-05,  1.1313e-02],
          [ 7.2181e-04,  1.3152e-02,  1.6257e-03],
          [ 1.1040e-02,  2.3369e-02,  1.8401e-02]],

         ...,

         [[-2.9669e-03,  2.8872e-03, -9.2818e-03],
          [ 7.1693e-04, -6.4429e-03, -1.9300e-03],
          [-1.8001e-03,  7.3982e-03,  1.3792e-02]],

         [[ 5.3111e-03, -3.7659e-03, -1.2466e-02],
          [-4.1694e-03, -3.2035e-03,  3.3814e-03],
          [-5.2441e-03,  5.9948e-04, -1.2047e-02]],

         [[ 2.9275e-04,  1.2286e-03, -5.0131e-03],
          [-1.5419e-02, -1.9695e-03, -2.0982e-04],
          [ 1.0780e-02, -6.5713e-03, -4.5631e-03]]],


        [[[ 2.9417e-03,  1.0375e-02,  3.1031e-03],
          [ 7.9492e-03,  1.4939e-02,  1.9262e-02],
          [-6.8335e-03, -2.0797e-03,  4.4254e-03]],

         [[ 1.2763e-03,  1.3419e-02,  8.6810e-03],
          [-1.1912e-02, -7.1952e-03,  3.7215e-03],
          [-9.9949e-03, -7.7885e-03, -5.8292e-03]],

         [[-3.8380e-03, -6.7990e-03,  6.1701e-03],
          [-6.0366e-03, -1.1438e-02,  9.5498e-03],
          [-1.2118e-03, -4.9962e-03, -1.2187e-03]],

         ...,

         [[ 7.8861e-03,  2.6860e-03, -9.4705e-03],
          [ 6.7951e-03, -1.9732e-03, -1.1730e-02],
          [ 8.7647e-03,  3.7932e-03,  1.5672e-03]],

         [[ 8.7736e-03,  5.2243e-03,  2.0530e-02],
          [-7.2202e-03, -5.9431e-03,  1.8374e-02],
          [-4.7661e-03,  2.9180e-03, -4.0897e-03]],

         [[-4.7025e-03,  5.0217e-03, -1.2654e-02],
          [-1.1978e-02,  3.1499e-03, -6.7509e-03],
          [-7.2339e-03,  1.0675e-02,  8.5109e-03]]],


        ...,


        [[[ 5.7803e-03, -3.9791e-03, -1.6208e-02],
          [-1.0429e-02,  5.3495e-03, -3.5800e-03],
          [ 1.4554e-03,  1.0851e-02, -1.3920e-02]],

         [[-6.6657e-03,  4.4185e-04,  3.8123e-03],
          [-8.1617e-03,  1.5417e-02, -3.3178e-04],
          [-7.0336e-03,  9.1713e-03, -8.0415e-03]],

         [[ 1.1245e-03, -9.6012e-03,  1.1218e-03],
          [-3.3020e-03, -2.6318e-02, -1.1355e-02],
          [ 1.2306e-02, -1.9986e-02, -1.7783e-02]],

         ...,

         [[-1.7925e-02,  6.2732e-03, -7.2426e-03],
          [-6.9569e-03, -4.8982e-03, -1.2104e-02],
          [-6.4229e-03,  1.1108e-02,  6.3105e-03]],

         [[-7.9662e-04, -3.5768e-03, -3.7506e-03],
          [-5.7339e-03,  5.8144e-03,  1.3580e-02],
          [ 9.8751e-03, -5.3420e-03, -9.4013e-03]],

         [[ 1.4238e-02,  1.3610e-02,  5.2881e-03],
          [-8.9777e-04,  4.8042e-03,  1.8798e-03],
          [-9.8649e-04,  3.8838e-04, -1.3474e-03]]],


        [[[-1.3703e-02, -3.6523e-03,  3.3538e-03],
          [-1.3032e-02,  8.3811e-03, -1.8331e-02],
          [-6.4226e-03, -2.6242e-03,  2.6880e-03]],

         [[ 1.2426e-03, -1.3994e-02, -2.2486e-02],
          [ 9.9229e-03,  4.2664e-03, -1.6258e-02],
          [ 8.8824e-03,  4.3431e-03,  4.8688e-03]],

         [[-6.9507e-03,  8.8360e-04,  1.0758e-02],
          [ 1.7241e-03, -1.1178e-03, -6.0784e-03],
          [-6.9612e-03, -5.0386e-03, -7.3613e-03]],

         ...,

         [[ 2.7038e-03,  1.1445e-03, -6.6939e-03],
          [-2.4344e-02, -1.7725e-02, -1.7088e-02],
          [-2.7458e-04, -2.4797e-03,  1.3753e-02]],

         [[ 4.4348e-03, -3.7256e-03,  3.4563e-03],
          [-3.1428e-03,  6.5886e-04, -1.4795e-04],
          [ 1.0602e-02,  9.9240e-03,  4.8750e-03]],

         [[ 7.8438e-03, -1.3972e-02, -1.3253e-02],
          [ 2.0084e-03, -3.2271e-03,  1.4236e-02],
          [-1.3651e-02,  3.1607e-03, -4.7612e-03]]],


        [[[ 8.4508e-03, -6.7542e-03, -3.5199e-03],
          [-1.4675e-02, -9.4481e-04,  4.6925e-03],
          [ 1.0549e-03, -9.5878e-03, -1.1809e-02]],

         [[-4.2427e-03,  1.4040e-02,  2.5045e-03],
          [ 5.5762e-03, -1.0241e-02,  5.4995e-04],
          [ 4.2132e-03,  6.9928e-03, -5.4881e-03]],

         [[-1.2109e-02, -3.4835e-03,  1.6639e-02],
          [-2.7719e-03,  4.4840e-03,  1.8357e-02],
          [-8.4575e-03, -1.3771e-03,  2.4861e-03]],

         ...,

         [[ 1.3798e-02,  5.3683e-03, -6.4647e-04],
          [ 1.1156e-02, -1.4346e-02,  5.5314e-03],
          [ 1.7851e-03, -1.5384e-02,  1.0221e-02]],

         [[ 1.9782e-02,  2.3714e-02,  2.5912e-03],
          [ 1.6444e-02,  1.8008e-03, -5.1420e-03],
          [-9.9782e-03,  1.2519e-02, -1.2567e-02]],

         [[-4.1912e-03, -1.3422e-02,  7.9689e-03],
          [-1.1551e-02, -6.0833e-03, -5.2707e-03],
          [ 1.1015e-02, -2.5979e-03,  2.0402e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.4642e+06, 3.9550e+05, 3.8338e+05,  ..., 4.7801e-01, 4.7159e-01,
        4.3564e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1465]) 

NULL SPACE BASIS :  tensor([[-3.1719e-02, -6.7632e-03,  1.9548e-02,  ...,  7.3277e-03,
          8.6920e-03,  2.2088e-03],
        [-1.7440e-02,  1.2790e-02,  9.2266e-03,  ..., -2.2880e-03,
          3.8606e-04, -9.1853e-04],
        [-2.0075e-02,  2.1530e-02,  3.7285e-03,  ...,  8.1518e-04,
          5.7502e-03, -1.6385e-03],
        ...,
        [-1.3246e-02,  5.4393e-03, -7.1897e-03,  ..., -5.2843e-03,
          4.3824e-03, -2.2068e-03],
        [-1.7302e-02,  5.6755e-03, -7.3580e-03,  ..., -1.6841e-03,
          8.2607e-03, -2.4163e-03],
        [ 1.7361e-02,  2.3035e-02,  3.9448e-05,  ...,  4.9975e-03,
         -2.9885e-05, -1.0771e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.8977e-03, -5.5114e-04, -3.2937e-04,  ..., -5.9361e-05,
         -2.6859e-04,  4.0876e-05],
        [-5.5114e-04,  4.9811e-03, -2.3611e-04,  ..., -2.5426e-04,
          9.0150e-05, -1.7473e-04],
        [-3.2937e-04, -2.3611e-04,  6.3159e-03,  ...,  1.1766e-04,
         -5.1498e-05,  1.3229e-05],
        ...,
        [-5.9361e-05, -2.5426e-04,  1.1766e-04,  ...,  1.0339e-02,
         -5.2702e-04, -1.8709e-05],
        [-2.6859e-04,  9.0150e-05, -5.1498e-05,  ..., -5.2702e-04,
          7.3124e-03, -6.0694e-04],
        [ 4.0876e-05, -1.7473e-04,  1.3229e-05,  ..., -1.8709e-05,
         -6.0694e-04,  8.4622e-03]], device='cuda:0') 

reserving basis 193/256; cond: 49354.5234375, radio:0.012315853498876095
PARAMETER       :  Parameter containing:
tensor([[[[-0.0174]],

         [[-0.0351]],

         [[-0.0372]],

         ...,

         [[ 0.0100]],

         [[-0.0465]],

         [[-0.0559]]],


        [[[ 0.0649]],

         [[-0.0605]],

         [[ 0.0179]],

         ...,

         [[ 0.0080]],

         [[ 0.0068]],

         [[ 0.0150]]],


        [[[ 0.0411]],

         [[ 0.0263]],

         [[-0.0056]],

         ...,

         [[ 0.0210]],

         [[ 0.0381]],

         [[-0.0281]]],


        ...,


        [[[-0.0273]],

         [[ 0.0090]],

         [[ 0.0456]],

         ...,

         [[ 0.0197]],

         [[-0.0169]],

         [[-0.0152]]],


        [[[ 0.0170]],

         [[ 0.0198]],

         [[-0.0244]],

         ...,

         [[ 0.0418]],

         [[ 0.0091]],

         [[ 0.0421]]],


        [[[-0.0372]],

         [[ 0.0246]],

         [[ 0.0413]],

         ...,

         [[ 0.0540]],

         [[-0.0617]],

         [[ 0.0420]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([4.1509e+05, 2.4614e+04, 2.0534e+04, 7.7640e+03, 5.9592e+03, 5.6248e+03,
        3.9131e+03, 2.0295e+03, 1.4852e+03, 1.2477e+03, 1.0248e+03, 9.4739e+02,
        8.0323e+02, 7.6599e+02, 7.3088e+02, 6.5830e+02, 6.3326e+02, 6.0134e+02,
        5.6673e+02, 5.1711e+02, 4.8786e+02, 4.7630e+02, 4.1109e+02, 4.0779e+02,
        3.6179e+02, 3.3810e+02, 3.1639e+02, 2.8494e+02, 2.7123e+02, 2.6685e+02,
        2.4922e+02, 2.3886e+02, 2.3060e+02, 2.2395e+02, 2.1446e+02, 2.0504e+02,
        1.9214e+02, 1.7628e+02, 1.7282e+02, 1.6775e+02, 1.5791e+02, 1.5660e+02,
        1.4889e+02, 1.4125e+02, 1.4035e+02, 1.3613e+02, 1.3193e+02, 1.2784e+02,
        1.2487e+02, 1.2058e+02, 1.1750e+02, 1.1020e+02, 1.0890e+02, 1.0549e+02,
        1.0464e+02, 1.0282e+02, 1.0081e+02, 9.8243e+01, 9.4604e+01, 9.1312e+01,
        8.9703e+01, 8.7882e+01, 8.5158e+01, 8.3805e+01, 8.1556e+01, 8.0908e+01,
        7.7966e+01, 7.7279e+01, 7.6099e+01, 7.5059e+01, 7.4367e+01, 7.3603e+01,
        7.0646e+01, 6.9836e+01, 6.8715e+01, 6.8212e+01, 6.6841e+01, 6.5587e+01,
        6.3665e+01, 6.2955e+01, 6.2521e+01, 6.0662e+01, 5.9427e+01, 5.9156e+01,
        5.8167e+01, 5.7882e+01, 5.6451e+01, 5.6199e+01, 5.5973e+01, 5.4550e+01,
        5.3979e+01, 5.3658e+01, 5.3126e+01, 5.2660e+01, 5.1232e+01, 5.0934e+01,
        5.0718e+01, 5.0378e+01, 4.8924e+01, 4.8423e+01, 4.7742e+01, 4.7028e+01,
        4.6871e+01, 4.6195e+01, 4.5886e+01, 4.5536e+01, 4.5046e+01, 4.4724e+01,
        4.3576e+01, 4.3145e+01, 4.2964e+01, 4.2695e+01, 4.1730e+01, 4.1356e+01,
        4.0811e+01, 4.0500e+01, 4.0136e+01, 3.9824e+01, 3.9202e+01, 3.8232e+01,
        3.8086e+01, 3.7960e+01, 3.7637e+01, 3.7506e+01, 3.7383e+01, 3.6940e+01,
        3.6400e+01, 3.6109e+01, 3.5857e+01, 3.5296e+01, 3.5092e+01, 3.4845e+01,
        3.4556e+01, 3.4151e+01, 3.4028e+01, 3.3607e+01, 3.3510e+01, 3.2913e+01,
        3.2773e+01, 3.2366e+01, 3.2293e+01, 3.1963e+01, 3.1668e+01, 3.1427e+01,
        3.0968e+01, 3.0947e+01, 3.0810e+01, 3.0492e+01, 3.0234e+01, 2.9673e+01,
        2.9625e+01, 2.9422e+01, 2.9168e+01, 2.8918e+01, 2.8761e+01, 2.8407e+01,
        2.8205e+01, 2.7856e+01, 2.7825e+01, 2.7644e+01, 2.7379e+01, 2.7273e+01,
        2.7036e+01, 2.6713e+01, 2.6525e+01, 2.6346e+01, 2.6135e+01, 2.5792e+01,
        2.5665e+01, 2.5334e+01, 2.5173e+01, 2.5091e+01, 2.4930e+01, 2.4749e+01,
        2.4594e+01, 2.4548e+01, 2.4159e+01, 2.4048e+01, 2.3908e+01, 2.3663e+01,
        2.3477e+01, 2.3165e+01, 2.3039e+01, 2.2705e+01, 2.2582e+01, 2.2186e+01,
        2.2129e+01, 2.2070e+01, 2.2018e+01, 2.1704e+01, 2.1543e+01, 2.1391e+01,
        2.1205e+01, 2.0984e+01, 2.0793e+01, 2.0621e+01, 2.0273e+01, 2.0133e+01,
        1.9926e+01, 1.9766e+01, 1.9641e+01, 1.9595e+01, 1.9330e+01, 1.9240e+01,
        1.9091e+01, 1.8837e+01, 1.8820e+01, 1.8621e+01, 1.8387e+01, 1.8244e+01,
        1.8124e+01, 1.7987e+01, 1.7933e+01, 1.7728e+01, 1.7572e+01, 1.7175e+01,
        1.7109e+01, 1.7035e+01, 1.6868e+01, 1.6788e+01, 1.6553e+01, 1.6455e+01,
        1.6262e+01, 1.6042e+01, 1.5761e+01, 1.5452e+01, 1.5365e+01, 1.5174e+01,
        1.5050e+01, 1.4892e+01, 1.4734e+01, 1.4676e+01, 1.4601e+01, 1.4480e+01,
        1.4315e+01, 1.4134e+01, 1.3910e+01, 1.3606e+01, 1.3465e+01, 1.3259e+01,
        1.3051e+01, 1.2900e+01, 1.2854e+01, 1.2708e+01, 1.2385e+01, 1.2114e+01,
        1.1930e+01, 1.1312e+01, 1.1160e+01, 1.1027e+01, 1.0890e+01, 1.0697e+01,
        1.0324e+01, 1.0026e+01, 9.7248e+00, 8.4104e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 193]) 

NULL SPACE BASIS :  tensor([[ 0.1773,  0.2086, -0.0534,  ..., -0.0019, -0.0138,  0.0049],
        [ 0.0831,  0.0665,  0.0262,  ...,  0.0209,  0.0043,  0.0347],
        [-0.0045,  0.0090,  0.0748,  ...,  0.0360, -0.0606,  0.0200],
        ...,
        [-0.0580,  0.0148,  0.0135,  ..., -0.0228,  0.0300,  0.0086],
        [-0.0428, -0.0037, -0.0682,  ...,  0.0279,  0.0058, -0.0037],
        [ 0.0115,  0.0289, -0.0386,  ...,  0.0124,  0.2612,  0.0367]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0295,  0.0027, -0.0023,  ...,  0.0009, -0.0035,  0.0020],
        [ 0.0027,  0.0578, -0.0030,  ..., -0.0054,  0.0022,  0.0012],
        [-0.0023, -0.0030,  0.0617,  ..., -0.0025,  0.0029, -0.0007],
        ...,
        [ 0.0009, -0.0054, -0.0025,  ...,  0.0558, -0.0017,  0.0015],
        [-0.0035,  0.0022,  0.0029,  ..., -0.0017,  0.0448, -0.0012],
        [ 0.0020,  0.0012, -0.0007,  ...,  0.0015, -0.0012,  0.0654]],
       device='cuda:0') 

reserving basis 1676/4608; cond: 6464648.0, radio:0.0007931137806735933
PARAMETER       :  Parameter containing:
tensor([[[[-4.1128e-03,  4.6094e-03, -1.2874e-02],
          [-7.9780e-03,  5.5884e-03, -1.6043e-02],
          [ 9.8021e-03, -2.9508e-03,  6.7543e-03]],

         [[-7.6403e-03, -2.5088e-03,  3.6974e-03],
          [ 9.2440e-03,  4.8559e-03, -3.8098e-03],
          [ 5.0900e-03, -5.2717e-03,  6.2293e-03]],

         [[ 1.2547e-03, -1.3765e-02, -1.4318e-02],
          [-4.2591e-03, -1.9099e-03, -1.2421e-02],
          [ 1.5398e-03,  1.4147e-02, -4.4666e-03]],

         ...,

         [[-3.5036e-03,  9.4408e-03, -1.2117e-03],
          [ 1.0267e-02,  3.6203e-04,  6.2723e-03],
          [ 5.1227e-03,  1.0683e-02, -4.9644e-03]],

         [[-1.2412e-02,  5.2000e-04, -1.1026e-02],
          [ 1.4929e-03, -7.0767e-03, -2.3184e-03],
          [-7.1408e-04, -1.0434e-02,  1.0461e-02]],

         [[-4.2016e-04,  1.0317e-02,  8.0756e-03],
          [ 6.7401e-03,  9.3870e-03,  1.2855e-02],
          [-1.2848e-02,  3.1937e-03, -9.6541e-03]]],


        [[[ 6.1008e-03, -3.9125e-03,  7.8977e-03],
          [-1.3616e-02, -9.0348e-03,  5.7376e-03],
          [-2.3101e-02,  1.0075e-03,  6.9326e-03]],

         [[-7.6180e-03,  1.6556e-03,  2.7174e-03],
          [-6.6987e-03,  1.6434e-02, -1.9632e-03],
          [ 1.1662e-02,  8.0960e-03,  1.5999e-02]],

         [[ 5.4883e-03, -6.9954e-03,  5.6604e-03],
          [ 1.2772e-03, -1.1084e-02,  5.1339e-03],
          [ 1.1680e-02,  4.4616e-06,  7.3195e-03]],

         ...,

         [[ 9.0434e-03,  3.5792e-03,  8.1546e-03],
          [ 1.1846e-02, -3.4580e-03, -4.4141e-03],
          [-6.2623e-03, -9.3814e-04,  2.6645e-03]],

         [[ 2.3308e-03, -1.9720e-03, -9.9211e-03],
          [ 9.2781e-04, -4.9266e-03, -2.3293e-03],
          [-3.1093e-03,  1.2696e-02,  2.9263e-03]],

         [[ 1.2297e-02, -7.3965e-03, -9.6747e-03],
          [ 1.2916e-02, -5.1837e-03,  1.8643e-03],
          [-1.8282e-03,  5.9891e-03, -1.0340e-02]]],


        [[[-5.6707e-03, -1.8261e-02, -7.9978e-03],
          [-2.4105e-03,  7.8179e-03, -1.4998e-02],
          [ 1.5537e-02,  1.8059e-02,  5.8356e-03]],

         [[ 6.9776e-03, -1.0527e-02,  1.0831e-03],
          [-5.7921e-03, -3.1918e-03, -2.1324e-02],
          [-4.5387e-03, -4.1813e-03, -2.0947e-02]],

         [[-1.3940e-03, -1.9487e-03,  5.3643e-03],
          [ 1.4035e-03,  2.2581e-03, -3.1114e-04],
          [-5.8332e-03,  1.0088e-02, -1.8811e-03]],

         ...,

         [[ 7.2705e-03,  1.3828e-02,  8.3651e-03],
          [-5.5326e-03,  9.5437e-03, -8.3631e-03],
          [ 4.8909e-04, -1.2587e-02, -2.2311e-03]],

         [[ 5.2056e-03, -1.1380e-02, -6.0478e-03],
          [ 4.0338e-03, -6.6845e-03, -1.9596e-02],
          [-3.6952e-03,  1.7593e-03, -1.3423e-02]],

         [[-1.2683e-02, -1.9078e-02, -3.8660e-03],
          [ 7.4327e-03,  6.1205e-03, -1.1411e-02],
          [-4.9930e-03, -1.1233e-02,  6.2504e-03]]],


        ...,


        [[[-1.1341e-02,  8.7667e-03, -8.3548e-03],
          [ 9.0316e-03,  7.1954e-03,  1.0395e-02],
          [-1.2633e-02, -1.7461e-02,  1.1312e-03]],

         [[-1.6233e-02, -1.8252e-03, -1.6038e-03],
          [ 4.2414e-03,  2.7680e-03, -1.7265e-03],
          [-9.6819e-03,  4.7293e-03, -3.2336e-03]],

         [[-1.5568e-02,  1.2596e-03,  1.6285e-03],
          [-1.3360e-02, -5.5747e-03, -1.5957e-02],
          [-8.2662e-03, -1.4401e-02, -3.2269e-03]],

         ...,

         [[ 1.0261e-02, -1.4702e-02,  5.6771e-03],
          [-3.1990e-03,  4.0578e-03, -1.0714e-02],
          [-8.2220e-03, -1.5220e-03,  5.3244e-03]],

         [[-1.6544e-03, -2.5533e-03, -1.5051e-03],
          [-1.2508e-02, -8.1610e-03, -1.9381e-02],
          [-1.4256e-02, -4.3747e-03, -4.1398e-03]],

         [[-3.3226e-03, -3.8577e-03, -1.0492e-02],
          [ 8.9126e-03,  9.7466e-03, -1.4963e-03],
          [ 1.0466e-02,  5.8424e-03,  1.0354e-02]]],


        [[[-1.4565e-02, -2.7068e-03,  9.7624e-03],
          [ 6.6328e-03, -9.3408e-04,  2.2652e-02],
          [-6.1548e-03, -7.6801e-03,  2.0901e-02]],

         [[ 1.0851e-02, -7.6118e-03, -1.6424e-03],
          [-3.8716e-03, -4.4664e-03,  8.8147e-03],
          [-1.9187e-02, -6.9903e-03, -1.8915e-02]],

         [[-1.5776e-02, -1.4515e-02, -1.3422e-02],
          [ 1.3645e-02,  1.4679e-03, -1.5371e-02],
          [ 8.5919e-03, -1.1956e-02, -3.0170e-03]],

         ...,

         [[ 1.2738e-02,  7.3515e-03, -2.5221e-03],
          [ 1.3006e-02,  1.0029e-02,  1.5623e-02],
          [-5.9552e-03,  9.7676e-03, -1.4961e-02]],

         [[-8.3064e-03,  2.1286e-03, -4.5494e-03],
          [ 5.1185e-03, -4.7138e-03,  1.3670e-03],
          [ 1.6067e-02,  5.5301e-03,  6.5435e-03]],

         [[ 3.6632e-03,  1.1846e-02,  1.0368e-02],
          [-9.4614e-04,  4.3830e-03,  3.1808e-03],
          [-1.6659e-03,  1.6758e-02,  2.2655e-03]]],


        [[[ 1.9601e-02,  4.8114e-03, -1.7070e-03],
          [ 2.7114e-03, -5.6227e-03, -7.2108e-03],
          [-1.0383e-02,  2.4434e-03,  3.5249e-03]],

         [[ 1.5529e-02,  6.0347e-04, -3.0654e-03],
          [-1.2456e-03,  1.7198e-02, -2.6891e-03],
          [ 6.6884e-03,  2.5431e-03,  1.4181e-02]],

         [[ 1.8324e-02,  2.4596e-03,  6.0298e-03],
          [-5.5364e-03, -9.7639e-03, -9.6654e-03],
          [-1.0908e-02, -6.0389e-04, -9.3341e-03]],

         ...,

         [[ 1.0273e-02, -1.4514e-02,  2.1341e-03],
          [ 3.6249e-03,  9.1761e-03,  4.3632e-03],
          [ 1.9411e-02,  2.3741e-02,  2.6610e-02]],

         [[-1.0765e-03,  1.1365e-02, -8.2992e-03],
          [-1.3529e-02,  1.1210e-03, -1.7627e-02],
          [ 7.4119e-03,  7.5753e-04, -1.6419e-03]],

         [[ 5.1452e-05,  3.8009e-03, -1.3645e-02],
          [-1.0333e-02,  6.3058e-03, -1.0868e-02],
          [-5.1363e-03, -1.1785e-02,  1.0540e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.2242e+06, 6.4222e+05, 6.2419e+05,  ..., 5.3931e-01, 5.2417e-01,
        4.9875e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1676]) 

NULL SPACE BASIS :  tensor([[-0.0294, -0.0072,  0.0103,  ...,  0.0067,  0.0042,  0.0057],
        [ 0.0053,  0.0105, -0.0165,  ...,  0.0021, -0.0118, -0.0091],
        [ 0.0130, -0.0014,  0.0086,  ..., -0.0026,  0.0068,  0.0042],
        ...,
        [ 0.0048, -0.0003,  0.0049,  ...,  0.0018, -0.0061,  0.0045],
        [-0.0035,  0.0078, -0.0185,  ..., -0.0054,  0.0030, -0.0010],
        [ 0.0192, -0.0088,  0.0010,  ..., -0.0035, -0.0033,  0.0026]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 9.1139e-03, -2.2011e-03, -1.2347e-04,  ...,  1.0247e-04,
         -1.1736e-04, -5.0034e-05],
        [-2.2011e-03,  7.2537e-03, -2.0117e-03,  ...,  1.0877e-04,
          7.7565e-07, -2.1220e-05],
        [-1.2347e-04, -2.0117e-03,  7.9497e-03,  ...,  2.4237e-05,
          7.5120e-06, -3.3151e-05],
        ...,
        [ 1.0247e-04,  1.0877e-04,  2.4237e-05,  ...,  4.4591e-03,
         -2.1854e-04, -2.4523e-05],
        [-1.1736e-04,  7.7565e-07,  7.5120e-06,  ..., -2.1854e-04,
          3.5431e-03, -1.7036e-04],
        [-5.0034e-05, -2.1220e-05, -3.3151e-05,  ..., -2.4523e-05,
         -1.7036e-04,  3.9594e-03]], device='cuda:0') 

reserving basis 702/4608; cond: 43100216.0, radio:4.2506806494202465e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 8.0985e-03,  5.0735e-03, -4.6031e-03],
          [-1.5952e-03, -8.5791e-03, -2.0235e-03],
          [ 1.6505e-03, -7.5947e-03,  6.1693e-03]],

         [[-4.2847e-03, -1.1310e-02,  1.0430e-03],
          [-4.2783e-03, -1.0358e-03, -2.5917e-03],
          [ 3.2854e-03,  9.3762e-03, -1.8942e-03]],

         [[-1.0920e-02,  6.3528e-03,  1.1761e-02],
          [ 1.5298e-02,  1.3191e-02,  3.2275e-03],
          [-9.9790e-04,  1.7522e-02,  5.2521e-03]],

         ...,

         [[ 1.1820e-02,  3.5282e-03,  3.6622e-04],
          [ 2.4440e-03, -8.4596e-03,  1.4441e-03],
          [ 6.3399e-03, -5.7191e-03,  7.6704e-03]],

         [[ 7.4321e-03, -1.2326e-03,  3.1418e-03],
          [ 3.7515e-03,  1.2338e-03,  7.3464e-03],
          [ 8.0044e-03,  8.4111e-03, -1.7736e-04]],

         [[-7.8138e-03,  1.2819e-02, -3.2077e-04],
          [-1.0024e-02, -1.3078e-02,  1.5717e-03],
          [-1.2230e-02,  5.0411e-03, -1.4433e-02]]],


        [[[-1.1731e-03,  7.4056e-03, -2.5204e-03],
          [ 1.4930e-03, -2.4780e-03, -8.9239e-03],
          [-4.7919e-03, -1.5146e-02,  6.2666e-03]],

         [[ 3.8582e-03,  6.7597e-03,  3.2651e-03],
          [ 9.8196e-03,  1.3553e-02, -1.2453e-02],
          [ 1.6602e-02,  7.5271e-03, -9.8684e-03]],

         [[ 7.1632e-03,  5.9335e-03,  2.7688e-03],
          [ 3.9559e-03, -4.4729e-03,  6.7128e-03],
          [-2.8992e-03, -7.3943e-03,  5.0571e-04]],

         ...,

         [[-2.2450e-03,  8.9840e-03,  4.8429e-03],
          [-7.9086e-03, -8.2409e-03,  7.2591e-03],
          [-7.5756e-03,  6.2573e-03, -3.4629e-03]],

         [[ 5.0900e-03, -3.1037e-03,  9.4320e-03],
          [-2.7767e-03, -1.4150e-02, -4.6382e-03],
          [ 4.8841e-03, -1.8103e-03, -1.4277e-02]],

         [[ 1.5797e-02,  1.9309e-02, -1.9314e-03],
          [ 3.1147e-04,  4.8574e-05,  5.9913e-03],
          [ 4.3986e-03, -2.7565e-03, -9.6626e-04]]],


        [[[-3.7543e-03, -4.8404e-03, -6.6330e-03],
          [-3.6794e-03, -1.2422e-02, -7.9769e-03],
          [-7.7222e-03, -3.3675e-03, -1.9735e-03]],

         [[-3.6155e-03,  5.3735e-03, -1.2236e-02],
          [ 1.6551e-03, -1.0157e-02, -1.3261e-02],
          [-5.3776e-03, -3.9910e-03, -8.8110e-03]],

         [[-1.8757e-03, -1.2516e-02,  4.0756e-03],
          [-5.5232e-03, -7.6264e-03, -1.3664e-02],
          [-4.9806e-03,  1.3634e-02, -6.2048e-03]],

         ...,

         [[ 3.1999e-03, -1.8801e-02, -6.7701e-04],
          [ 1.5945e-02,  9.7700e-04, -5.6058e-03],
          [-4.7392e-03,  8.1535e-03, -2.1780e-03]],

         [[ 3.7645e-03,  1.8485e-02,  4.6954e-03],
          [-2.6943e-03, -5.2449e-03,  9.0064e-03],
          [ 2.9797e-04, -4.2809e-03,  4.8228e-04]],

         [[-1.3799e-02, -1.5074e-02, -1.6444e-02],
          [-1.2484e-02, -2.3945e-04,  1.1966e-03],
          [-1.2622e-02, -1.1087e-02, -1.7085e-02]]],


        ...,


        [[[-1.3143e-03,  1.2425e-03,  9.0965e-03],
          [ 4.0337e-04,  9.1079e-03, -7.0388e-03],
          [ 7.6223e-03, -4.2034e-04,  5.3098e-03]],

         [[-8.2116e-03, -3.7960e-03,  8.9236e-03],
          [ 9.0683e-03,  6.9513e-03, -7.7340e-03],
          [ 1.2498e-02, -4.1368e-03, -3.1361e-03]],

         [[-1.4408e-02, -1.5835e-03, -1.1512e-02],
          [-1.2598e-02, -1.1395e-02, -3.6772e-03],
          [ 1.1664e-02,  1.6597e-02, -7.2341e-03]],

         ...,

         [[-3.6685e-03,  6.6369e-03,  5.8773e-03],
          [ 9.6347e-03,  2.0531e-03,  5.6069e-03],
          [-4.8836e-03, -3.4230e-03, -2.7637e-03]],

         [[-8.7383e-03,  1.1752e-02, -9.0864e-03],
          [ 9.7991e-03, -1.2143e-02, -8.3082e-03],
          [ 4.8276e-04,  1.4417e-03, -1.6887e-02]],

         [[ 1.6982e-02,  2.0347e-02,  2.4201e-02],
          [ 1.0961e-02, -2.2162e-03,  3.9287e-03],
          [ 1.0422e-02,  1.5959e-02,  6.5877e-03]]],


        [[[ 2.1376e-03, -2.3617e-03, -8.6354e-05],
          [-2.7387e-03, -1.1745e-02, -1.0146e-02],
          [-1.0764e-02,  1.0019e-03, -3.0483e-03]],

         [[-4.3224e-03,  9.9309e-03,  2.4113e-03],
          [-1.1123e-03,  6.0185e-03,  1.3015e-02],
          [-6.6737e-03, -1.0576e-02,  1.1619e-03]],

         [[ 4.5792e-03,  7.6984e-04,  4.4812e-03],
          [-1.2413e-02, -8.4045e-03,  9.7961e-03],
          [ 3.2719e-03,  9.7914e-03, -6.1786e-03]],

         ...,

         [[ 4.7116e-03,  4.0901e-03, -4.1067e-03],
          [ 8.5509e-03,  3.8953e-03,  3.1931e-03],
          [ 2.4502e-03,  1.7209e-03, -1.4392e-02]],

         [[-8.9536e-03, -3.3733e-03, -5.1944e-03],
          [ 2.4261e-03,  1.5243e-03, -1.4695e-02],
          [-4.6481e-04, -8.1636e-03,  2.7199e-03]],

         [[-1.3131e-02,  5.4926e-04,  1.2180e-04],
          [ 3.0934e-03, -4.7134e-03,  4.7458e-03],
          [-1.1608e-02, -1.1673e-02,  1.3955e-03]]],


        [[[-6.0019e-03, -9.8152e-03, -8.5781e-03],
          [ 3.1354e-03, -6.7437e-03,  1.1580e-03],
          [ 3.0901e-03,  2.4257e-03, -5.1263e-03]],

         [[-5.9933e-03,  9.7289e-03,  7.6118e-04],
          [ 1.9287e-03,  8.2212e-03,  2.0668e-03],
          [ 2.8940e-03,  1.1608e-02,  2.5567e-03]],

         [[-1.9400e-03,  5.9031e-03,  6.0621e-03],
          [-7.1784e-03,  1.0352e-02, -1.2865e-02],
          [ 4.6300e-04,  2.1504e-03,  2.7802e-03]],

         ...,

         [[ 4.4267e-03, -7.1598e-04,  1.3300e-02],
          [ 1.0380e-02,  3.7763e-03,  1.4471e-02],
          [ 7.7173e-03,  1.1192e-02,  4.0605e-03]],

         [[-8.0806e-03, -1.9517e-02, -6.1509e-04],
          [-1.1421e-02, -1.2265e-02,  4.3721e-03],
          [ 5.7572e-03, -2.0568e-04,  4.7173e-03]],

         [[-7.0297e-03,  5.0668e-03, -1.5224e-02],
          [-4.8506e-03,  1.0302e-02, -1.2522e-02],
          [ 7.0492e-03, -3.7374e-03, -5.7071e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([7.6949e+05, 1.7083e+05, 1.5351e+05,  ..., 2.5386e-02, 2.2680e-02,
        1.7854e-02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 702]) 

NULL SPACE BASIS :  tensor([[-0.0286, -0.0150, -0.0090,  ...,  0.0088,  0.0150, -0.0173],
        [-0.0142,  0.0095, -0.0091,  ..., -0.0221,  0.0009,  0.0221],
        [ 0.0244,  0.0169, -0.0314,  ...,  0.0166, -0.0112,  0.0114],
        ...,
        [ 0.0038,  0.0071, -0.0028,  ..., -0.0012,  0.0004, -0.0025],
        [ 0.0010,  0.0023, -0.0067,  ..., -0.0009,  0.0025,  0.0013],
        [ 0.0054, -0.0012, -0.0040,  ..., -0.0002,  0.0007,  0.0017]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.9382e-02, -3.1926e-03,  1.2906e-04,  ..., -2.1401e-04,
          2.8151e-04,  7.8203e-06],
        [-3.1926e-03,  1.6322e-02, -3.1035e-03,  ...,  2.7335e-04,
         -5.8846e-05,  7.3626e-05],
        [ 1.2906e-04, -3.1035e-03,  1.8466e-02,  ..., -4.8020e-05,
          2.6861e-04, -1.4699e-04],
        ...,
        [-2.1401e-04,  2.7335e-04, -4.8020e-05,  ...,  1.1946e-03,
         -6.3376e-04,  1.8597e-04],
        [ 2.8151e-04, -5.8846e-05,  2.6861e-04,  ..., -6.3376e-04,
          1.4077e-03, -6.4959e-04],
        [ 7.8203e-06,  7.3626e-05, -1.4699e-04,  ...,  1.8597e-04,
         -6.4959e-04,  1.1606e-03]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 85.100, Total time 0.56
 * Val loss 0.760, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 98.540, Total time 3.11
 * Val loss 0.046, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 71.900, Total time 0.57
 * Val loss 0.808, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 73.780, Total time 3.13
 * Val loss 0.754, Total time 0.00
**************************************************
validation split name: 3
 * Val Acc 69.100, Total time 0.56
 * Val loss 0.931, Total time 0.00
**************************************************
training split name: 3
 * Val Acc 71.660, Total time 3.15
 * Val loss 0.843, Total time 0.00
**************************************************
validation split name: 4
 * Val Acc 69.700, Total time 0.56
 * Val loss 0.900, Total time 0.00
**************************************************
training split name: 4
 * Val Acc 71.260, Total time 3.15
 * Val loss 0.816, Total time 0.00
**************************************************
validation split name: 5
 * Val Acc 74.400, Total time 0.57
 * Val loss 0.765, Total time 0.00
**************************************************
training split name: 5
 * Val Acc 78.320, Total time 3.17
 * Val loss 0.658, Total time 0.00
**************************************************
====================== 6 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0086 (0.0086)	2.460 (2.460)	3.12 (3.12)
[10/157]	0.0935 (0.0921)	0.0561 (0.0540)	2.160 (2.286)	34.38 (22.16)
[20/157]	0.0947 (0.0939)	0.0570 (0.0561)	1.730 (2.119)	53.12 (32.89)
[30/157]	0.0964 (0.0944)	0.0585 (0.0569)	1.689 (2.012)	50.00 (37.40)
[40/157]	0.0951 (0.0946)	0.0580 (0.0572)	1.573 (1.905)	53.12 (41.54)
[50/157]	0.0947 (0.0947)	0.0572 (0.0574)	1.506 (1.836)	46.88 (43.20)
[60/157]	0.0959 (0.0948)	0.0577 (0.0575)	1.510 (1.780)	50.00 (44.62)
[70/157]	0.1019 (0.0949)	0.0619 (0.0577)	1.371 (1.740)	56.25 (45.73)
[80/157]	0.0949 (0.0957)	0.0575 (0.0584)	1.324 (1.699)	59.38 (46.91)
[90/157]	0.0973 (0.0956)	0.0591 (0.0584)	1.292 (1.665)	59.38 (47.56)
[100/157]	0.0948 (0.0956)	0.0575 (0.0584)	1.415 (1.642)	46.88 (48.14)
[110/157]	0.0951 (0.0956)	0.0578 (0.0583)	1.631 (1.616)	46.88 (49.16)
[120/157]	0.0955 (0.0955)	0.0573 (0.0583)	1.265 (1.591)	59.38 (49.72)
[130/157]	0.1169 (0.0966)	0.0771 (0.0592)	1.204 (1.565)	59.38 (50.52)
[140/157]	0.0943 (0.0966)	0.0567 (0.0592)	1.311 (1.548)	37.50 (51.02)
[150/157]	0.0955 (0.0965)	0.0579 (0.0592)	1.126 (1.531)	53.12 (51.35)
[156/157]	0.0796 (0.0963)	0.0546 (0.0591)	1.796 (1.529)	25.00 (51.26)
 * Train Acc 51.260
 * Val Acc 56.300, Total time 0.62
 * Val loss 1.205, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0087 (0.0087)	1.337 (1.337)	53.12 (53.12)
[10/157]	0.0970 (0.0914)	0.0588 (0.0547)	1.448 (1.306)	56.25 (53.98)
[20/157]	0.0975 (0.0939)	0.0586 (0.0566)	1.535 (1.317)	43.75 (54.46)
[30/157]	0.0978 (0.0947)	0.0592 (0.0572)	1.266 (1.321)	62.50 (54.74)
[40/157]	0.0942 (0.0951)	0.0574 (0.0574)	1.151 (1.314)	65.62 (55.56)
[50/157]	0.0990 (0.0955)	0.0591 (0.0578)	1.248 (1.324)	56.25 (55.21)
[60/157]	0.0978 (0.0956)	0.0587 (0.0580)	0.978 (1.305)	68.75 (56.20)
[70/157]	0.0969 (0.0957)	0.0584 (0.0580)	0.953 (1.276)	75.00 (57.39)
[80/157]	0.0960 (0.0956)	0.0584 (0.0580)	0.995 (1.257)	78.12 (58.02)
[90/157]	0.0972 (0.0968)	0.0582 (0.0591)	1.279 (1.247)	59.38 (58.45)
[100/157]	0.0988 (0.0969)	0.0594 (0.0592)	1.079 (1.244)	65.62 (58.29)
[110/157]	0.0977 (0.0970)	0.0590 (0.0592)	1.345 (1.236)	50.00 (58.47)
[120/157]	0.0965 (0.0970)	0.0587 (0.0592)	1.647 (1.246)	37.50 (57.98)
[130/157]	0.0968 (0.0971)	0.0585 (0.0593)	1.383 (1.245)	56.25 (58.23)
[140/157]	0.0977 (0.0972)	0.0590 (0.0593)	1.564 (1.242)	53.12 (58.60)
[150/157]	0.0973 (0.0972)	0.0577 (0.0594)	1.281 (1.241)	53.12 (58.67)
[156/157]	0.0804 (0.0971)	0.0554 (0.0593)	1.247 (1.243)	75.00 (58.64)
 * Train Acc 58.640
 * Val Acc 61.200, Total time 0.60
 * Val loss 1.107, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0090 (0.0090)	1.101 (1.101)	56.25 (56.25)
[10/157]	0.0980 (0.0923)	0.0602 (0.0550)	1.348 (1.293)	56.25 (54.26)
[20/157]	0.0984 (0.0951)	0.0603 (0.0574)	1.071 (1.266)	68.75 (56.99)
[30/157]	0.0993 (0.0960)	0.0605 (0.0583)	1.164 (1.267)	59.38 (57.06)
[40/157]	0.0985 (0.0965)	0.0601 (0.0586)	1.273 (1.253)	56.25 (58.00)
[50/157]	0.0985 (0.0970)	0.0599 (0.0589)	1.435 (1.263)	62.50 (58.21)
[60/157]	0.0980 (0.0971)	0.0587 (0.0590)	1.199 (1.259)	53.12 (58.35)
[70/157]	0.0987 (0.0973)	0.0592 (0.0591)	1.408 (1.242)	62.50 (59.11)
[80/157]	0.0978 (0.0974)	0.0582 (0.0591)	1.102 (1.240)	59.38 (58.95)
[90/157]	0.0981 (0.0975)	0.0592 (0.0592)	0.909 (1.226)	75.00 (59.75)
[100/157]	0.1156 (0.0976)	0.0742 (0.0593)	1.522 (1.220)	46.88 (59.93)
[110/157]	0.1002 (0.0986)	0.0600 (0.0601)	1.112 (1.218)	65.62 (59.77)
[120/157]	0.1006 (0.0987)	0.0604 (0.0602)	1.016 (1.221)	68.75 (59.63)
[130/157]	0.0989 (0.0988)	0.0595 (0.0602)	1.047 (1.216)	65.62 (59.90)
[140/157]	0.1000 (0.0989)	0.0604 (0.0603)	1.515 (1.213)	46.88 (60.13)
[150/157]	0.1006 (0.0990)	0.0607 (0.0603)	0.944 (1.207)	78.12 (60.41)
[156/157]	0.0839 (0.0989)	0.0559 (0.0603)	1.089 (1.207)	62.50 (60.26)
 * Train Acc 60.260
 * Val Acc 62.500, Total time 0.60
 * Val loss 1.068, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0088 (0.0088)	0.855 (0.855)	78.12 (78.12)
[10/157]	0.1003 (0.0942)	0.0605 (0.0559)	1.068 (1.175)	62.50 (57.67)
[20/157]	0.1003 (0.0968)	0.0592 (0.0580)	1.171 (1.129)	65.62 (61.31)
[30/157]	0.0993 (0.0977)	0.0596 (0.0587)	0.954 (1.122)	71.88 (61.90)
[40/157]	0.1008 (0.0983)	0.0612 (0.0589)	1.018 (1.126)	68.75 (62.42)
[50/157]	0.1018 (0.0986)	0.0610 (0.0593)	1.390 (1.134)	46.88 (62.25)
[60/157]	0.1010 (0.0988)	0.0604 (0.0595)	1.236 (1.158)	53.12 (61.27)
[70/157]	0.0977 (0.0990)	0.0578 (0.0596)	1.178 (1.147)	68.75 (61.93)
[80/157]	0.1001 (0.0991)	0.0602 (0.0598)	1.258 (1.139)	56.25 (62.31)
[90/157]	0.1001 (0.0992)	0.0599 (0.0598)	1.249 (1.142)	46.88 (61.92)
[100/157]	0.0987 (0.0993)	0.0596 (0.0599)	1.045 (1.140)	59.38 (61.70)
[110/157]	0.0994 (0.0993)	0.0610 (0.0600)	1.330 (1.156)	65.62 (61.06)
[120/157]	0.1000 (0.0993)	0.0611 (0.0601)	1.434 (1.158)	46.88 (60.92)
[130/157]	0.1004 (0.0994)	0.0617 (0.0602)	0.918 (1.161)	78.12 (61.04)
[140/157]	0.0996 (0.0994)	0.0606 (0.0603)	1.030 (1.159)	65.62 (61.08)
[150/157]	0.0995 (0.0994)	0.0601 (0.0603)	1.208 (1.157)	62.50 (61.18)
[156/157]	0.0820 (0.0993)	0.0555 (0.0603)	1.890 (1.155)	50.00 (61.22)
 * Train Acc 61.220
 * Val Acc 63.900, Total time 0.59
 * Val loss 1.022, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0086 (0.0086)	1.024 (1.024)	68.75 (68.75)
[10/157]	0.0997 (0.0934)	0.0612 (0.0559)	1.293 (1.132)	62.50 (61.65)
[20/157]	0.1005 (0.0964)	0.0621 (0.0587)	1.351 (1.131)	53.12 (62.35)
[30/157]	0.0994 (0.0973)	0.0603 (0.0595)	1.014 (1.101)	62.50 (63.71)
[40/157]	0.1003 (0.0979)	0.0607 (0.0598)	1.023 (1.118)	62.50 (62.73)
[50/157]	0.0990 (0.0982)	0.0605 (0.0600)	1.152 (1.111)	50.00 (62.44)
[60/157]	0.0996 (0.0984)	0.0609 (0.0602)	0.992 (1.110)	75.00 (62.24)
[70/157]	0.0993 (0.0986)	0.0612 (0.0604)	1.042 (1.113)	71.88 (62.28)
[80/157]	0.1000 (0.0987)	0.0606 (0.0604)	1.003 (1.117)	62.50 (62.04)
[90/157]	0.1011 (0.0988)	0.0608 (0.0605)	1.494 (1.128)	46.88 (61.40)
[100/157]	0.0995 (0.0989)	0.0600 (0.0605)	1.168 (1.127)	59.38 (61.63)
[110/157]	0.1009 (0.0990)	0.0612 (0.0606)	1.196 (1.122)	59.38 (61.97)
[120/157]	0.1007 (0.0990)	0.0619 (0.0606)	1.209 (1.121)	59.38 (62.09)
[130/157]	0.0991 (0.0991)	0.0602 (0.0607)	1.113 (1.126)	62.50 (62.00)
[140/157]	0.1001 (0.0991)	0.0612 (0.0607)	1.155 (1.124)	53.12 (61.95)
[150/157]	0.1001 (0.0992)	0.0610 (0.0607)	1.348 (1.128)	56.25 (62.04)
[156/157]	0.0833 (0.0990)	0.0568 (0.0607)	1.156 (1.126)	50.00 (62.12)
 * Train Acc 62.120
 * Val Acc 64.300, Total time 0.59
 * Val loss 0.997, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0083 (0.0083)	1.023 (1.023)	68.75 (68.75)
[10/157]	0.1007 (0.0942)	0.0603 (0.0554)	0.894 (1.011)	75.00 (68.18)
[20/157]	0.1000 (0.0966)	0.0612 (0.0580)	1.366 (1.067)	53.12 (66.37)
[30/157]	0.0997 (0.0976)	0.0611 (0.0591)	0.955 (1.082)	68.75 (64.92)
[40/157]	0.0996 (0.0981)	0.0611 (0.0596)	1.127 (1.088)	68.75 (64.33)
[50/157]	0.0991 (0.0984)	0.0601 (0.0600)	0.927 (1.081)	68.75 (64.77)
[60/157]	0.0989 (0.0986)	0.0593 (0.0601)	1.108 (1.090)	59.38 (64.55)
[70/157]	0.0982 (0.0987)	0.0592 (0.0602)	1.011 (1.081)	65.62 (64.92)
[80/157]	0.1001 (0.0988)	0.0612 (0.0603)	1.423 (1.086)	53.12 (64.47)
[90/157]	0.1005 (0.0990)	0.0608 (0.0604)	1.043 (1.092)	62.50 (64.18)
[100/157]	0.1010 (0.0990)	0.0619 (0.0606)	0.927 (1.093)	65.62 (64.08)
[110/157]	0.1001 (0.0991)	0.0617 (0.0607)	0.906 (1.096)	65.62 (64.05)
[120/157]	0.1005 (0.0992)	0.0616 (0.0608)	1.220 (1.086)	59.38 (64.33)
[130/157]	0.0990 (0.0992)	0.0614 (0.0609)	1.126 (1.088)	71.88 (64.31)
[140/157]	0.0973 (0.0993)	0.0591 (0.0609)	1.111 (1.088)	65.62 (64.27)
[150/157]	0.0988 (0.0993)	0.0606 (0.0610)	0.813 (1.090)	78.12 (64.28)
[156/157]	0.0838 (0.0992)	0.0556 (0.0609)	1.665 (1.091)	50.00 (64.20)
 * Train Acc 64.200
 * Val Acc 65.300, Total time 0.60
 * Val loss 0.973, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0084 (0.0084)	1.074 (1.074)	59.38 (59.38)
[10/157]	0.0996 (0.0938)	0.0599 (0.0557)	1.343 (1.071)	53.12 (67.33)
[20/157]	0.0998 (0.0966)	0.0605 (0.0581)	1.038 (1.086)	59.38 (65.92)
[30/157]	0.0998 (0.0977)	0.0604 (0.0590)	0.908 (1.076)	81.25 (66.43)
[40/157]	0.1006 (0.0982)	0.0615 (0.0595)	1.019 (1.083)	68.75 (65.17)
[50/157]	0.1012 (0.0985)	0.0619 (0.0598)	1.116 (1.073)	65.62 (65.32)
[60/157]	0.0987 (0.0988)	0.0606 (0.0600)	0.797 (1.070)	81.25 (65.47)
[70/157]	0.1005 (0.0989)	0.0611 (0.0602)	1.102 (1.073)	56.25 (65.58)
[80/157]	0.0995 (0.0990)	0.0608 (0.0603)	1.282 (1.076)	62.50 (65.28)
[90/157]	0.0998 (0.0991)	0.0612 (0.0605)	0.870 (1.076)	68.75 (65.28)
[100/157]	0.0999 (0.0992)	0.0616 (0.0606)	1.095 (1.071)	62.50 (65.22)
[110/157]	0.0987 (0.0992)	0.0601 (0.0607)	1.152 (1.073)	56.25 (64.89)
[120/157]	0.1001 (0.0992)	0.0603 (0.0606)	0.887 (1.079)	68.75 (64.54)
[130/157]	0.0986 (0.0993)	0.0588 (0.0606)	1.066 (1.075)	62.50 (64.62)
[140/157]	0.0991 (0.0993)	0.0604 (0.0606)	1.259 (1.076)	56.25 (64.49)
[150/157]	0.1001 (0.0994)	0.0602 (0.0607)	1.293 (1.069)	56.25 (64.86)
[156/157]	0.0871 (0.0993)	0.0595 (0.0607)	1.430 (1.071)	50.00 (64.92)
 * Train Acc 64.920
 * Val Acc 64.900, Total time 0.59
 * Val loss 0.974, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0088 (0.0088)	1.441 (1.441)	43.75 (43.75)
[10/157]	0.1035 (0.0966)	0.0634 (0.0585)	0.937 (1.073)	68.75 (66.48)
[20/157]	0.1029 (0.0993)	0.0635 (0.0610)	1.445 (1.041)	46.88 (66.22)
[30/157]	0.1034 (0.1004)	0.0643 (0.0620)	1.194 (1.080)	59.38 (65.02)
[40/157]	0.1046 (0.1010)	0.0643 (0.0625)	1.353 (1.094)	46.88 (64.48)
[50/157]	0.1034 (0.1013)	0.0642 (0.0628)	0.999 (1.078)	75.00 (64.95)
[60/157]	0.1013 (0.1014)	0.0626 (0.0629)	0.744 (1.076)	78.12 (65.27)
[70/157]	0.1027 (0.1016)	0.0635 (0.0630)	1.025 (1.084)	68.75 (64.44)
[80/157]	0.1038 (0.1017)	0.0643 (0.0631)	1.151 (1.075)	50.00 (64.43)
[90/157]	0.1026 (0.1017)	0.0631 (0.0632)	1.199 (1.086)	62.50 (64.25)
[100/157]	0.1014 (0.1018)	0.0620 (0.0632)	1.101 (1.079)	65.62 (64.63)
[110/157]	0.1013 (0.1018)	0.0624 (0.0632)	1.124 (1.075)	62.50 (64.84)
[120/157]	0.1005 (0.1019)	0.0607 (0.0632)	1.401 (1.075)	53.12 (64.85)
[130/157]	0.1038 (0.1020)	0.0629 (0.0632)	0.989 (1.071)	68.75 (65.03)
[140/157]	0.1026 (0.1020)	0.0635 (0.0633)	1.296 (1.065)	56.25 (65.16)
[150/157]	0.1037 (0.1021)	0.0639 (0.0633)	1.341 (1.063)	56.25 (65.25)
[156/157]	0.0869 (0.1020)	0.0591 (0.0633)	1.202 (1.056)	62.50 (65.60)
 * Train Acc 65.600
 * Val Acc 67.600, Total time 0.57
 * Val loss 0.934, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0089 (0.0089)	1.437 (1.437)	46.88 (46.88)
[10/157]	0.1038 (0.0952)	0.0631 (0.0568)	1.050 (1.062)	59.38 (64.20)
[20/157]	0.1005 (0.0977)	0.0610 (0.0592)	1.234 (1.039)	56.25 (66.07)
[30/157]	0.0999 (0.0984)	0.0611 (0.0600)	1.048 (1.083)	68.75 (63.71)
[40/157]	0.1009 (0.0988)	0.0611 (0.0604)	0.949 (1.080)	65.62 (63.41)
[50/157]	0.1017 (0.0993)	0.0621 (0.0608)	0.916 (1.076)	78.12 (63.60)
[60/157]	0.1008 (0.0996)	0.0616 (0.0609)	1.009 (1.067)	68.75 (63.78)
[70/157]	0.0996 (0.0997)	0.0614 (0.0611)	0.882 (1.055)	71.88 (64.30)
[80/157]	0.1005 (0.0999)	0.0613 (0.0613)	1.054 (1.059)	68.75 (64.16)
[90/157]	0.0999 (0.0999)	0.0611 (0.0612)	1.234 (1.061)	56.25 (64.32)
[100/157]	0.0998 (0.0999)	0.0607 (0.0613)	0.711 (1.061)	75.00 (64.57)
[110/157]	0.1015 (0.1000)	0.0622 (0.0615)	1.206 (1.069)	56.25 (64.02)
[120/157]	0.0991 (0.1001)	0.0608 (0.0615)	0.904 (1.067)	68.75 (63.97)
[130/157]	0.0995 (0.1001)	0.0614 (0.0615)	1.150 (1.064)	53.12 (64.00)
[140/157]	0.1031 (0.1001)	0.0633 (0.0616)	0.834 (1.057)	75.00 (64.32)
[150/157]	0.1007 (0.1002)	0.0617 (0.0616)	1.091 (1.058)	59.38 (64.36)
[156/157]	0.0846 (0.1001)	0.0577 (0.0617)	0.993 (1.058)	62.50 (64.50)
 * Train Acc 64.500
 * Val Acc 67.700, Total time 0.59
 * Val loss 0.906, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0089 (0.0089)	1.043 (1.043)	65.62 (65.62)
[10/157]	0.1006 (0.0944)	0.0617 (0.0566)	0.933 (0.968)	68.75 (66.48)
[20/157]	0.0998 (0.0970)	0.0611 (0.0590)	1.132 (1.012)	65.62 (66.67)
[30/157]	0.0999 (0.0980)	0.0612 (0.0599)	1.367 (1.034)	50.00 (65.42)
[40/157]	0.1007 (0.0986)	0.0625 (0.0603)	0.919 (1.028)	65.62 (65.78)
[50/157]	0.1011 (0.0991)	0.0616 (0.0608)	0.850 (1.038)	65.62 (65.75)
[60/157]	0.1004 (0.0993)	0.0614 (0.0610)	1.070 (1.025)	75.00 (66.09)
[70/157]	0.0991 (0.0996)	0.0596 (0.0612)	0.831 (1.023)	75.00 (66.46)
[80/157]	0.1014 (0.0997)	0.0622 (0.0612)	1.005 (1.018)	62.50 (66.59)
[90/157]	0.1015 (0.0998)	0.0612 (0.0613)	1.289 (1.019)	53.12 (66.72)
[100/157]	0.1024 (0.0999)	0.0618 (0.0614)	0.766 (1.009)	71.88 (66.77)
[110/157]	0.1007 (0.1000)	0.0613 (0.0614)	0.775 (1.006)	78.12 (66.86)
[120/157]	0.1000 (0.1000)	0.0612 (0.0614)	0.881 (1.001)	75.00 (67.25)
[130/157]	0.1007 (0.1001)	0.0614 (0.0615)	1.086 (1.004)	65.62 (67.20)
[140/157]	0.0999 (0.1001)	0.0607 (0.0615)	0.996 (1.005)	65.62 (67.24)
[150/157]	0.1026 (0.1002)	0.0631 (0.0616)	1.243 (1.011)	68.75 (66.97)
[156/157]	0.0841 (0.1001)	0.0570 (0.0615)	1.461 (1.017)	62.50 (66.80)
 * Train Acc 66.800
 * Val Acc 66.800, Total time 0.59
 * Val loss 0.921, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0086 (0.0086)	0.896 (0.896)	71.88 (71.88)
[10/157]	0.1015 (0.0949)	0.0627 (0.0574)	1.007 (1.005)	62.50 (64.77)
[20/157]	0.1000 (0.0977)	0.0610 (0.0597)	1.033 (0.997)	59.38 (67.11)
[30/157]	0.1035 (0.0989)	0.0635 (0.0606)	1.075 (1.000)	59.38 (67.24)
[40/157]	0.1006 (0.0993)	0.0615 (0.0608)	1.025 (1.009)	62.50 (67.15)
[50/157]	0.0997 (0.0994)	0.0608 (0.0609)	1.045 (1.003)	65.62 (67.46)
[60/157]	0.0994 (0.0995)	0.0608 (0.0610)	0.870 (1.014)	71.88 (66.91)
[70/157]	0.1001 (0.0996)	0.0618 (0.0611)	0.813 (1.004)	71.88 (66.95)
[80/157]	0.1013 (0.0999)	0.0615 (0.0613)	0.897 (1.010)	71.88 (66.86)
[90/157]	0.1000 (0.0999)	0.0603 (0.0613)	0.660 (1.013)	78.12 (66.83)
[100/157]	0.1007 (0.0999)	0.0608 (0.0613)	1.222 (1.012)	65.62 (67.11)
[110/157]	0.1003 (0.0999)	0.0611 (0.0613)	0.954 (1.012)	62.50 (66.95)
[120/157]	0.1013 (0.1001)	0.0619 (0.0614)	1.018 (1.010)	59.38 (66.86)
[130/157]	0.1001 (0.1001)	0.0602 (0.0614)	0.957 (1.012)	68.75 (66.84)
[140/157]	0.1029 (0.1001)	0.0629 (0.0615)	0.790 (1.006)	78.12 (67.18)
[150/157]	0.1015 (0.1002)	0.0622 (0.0615)	1.020 (1.010)	65.62 (67.03)
[156/157]	0.0841 (0.1001)	0.0565 (0.0615)	1.631 (1.010)	25.00 (66.94)
 * Train Acc 66.940
 * Val Acc 67.500, Total time 0.59
 * Val loss 0.903, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0091 (0.0091)	0.848 (0.848)	68.75 (68.75)
[10/157]	0.1001 (0.0947)	0.0610 (0.0565)	1.183 (1.004)	65.62 (68.18)
[20/157]	0.1002 (0.0973)	0.0608 (0.0588)	0.773 (0.956)	75.00 (69.05)
[30/157]	0.0996 (0.0984)	0.0607 (0.0598)	1.141 (0.961)	65.62 (70.06)
[40/157]	0.0998 (0.0990)	0.0607 (0.0603)	1.186 (0.986)	56.25 (68.67)
[50/157]	0.0989 (0.0992)	0.0604 (0.0605)	1.426 (1.004)	43.75 (67.34)
[60/157]	0.1012 (0.0994)	0.0608 (0.0607)	0.888 (1.004)	68.75 (67.26)
[70/157]	0.0967 (0.0987)	0.0585 (0.0603)	1.181 (1.008)	65.62 (67.03)
[80/157]	0.0969 (0.0983)	0.0586 (0.0600)	1.291 (1.022)	50.00 (66.51)
[90/157]	0.0949 (0.0979)	0.0570 (0.0598)	1.224 (1.025)	68.75 (66.38)
[100/157]	0.0949 (0.0985)	0.0566 (0.0602)	0.740 (1.019)	81.25 (66.62)
[110/157]	0.1004 (0.0983)	0.0613 (0.0600)	1.022 (1.012)	65.62 (66.89)
[120/157]	0.1004 (0.0985)	0.0601 (0.0601)	0.713 (1.015)	75.00 (66.84)
[130/157]	0.1009 (0.0986)	0.0611 (0.0601)	1.028 (1.011)	68.75 (67.01)
[140/157]	0.0993 (0.0987)	0.0600 (0.0601)	1.038 (1.017)	62.50 (66.80)
[150/157]	0.1014 (0.0988)	0.0605 (0.0602)	0.951 (1.010)	78.12 (67.16)
[156/157]	0.0820 (0.0987)	0.0554 (0.0602)	1.019 (1.012)	87.50 (67.06)
 * Train Acc 67.060
 * Val Acc 69.200, Total time 0.58
 * Val loss 0.883, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0088 (0.0088)	1.143 (1.143)	53.12 (53.12)
[10/157]	0.0998 (0.0936)	0.0606 (0.0563)	0.915 (1.078)	71.88 (62.78)
[20/157]	0.0985 (0.0964)	0.0600 (0.0587)	0.969 (1.036)	68.75 (65.03)
[30/157]	0.0970 (0.0974)	0.0586 (0.0595)	0.802 (1.002)	75.00 (67.54)
[40/157]	0.0991 (0.0979)	0.0601 (0.0599)	1.100 (1.013)	62.50 (66.84)
[50/157]	0.1015 (0.0983)	0.0615 (0.0600)	1.030 (1.005)	59.38 (67.03)
[60/157]	0.0981 (0.0985)	0.0601 (0.0601)	1.129 (1.014)	59.38 (66.96)
[70/157]	0.0994 (0.0986)	0.0611 (0.0604)	0.931 (1.014)	71.88 (66.68)
[80/157]	0.0988 (0.0987)	0.0606 (0.0605)	1.054 (1.006)	62.50 (66.82)
[90/157]	0.1001 (0.0988)	0.0606 (0.0605)	1.139 (1.000)	62.50 (66.93)
[100/157]	0.0997 (0.0989)	0.0603 (0.0605)	1.198 (1.003)	59.38 (66.49)
[110/157]	0.1003 (0.0990)	0.0608 (0.0606)	1.331 (1.008)	53.12 (66.33)
[120/157]	0.0997 (0.0990)	0.0610 (0.0606)	0.904 (1.005)	68.75 (66.61)
[130/157]	0.1004 (0.0991)	0.0615 (0.0606)	1.096 (1.004)	65.62 (66.58)
[140/157]	0.0997 (0.0991)	0.0608 (0.0607)	1.117 (1.002)	56.25 (66.64)
[150/157]	0.1010 (0.0991)	0.0613 (0.0607)	1.118 (1.007)	65.62 (66.66)
[156/157]	0.0813 (0.0990)	0.0556 (0.0606)	1.216 (1.010)	50.00 (66.48)
 * Train Acc 66.480
 * Val Acc 69.900, Total time 0.60
 * Val loss 0.870, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0089 (0.0089)	1.075 (1.075)	68.75 (68.75)
[10/157]	0.0999 (0.0943)	0.0583 (0.0562)	0.835 (0.993)	78.12 (69.60)
[20/157]	0.0993 (0.0968)	0.0596 (0.0583)	0.814 (0.941)	75.00 (70.83)
[30/157]	0.0988 (0.0978)	0.0603 (0.0590)	0.950 (0.959)	71.88 (69.76)
[40/157]	0.1002 (0.0983)	0.0602 (0.0595)	1.048 (0.959)	62.50 (69.05)
[50/157]	0.1007 (0.0986)	0.0601 (0.0597)	1.226 (0.958)	56.25 (69.06)
[60/157]	0.0991 (0.0988)	0.0599 (0.0597)	0.948 (0.955)	75.00 (69.42)
[70/157]	0.1004 (0.0989)	0.0615 (0.0600)	1.104 (0.961)	56.25 (68.97)
[80/157]	0.1002 (0.0991)	0.0613 (0.0601)	0.918 (0.966)	59.38 (68.40)
[90/157]	0.1024 (0.0992)	0.0620 (0.0602)	0.930 (0.969)	68.75 (68.23)
[100/157]	0.1003 (0.0992)	0.0610 (0.0603)	1.016 (0.970)	71.88 (68.13)
[110/157]	0.1004 (0.0993)	0.0611 (0.0604)	1.106 (0.973)	62.50 (67.99)
[120/157]	0.1015 (0.0994)	0.0616 (0.0604)	1.060 (0.973)	65.62 (67.87)
[130/157]	0.1004 (0.0994)	0.0614 (0.0605)	1.039 (0.973)	68.75 (67.68)
[140/157]	0.1012 (0.0994)	0.0615 (0.0606)	0.808 (0.975)	75.00 (67.86)
[150/157]	0.1011 (0.0995)	0.0625 (0.0606)	0.937 (0.982)	68.75 (67.51)
[156/157]	0.0852 (0.0994)	0.0572 (0.0607)	1.166 (0.980)	62.50 (67.58)
 * Train Acc 67.580
 * Val Acc 68.700, Total time 0.59
 * Val loss 0.872, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0085 (0.0085)	1.294 (1.294)	59.38 (59.38)
[10/157]	0.0979 (0.0940)	0.0596 (0.0561)	1.119 (1.009)	65.62 (67.90)
[20/157]	0.1005 (0.0967)	0.0611 (0.0587)	1.215 (1.000)	56.25 (68.01)
[30/157]	0.1006 (0.0977)	0.0610 (0.0596)	0.818 (0.988)	71.88 (68.25)
[40/157]	0.1015 (0.0984)	0.0617 (0.0600)	1.285 (1.000)	50.00 (67.38)
[50/157]	0.1003 (0.0987)	0.0605 (0.0602)	0.867 (0.983)	75.00 (67.83)
[60/157]	0.1004 (0.0990)	0.0605 (0.0602)	1.016 (0.987)	68.75 (67.57)
[70/157]	0.1005 (0.0992)	0.0597 (0.0603)	0.922 (0.977)	65.62 (68.13)
[80/157]	0.1016 (0.0993)	0.0606 (0.0602)	0.876 (0.969)	78.12 (68.17)
[90/157]	0.1000 (0.0993)	0.0601 (0.0603)	1.019 (0.975)	59.38 (67.75)
[100/157]	0.1000 (0.0994)	0.0605 (0.0603)	0.953 (0.971)	65.62 (67.70)
[110/157]	0.0990 (0.0994)	0.0595 (0.0603)	0.697 (0.970)	78.12 (67.71)
[120/157]	0.1014 (0.0995)	0.0614 (0.0603)	0.976 (0.967)	71.88 (67.98)
[130/157]	0.0984 (0.0995)	0.0597 (0.0604)	0.771 (0.969)	75.00 (67.84)
[140/157]	0.1010 (0.0995)	0.0615 (0.0605)	0.963 (0.971)	65.62 (67.71)
[150/157]	0.1001 (0.0995)	0.0606 (0.0605)	0.852 (0.970)	78.12 (67.72)
[156/157]	0.0835 (0.0994)	0.0561 (0.0605)	1.386 (0.972)	75.00 (67.72)
 * Train Acc 67.720
 * Val Acc 68.600, Total time 0.59
 * Val loss 0.866, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0087 (0.0087)	0.855 (0.855)	75.00 (75.00)
[10/157]	0.0993 (0.0942)	0.0596 (0.0556)	0.780 (1.113)	84.38 (64.77)
[20/157]	0.1014 (0.0969)	0.0617 (0.0582)	0.880 (1.023)	75.00 (67.56)
[30/157]	0.1012 (0.0979)	0.0611 (0.0591)	0.888 (1.007)	71.88 (66.94)
[40/157]	0.0999 (0.0984)	0.0604 (0.0596)	0.847 (1.006)	75.00 (67.23)
[50/157]	0.0999 (0.0987)	0.0602 (0.0598)	0.826 (0.991)	75.00 (67.65)
[60/157]	0.0993 (0.0989)	0.0602 (0.0599)	0.852 (0.984)	71.88 (67.62)
[70/157]	0.0999 (0.0990)	0.0614 (0.0601)	0.891 (0.974)	68.75 (67.78)
[80/157]	0.1001 (0.0992)	0.0600 (0.0603)	0.924 (0.977)	68.75 (67.55)
[90/157]	0.1004 (0.0993)	0.0611 (0.0603)	1.096 (0.971)	62.50 (67.96)
[100/157]	0.1009 (0.0993)	0.0612 (0.0604)	1.098 (0.979)	68.75 (67.61)
[110/157]	0.1003 (0.0994)	0.0606 (0.0605)	0.896 (0.974)	62.50 (67.54)
[120/157]	0.1015 (0.0994)	0.0623 (0.0605)	0.812 (0.977)	71.88 (67.56)
[130/157]	0.0996 (0.0994)	0.0609 (0.0606)	1.287 (0.983)	50.00 (67.29)
[140/157]	0.0997 (0.0994)	0.0614 (0.0606)	1.240 (0.985)	53.12 (67.42)
[150/157]	0.0992 (0.0994)	0.0606 (0.0607)	0.986 (0.986)	71.88 (67.36)
[156/157]	0.0828 (0.0993)	0.0565 (0.0607)	1.985 (0.982)	37.50 (67.62)
 * Train Acc 67.620
 * Val Acc 69.700, Total time 0.60
 * Val loss 0.851, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0457 (0.0457)	0.0090 (0.0090)	0.982 (0.982)	65.62 (65.62)
[10/157]	0.1000 (0.0941)	0.0612 (0.0564)	1.178 (0.984)	53.12 (66.19)
[20/157]	0.0996 (0.0966)	0.0610 (0.0588)	0.794 (0.960)	78.12 (67.41)
[30/157]	0.1006 (0.0976)	0.0608 (0.0595)	0.998 (0.981)	68.75 (67.44)
[40/157]	0.0999 (0.0982)	0.0605 (0.0598)	0.916 (0.983)	62.50 (67.23)
[50/157]	0.0994 (0.0984)	0.0598 (0.0600)	1.164 (0.982)	56.25 (67.28)
[60/157]	0.0989 (0.0987)	0.0605 (0.0601)	1.169 (0.981)	68.75 (67.88)
[70/157]	0.0992 (0.0988)	0.0602 (0.0602)	0.899 (0.974)	75.00 (68.27)
[80/157]	0.0994 (0.0990)	0.0605 (0.0604)	0.810 (0.976)	78.12 (68.36)
[90/157]	0.1010 (0.0991)	0.0602 (0.0605)	0.763 (0.969)	75.00 (68.72)
[100/157]	0.1003 (0.0992)	0.0616 (0.0606)	0.926 (0.968)	68.75 (68.63)
[110/157]	0.1012 (0.0993)	0.0617 (0.0607)	1.163 (0.970)	56.25 (68.52)
[120/157]	0.1002 (0.0994)	0.0605 (0.0607)	0.984 (0.960)	71.88 (68.78)
[130/157]	0.0999 (0.0994)	0.0599 (0.0607)	0.875 (0.956)	65.62 (68.75)
[140/157]	0.0993 (0.0994)	0.0594 (0.0607)	0.767 (0.957)	78.12 (68.79)
[150/157]	0.1005 (0.0995)	0.0607 (0.0606)	0.935 (0.956)	71.88 (68.77)
[156/157]	0.0830 (0.0994)	0.0551 (0.0606)	1.078 (0.956)	87.50 (68.76)
 * Train Acc 68.760
 * Val Acc 70.300, Total time 0.60
 * Val loss 0.851, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0091 (0.0091)	0.968 (0.968)	65.62 (65.62)
[10/157]	0.0995 (0.0940)	0.0602 (0.0561)	0.785 (0.983)	78.12 (66.48)
[20/157]	0.1002 (0.0966)	0.0604 (0.0582)	0.932 (0.955)	75.00 (68.01)
[30/157]	0.1010 (0.0977)	0.0607 (0.0590)	0.876 (0.945)	71.88 (68.15)
[40/157]	0.1001 (0.0983)	0.0602 (0.0595)	0.859 (0.953)	65.62 (67.76)
[50/157]	0.0995 (0.0986)	0.0601 (0.0597)	1.076 (0.967)	65.62 (67.16)
[60/157]	0.1014 (0.0988)	0.0612 (0.0599)	0.921 (0.953)	71.88 (68.08)
[70/157]	0.1004 (0.0990)	0.0602 (0.0599)	0.947 (0.949)	71.88 (68.57)
[80/157]	0.1001 (0.0991)	0.0604 (0.0600)	1.278 (0.965)	53.12 (67.94)
[90/157]	0.1002 (0.0992)	0.0606 (0.0601)	0.708 (0.964)	71.88 (68.03)
[100/157]	0.1003 (0.0993)	0.0607 (0.0602)	1.192 (0.968)	78.12 (68.07)
[110/157]	0.1005 (0.0994)	0.0608 (0.0602)	0.989 (0.971)	68.75 (68.05)
[120/157]	0.1000 (0.0994)	0.0604 (0.0602)	1.051 (0.970)	62.50 (68.13)
[130/157]	0.0996 (0.0994)	0.0596 (0.0602)	0.882 (0.965)	75.00 (68.44)
[140/157]	0.1009 (0.0995)	0.0608 (0.0602)	0.900 (0.962)	78.12 (68.59)
[150/157]	0.1011 (0.0995)	0.0613 (0.0602)	1.016 (0.966)	65.62 (68.50)
[156/157]	0.0819 (0.0994)	0.0549 (0.0602)	1.624 (0.965)	37.50 (68.52)
 * Train Acc 68.520
 * Val Acc 70.300, Total time 0.59
 * Val loss 0.845, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0088 (0.0088)	0.828 (0.828)	71.88 (71.88)
[10/157]	0.0989 (0.0939)	0.0592 (0.0556)	1.012 (0.896)	65.62 (69.89)
[20/157]	0.1007 (0.0968)	0.0599 (0.0580)	1.039 (0.896)	65.62 (70.98)
[30/157]	0.1006 (0.0978)	0.0613 (0.0589)	0.818 (0.915)	71.88 (69.15)
[40/157]	0.1000 (0.0984)	0.0594 (0.0594)	0.753 (0.910)	78.12 (70.05)
[50/157]	0.1005 (0.0987)	0.0605 (0.0595)	0.801 (0.927)	78.12 (69.85)
[60/157]	0.1000 (0.0989)	0.0595 (0.0597)	1.129 (0.927)	59.38 (69.62)
[70/157]	0.0999 (0.0990)	0.0605 (0.0598)	0.769 (0.926)	78.12 (69.63)
[80/157]	0.1005 (0.0992)	0.0608 (0.0599)	0.831 (0.925)	71.88 (69.91)
[90/157]	0.1004 (0.0993)	0.0600 (0.0600)	0.984 (0.939)	62.50 (69.40)
[100/157]	0.0996 (0.0993)	0.0599 (0.0600)	1.125 (0.937)	62.50 (69.62)
[110/157]	0.0995 (0.0994)	0.0599 (0.0600)	0.911 (0.932)	68.75 (69.82)
[120/157]	0.1001 (0.0994)	0.0604 (0.0601)	1.173 (0.935)	68.75 (69.65)
[130/157]	0.0999 (0.0995)	0.0601 (0.0602)	0.881 (0.930)	75.00 (69.82)
[140/157]	0.0996 (0.0995)	0.0602 (0.0602)	1.202 (0.933)	53.12 (69.61)
[150/157]	0.1014 (0.0995)	0.0613 (0.0602)	1.223 (0.935)	65.62 (69.43)
[156/157]	0.0843 (0.0994)	0.0561 (0.0602)	0.935 (0.938)	75.00 (69.40)
 * Train Acc 69.400
 * Val Acc 71.000, Total time 0.59
 * Val loss 0.841, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0460 (0.0460)	0.0089 (0.0089)	0.659 (0.659)	78.12 (78.12)
[10/157]	0.1003 (0.0943)	0.0611 (0.0561)	0.939 (0.885)	75.00 (70.17)
[20/157]	0.1000 (0.0969)	0.0607 (0.0582)	1.022 (0.941)	68.75 (70.24)
[30/157]	0.1003 (0.0979)	0.0595 (0.0588)	0.797 (0.933)	81.25 (70.36)
[40/157]	0.0985 (0.0983)	0.0591 (0.0592)	1.235 (0.934)	62.50 (69.89)
[50/157]	0.1004 (0.0986)	0.0606 (0.0596)	1.164 (0.935)	65.62 (69.85)
[60/157]	0.1002 (0.0989)	0.0607 (0.0597)	0.890 (0.938)	68.75 (69.36)
[70/157]	0.1007 (0.0990)	0.0608 (0.0598)	0.992 (0.942)	65.62 (69.10)
[80/157]	0.1007 (0.0991)	0.0615 (0.0600)	1.007 (0.933)	59.38 (69.21)
[90/157]	0.0999 (0.0992)	0.0599 (0.0601)	0.944 (0.936)	68.75 (69.13)
[100/157]	0.1005 (0.0993)	0.0616 (0.0602)	0.751 (0.933)	78.12 (69.68)
[110/157]	0.0997 (0.0994)	0.0597 (0.0602)	0.864 (0.933)	78.12 (69.68)
[120/157]	0.1014 (0.0994)	0.0615 (0.0603)	1.068 (0.935)	68.75 (69.60)
[130/157]	0.1000 (0.0995)	0.0595 (0.0604)	0.855 (0.934)	75.00 (69.61)
[140/157]	0.1021 (0.0995)	0.0609 (0.0604)	1.068 (0.940)	68.75 (69.44)
[150/157]	0.1008 (0.0995)	0.0613 (0.0604)	1.123 (0.940)	68.75 (69.37)
[156/157]	0.0810 (0.0994)	0.0543 (0.0603)	0.849 (0.941)	75.00 (69.36)
 * Train Acc 69.360
 * Val Acc 70.900, Total time 0.60
 * Val loss 0.836, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0088 (0.0088)	0.928 (0.928)	65.62 (65.62)
[10/157]	0.1018 (0.0942)	0.0612 (0.0558)	0.832 (0.990)	84.38 (65.62)
[20/157]	0.1012 (0.0967)	0.0615 (0.0581)	1.015 (0.944)	75.00 (69.20)
[30/157]	0.1011 (0.0978)	0.0613 (0.0590)	1.080 (0.944)	71.88 (68.95)
[40/157]	0.1002 (0.0984)	0.0602 (0.0594)	1.091 (0.927)	50.00 (68.75)
[50/157]	0.1001 (0.0987)	0.0602 (0.0596)	0.907 (0.934)	75.00 (68.44)
[60/157]	0.1011 (0.0989)	0.0607 (0.0598)	0.741 (0.942)	78.12 (67.88)
[70/157]	0.0992 (0.0990)	0.0597 (0.0599)	0.874 (0.946)	71.88 (68.09)
[80/157]	0.0954 (0.0992)	0.0558 (0.0600)	0.990 (0.954)	71.88 (68.13)
[90/157]	0.1003 (0.0993)	0.0607 (0.0601)	0.810 (0.949)	65.62 (68.13)
[100/157]	0.0995 (0.0993)	0.0602 (0.0602)	1.114 (0.940)	65.62 (68.97)
[110/157]	0.1002 (0.0994)	0.0600 (0.0602)	0.685 (0.932)	78.12 (69.40)
[120/157]	0.1000 (0.0994)	0.0599 (0.0602)	0.891 (0.942)	71.88 (69.19)
[130/157]	0.1001 (0.0995)	0.0607 (0.0602)	0.971 (0.941)	59.38 (69.08)
[140/157]	0.0992 (0.0995)	0.0591 (0.0602)	0.833 (0.947)	71.88 (68.82)
[150/157]	0.1004 (0.0995)	0.0606 (0.0602)	1.208 (0.946)	62.50 (68.96)
[156/157]	0.0836 (0.0994)	0.0563 (0.0602)	0.662 (0.945)	75.00 (68.92)
 * Train Acc 68.920
 * Val Acc 70.800, Total time 0.59
 * Val loss 0.827, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0077 (0.0077)	1.058 (1.058)	56.25 (56.25)
[10/157]	0.1014 (0.0943)	0.0605 (0.0558)	0.629 (0.852)	87.50 (72.44)
[20/157]	0.1009 (0.0968)	0.0599 (0.0579)	0.977 (0.887)	62.50 (71.73)
[30/157]	0.1011 (0.0979)	0.0612 (0.0588)	1.133 (0.908)	65.62 (70.87)
[40/157]	0.1004 (0.0984)	0.0606 (0.0593)	0.991 (0.906)	68.75 (70.81)
[50/157]	0.1005 (0.0987)	0.0610 (0.0596)	0.915 (0.912)	68.75 (70.77)
[60/157]	0.1009 (0.0989)	0.0604 (0.0598)	1.081 (0.907)	68.75 (71.26)
[70/157]	0.0997 (0.0990)	0.0598 (0.0599)	0.779 (0.918)	78.12 (70.86)
[80/157]	0.1030 (0.0992)	0.0620 (0.0600)	1.170 (0.916)	59.38 (70.79)
[90/157]	0.0987 (0.0992)	0.0600 (0.0601)	1.203 (0.929)	53.12 (70.05)
[100/157]	0.1014 (0.0993)	0.0614 (0.0603)	0.765 (0.933)	75.00 (69.65)
[110/157]	0.0998 (0.0994)	0.0606 (0.0603)	1.317 (0.947)	59.38 (69.23)
[120/157]	0.1007 (0.0994)	0.0603 (0.0604)	1.001 (0.942)	75.00 (69.42)
[130/157]	0.0990 (0.0995)	0.0594 (0.0604)	0.610 (0.945)	78.12 (69.37)
[140/157]	0.1001 (0.0995)	0.0608 (0.0604)	1.218 (0.939)	62.50 (69.53)
[150/157]	0.1006 (0.0995)	0.0608 (0.0605)	0.769 (0.939)	78.12 (69.62)
[156/157]	0.0822 (0.0994)	0.0556 (0.0604)	1.187 (0.936)	62.50 (69.74)
 * Train Acc 69.740
 * Val Acc 69.400, Total time 0.60
 * Val loss 0.849, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0091 (0.0091)	0.858 (0.858)	81.25 (81.25)
[10/157]	0.1098 (0.1068)	0.0700 (0.0678)	0.857 (0.970)	65.62 (69.32)
[20/157]	0.0956 (0.1059)	0.0570 (0.0670)	0.784 (0.922)	75.00 (69.49)
[30/157]	0.0951 (0.1026)	0.0578 (0.0642)	1.064 (0.958)	65.62 (67.74)
[40/157]	0.1033 (0.1011)	0.0625 (0.0629)	0.734 (0.946)	78.12 (68.45)
[50/157]	0.1033 (0.1014)	0.0637 (0.0631)	0.852 (0.938)	68.75 (68.87)
[60/157]	0.1046 (0.1018)	0.0644 (0.0632)	1.051 (0.940)	68.75 (68.95)
[70/157]	0.1028 (0.1019)	0.0634 (0.0632)	1.226 (0.936)	59.38 (68.71)
[80/157]	0.1053 (0.1021)	0.0654 (0.0633)	0.914 (0.934)	71.88 (69.06)
[90/157]	0.1031 (0.1021)	0.0642 (0.0634)	0.816 (0.932)	71.88 (68.96)
[100/157]	0.1015 (0.1023)	0.0625 (0.0635)	1.203 (0.938)	56.25 (68.41)
[110/157]	0.1046 (0.1024)	0.0651 (0.0636)	0.782 (0.934)	81.25 (68.33)
[120/157]	0.0961 (0.1021)	0.0573 (0.0633)	0.717 (0.931)	75.00 (68.49)
[130/157]	0.1108 (0.1024)	0.0696 (0.0636)	0.636 (0.931)	87.50 (68.51)
[140/157]	0.0969 (0.1020)	0.0583 (0.0633)	0.845 (0.931)	65.62 (68.68)
[150/157]	0.0975 (0.1017)	0.0584 (0.0630)	0.782 (0.930)	81.25 (68.79)
[156/157]	0.0799 (0.1014)	0.0530 (0.0628)	0.849 (0.926)	50.00 (69.02)
 * Train Acc 69.020
 * Val Acc 72.100, Total time 0.59
 * Val loss 0.813, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0087 (0.0087)	1.028 (1.028)	62.50 (62.50)
[10/157]	0.1048 (0.0941)	0.0649 (0.0564)	1.098 (0.907)	65.62 (69.89)
[20/157]	0.1052 (0.0987)	0.0657 (0.0603)	0.850 (0.906)	71.88 (69.05)
[30/157]	0.1034 (0.1005)	0.0629 (0.0616)	1.316 (0.907)	56.25 (68.95)
[40/157]	0.1045 (0.1013)	0.0648 (0.0623)	1.075 (0.919)	59.38 (68.83)
[50/157]	0.1044 (0.1018)	0.0650 (0.0627)	1.073 (0.926)	68.75 (68.32)
[60/157]	0.1043 (0.1021)	0.0648 (0.0631)	0.816 (0.929)	75.00 (68.44)
[70/157]	0.1063 (0.1023)	0.0653 (0.0633)	0.749 (0.917)	78.12 (69.28)
[80/157]	0.0954 (0.1024)	0.0579 (0.0633)	1.157 (0.926)	65.62 (68.90)
[90/157]	0.0953 (0.1016)	0.0577 (0.0628)	1.218 (0.930)	65.62 (68.96)
[100/157]	0.0964 (0.1009)	0.0586 (0.0623)	0.885 (0.932)	71.88 (69.00)
[110/157]	0.0974 (0.1004)	0.0567 (0.0619)	1.029 (0.938)	62.50 (68.89)
[120/157]	0.0976 (0.1003)	0.0593 (0.0618)	0.699 (0.931)	84.38 (69.19)
[130/157]	0.0995 (0.1003)	0.0604 (0.0618)	0.643 (0.925)	84.38 (69.49)
[140/157]	0.0994 (0.1002)	0.0604 (0.0617)	0.802 (0.930)	68.75 (69.22)
[150/157]	0.0999 (0.1002)	0.0603 (0.0616)	0.715 (0.927)	71.88 (69.37)
[156/157]	0.0965 (0.1007)	0.0698 (0.0622)	0.813 (0.922)	75.00 (69.50)
 * Train Acc 69.500
 * Val Acc 70.200, Total time 0.58
 * Val loss 0.824, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0413 (0.0413)	0.0084 (0.0084)	1.048 (1.048)	65.62 (65.62)
[10/157]	0.1129 (0.1011)	0.0736 (0.0627)	1.068 (0.978)	62.50 (67.33)
[20/157]	0.0965 (0.1038)	0.0587 (0.0652)	0.735 (0.952)	75.00 (68.60)
[30/157]	0.0988 (0.1018)	0.0593 (0.0632)	0.864 (0.931)	78.12 (69.35)
[40/157]	0.0985 (0.1008)	0.0593 (0.0623)	1.098 (0.913)	59.38 (69.66)
[50/157]	0.0987 (0.1002)	0.0593 (0.0617)	0.975 (0.931)	65.62 (68.87)
[60/157]	0.0990 (0.0998)	0.0567 (0.0612)	0.834 (0.924)	78.12 (69.57)
[70/157]	0.1205 (0.1000)	0.0801 (0.0615)	0.730 (0.911)	78.12 (70.29)
[80/157]	0.0979 (0.1004)	0.0591 (0.0620)	1.012 (0.920)	65.62 (69.56)
[90/157]	0.0956 (0.0999)	0.0578 (0.0615)	0.984 (0.923)	68.75 (69.75)
[100/157]	0.0960 (0.0995)	0.0582 (0.0612)	0.739 (0.919)	81.25 (70.20)
[110/157]	0.1034 (0.0998)	0.0641 (0.0615)	0.786 (0.919)	78.12 (70.27)
[120/157]	0.1038 (0.1001)	0.0641 (0.0617)	0.999 (0.922)	68.75 (70.17)
[130/157]	0.1057 (0.1004)	0.0663 (0.0620)	1.119 (0.918)	59.38 (70.23)
[140/157]	0.1049 (0.1006)	0.0657 (0.0622)	0.793 (0.912)	75.00 (70.37)
[150/157]	0.1023 (0.1009)	0.0633 (0.0624)	1.179 (0.912)	50.00 (70.26)
[156/157]	0.0873 (0.1009)	0.0607 (0.0625)	0.869 (0.916)	62.50 (70.04)
 * Train Acc 70.040
 * Val Acc 71.700, Total time 0.60
 * Val loss 0.800, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0089 (0.0089)	1.044 (1.044)	62.50 (62.50)
[10/157]	0.1041 (0.0975)	0.0650 (0.0595)	0.743 (0.889)	78.12 (70.17)
[20/157]	0.1060 (0.1006)	0.0664 (0.0623)	0.909 (0.899)	78.12 (69.64)
[30/157]	0.1053 (0.1016)	0.0657 (0.0630)	0.899 (0.895)	71.88 (70.77)
[40/157]	0.1045 (0.1021)	0.0643 (0.0634)	1.105 (0.913)	62.50 (69.97)
[50/157]	0.1034 (0.1023)	0.0640 (0.0637)	0.781 (0.915)	68.75 (69.36)
[60/157]	0.1030 (0.1026)	0.0637 (0.0639)	0.724 (0.921)	81.25 (69.31)
[70/157]	0.1029 (0.1027)	0.0638 (0.0641)	1.284 (0.934)	56.25 (69.10)
[80/157]	0.1048 (0.1029)	0.0658 (0.0642)	0.962 (0.933)	68.75 (69.06)
[90/157]	0.0965 (0.1023)	0.0581 (0.0637)	0.968 (0.930)	62.50 (69.16)
[100/157]	0.0941 (0.1016)	0.0557 (0.0631)	0.928 (0.919)	68.75 (69.77)
[110/157]	0.1022 (0.1014)	0.0630 (0.0629)	0.864 (0.914)	71.88 (70.07)
[120/157]	0.1019 (0.1016)	0.0628 (0.0630)	0.867 (0.913)	81.25 (70.14)
[130/157]	0.1018 (0.1016)	0.0626 (0.0630)	0.953 (0.907)	71.88 (70.61)
[140/157]	0.1028 (0.1017)	0.0638 (0.0630)	0.792 (0.902)	68.75 (70.68)
[150/157]	0.1023 (0.1018)	0.0625 (0.0631)	1.055 (0.909)	65.62 (70.38)
[156/157]	0.0862 (0.1017)	0.0588 (0.0630)	1.316 (0.909)	37.50 (70.40)
 * Train Acc 70.400
 * Val Acc 70.400, Total time 0.60
 * Val loss 0.813, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0091 (0.0091)	0.840 (0.840)	71.88 (71.88)
[10/157]	0.1050 (0.0962)	0.0631 (0.0578)	0.861 (0.900)	71.88 (69.03)
[20/157]	0.0948 (0.0968)	0.0567 (0.0586)	1.178 (0.925)	65.62 (68.30)
[30/157]	0.0962 (0.0964)	0.0582 (0.0584)	0.724 (0.905)	78.12 (69.25)
[40/157]	0.0975 (0.0974)	0.0591 (0.0592)	0.832 (0.898)	65.62 (69.74)
[50/157]	0.1001 (0.0982)	0.0607 (0.0598)	0.940 (0.907)	65.62 (69.85)
[60/157]	0.1026 (0.0987)	0.0625 (0.0602)	0.742 (0.913)	81.25 (69.31)
[70/157]	0.1042 (0.0991)	0.0638 (0.0605)	0.855 (0.909)	75.00 (69.94)
[80/157]	0.1017 (0.0994)	0.0612 (0.0607)	1.026 (0.905)	71.88 (70.02)
[90/157]	0.1001 (0.0997)	0.0613 (0.0609)	0.946 (0.899)	68.75 (70.36)
[100/157]	0.1012 (0.0999)	0.0609 (0.0610)	1.072 (0.906)	56.25 (70.08)
[110/157]	0.1008 (0.1000)	0.0617 (0.0612)	0.915 (0.905)	71.88 (69.99)
[120/157]	0.0993 (0.1001)	0.0609 (0.0613)	1.012 (0.900)	71.88 (70.40)
[130/157]	0.1212 (0.1001)	0.0807 (0.0614)	0.767 (0.901)	78.12 (70.54)
[140/157]	0.0955 (0.0999)	0.0573 (0.0613)	0.750 (0.901)	71.88 (70.61)
[150/157]	0.0961 (0.1003)	0.0578 (0.0616)	1.040 (0.900)	65.62 (70.57)
[156/157]	0.0896 (0.1004)	0.0627 (0.0618)	0.484 (0.897)	87.50 (70.74)
 * Train Acc 70.740
 * Val Acc 71.700, Total time 0.57
 * Val loss 0.803, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0090 (0.0090)	1.038 (1.038)	59.38 (59.38)
[10/157]	0.0992 (0.1030)	0.0601 (0.0639)	0.878 (0.917)	71.88 (69.03)
[20/157]	0.0991 (0.1013)	0.0598 (0.0624)	0.999 (0.904)	62.50 (69.20)
[30/157]	0.1017 (0.1008)	0.0611 (0.0618)	0.882 (0.921)	65.62 (68.55)
[40/157]	0.0970 (0.1006)	0.0572 (0.0616)	0.859 (0.900)	71.88 (69.59)
[50/157]	0.1001 (0.1004)	0.0613 (0.0614)	0.665 (0.889)	81.25 (69.79)
[60/157]	0.1001 (0.1003)	0.0617 (0.0614)	1.041 (0.898)	68.75 (69.57)
[70/157]	0.0963 (0.1002)	0.0577 (0.0614)	0.737 (0.899)	75.00 (70.07)
[80/157]	0.1003 (0.1001)	0.0619 (0.0614)	1.188 (0.908)	65.62 (69.87)
[90/157]	0.1008 (0.1001)	0.0623 (0.0614)	0.840 (0.909)	78.12 (70.02)
[100/157]	0.0996 (0.1001)	0.0612 (0.0615)	0.689 (0.899)	84.38 (70.42)
[110/157]	0.1001 (0.1000)	0.0610 (0.0615)	0.935 (0.897)	65.62 (70.55)
[120/157]	0.1002 (0.1000)	0.0608 (0.0614)	0.974 (0.907)	75.00 (70.30)
[130/157]	0.1005 (0.1000)	0.0608 (0.0614)	0.830 (0.908)	75.00 (70.16)
[140/157]	0.0999 (0.1000)	0.0609 (0.0614)	0.888 (0.907)	65.62 (70.41)
[150/157]	0.0999 (0.1000)	0.0605 (0.0613)	0.857 (0.908)	78.12 (70.41)
[156/157]	0.0814 (0.0998)	0.0544 (0.0613)	1.469 (0.908)	37.50 (70.26)
 * Train Acc 70.260
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.815, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0086 (0.0086)	0.823 (0.823)	75.00 (75.00)
[10/157]	0.1007 (0.0940)	0.0603 (0.0557)	1.091 (0.925)	56.25 (66.48)
[20/157]	0.1010 (0.0966)	0.0616 (0.0582)	0.739 (0.905)	78.12 (68.90)
[30/157]	0.1001 (0.0977)	0.0612 (0.0591)	0.808 (0.919)	81.25 (69.05)
[40/157]	0.0995 (0.0982)	0.0601 (0.0595)	1.009 (0.909)	56.25 (69.36)
[50/157]	0.1005 (0.0988)	0.0602 (0.0597)	0.920 (0.901)	71.88 (70.10)
[60/157]	0.1007 (0.0989)	0.0604 (0.0598)	0.828 (0.914)	71.88 (69.57)
[70/157]	0.0999 (0.0991)	0.0603 (0.0599)	0.543 (0.895)	84.38 (70.64)
[80/157]	0.0999 (0.0991)	0.0610 (0.0600)	1.097 (0.907)	68.75 (70.52)
[90/157]	0.0996 (0.0991)	0.0610 (0.0601)	0.981 (0.905)	62.50 (70.81)
[100/157]	0.1001 (0.0991)	0.0608 (0.0602)	0.980 (0.906)	65.62 (70.73)
[110/157]	0.0993 (0.0992)	0.0604 (0.0602)	0.787 (0.899)	71.88 (70.95)
[120/157]	0.0961 (0.0991)	0.0580 (0.0602)	0.762 (0.895)	71.88 (71.15)
[130/157]	0.1225 (0.1005)	0.0810 (0.0615)	0.804 (0.893)	71.88 (71.21)
[140/157]	0.0965 (0.1002)	0.0587 (0.0614)	1.214 (0.898)	46.88 (70.86)
[150/157]	0.0987 (0.1001)	0.0583 (0.0613)	0.873 (0.895)	78.12 (70.99)
[156/157]	0.0812 (0.0999)	0.0544 (0.0611)	1.386 (0.895)	62.50 (71.00)
 * Train Acc 71.000
 * Val Acc 71.700, Total time 0.60
 * Val loss 0.795, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0089 (0.0089)	1.258 (1.258)	53.12 (53.12)
[10/157]	0.0983 (0.0924)	0.0590 (0.0546)	0.989 (0.871)	78.12 (71.88)
[20/157]	0.1215 (0.0994)	0.0793 (0.0611)	0.880 (0.893)	65.62 (70.83)
[30/157]	0.1039 (0.1013)	0.0646 (0.0627)	0.775 (0.871)	75.00 (72.28)
[40/157]	0.1041 (0.1020)	0.0653 (0.0634)	1.029 (0.880)	62.50 (71.72)
[50/157]	0.1065 (0.1025)	0.0662 (0.0639)	0.944 (0.881)	65.62 (71.20)
[60/157]	0.1055 (0.1029)	0.0660 (0.0642)	0.810 (0.869)	75.00 (71.82)
[70/157]	0.1056 (0.1032)	0.0657 (0.0644)	0.751 (0.877)	75.00 (71.04)
[80/157]	0.1064 (0.1034)	0.0654 (0.0645)	0.904 (0.872)	65.62 (71.14)
[90/157]	0.1044 (0.1036)	0.0651 (0.0646)	0.896 (0.867)	65.62 (71.63)
[100/157]	0.0995 (0.1031)	0.0596 (0.0642)	1.082 (0.872)	59.38 (71.32)
[110/157]	0.0999 (0.1028)	0.0600 (0.0639)	0.982 (0.872)	71.88 (71.45)
[120/157]	0.1004 (0.1026)	0.0610 (0.0636)	0.637 (0.876)	84.38 (71.31)
[130/157]	0.0996 (0.1024)	0.0604 (0.0634)	0.769 (0.880)	71.88 (70.99)
[140/157]	0.1004 (0.1022)	0.0601 (0.0632)	0.739 (0.885)	75.00 (70.57)
[150/157]	0.1006 (0.1021)	0.0604 (0.0630)	1.045 (0.890)	68.75 (70.57)
[156/157]	0.0836 (0.1019)	0.0564 (0.0629)	0.736 (0.887)	87.50 (70.52)
 * Train Acc 70.520
 * Val Acc 72.300, Total time 0.61
 * Val loss 0.789, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0091 (0.0091)	1.087 (1.087)	68.75 (68.75)
[10/157]	0.1026 (0.0943)	0.0626 (0.0564)	0.871 (0.896)	71.88 (70.17)
[20/157]	0.1002 (0.0972)	0.0603 (0.0586)	0.703 (0.894)	81.25 (71.13)
[30/157]	0.1005 (0.0981)	0.0602 (0.0591)	0.874 (0.909)	71.88 (70.56)
[40/157]	0.1000 (0.0986)	0.0591 (0.0594)	0.791 (0.896)	75.00 (71.27)
[50/157]	0.1006 (0.0988)	0.0604 (0.0596)	0.928 (0.874)	78.12 (72.00)
[60/157]	0.0993 (0.0990)	0.0598 (0.0598)	1.135 (0.861)	68.75 (72.39)
[70/157]	0.1011 (0.0991)	0.0610 (0.0599)	0.870 (0.867)	78.12 (71.88)
[80/157]	0.0997 (0.0992)	0.0610 (0.0601)	0.858 (0.858)	78.12 (72.26)
[90/157]	0.0993 (0.0993)	0.0603 (0.0602)	1.134 (0.860)	56.25 (72.18)
[100/157]	0.1026 (0.0994)	0.0621 (0.0603)	0.830 (0.865)	71.88 (71.72)
[110/157]	0.0997 (0.0994)	0.0594 (0.0604)	1.006 (0.868)	65.62 (71.71)
[120/157]	0.1000 (0.0995)	0.0603 (0.0604)	0.687 (0.869)	78.12 (71.69)
[130/157]	0.1004 (0.0995)	0.0596 (0.0604)	0.816 (0.867)	87.50 (71.83)
[140/157]	0.1016 (0.0996)	0.0606 (0.0604)	0.811 (0.867)	75.00 (71.81)
[150/157]	0.1001 (0.0996)	0.0608 (0.0605)	1.003 (0.867)	71.88 (71.85)
[156/157]	0.0857 (0.0995)	0.0575 (0.0604)	1.177 (0.870)	87.50 (71.66)
 * Train Acc 71.660
 * Val Acc 72.400, Total time 0.60
 * Val loss 0.777, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0088 (0.0088)	0.992 (0.992)	68.75 (68.75)
[10/157]	0.1028 (0.0946)	0.0637 (0.0568)	0.627 (0.833)	84.38 (73.86)
[20/157]	0.1051 (0.0986)	0.0645 (0.0602)	0.984 (0.859)	65.62 (72.47)
[30/157]	0.1042 (0.0999)	0.0639 (0.0613)	0.950 (0.854)	75.00 (73.19)
[40/157]	0.1044 (0.1006)	0.0648 (0.0620)	0.790 (0.873)	71.88 (71.95)
[50/157]	0.1021 (0.1010)	0.0630 (0.0623)	0.840 (0.871)	68.75 (72.00)
[60/157]	0.1022 (0.1013)	0.0629 (0.0626)	0.741 (0.872)	78.12 (72.34)
[70/157]	0.1036 (0.1016)	0.0632 (0.0628)	1.146 (0.879)	62.50 (71.79)
[80/157]	0.1021 (0.1017)	0.0632 (0.0629)	1.042 (0.871)	62.50 (72.18)
[90/157]	0.1055 (0.1019)	0.0647 (0.0630)	0.786 (0.864)	78.12 (72.36)
[100/157]	0.1036 (0.1020)	0.0645 (0.0630)	0.835 (0.869)	68.75 (71.97)
[110/157]	0.1055 (0.1021)	0.0642 (0.0631)	0.962 (0.877)	65.62 (71.40)
[120/157]	0.1006 (0.1021)	0.0616 (0.0631)	0.750 (0.873)	75.00 (71.41)
[130/157]	0.1028 (0.1021)	0.0634 (0.0631)	0.969 (0.873)	75.00 (71.47)
[140/157]	0.1048 (0.1022)	0.0643 (0.0632)	1.209 (0.877)	56.25 (71.28)
[150/157]	0.0951 (0.1019)	0.0568 (0.0629)	0.999 (0.880)	65.62 (71.07)
[156/157]	0.0784 (0.1015)	0.0533 (0.0627)	0.806 (0.878)	75.00 (71.20)
 * Train Acc 71.200
 * Val Acc 71.200, Total time 0.58
 * Val loss 0.806, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0078 (0.0078)	0.804 (0.804)	68.75 (68.75)
[10/157]	0.1075 (0.1013)	0.0674 (0.0627)	1.274 (0.854)	59.38 (72.16)
[20/157]	0.0949 (0.0987)	0.0577 (0.0610)	0.692 (0.873)	87.50 (71.43)
[30/157]	0.0964 (0.0977)	0.0581 (0.0602)	0.676 (0.849)	78.12 (72.58)
[40/157]	0.0951 (0.0970)	0.0574 (0.0597)	1.144 (0.859)	68.75 (71.80)
[50/157]	0.1205 (0.0990)	0.0800 (0.0614)	0.597 (0.878)	81.25 (71.08)
[60/157]	0.1035 (0.0998)	0.0644 (0.0620)	0.818 (0.870)	68.75 (71.47)
[70/157]	0.1025 (0.1003)	0.0632 (0.0623)	0.888 (0.860)	65.62 (71.74)
[80/157]	0.1036 (0.1007)	0.0642 (0.0626)	1.152 (0.866)	59.38 (71.84)
[90/157]	0.1023 (0.1010)	0.0631 (0.0627)	1.083 (0.869)	68.75 (71.74)
[100/157]	0.1030 (0.1012)	0.0633 (0.0629)	0.686 (0.868)	84.38 (71.97)
[110/157]	0.1014 (0.1015)	0.0623 (0.0630)	0.996 (0.869)	65.62 (71.90)
[120/157]	0.0971 (0.1009)	0.0592 (0.0626)	0.659 (0.870)	78.12 (71.88)
[130/157]	0.0944 (0.1005)	0.0570 (0.0622)	0.865 (0.865)	75.00 (72.04)
[140/157]	0.0996 (0.1010)	0.0605 (0.0627)	1.094 (0.866)	62.50 (71.92)
[150/157]	0.0980 (0.1008)	0.0596 (0.0625)	0.909 (0.871)	59.38 (71.69)
[156/157]	0.0824 (0.1006)	0.0551 (0.0623)	0.953 (0.870)	75.00 (71.82)
 * Train Acc 71.820
 * Val Acc 71.700, Total time 0.59
 * Val loss 0.788, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0087 (0.0087)	0.748 (0.748)	71.88 (71.88)
[10/157]	0.0985 (0.0930)	0.0603 (0.0549)	0.663 (0.760)	84.38 (74.72)
[20/157]	0.1000 (0.0958)	0.0592 (0.0575)	0.960 (0.788)	62.50 (74.85)
[30/157]	0.0993 (0.0966)	0.0588 (0.0581)	1.236 (0.800)	59.38 (73.89)
[40/157]	0.0992 (0.0971)	0.0606 (0.0585)	0.875 (0.818)	68.75 (73.09)
[50/157]	0.0992 (0.0974)	0.0590 (0.0588)	0.660 (0.834)	87.50 (72.49)
[60/157]	0.1001 (0.0976)	0.0605 (0.0590)	1.005 (0.846)	71.88 (71.88)
[70/157]	0.0980 (0.0978)	0.0593 (0.0590)	1.131 (0.864)	62.50 (71.17)
[80/157]	0.0978 (0.0979)	0.0592 (0.0591)	0.646 (0.856)	81.25 (71.53)
[90/157]	0.1002 (0.0980)	0.0615 (0.0593)	0.990 (0.859)	71.88 (71.22)
[100/157]	0.0990 (0.0980)	0.0608 (0.0593)	0.938 (0.864)	68.75 (71.19)
[110/157]	0.1007 (0.0981)	0.0608 (0.0594)	0.985 (0.867)	65.62 (71.31)
[120/157]	0.1005 (0.0981)	0.0603 (0.0594)	1.012 (0.867)	62.50 (71.33)
[130/157]	0.0990 (0.0981)	0.0593 (0.0594)	0.674 (0.869)	75.00 (71.33)
[140/157]	0.0983 (0.0981)	0.0592 (0.0595)	0.767 (0.867)	75.00 (71.50)
[150/157]	0.1006 (0.0982)	0.0609 (0.0595)	0.780 (0.865)	75.00 (71.59)
[156/157]	0.0841 (0.0982)	0.0565 (0.0595)	0.791 (0.865)	87.50 (71.52)
 * Train Acc 71.520
 * Val Acc 72.600, Total time 0.60
 * Val loss 0.778, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0463 (0.0463)	0.0091 (0.0091)	0.694 (0.694)	78.12 (78.12)
[10/157]	0.1001 (0.0945)	0.0611 (0.0563)	0.746 (0.746)	81.25 (75.85)
[20/157]	0.1001 (0.0971)	0.0614 (0.0589)	1.280 (0.791)	56.25 (74.40)
[30/157]	0.1003 (0.0982)	0.0610 (0.0598)	0.767 (0.805)	75.00 (75.00)
[40/157]	0.0999 (0.0986)	0.0605 (0.0601)	0.883 (0.826)	75.00 (73.63)
[50/157]	0.0997 (0.0989)	0.0600 (0.0603)	0.823 (0.826)	68.75 (73.35)
[60/157]	0.0997 (0.0991)	0.0595 (0.0605)	1.108 (0.823)	65.62 (73.87)
[70/157]	0.0999 (0.0992)	0.0609 (0.0606)	0.789 (0.821)	75.00 (73.77)
[80/157]	0.1039 (0.0993)	0.0609 (0.0607)	1.182 (0.831)	50.00 (73.30)
[90/157]	0.0999 (0.0994)	0.0603 (0.0607)	0.880 (0.833)	65.62 (73.01)
[100/157]	0.0998 (0.0994)	0.0606 (0.0608)	1.044 (0.839)	62.50 (72.59)
[110/157]	0.1014 (0.0995)	0.0615 (0.0608)	0.887 (0.839)	78.12 (72.47)
[120/157]	0.0983 (0.0995)	0.0604 (0.0609)	1.029 (0.844)	71.88 (72.31)
[130/157]	0.1006 (0.0996)	0.0608 (0.0609)	0.883 (0.844)	75.00 (72.35)
[140/157]	0.0992 (0.0996)	0.0595 (0.0609)	1.068 (0.843)	65.62 (72.34)
[150/157]	0.1003 (0.0996)	0.0605 (0.0610)	0.900 (0.846)	78.12 (72.14)
[156/157]	0.0858 (0.0995)	0.0572 (0.0609)	0.970 (0.847)	75.00 (72.16)
 * Train Acc 72.160
 * Val Acc 71.500, Total time 0.60
 * Val loss 0.779, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0084 (0.0084)	0.714 (0.714)	75.00 (75.00)
[10/157]	0.1001 (0.0940)	0.0609 (0.0562)	0.844 (0.833)	78.12 (74.15)
[20/157]	0.1002 (0.0968)	0.0608 (0.0585)	1.108 (0.866)	53.12 (71.73)
[30/157]	0.0999 (0.0980)	0.0605 (0.0594)	0.668 (0.876)	71.88 (71.47)
[40/157]	0.1001 (0.0984)	0.0607 (0.0599)	0.832 (0.869)	68.75 (71.80)
[50/157]	0.1004 (0.0987)	0.0613 (0.0602)	0.688 (0.876)	81.25 (71.69)
[60/157]	0.0992 (0.0989)	0.0600 (0.0602)	0.866 (0.877)	71.88 (71.41)
[70/157]	0.0997 (0.0990)	0.0599 (0.0603)	0.697 (0.877)	71.88 (71.08)
[80/157]	0.0996 (0.0991)	0.0594 (0.0604)	0.953 (0.871)	75.00 (71.26)
[90/157]	0.0990 (0.0992)	0.0602 (0.0605)	0.795 (0.859)	84.38 (71.63)
[100/157]	0.1005 (0.0993)	0.0608 (0.0605)	0.656 (0.872)	84.38 (71.07)
[110/157]	0.1009 (0.0994)	0.0612 (0.0606)	0.550 (0.867)	84.38 (71.37)
[120/157]	0.1011 (0.0994)	0.0612 (0.0606)	0.770 (0.870)	68.75 (71.13)
[130/157]	0.0992 (0.0995)	0.0597 (0.0606)	0.832 (0.871)	65.62 (71.02)
[140/157]	0.1005 (0.0995)	0.0605 (0.0606)	1.057 (0.869)	68.75 (70.88)
[150/157]	0.1005 (0.0995)	0.0610 (0.0606)	1.052 (0.870)	68.75 (70.80)
[156/157]	0.0833 (0.0994)	0.0559 (0.0606)	0.697 (0.873)	87.50 (70.64)
 * Train Acc 70.640
 * Val Acc 71.500, Total time 0.60
 * Val loss 0.766, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0090 (0.0090)	0.913 (0.913)	71.88 (71.88)
[10/157]	0.0994 (0.0947)	0.0604 (0.0568)	0.980 (0.923)	68.75 (67.33)
[20/157]	0.1000 (0.0974)	0.0605 (0.0590)	0.806 (0.891)	71.88 (69.49)
[30/157]	0.1018 (0.0983)	0.0604 (0.0596)	0.963 (0.866)	65.62 (69.86)
[40/157]	0.1008 (0.0987)	0.0613 (0.0599)	0.859 (0.861)	75.00 (71.04)
[50/157]	0.0995 (0.0989)	0.0607 (0.0601)	0.855 (0.865)	71.88 (71.08)
[60/157]	0.0995 (0.0991)	0.0603 (0.0603)	0.947 (0.866)	71.88 (71.57)
[70/157]	0.1009 (0.0992)	0.0613 (0.0605)	0.803 (0.862)	81.25 (71.92)
[80/157]	0.0994 (0.0994)	0.0603 (0.0606)	0.858 (0.855)	71.88 (72.34)
[90/157]	0.1000 (0.0994)	0.0609 (0.0607)	0.585 (0.846)	81.25 (72.84)
[100/157]	0.1008 (0.0994)	0.0612 (0.0607)	0.820 (0.851)	78.12 (72.80)
[110/157]	0.0990 (0.0995)	0.0593 (0.0607)	0.941 (0.849)	65.62 (72.89)
[120/157]	0.1002 (0.0995)	0.0615 (0.0607)	0.704 (0.852)	84.38 (72.83)
[130/157]	0.0999 (0.0996)	0.0606 (0.0608)	0.734 (0.857)	75.00 (72.71)
[140/157]	0.1000 (0.0996)	0.0603 (0.0608)	0.890 (0.864)	71.88 (72.47)
[150/157]	0.1007 (0.0996)	0.0611 (0.0608)	0.654 (0.865)	84.38 (72.35)
[156/157]	0.0833 (0.0995)	0.0557 (0.0607)	1.778 (0.867)	50.00 (72.36)
 * Train Acc 72.360
 * Val Acc 71.500, Total time 0.60
 * Val loss 0.782, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0452 (0.0452)	0.0096 (0.0096)	0.589 (0.589)	87.50 (87.50)
[10/157]	0.1004 (0.0947)	0.0606 (0.0565)	0.660 (0.754)	84.38 (78.41)
[20/157]	0.1013 (0.0975)	0.0617 (0.0588)	0.668 (0.814)	84.38 (73.81)
[30/157]	0.0988 (0.0982)	0.0598 (0.0593)	0.760 (0.840)	75.00 (72.28)
[40/157]	0.0995 (0.0987)	0.0599 (0.0598)	0.899 (0.834)	75.00 (73.40)
[50/157]	0.1015 (0.0989)	0.0612 (0.0601)	0.975 (0.857)	68.75 (72.73)
[60/157]	0.1004 (0.0991)	0.0610 (0.0603)	0.958 (0.857)	59.38 (72.23)
[70/157]	0.0995 (0.0992)	0.0597 (0.0603)	1.043 (0.861)	68.75 (72.36)
[80/157]	0.1006 (0.0993)	0.0613 (0.0605)	0.921 (0.864)	75.00 (72.15)
[90/157]	0.1177 (0.0996)	0.0773 (0.0607)	0.649 (0.871)	81.25 (72.05)
[100/157]	0.1095 (0.1010)	0.0686 (0.0621)	0.957 (0.867)	65.62 (71.97)
[110/157]	0.0975 (0.1008)	0.0589 (0.0620)	0.843 (0.866)	68.75 (71.71)
[120/157]	0.1145 (0.1006)	0.0733 (0.0618)	1.098 (0.872)	59.38 (71.44)
[130/157]	0.1114 (0.1014)	0.0713 (0.0626)	0.657 (0.859)	81.25 (72.16)
[140/157]	0.0989 (0.1013)	0.0591 (0.0625)	1.107 (0.868)	71.88 (71.68)
[150/157]	0.0969 (0.1010)	0.0586 (0.0623)	0.752 (0.867)	75.00 (71.69)
[156/157]	0.0789 (0.1007)	0.0537 (0.0621)	0.887 (0.870)	75.00 (71.60)
 * Train Acc 71.600
 * Val Acc 71.900, Total time 0.58
 * Val loss 0.775, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0086 (0.0086)	0.900 (0.900)	65.62 (65.62)
[10/157]	0.1087 (0.0981)	0.0693 (0.0595)	1.027 (0.898)	56.25 (69.03)
[20/157]	0.0981 (0.1027)	0.0586 (0.0638)	0.806 (0.873)	78.12 (69.64)
[30/157]	0.0955 (0.1007)	0.0579 (0.0622)	0.726 (0.855)	75.00 (70.87)
[40/157]	0.0978 (0.0997)	0.0583 (0.0613)	0.803 (0.867)	71.88 (71.11)
[50/157]	0.1004 (0.1010)	0.0611 (0.0625)	1.220 (0.861)	56.25 (71.08)
[60/157]	0.1010 (0.1009)	0.0622 (0.0624)	1.000 (0.860)	71.88 (71.41)
[70/157]	0.1026 (0.1010)	0.0631 (0.0624)	1.202 (0.854)	50.00 (71.88)
[80/157]	0.1001 (0.1009)	0.0611 (0.0624)	0.785 (0.849)	75.00 (71.91)
[90/157]	0.1008 (0.1010)	0.0615 (0.0624)	0.934 (0.847)	71.88 (72.01)
[100/157]	0.1015 (0.1010)	0.0625 (0.0624)	0.960 (0.849)	59.38 (71.81)
[110/157]	0.0998 (0.1010)	0.0607 (0.0624)	0.633 (0.841)	75.00 (71.79)
[120/157]	0.1009 (0.1011)	0.0617 (0.0624)	0.965 (0.835)	71.88 (72.16)
[130/157]	0.1034 (0.1010)	0.0635 (0.0624)	0.900 (0.847)	71.88 (71.73)
[140/157]	0.1019 (0.1010)	0.0629 (0.0624)	0.652 (0.847)	78.12 (72.01)
[150/157]	0.1016 (0.1010)	0.0625 (0.0623)	0.616 (0.843)	84.38 (72.19)
[156/157]	0.0844 (0.1009)	0.0564 (0.0623)	0.823 (0.842)	87.50 (72.26)
 * Train Acc 72.260
 * Val Acc 72.400, Total time 0.60
 * Val loss 0.767, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0088 (0.0088)	0.886 (0.886)	68.75 (68.75)
[10/157]	0.1017 (0.0950)	0.0626 (0.0575)	0.972 (0.855)	68.75 (70.45)
[20/157]	0.1003 (0.0978)	0.0602 (0.0597)	0.892 (0.862)	68.75 (71.43)
[30/157]	0.0990 (0.0987)	0.0600 (0.0603)	0.856 (0.845)	75.00 (72.38)
[40/157]	0.1005 (0.0994)	0.0614 (0.0609)	0.966 (0.853)	62.50 (71.72)
[50/157]	0.1032 (0.0999)	0.0627 (0.0611)	1.088 (0.856)	62.50 (71.63)
[60/157]	0.1026 (0.1001)	0.0620 (0.0613)	0.723 (0.853)	75.00 (71.31)
[70/157]	0.0999 (0.1002)	0.0607 (0.0613)	0.827 (0.854)	71.88 (71.26)
[80/157]	0.0995 (0.1004)	0.0594 (0.0614)	0.809 (0.851)	68.75 (71.57)
[90/157]	0.1028 (0.1005)	0.0628 (0.0615)	0.726 (0.851)	71.88 (71.67)
[100/157]	0.1004 (0.1005)	0.0619 (0.0616)	0.800 (0.858)	75.00 (71.72)
[110/157]	0.1027 (0.1006)	0.0625 (0.0617)	1.061 (0.856)	68.75 (71.99)
[120/157]	0.1008 (0.1006)	0.0621 (0.0617)	0.670 (0.861)	84.38 (71.90)
[130/157]	0.1010 (0.1006)	0.0616 (0.0617)	0.749 (0.858)	75.00 (71.99)
[140/157]	0.1012 (0.1007)	0.0617 (0.0618)	0.591 (0.853)	81.25 (72.14)
[150/157]	0.1016 (0.1007)	0.0616 (0.0618)	0.767 (0.852)	71.88 (72.31)
[156/157]	0.0834 (0.1006)	0.0559 (0.0618)	1.031 (0.859)	87.50 (72.00)
 * Train Acc 72.000
 * Val Acc 72.500, Total time 0.59
 * Val loss 0.770, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0458 (0.0458)	0.0090 (0.0090)	0.564 (0.564)	81.25 (81.25)
[10/157]	0.1003 (0.0956)	0.0605 (0.0567)	0.918 (0.928)	71.88 (70.17)
[20/157]	0.1040 (0.0982)	0.0638 (0.0595)	1.143 (0.898)	68.75 (70.39)
[30/157]	0.1005 (0.0990)	0.0607 (0.0602)	0.722 (0.889)	68.75 (70.46)
[40/157]	0.0934 (0.0989)	0.0565 (0.0603)	0.716 (0.862)	78.12 (72.03)
[50/157]	0.0955 (0.0982)	0.0579 (0.0599)	0.787 (0.845)	78.12 (72.73)
[60/157]	0.0946 (0.0977)	0.0565 (0.0596)	1.011 (0.834)	62.50 (72.95)
[70/157]	0.0956 (0.0974)	0.0574 (0.0593)	0.935 (0.847)	71.88 (72.23)
[80/157]	0.0974 (0.0972)	0.0589 (0.0592)	0.956 (0.857)	75.00 (71.84)
[90/157]	0.0978 (0.0970)	0.0584 (0.0591)	1.007 (0.863)	65.62 (71.60)
[100/157]	0.1028 (0.0982)	0.0626 (0.0601)	0.716 (0.858)	78.12 (71.94)
[110/157]	0.1036 (0.0985)	0.0640 (0.0604)	0.850 (0.856)	65.62 (71.90)
[120/157]	0.1013 (0.0987)	0.0624 (0.0605)	1.084 (0.859)	62.50 (71.88)
[130/157]	0.1041 (0.0989)	0.0643 (0.0607)	0.769 (0.857)	78.12 (71.90)
[140/157]	0.1004 (0.0991)	0.0620 (0.0608)	1.282 (0.865)	71.88 (71.70)
[150/157]	0.1005 (0.0992)	0.0613 (0.0610)	0.728 (0.860)	71.88 (71.88)
[156/157]	0.0851 (0.0992)	0.0582 (0.0610)	0.890 (0.862)	75.00 (71.80)
 * Train Acc 71.800
 * Val Acc 72.400, Total time 0.57
 * Val loss 0.776, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0088 (0.0088)	0.896 (0.896)	68.75 (68.75)
[10/157]	0.1023 (0.0948)	0.0635 (0.0573)	0.590 (0.879)	84.38 (70.17)
[20/157]	0.1004 (0.0978)	0.0615 (0.0597)	1.138 (0.855)	56.25 (72.62)
[30/157]	0.1023 (0.0989)	0.0630 (0.0607)	1.031 (0.873)	65.62 (71.67)
[40/157]	0.1001 (0.0994)	0.0607 (0.0610)	0.950 (0.865)	65.62 (71.65)
[50/157]	0.1046 (0.0999)	0.0631 (0.0613)	0.496 (0.846)	90.62 (72.12)
[60/157]	0.1010 (0.1002)	0.0609 (0.0615)	0.920 (0.853)	71.88 (71.82)
[70/157]	0.0999 (0.1002)	0.0602 (0.0615)	0.767 (0.864)	71.88 (71.26)
[80/157]	0.1015 (0.1004)	0.0617 (0.0616)	0.646 (0.857)	84.38 (71.57)
[90/157]	0.1015 (0.1004)	0.0630 (0.0616)	0.617 (0.851)	75.00 (71.91)
[100/157]	0.1004 (0.1005)	0.0606 (0.0617)	0.890 (0.857)	65.62 (71.69)
[110/157]	0.1006 (0.1006)	0.0602 (0.0617)	0.939 (0.862)	75.00 (71.79)
[120/157]	0.1007 (0.1006)	0.0615 (0.0617)	1.086 (0.860)	65.62 (72.06)
[130/157]	0.1013 (0.1006)	0.0622 (0.0617)	0.907 (0.857)	68.75 (72.09)
[140/157]	0.0995 (0.1006)	0.0595 (0.0617)	0.756 (0.856)	68.75 (72.21)
[150/157]	0.1005 (0.1007)	0.0618 (0.0618)	1.022 (0.859)	68.75 (72.19)
[156/157]	0.0829 (0.1006)	0.0559 (0.0617)	0.852 (0.861)	62.50 (72.10)
 * Train Acc 72.100
 * Val Acc 73.000, Total time 0.60
 * Val loss 0.767, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0083 (0.0083)	0.928 (0.928)	65.62 (65.62)
[10/157]	0.1009 (0.0954)	0.0617 (0.0569)	0.996 (0.807)	62.50 (71.59)
[20/157]	0.1041 (0.0982)	0.0642 (0.0596)	0.967 (0.848)	65.62 (70.98)
[30/157]	0.1016 (0.0991)	0.0610 (0.0603)	1.309 (0.843)	50.00 (71.57)
[40/157]	0.0927 (0.0995)	0.0553 (0.0606)	0.776 (0.845)	71.88 (71.88)
[50/157]	0.0968 (0.0986)	0.0588 (0.0601)	1.051 (0.850)	68.75 (71.51)
[60/157]	0.1002 (0.0982)	0.0612 (0.0599)	1.146 (0.861)	56.25 (70.39)
[70/157]	0.1004 (0.0984)	0.0609 (0.0600)	0.659 (0.853)	78.12 (70.77)
[80/157]	0.0995 (0.0985)	0.0605 (0.0601)	0.698 (0.861)	78.12 (70.79)
[90/157]	0.0994 (0.0986)	0.0598 (0.0602)	0.649 (0.859)	84.38 (70.95)
[100/157]	0.0998 (0.0987)	0.0605 (0.0602)	1.028 (0.864)	62.50 (70.95)
[110/157]	0.0983 (0.0987)	0.0602 (0.0602)	0.853 (0.867)	68.75 (70.92)
[120/157]	0.0994 (0.0987)	0.0611 (0.0603)	0.646 (0.865)	87.50 (71.44)
[130/157]	0.0998 (0.0988)	0.0602 (0.0603)	0.869 (0.865)	71.88 (71.33)
[140/157]	0.0999 (0.0988)	0.0617 (0.0603)	1.134 (0.868)	62.50 (71.19)
[150/157]	0.1002 (0.0989)	0.0618 (0.0604)	0.717 (0.867)	87.50 (71.15)
[156/157]	0.0819 (0.0987)	0.0558 (0.0604)	1.120 (0.864)	50.00 (71.36)
 * Train Acc 71.360
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.781, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0086 (0.0086)	1.097 (1.097)	59.38 (59.38)
[10/157]	0.0991 (0.0934)	0.0598 (0.0559)	0.625 (0.822)	78.12 (72.73)
[20/157]	0.0996 (0.0965)	0.0603 (0.0583)	0.976 (0.824)	68.75 (72.47)
[30/157]	0.0986 (0.0972)	0.0600 (0.0591)	0.663 (0.845)	84.38 (72.18)
[40/157]	0.1006 (0.0978)	0.0608 (0.0596)	0.658 (0.841)	81.25 (72.10)
[50/157]	0.0994 (0.0980)	0.0609 (0.0598)	0.791 (0.863)	81.25 (71.45)
[60/157]	0.0990 (0.0982)	0.0606 (0.0601)	1.008 (0.856)	68.75 (71.98)
[70/157]	0.0991 (0.0984)	0.0600 (0.0602)	0.858 (0.854)	71.88 (72.01)
[80/157]	0.0995 (0.0985)	0.0612 (0.0602)	0.846 (0.858)	62.50 (71.57)
[90/157]	0.0987 (0.0986)	0.0590 (0.0603)	0.878 (0.852)	62.50 (71.84)
[100/157]	0.0997 (0.0987)	0.0597 (0.0604)	0.867 (0.859)	71.88 (71.50)
[110/157]	0.0994 (0.0987)	0.0607 (0.0604)	0.665 (0.850)	75.00 (71.68)
[120/157]	0.0995 (0.0988)	0.0611 (0.0605)	0.933 (0.852)	65.62 (71.54)
[130/157]	0.0998 (0.0988)	0.0601 (0.0605)	0.960 (0.850)	65.62 (71.80)
[140/157]	0.1008 (0.0989)	0.0612 (0.0605)	0.936 (0.848)	68.75 (71.83)
[150/157]	0.1002 (0.0989)	0.0609 (0.0605)	0.772 (0.847)	71.88 (71.75)
[156/157]	0.0830 (0.0988)	0.0551 (0.0605)	0.556 (0.845)	87.50 (71.82)
 * Train Acc 71.820
 * Val Acc 72.800, Total time 0.60
 * Val loss 0.760, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0088 (0.0088)	0.865 (0.865)	65.62 (65.62)
[10/157]	0.1010 (0.0937)	0.0616 (0.0559)	0.811 (0.811)	75.00 (73.01)
[20/157]	0.0988 (0.0965)	0.0602 (0.0584)	0.670 (0.818)	78.12 (72.62)
[30/157]	0.1001 (0.0975)	0.0610 (0.0593)	0.605 (0.817)	81.25 (72.98)
[40/157]	0.0988 (0.0979)	0.0593 (0.0596)	1.000 (0.811)	71.88 (73.09)
[50/157]	0.0986 (0.0982)	0.0604 (0.0597)	0.752 (0.815)	75.00 (72.98)
[60/157]	0.0990 (0.0984)	0.0603 (0.0600)	0.743 (0.824)	75.00 (72.44)
[70/157]	0.0998 (0.0985)	0.0601 (0.0601)	0.789 (0.829)	71.88 (72.18)
[80/157]	0.1000 (0.0986)	0.0614 (0.0602)	0.814 (0.833)	78.12 (72.22)
[90/157]	0.0998 (0.0987)	0.0613 (0.0604)	1.023 (0.843)	75.00 (71.94)
[100/157]	0.0994 (0.0988)	0.0598 (0.0604)	0.958 (0.843)	71.88 (72.28)
[110/157]	0.1004 (0.0989)	0.0606 (0.0604)	0.716 (0.842)	81.25 (72.35)
[120/157]	0.1002 (0.0989)	0.0601 (0.0604)	0.895 (0.847)	68.75 (72.18)
[130/157]	0.1001 (0.0990)	0.0604 (0.0604)	0.750 (0.849)	78.12 (72.19)
[140/157]	0.1001 (0.0990)	0.0613 (0.0605)	0.688 (0.843)	78.12 (72.27)
[150/157]	0.0998 (0.0990)	0.0605 (0.0605)	1.056 (0.847)	65.62 (72.00)
[156/157]	0.0810 (0.0989)	0.0552 (0.0605)	0.606 (0.848)	87.50 (71.92)
 * Train Acc 71.920
 * Val Acc 73.300, Total time 0.59
 * Val loss 0.756, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0088 (0.0088)	0.738 (0.738)	78.12 (78.12)
[10/157]	0.1003 (0.0934)	0.0615 (0.0556)	0.744 (0.872)	75.00 (71.02)
[20/157]	0.0995 (0.0960)	0.0602 (0.0579)	0.816 (0.875)	71.88 (72.62)
[30/157]	0.0992 (0.0969)	0.0605 (0.0587)	1.176 (0.862)	59.38 (71.77)
[40/157]	0.0994 (0.0976)	0.0604 (0.0593)	1.028 (0.857)	59.38 (71.72)
[50/157]	0.0996 (0.0979)	0.0599 (0.0596)	0.858 (0.838)	65.62 (71.94)
[60/157]	0.0991 (0.0981)	0.0604 (0.0596)	0.869 (0.843)	71.88 (71.82)
[70/157]	0.1014 (0.0984)	0.0618 (0.0598)	0.740 (0.839)	81.25 (72.32)
[80/157]	0.1015 (0.0986)	0.0617 (0.0600)	0.626 (0.848)	81.25 (71.88)
[90/157]	0.1006 (0.0987)	0.0612 (0.0601)	0.794 (0.848)	78.12 (71.77)
[100/157]	0.1020 (0.0988)	0.0623 (0.0602)	0.814 (0.854)	78.12 (71.66)
[110/157]	0.1001 (0.0988)	0.0593 (0.0602)	0.878 (0.856)	71.88 (71.71)
[120/157]	0.1000 (0.0989)	0.0602 (0.0603)	0.692 (0.852)	78.12 (71.95)
[130/157]	0.1003 (0.0990)	0.0603 (0.0603)	0.800 (0.856)	71.88 (71.92)
[140/157]	0.1050 (0.0999)	0.0657 (0.0612)	0.872 (0.856)	71.88 (72.01)
[150/157]	0.1018 (0.1002)	0.0621 (0.0614)	0.686 (0.854)	84.38 (71.98)
[156/157]	0.0875 (0.1003)	0.0607 (0.0615)	0.524 (0.850)	87.50 (72.22)
 * Train Acc 72.220
 * Val Acc 72.800, Total time 0.61
 * Val loss 0.758, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0087 (0.0087)	0.770 (0.770)	81.25 (81.25)
[10/157]	0.1058 (0.0979)	0.0651 (0.0597)	0.914 (0.863)	71.88 (71.88)
[20/157]	0.1059 (0.1010)	0.0664 (0.0625)	1.016 (0.866)	59.38 (71.43)
[30/157]	0.0953 (0.1013)	0.0574 (0.0627)	0.826 (0.865)	75.00 (71.27)
[40/157]	0.0939 (0.0997)	0.0565 (0.0614)	0.757 (0.853)	75.00 (71.49)
[50/157]	0.1022 (0.0992)	0.0627 (0.0610)	0.991 (0.850)	65.62 (71.69)
[60/157]	0.1039 (0.0996)	0.0636 (0.0614)	0.921 (0.850)	75.00 (71.93)
[70/157]	0.1003 (0.1000)	0.0611 (0.0616)	0.949 (0.843)	75.00 (72.76)
[80/157]	0.1021 (0.1002)	0.0625 (0.0618)	0.688 (0.844)	78.12 (72.88)
[90/157]	0.1017 (0.1004)	0.0626 (0.0620)	0.696 (0.840)	84.38 (72.87)
[100/157]	0.1002 (0.1006)	0.0609 (0.0620)	0.694 (0.839)	78.12 (72.80)
[110/157]	0.1009 (0.1007)	0.0620 (0.0621)	0.925 (0.844)	65.62 (72.38)
[120/157]	0.1039 (0.1008)	0.0628 (0.0622)	0.823 (0.846)	75.00 (72.39)
[130/157]	0.1014 (0.1009)	0.0624 (0.0623)	0.552 (0.846)	84.38 (72.35)
[140/157]	0.1016 (0.1010)	0.0623 (0.0624)	0.925 (0.849)	68.75 (72.30)
[150/157]	0.1027 (0.1011)	0.0627 (0.0624)	0.964 (0.847)	68.75 (72.39)
[156/157]	0.0851 (0.1010)	0.0572 (0.0624)	0.560 (0.844)	87.50 (72.58)
 * Train Acc 72.580
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.761, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0092 (0.0092)	1.086 (1.086)	62.50 (62.50)
[10/157]	0.1038 (0.0964)	0.0634 (0.0577)	0.803 (0.886)	75.00 (71.59)
[20/157]	0.1012 (0.0992)	0.0617 (0.0602)	0.918 (0.841)	68.75 (73.07)
[30/157]	0.1011 (0.1001)	0.0615 (0.0610)	0.895 (0.859)	71.88 (72.88)
[40/157]	0.1031 (0.1006)	0.0630 (0.0614)	0.644 (0.857)	78.12 (73.40)
[50/157]	0.1018 (0.1008)	0.0623 (0.0617)	0.713 (0.835)	81.25 (74.14)
[60/157]	0.1006 (0.1010)	0.0617 (0.0618)	0.996 (0.848)	56.25 (73.41)
[70/157]	0.1019 (0.1012)	0.0629 (0.0620)	0.969 (0.848)	71.88 (73.28)
[80/157]	0.1009 (0.1013)	0.0617 (0.0621)	0.847 (0.850)	71.88 (72.69)
[90/157]	0.1009 (0.1013)	0.0620 (0.0622)	1.167 (0.849)	59.38 (72.63)
[100/157]	0.1006 (0.1014)	0.0608 (0.0623)	1.042 (0.847)	62.50 (72.68)
[110/157]	0.1020 (0.1014)	0.0628 (0.0623)	0.722 (0.854)	81.25 (72.49)
[120/157]	0.0956 (0.1011)	0.0581 (0.0621)	0.766 (0.853)	75.00 (72.52)
[130/157]	0.0941 (0.1007)	0.0566 (0.0619)	0.948 (0.853)	68.75 (72.61)
[140/157]	0.0957 (0.1003)	0.0581 (0.0616)	0.810 (0.853)	68.75 (72.41)
[150/157]	0.0967 (0.1011)	0.0579 (0.0623)	0.695 (0.853)	78.12 (72.48)
[156/157]	0.0794 (0.1008)	0.0534 (0.0621)	0.686 (0.854)	75.00 (72.28)
 * Train Acc 72.280
 * Val Acc 72.300, Total time 0.58
 * Val loss 0.771, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0087 (0.0087)	0.987 (0.987)	75.00 (75.00)
[10/157]	0.0976 (0.0913)	0.0591 (0.0540)	0.729 (0.926)	78.12 (73.30)
[20/157]	0.1010 (0.0980)	0.0615 (0.0601)	0.671 (0.875)	84.38 (74.11)
[30/157]	0.1043 (0.0991)	0.0637 (0.0611)	0.794 (0.870)	68.75 (72.88)
[40/157]	0.1011 (0.0999)	0.0613 (0.0615)	1.017 (0.854)	59.38 (72.94)
[50/157]	0.1026 (0.1002)	0.0621 (0.0618)	1.140 (0.881)	59.38 (71.63)
[60/157]	0.1042 (0.1005)	0.0640 (0.0620)	1.058 (0.879)	59.38 (71.77)
[70/157]	0.1022 (0.1008)	0.0620 (0.0622)	0.887 (0.879)	71.88 (71.26)
[80/157]	0.1016 (0.1010)	0.0623 (0.0623)	0.793 (0.876)	75.00 (71.33)
[90/157]	0.1020 (0.1011)	0.0627 (0.0624)	0.814 (0.866)	71.88 (71.74)
[100/157]	0.1010 (0.1012)	0.0616 (0.0624)	0.868 (0.860)	68.75 (71.97)
[110/157]	0.1012 (0.1012)	0.0617 (0.0625)	0.700 (0.861)	71.88 (71.82)
[120/157]	0.1007 (0.1013)	0.0609 (0.0626)	0.726 (0.856)	75.00 (71.95)
[130/157]	0.1035 (0.1013)	0.0631 (0.0626)	0.900 (0.855)	65.62 (71.88)
[140/157]	0.1019 (0.1014)	0.0627 (0.0626)	0.691 (0.854)	84.38 (71.99)
[150/157]	0.1031 (0.1013)	0.0639 (0.0626)	0.719 (0.848)	71.88 (72.27)
[156/157]	0.0858 (0.1012)	0.0584 (0.0626)	0.783 (0.851)	62.50 (72.10)
 * Train Acc 72.100
 * Val Acc 73.100, Total time 0.60
 * Val loss 0.751, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0091 (0.0091)	0.963 (0.963)	71.88 (71.88)
[10/157]	0.1015 (0.0962)	0.0623 (0.0576)	0.767 (0.814)	78.12 (76.14)
[20/157]	0.1012 (0.0990)	0.0614 (0.0601)	0.920 (0.797)	65.62 (75.89)
[30/157]	0.1005 (0.0999)	0.0614 (0.0611)	0.757 (0.833)	75.00 (74.29)
[40/157]	0.1012 (0.1005)	0.0619 (0.0616)	0.987 (0.824)	75.00 (74.39)
[50/157]	0.1013 (0.1008)	0.0615 (0.0619)	0.863 (0.841)	75.00 (73.47)
[60/157]	0.1023 (0.1010)	0.0626 (0.0621)	0.723 (0.824)	78.12 (74.08)
[70/157]	0.1011 (0.1011)	0.0617 (0.0622)	0.988 (0.839)	56.25 (73.15)
[80/157]	0.1010 (0.1012)	0.0616 (0.0623)	0.769 (0.841)	78.12 (73.23)
[90/157]	0.1011 (0.1013)	0.0616 (0.0624)	0.480 (0.846)	90.62 (73.28)
[100/157]	0.1010 (0.1013)	0.0618 (0.0624)	0.736 (0.838)	78.12 (73.70)
[110/157]	0.1002 (0.1014)	0.0615 (0.0625)	0.914 (0.838)	84.38 (73.73)
[120/157]	0.1005 (0.1014)	0.0612 (0.0625)	0.577 (0.832)	84.38 (73.99)
[130/157]	0.1032 (0.1015)	0.0630 (0.0626)	0.937 (0.834)	71.88 (73.85)
[140/157]	0.1012 (0.1015)	0.0619 (0.0626)	0.574 (0.831)	84.38 (74.00)
[150/157]	0.1007 (0.1015)	0.0612 (0.0627)	0.871 (0.831)	62.50 (73.84)
[156/157]	0.0870 (0.1014)	0.0597 (0.0626)	0.728 (0.828)	87.50 (73.90)
 * Train Acc 73.900
 * Val Acc 72.600, Total time 0.57
 * Val loss 0.762, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0087 (0.0087)	1.395 (1.395)	53.12 (53.12)
[10/157]	0.0954 (0.1007)	0.0577 (0.0626)	0.761 (0.907)	81.25 (72.16)
[20/157]	0.0967 (0.0982)	0.0587 (0.0605)	0.955 (0.875)	75.00 (72.92)
[30/157]	0.0942 (0.0972)	0.0561 (0.0596)	0.647 (0.860)	71.88 (72.78)
[40/157]	0.1205 (0.0991)	0.0801 (0.0613)	0.721 (0.851)	81.25 (72.79)
[50/157]	0.0969 (0.0988)	0.0583 (0.0610)	0.674 (0.833)	81.25 (73.35)
[60/157]	0.0934 (0.0983)	0.0556 (0.0604)	0.958 (0.848)	62.50 (72.54)
[70/157]	0.1139 (0.0990)	0.0739 (0.0611)	1.065 (0.848)	59.38 (72.45)
[80/157]	0.0979 (0.0993)	0.0600 (0.0614)	0.775 (0.846)	71.88 (72.11)
[90/157]	0.0991 (0.0991)	0.0605 (0.0612)	0.823 (0.843)	68.75 (72.39)
[100/157]	0.0975 (0.0989)	0.0587 (0.0611)	1.078 (0.843)	65.62 (72.40)
[110/157]	0.0967 (0.0988)	0.0585 (0.0609)	0.769 (0.835)	78.12 (72.83)
[120/157]	0.1037 (0.0996)	0.0646 (0.0617)	0.753 (0.831)	75.00 (72.70)
[130/157]	0.1008 (0.0999)	0.0615 (0.0619)	0.864 (0.830)	75.00 (72.97)
[140/157]	0.1041 (0.1001)	0.0638 (0.0621)	0.830 (0.833)	81.25 (72.98)
[150/157]	0.1022 (0.1002)	0.0631 (0.0622)	0.860 (0.829)	68.75 (73.10)
[156/157]	0.0857 (0.1002)	0.0590 (0.0622)	1.442 (0.831)	50.00 (72.98)
 * Train Acc 72.980
 * Val Acc 72.400, Total time 0.60
 * Val loss 0.767, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0093 (0.0093)	0.844 (0.844)	71.88 (71.88)
[10/157]	0.1026 (0.0964)	0.0635 (0.0585)	0.713 (0.845)	75.00 (71.88)
[20/157]	0.1044 (0.0994)	0.0645 (0.0613)	0.945 (0.837)	68.75 (72.77)
[30/157]	0.1039 (0.1007)	0.0639 (0.0622)	0.570 (0.819)	87.50 (73.69)
[40/157]	0.1027 (0.1011)	0.0639 (0.0626)	0.698 (0.832)	84.38 (73.02)
[50/157]	0.1050 (0.1015)	0.0637 (0.0628)	0.699 (0.829)	75.00 (73.16)
[60/157]	0.0925 (0.1008)	0.0545 (0.0621)	0.642 (0.831)	87.50 (73.00)
[70/157]	0.0993 (0.1016)	0.0606 (0.0630)	0.950 (0.834)	65.62 (73.11)
[80/157]	0.0976 (0.1013)	0.0590 (0.0627)	0.723 (0.828)	81.25 (73.34)
[90/157]	0.1005 (0.1010)	0.0615 (0.0625)	0.639 (0.818)	78.12 (73.59)
[100/157]	0.1005 (0.1009)	0.0607 (0.0623)	0.941 (0.824)	71.88 (73.17)
[110/157]	0.1016 (0.1007)	0.0614 (0.0621)	0.743 (0.825)	81.25 (73.31)
[120/157]	0.0988 (0.1006)	0.0606 (0.0619)	0.775 (0.822)	75.00 (73.63)
[130/157]	0.0986 (0.1005)	0.0593 (0.0618)	1.053 (0.823)	65.62 (73.57)
[140/157]	0.1002 (0.1004)	0.0607 (0.0617)	0.886 (0.827)	71.88 (73.40)
[150/157]	0.1011 (0.1003)	0.0612 (0.0617)	0.825 (0.834)	71.88 (72.97)
[156/157]	0.0832 (0.1002)	0.0554 (0.0616)	1.501 (0.835)	50.00 (72.96)
 * Train Acc 72.960
 * Val Acc 73.100, Total time 0.59
 * Val loss 0.753, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0422 (0.0422)	0.0083 (0.0083)	0.896 (0.896)	65.62 (65.62)
[10/157]	0.1212 (0.1011)	0.0804 (0.0632)	0.891 (0.881)	68.75 (72.44)
[20/157]	0.1051 (0.1040)	0.0656 (0.0655)	0.891 (0.853)	75.00 (72.92)
[30/157]	0.1061 (0.1047)	0.0664 (0.0661)	0.839 (0.838)	78.12 (73.69)
[40/157]	0.0966 (0.1029)	0.0586 (0.0645)	0.888 (0.826)	75.00 (74.39)
[50/157]	0.1023 (0.1025)	0.0637 (0.0641)	0.699 (0.835)	84.38 (73.59)
[60/157]	0.1050 (0.1025)	0.0648 (0.0641)	0.548 (0.816)	78.12 (74.33)
[70/157]	0.1028 (0.1026)	0.0637 (0.0640)	0.709 (0.823)	87.50 (74.21)
[80/157]	0.1027 (0.1025)	0.0633 (0.0639)	0.860 (0.823)	71.88 (74.31)
[90/157]	0.1040 (0.1026)	0.0647 (0.0640)	0.895 (0.826)	68.75 (74.00)
[100/157]	0.1038 (0.1026)	0.0641 (0.0639)	0.986 (0.833)	56.25 (73.58)
[110/157]	0.1048 (0.1027)	0.0637 (0.0639)	0.807 (0.836)	81.25 (73.34)
[120/157]	0.1027 (0.1026)	0.0626 (0.0639)	0.926 (0.833)	75.00 (73.19)
[130/157]	0.1036 (0.1027)	0.0637 (0.0639)	0.933 (0.828)	68.75 (73.47)
[140/157]	0.1037 (0.1027)	0.0640 (0.0638)	1.086 (0.836)	56.25 (72.96)
[150/157]	0.1027 (0.1026)	0.0634 (0.0638)	0.810 (0.833)	68.75 (72.87)
[156/157]	0.0867 (0.1025)	0.0583 (0.0638)	1.486 (0.836)	50.00 (72.74)
 * Train Acc 72.740
 * Val Acc 72.900, Total time 0.60
 * Val loss 0.756, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0087 (0.0087)	0.787 (0.787)	68.75 (68.75)
[10/157]	0.1031 (0.0965)	0.0642 (0.0586)	0.746 (0.774)	71.88 (78.12)
[20/157]	0.1036 (0.0993)	0.0641 (0.0611)	0.737 (0.795)	78.12 (75.60)
[30/157]	0.1051 (0.1003)	0.0650 (0.0620)	1.031 (0.789)	71.88 (76.01)
[40/157]	0.1032 (0.1008)	0.0636 (0.0624)	0.872 (0.785)	75.00 (76.14)
[50/157]	0.1018 (0.1011)	0.0628 (0.0627)	0.972 (0.799)	62.50 (75.61)
[60/157]	0.1035 (0.1014)	0.0635 (0.0628)	0.576 (0.803)	84.38 (75.51)
[70/157]	0.1036 (0.1016)	0.0643 (0.0630)	0.606 (0.806)	84.38 (75.18)
[80/157]	0.1021 (0.1017)	0.0633 (0.0631)	0.765 (0.815)	78.12 (74.81)
[90/157]	0.1030 (0.1017)	0.0635 (0.0631)	0.774 (0.815)	71.88 (74.42)
[100/157]	0.1045 (0.1018)	0.0635 (0.0632)	0.770 (0.820)	81.25 (74.20)
[110/157]	0.1004 (0.1019)	0.0624 (0.0632)	0.939 (0.827)	65.62 (73.82)
[120/157]	0.1020 (0.1019)	0.0630 (0.0633)	0.748 (0.830)	78.12 (73.66)
[130/157]	0.1033 (0.1020)	0.0638 (0.0633)	0.686 (0.830)	78.12 (73.66)
[140/157]	0.1038 (0.1020)	0.0639 (0.0633)	0.740 (0.830)	90.62 (73.65)
[150/157]	0.1046 (0.1020)	0.0638 (0.0633)	0.650 (0.832)	84.38 (73.53)
[156/157]	0.0865 (0.1019)	0.0591 (0.0633)	0.609 (0.829)	87.50 (73.70)
 * Train Acc 73.700
 * Val Acc 73.400, Total time 0.58
 * Val loss 0.754, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0415 (0.0415)	0.0084 (0.0084)	0.760 (0.760)	71.88 (71.88)
[10/157]	0.0959 (0.0925)	0.0580 (0.0543)	0.691 (0.756)	81.25 (76.42)
[20/157]	0.0968 (0.0942)	0.0577 (0.0561)	0.709 (0.760)	78.12 (76.34)
[30/157]	0.0974 (0.0947)	0.0584 (0.0567)	0.905 (0.772)	71.88 (75.81)
[40/157]	0.0979 (0.0949)	0.0583 (0.0570)	0.707 (0.779)	81.25 (75.23)
[50/157]	0.0995 (0.0958)	0.0605 (0.0578)	0.902 (0.797)	68.75 (74.39)
[60/157]	0.0973 (0.0964)	0.0589 (0.0583)	0.677 (0.800)	84.38 (74.18)
[70/157]	0.0982 (0.0968)	0.0596 (0.0586)	0.902 (0.806)	71.88 (73.94)
[80/157]	0.0997 (0.0971)	0.0603 (0.0589)	1.135 (0.810)	56.25 (73.69)
[90/157]	0.0994 (0.0973)	0.0597 (0.0591)	0.931 (0.812)	56.25 (73.73)
[100/157]	0.0998 (0.0975)	0.0607 (0.0592)	0.549 (0.819)	87.50 (73.36)
[110/157]	0.1008 (0.0977)	0.0616 (0.0594)	1.011 (0.822)	65.62 (73.34)
[120/157]	0.0998 (0.0978)	0.0611 (0.0595)	0.659 (0.823)	84.38 (73.40)
[130/157]	0.0995 (0.0979)	0.0597 (0.0596)	0.931 (0.820)	71.88 (73.40)
[140/157]	0.1090 (0.0984)	0.0704 (0.0600)	0.877 (0.823)	75.00 (73.71)
[150/157]	0.0974 (0.0983)	0.0586 (0.0599)	0.645 (0.824)	75.00 (73.61)
[156/157]	0.0835 (0.0982)	0.0556 (0.0599)	1.405 (0.829)	50.00 (73.32)
 * Train Acc 73.320
 * Val Acc 72.900, Total time 0.59
 * Val loss 0.756, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0087 (0.0087)	0.734 (0.734)	71.88 (71.88)
[10/157]	0.1138 (0.1029)	0.0731 (0.0644)	0.625 (0.807)	78.12 (72.73)
[20/157]	0.1042 (0.1039)	0.0645 (0.0649)	0.661 (0.802)	84.38 (72.92)
[30/157]	0.1060 (0.1042)	0.0661 (0.0652)	0.888 (0.798)	75.00 (73.69)
[40/157]	0.0940 (0.1041)	0.0568 (0.0652)	1.011 (0.807)	62.50 (73.40)
[50/157]	0.0956 (0.1024)	0.0579 (0.0637)	0.894 (0.821)	68.75 (73.10)
[60/157]	0.0958 (0.1012)	0.0579 (0.0628)	0.625 (0.815)	78.12 (73.36)
[70/157]	0.1002 (0.1022)	0.0606 (0.0637)	0.818 (0.806)	71.88 (73.68)
[80/157]	0.0979 (0.1019)	0.0586 (0.0632)	0.989 (0.813)	59.38 (73.11)
[90/157]	0.1002 (0.1017)	0.0613 (0.0630)	0.871 (0.819)	71.88 (72.97)
[100/157]	0.0999 (0.1015)	0.0605 (0.0628)	0.900 (0.827)	71.88 (72.77)
[110/157]	0.0998 (0.1014)	0.0606 (0.0626)	0.713 (0.829)	71.88 (72.69)
[120/157]	0.0999 (0.1013)	0.0607 (0.0625)	0.706 (0.826)	81.25 (72.83)
[130/157]	0.1009 (0.1012)	0.0613 (0.0623)	0.757 (0.823)	78.12 (73.00)
[140/157]	0.0999 (0.1011)	0.0602 (0.0622)	0.861 (0.827)	68.75 (72.92)
[150/157]	0.1003 (0.1010)	0.0609 (0.0621)	0.753 (0.823)	68.75 (73.18)
[156/157]	0.0838 (0.1008)	0.0564 (0.0621)	1.717 (0.825)	25.00 (72.98)
 * Train Acc 72.980
 * Val Acc 73.100, Total time 0.59
 * Val loss 0.753, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0088 (0.0088)	0.968 (0.968)	68.75 (68.75)
[10/157]	0.1009 (0.1011)	0.0612 (0.0623)	0.413 (0.789)	93.75 (74.72)
[20/157]	0.1019 (0.1013)	0.0620 (0.0623)	1.196 (0.819)	56.25 (72.47)
[30/157]	0.1036 (0.1013)	0.0625 (0.0624)	0.637 (0.806)	78.12 (73.99)
[40/157]	0.0988 (0.1012)	0.0589 (0.0621)	0.567 (0.806)	87.50 (74.01)
[50/157]	0.1009 (0.1011)	0.0633 (0.0621)	0.948 (0.817)	62.50 (73.41)
[60/157]	0.0999 (0.1011)	0.0605 (0.0621)	0.922 (0.825)	68.75 (73.57)
[70/157]	0.1026 (0.1012)	0.0621 (0.0622)	0.706 (0.824)	78.12 (73.86)
[80/157]	0.1008 (0.1011)	0.0605 (0.0621)	0.859 (0.821)	71.88 (73.96)
[90/157]	0.1009 (0.1012)	0.0615 (0.0621)	0.888 (0.825)	71.88 (73.45)
[100/157]	0.1032 (0.1012)	0.0628 (0.0621)	0.863 (0.827)	68.75 (73.17)
[110/157]	0.0998 (0.1011)	0.0606 (0.0620)	0.760 (0.827)	75.00 (73.17)
[120/157]	0.1000 (0.1010)	0.0611 (0.0620)	0.906 (0.819)	75.00 (73.45)
[130/157]	0.1013 (0.1010)	0.0621 (0.0620)	0.914 (0.824)	75.00 (73.52)
[140/157]	0.1025 (0.1011)	0.0615 (0.0620)	1.180 (0.830)	65.62 (73.01)
[150/157]	0.1014 (0.1011)	0.0613 (0.0620)	0.956 (0.833)	65.62 (72.95)
[156/157]	0.0892 (0.1010)	0.0583 (0.0620)	1.229 (0.834)	50.00 (72.78)
 * Train Acc 72.780
 * Val Acc 73.400, Total time 0.60
 * Val loss 0.742, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0091 (0.0091)	0.843 (0.843)	75.00 (75.00)
[10/157]	0.0978 (0.0948)	0.0585 (0.0561)	0.790 (0.840)	75.00 (74.72)
[20/157]	0.1006 (0.0974)	0.0613 (0.0588)	0.712 (0.818)	71.88 (75.00)
[30/157]	0.1008 (0.0985)	0.0617 (0.0601)	0.712 (0.804)	78.12 (75.20)
[40/157]	0.0999 (0.0991)	0.0607 (0.0606)	0.853 (0.791)	65.62 (75.61)
[50/157]	0.1005 (0.0993)	0.0616 (0.0608)	0.464 (0.793)	90.62 (75.25)
[60/157]	0.1013 (0.0995)	0.0627 (0.0611)	1.039 (0.805)	71.88 (75.20)
[70/157]	0.1005 (0.0997)	0.0614 (0.0613)	1.191 (0.815)	71.88 (74.65)
[80/157]	0.1015 (0.0998)	0.0619 (0.0614)	1.075 (0.827)	65.62 (74.07)
[90/157]	0.1017 (0.1000)	0.0613 (0.0615)	0.858 (0.826)	68.75 (74.18)
[100/157]	0.0997 (0.1000)	0.0608 (0.0615)	0.789 (0.822)	81.25 (74.32)
[110/157]	0.1007 (0.1000)	0.0615 (0.0616)	0.818 (0.817)	75.00 (74.47)
[120/157]	0.1018 (0.1000)	0.0628 (0.0616)	0.812 (0.818)	78.12 (74.38)
[130/157]	0.1007 (0.1001)	0.0612 (0.0617)	0.804 (0.820)	68.75 (74.21)
[140/157]	0.0998 (0.1001)	0.0607 (0.0617)	1.012 (0.822)	65.62 (74.09)
[150/157]	0.1020 (0.1002)	0.0632 (0.0617)	0.923 (0.825)	71.88 (73.97)
[156/157]	0.0838 (0.1001)	0.0560 (0.0617)	0.873 (0.828)	50.00 (73.78)
 * Train Acc 73.780
 * Val Acc 72.800, Total time 0.60
 * Val loss 0.753, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0078 (0.0078)	0.597 (0.597)	87.50 (87.50)
[10/157]	0.0993 (0.0944)	0.0587 (0.0562)	0.989 (0.860)	68.75 (73.86)
[20/157]	0.1031 (0.0973)	0.0641 (0.0594)	0.585 (0.813)	84.38 (74.85)
[30/157]	0.1004 (0.0985)	0.0616 (0.0603)	0.797 (0.816)	78.12 (73.59)
[40/157]	0.1004 (0.0990)	0.0609 (0.0605)	0.639 (0.821)	84.38 (72.41)
[50/157]	0.1008 (0.0991)	0.0619 (0.0608)	0.618 (0.805)	81.25 (73.35)
[60/157]	0.1006 (0.0993)	0.0620 (0.0610)	0.887 (0.812)	71.88 (73.57)
[70/157]	0.0997 (0.0996)	0.0606 (0.0611)	1.158 (0.821)	59.38 (73.50)
[80/157]	0.1002 (0.0997)	0.0613 (0.0612)	1.045 (0.824)	71.88 (73.53)
[90/157]	0.1010 (0.0998)	0.0622 (0.0613)	0.934 (0.829)	65.62 (73.32)
[100/157]	0.1004 (0.0999)	0.0614 (0.0615)	0.759 (0.832)	71.88 (73.17)
[110/157]	0.1001 (0.0999)	0.0615 (0.0615)	0.830 (0.831)	68.75 (73.25)
[120/157]	0.0998 (0.1000)	0.0613 (0.0615)	0.888 (0.843)	68.75 (72.68)
[130/157]	0.1006 (0.1001)	0.0613 (0.0616)	0.708 (0.845)	75.00 (72.52)
[140/157]	0.1002 (0.1001)	0.0615 (0.0616)	1.022 (0.846)	62.50 (72.63)
[150/157]	0.1008 (0.1002)	0.0617 (0.0617)	1.034 (0.843)	65.62 (72.66)
[156/157]	0.0853 (0.1001)	0.0574 (0.0616)	1.179 (0.842)	50.00 (72.74)
 * Train Acc 72.740
 * Val Acc 73.400, Total time 0.60
 * Val loss 0.757, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0094 (0.0094)	0.924 (0.924)	71.88 (71.88)
[10/157]	0.1006 (0.0949)	0.0615 (0.0563)	0.994 (0.845)	68.75 (70.74)
[20/157]	0.1011 (0.0980)	0.0615 (0.0592)	0.777 (0.835)	71.88 (72.02)
[30/157]	0.1009 (0.0988)	0.0609 (0.0599)	0.891 (0.878)	68.75 (72.08)
[40/157]	0.1002 (0.0995)	0.0608 (0.0604)	0.835 (0.852)	75.00 (73.17)
[50/157]	0.1010 (0.0997)	0.0618 (0.0607)	0.793 (0.843)	68.75 (73.04)
[60/157]	0.1017 (0.0999)	0.0622 (0.0610)	1.124 (0.839)	53.12 (73.10)
[70/157]	0.0997 (0.1000)	0.0606 (0.0611)	0.654 (0.826)	81.25 (73.24)
[80/157]	0.1017 (0.1001)	0.0627 (0.0612)	0.795 (0.822)	68.75 (73.26)
[90/157]	0.1004 (0.1002)	0.0601 (0.0613)	0.711 (0.821)	78.12 (73.45)
[100/157]	0.1005 (0.1002)	0.0613 (0.0613)	0.704 (0.823)	78.12 (73.36)
[110/157]	0.1026 (0.1003)	0.0609 (0.0614)	0.788 (0.835)	65.62 (73.09)
[120/157]	0.1007 (0.1003)	0.0609 (0.0614)	0.849 (0.828)	62.50 (73.30)
[130/157]	0.1007 (0.1003)	0.0606 (0.0614)	0.903 (0.824)	68.75 (73.40)
[140/157]	0.1002 (0.1004)	0.0611 (0.0615)	0.808 (0.826)	81.25 (73.45)
[150/157]	0.1011 (0.1004)	0.0617 (0.0615)	0.949 (0.832)	71.88 (73.43)
[156/157]	0.0842 (0.1002)	0.0567 (0.0614)	0.696 (0.834)	75.00 (73.30)
 * Train Acc 73.300
 * Val Acc 73.400, Total time 0.60
 * Val loss 0.749, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0455 (0.0455)	0.0096 (0.0096)	0.890 (0.890)	65.62 (65.62)
[10/157]	0.1000 (0.0947)	0.0609 (0.0568)	1.043 (0.880)	62.50 (71.02)
[20/157]	0.1023 (0.0976)	0.0628 (0.0594)	0.824 (0.839)	75.00 (72.47)
[30/157]	0.0999 (0.0987)	0.0602 (0.0602)	0.823 (0.850)	65.62 (71.67)
[40/157]	0.1004 (0.0991)	0.0612 (0.0605)	0.757 (0.849)	78.12 (71.95)
[50/157]	0.1020 (0.0994)	0.0625 (0.0608)	0.842 (0.833)	71.88 (72.67)
[60/157]	0.1003 (0.0997)	0.0607 (0.0611)	1.001 (0.842)	62.50 (72.08)
[70/157]	0.1012 (0.0999)	0.0623 (0.0612)	0.772 (0.831)	81.25 (72.80)
[80/157]	0.1005 (0.1000)	0.0607 (0.0613)	0.767 (0.810)	78.12 (73.69)
[90/157]	0.0995 (0.1000)	0.0606 (0.0613)	0.864 (0.817)	68.75 (73.32)
[100/157]	0.1007 (0.1000)	0.0618 (0.0614)	0.906 (0.815)	68.75 (73.36)
[110/157]	0.1013 (0.1002)	0.0608 (0.0614)	0.581 (0.814)	87.50 (73.62)
[120/157]	0.1008 (0.1001)	0.0610 (0.0614)	0.920 (0.812)	62.50 (73.55)
[130/157]	0.1004 (0.1001)	0.0618 (0.0614)	0.655 (0.813)	75.00 (73.33)
[140/157]	0.1018 (0.1002)	0.0623 (0.0615)	1.048 (0.816)	78.12 (73.27)
[150/157]	0.1003 (0.1002)	0.0612 (0.0615)	0.719 (0.811)	68.75 (73.41)
[156/157]	0.0839 (0.1001)	0.0573 (0.0615)	1.122 (0.813)	62.50 (73.32)
 * Train Acc 73.320
 * Val Acc 73.400, Total time 0.60
 * Val loss 0.744, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0089 (0.0089)	0.694 (0.694)	75.00 (75.00)
[10/157]	0.1004 (0.0952)	0.0615 (0.0570)	0.575 (0.695)	84.38 (76.99)
[20/157]	0.1014 (0.0975)	0.0624 (0.0593)	0.744 (0.742)	71.88 (74.55)
[30/157]	0.1002 (0.0984)	0.0610 (0.0600)	0.702 (0.765)	78.12 (73.59)
[40/157]	0.1003 (0.0988)	0.0619 (0.0603)	1.103 (0.774)	62.50 (73.93)
[50/157]	0.1006 (0.0992)	0.0608 (0.0607)	0.972 (0.783)	65.62 (74.26)
[60/157]	0.0996 (0.0994)	0.0606 (0.0607)	0.888 (0.779)	68.75 (74.33)
[70/157]	0.1004 (0.0996)	0.0608 (0.0608)	0.557 (0.786)	84.38 (74.30)
[80/157]	0.1016 (0.0998)	0.0620 (0.0610)	0.666 (0.797)	75.00 (73.84)
[90/157]	0.0999 (0.0999)	0.0610 (0.0611)	0.850 (0.794)	75.00 (74.18)
[100/157]	0.1003 (0.0999)	0.0612 (0.0611)	0.786 (0.801)	71.88 (73.51)
[110/157]	0.1007 (0.1000)	0.0614 (0.0612)	0.960 (0.801)	59.38 (73.54)
[120/157]	0.1022 (0.1002)	0.0623 (0.0613)	0.774 (0.799)	68.75 (73.63)
[130/157]	0.0998 (0.1002)	0.0607 (0.0613)	0.898 (0.803)	68.75 (73.47)
[140/157]	0.0999 (0.1002)	0.0604 (0.0613)	1.086 (0.805)	56.25 (73.45)
[150/157]	0.1011 (0.1002)	0.0613 (0.0613)	0.859 (0.806)	68.75 (73.43)
[156/157]	0.0842 (0.1000)	0.0566 (0.0613)	0.830 (0.809)	50.00 (73.36)
 * Train Acc 73.360
 * Val Acc 73.900, Total time 0.58
 * Val loss 0.732, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0085 (0.0085)	0.763 (0.763)	62.50 (62.50)
[10/157]	0.0956 (0.0919)	0.0576 (0.0544)	0.856 (0.837)	71.88 (71.59)
[20/157]	0.0965 (0.0937)	0.0583 (0.0559)	0.990 (0.839)	68.75 (73.07)
[30/157]	0.0973 (0.0943)	0.0581 (0.0566)	0.781 (0.832)	75.00 (73.49)
[40/157]	0.0943 (0.0945)	0.0566 (0.0569)	0.899 (0.823)	68.75 (73.09)
[50/157]	0.0962 (0.0947)	0.0581 (0.0572)	0.747 (0.805)	68.75 (73.35)
[60/157]	0.1007 (0.0966)	0.0631 (0.0590)	0.888 (0.841)	68.75 (71.82)
[70/157]	0.1208 (0.0970)	0.0805 (0.0595)	0.793 (0.846)	75.00 (71.92)
[80/157]	0.0962 (0.0978)	0.0584 (0.0601)	0.708 (0.838)	71.88 (72.18)
[90/157]	0.1091 (0.0980)	0.0698 (0.0604)	0.752 (0.825)	75.00 (72.63)
[100/157]	0.0946 (0.0987)	0.0568 (0.0609)	0.866 (0.826)	75.00 (72.87)
[110/157]	0.0971 (0.0984)	0.0586 (0.0606)	0.889 (0.826)	68.75 (72.94)
[120/157]	0.0971 (0.0982)	0.0584 (0.0604)	0.948 (0.821)	68.75 (73.27)
[130/157]	0.1185 (0.0992)	0.0771 (0.0613)	0.871 (0.819)	65.62 (73.57)
[140/157]	0.1014 (0.0993)	0.0618 (0.0614)	0.653 (0.818)	84.38 (73.54)
[150/157]	0.1008 (0.0994)	0.0603 (0.0614)	0.848 (0.815)	68.75 (73.65)
[156/157]	0.0840 (0.0994)	0.0564 (0.0614)	0.444 (0.816)	100.00 (73.72)
 * Train Acc 73.720
 * Val Acc 74.000, Total time 0.61
 * Val loss 0.748, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0084 (0.0084)	1.114 (1.114)	62.50 (62.50)
[10/157]	0.1005 (0.0948)	0.0620 (0.0563)	0.994 (0.951)	65.62 (69.60)
[20/157]	0.1000 (0.0975)	0.0613 (0.0591)	0.759 (0.897)	71.88 (70.09)
[30/157]	0.1012 (0.0988)	0.0615 (0.0600)	0.793 (0.842)	71.88 (71.57)
[40/157]	0.0999 (0.0990)	0.0599 (0.0603)	0.973 (0.850)	68.75 (71.34)
[50/157]	0.1005 (0.0993)	0.0604 (0.0605)	1.159 (0.853)	62.50 (70.65)
[60/157]	0.1003 (0.0994)	0.0617 (0.0608)	0.611 (0.844)	84.38 (71.31)
[70/157]	0.1000 (0.0997)	0.0609 (0.0609)	0.901 (0.841)	65.62 (71.65)
[80/157]	0.1010 (0.0997)	0.0618 (0.0610)	0.560 (0.831)	87.50 (72.30)
[90/157]	0.1001 (0.0998)	0.0613 (0.0611)	0.892 (0.827)	75.00 (72.49)
[100/157]	0.1017 (0.0999)	0.0617 (0.0612)	0.841 (0.822)	71.88 (72.93)
[110/157]	0.1004 (0.1000)	0.0611 (0.0612)	0.485 (0.819)	87.50 (72.94)
[120/157]	0.0983 (0.1000)	0.0585 (0.0612)	0.824 (0.820)	68.75 (72.91)
[130/157]	0.0998 (0.1000)	0.0611 (0.0612)	0.852 (0.819)	75.00 (72.97)
[140/157]	0.1000 (0.1001)	0.0604 (0.0613)	0.895 (0.821)	71.88 (72.89)
[150/157]	0.0999 (0.1001)	0.0610 (0.0613)	0.935 (0.820)	59.38 (73.10)
[156/157]	0.0840 (0.1000)	0.0568 (0.0613)	0.789 (0.818)	75.00 (73.14)
 * Train Acc 73.140
 * Val Acc 73.700, Total time 0.60
 * Val loss 0.740, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0092 (0.0092)	0.921 (0.921)	68.75 (68.75)
[10/157]	0.1000 (0.0952)	0.0603 (0.0564)	0.577 (0.799)	84.38 (73.30)
[20/157]	0.1005 (0.0975)	0.0610 (0.0588)	0.655 (0.835)	87.50 (73.66)
[30/157]	0.1000 (0.0982)	0.0611 (0.0597)	0.887 (0.835)	81.25 (73.79)
[40/157]	0.1000 (0.0987)	0.0607 (0.0602)	0.805 (0.835)	68.75 (73.93)
[50/157]	0.1010 (0.0991)	0.0620 (0.0605)	0.696 (0.837)	78.12 (73.22)
[60/157]	0.1006 (0.0994)	0.0606 (0.0608)	0.753 (0.829)	71.88 (73.21)
[70/157]	0.1010 (0.0995)	0.0613 (0.0608)	0.859 (0.816)	68.75 (73.68)
[80/157]	0.1002 (0.0996)	0.0613 (0.0609)	0.724 (0.820)	78.12 (73.84)
[90/157]	0.1009 (0.0996)	0.0605 (0.0610)	0.776 (0.815)	71.88 (74.04)
[100/157]	0.1039 (0.0998)	0.0634 (0.0611)	0.850 (0.811)	75.00 (74.41)
[110/157]	0.1003 (0.0999)	0.0609 (0.0612)	0.857 (0.810)	65.62 (74.52)
[120/157]	0.1009 (0.0999)	0.0617 (0.0613)	0.592 (0.812)	81.25 (74.17)
[130/157]	0.1003 (0.1000)	0.0599 (0.0613)	0.908 (0.814)	75.00 (73.95)
[140/157]	0.1003 (0.1000)	0.0610 (0.0613)	0.684 (0.824)	75.00 (73.56)
[150/157]	0.1020 (0.1001)	0.0628 (0.0613)	0.706 (0.822)	71.88 (73.57)
[156/157]	0.0850 (0.1000)	0.0570 (0.0613)	1.223 (0.820)	62.50 (73.60)
 * Train Acc 73.600
 * Val Acc 73.600, Total time 0.60
 * Val loss 0.741, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0087 (0.0087)	1.014 (1.014)	59.38 (59.38)
[10/157]	0.1000 (0.0953)	0.0601 (0.0561)	1.030 (0.867)	62.50 (71.88)
[20/157]	0.0998 (0.0976)	0.0604 (0.0586)	0.891 (0.851)	71.88 (71.58)
[30/157]	0.1006 (0.0983)	0.0610 (0.0595)	0.979 (0.855)	71.88 (71.67)
[40/157]	0.1010 (0.0989)	0.0625 (0.0602)	0.507 (0.845)	84.38 (72.48)
[50/157]	0.0993 (0.0993)	0.0604 (0.0605)	0.767 (0.841)	81.25 (72.61)
[60/157]	0.0997 (0.0994)	0.0608 (0.0607)	0.694 (0.834)	75.00 (72.85)
[70/157]	0.1004 (0.0995)	0.0616 (0.0609)	0.891 (0.822)	71.88 (73.42)
[80/157]	0.1018 (0.0997)	0.0633 (0.0610)	0.738 (0.824)	75.00 (73.07)
[90/157]	0.1021 (0.0997)	0.0630 (0.0612)	0.729 (0.828)	78.12 (73.15)
[100/157]	0.0991 (0.0998)	0.0605 (0.0612)	1.127 (0.831)	62.50 (72.96)
[110/157]	0.0997 (0.0999)	0.0611 (0.0612)	0.712 (0.824)	81.25 (73.31)
[120/157]	0.1030 (0.1000)	0.0626 (0.0613)	1.019 (0.830)	65.62 (73.27)
[130/157]	0.0991 (0.1000)	0.0599 (0.0613)	0.839 (0.819)	71.88 (73.66)
[140/157]	0.0983 (0.1000)	0.0597 (0.0613)	0.696 (0.822)	78.12 (73.56)
[150/157]	0.1006 (0.1000)	0.0603 (0.0613)	0.654 (0.818)	81.25 (73.68)
[156/157]	0.0845 (0.0999)	0.0574 (0.0613)	0.975 (0.821)	62.50 (73.48)
 * Train Acc 73.480
 * Val Acc 73.700, Total time 0.59
 * Val loss 0.743, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0088 (0.0088)	0.864 (0.864)	71.88 (71.88)
[10/157]	0.1007 (0.0943)	0.0616 (0.0565)	0.742 (0.724)	81.25 (78.41)
[20/157]	0.1000 (0.0970)	0.0610 (0.0589)	0.659 (0.745)	68.75 (77.08)
[30/157]	0.1013 (0.0980)	0.0625 (0.0597)	0.691 (0.761)	75.00 (75.50)
[40/157]	0.1008 (0.0984)	0.0616 (0.0601)	1.043 (0.776)	56.25 (75.15)
[50/157]	0.0998 (0.0987)	0.0608 (0.0604)	0.858 (0.800)	81.25 (73.96)
[60/157]	0.1006 (0.0990)	0.0612 (0.0606)	0.702 (0.788)	71.88 (74.28)
[70/157]	0.1005 (0.0991)	0.0615 (0.0607)	0.718 (0.787)	78.12 (74.47)
[80/157]	0.1006 (0.0993)	0.0615 (0.0608)	0.673 (0.785)	78.12 (74.34)
[90/157]	0.1011 (0.0994)	0.0620 (0.0609)	0.726 (0.793)	78.12 (74.21)
[100/157]	0.0997 (0.0996)	0.0616 (0.0610)	1.157 (0.802)	59.38 (73.61)
[110/157]	0.0998 (0.0996)	0.0603 (0.0610)	0.752 (0.812)	62.50 (73.23)
[120/157]	0.1003 (0.0996)	0.0616 (0.0611)	0.778 (0.819)	78.12 (73.06)
[130/157]	0.1002 (0.0996)	0.0611 (0.0611)	0.628 (0.816)	78.12 (73.21)
[140/157]	0.0996 (0.0997)	0.0610 (0.0612)	0.638 (0.811)	90.62 (73.40)
[150/157]	0.1028 (0.0998)	0.0630 (0.0612)	0.845 (0.805)	71.88 (73.70)
[156/157]	0.0851 (0.0997)	0.0570 (0.0612)	1.496 (0.810)	50.00 (73.48)
 * Train Acc 73.480
 * Val Acc 73.300, Total time 0.60
 * Val loss 0.733, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0081 (0.0081)	0.731 (0.731)	87.50 (87.50)
[10/157]	0.1015 (0.0949)	0.0621 (0.0565)	0.851 (0.741)	68.75 (78.41)
[20/157]	0.1005 (0.0978)	0.0609 (0.0594)	0.560 (0.747)	87.50 (76.04)
[30/157]	0.1008 (0.0987)	0.0614 (0.0601)	0.759 (0.774)	75.00 (74.60)
[40/157]	0.1022 (0.0993)	0.0617 (0.0607)	1.001 (0.783)	65.62 (74.70)
[50/157]	0.1010 (0.0995)	0.0613 (0.0609)	0.936 (0.819)	71.88 (73.59)
[60/157]	0.1005 (0.0996)	0.0617 (0.0609)	0.649 (0.813)	87.50 (73.82)
[70/157]	0.1006 (0.0997)	0.0619 (0.0610)	0.721 (0.809)	78.12 (73.42)
[80/157]	0.1008 (0.0999)	0.0610 (0.0612)	0.676 (0.809)	84.38 (73.30)
[90/157]	0.0993 (0.0999)	0.0608 (0.0611)	0.755 (0.813)	68.75 (73.32)
[100/157]	0.0992 (0.0999)	0.0610 (0.0612)	0.947 (0.816)	62.50 (73.24)
[110/157]	0.0999 (0.0999)	0.0605 (0.0612)	1.061 (0.818)	65.62 (73.20)
[120/157]	0.1015 (0.1000)	0.0620 (0.0612)	0.772 (0.816)	78.12 (73.40)
[130/157]	0.1032 (0.1001)	0.0601 (0.0613)	0.875 (0.816)	75.00 (73.40)
[140/157]	0.0997 (0.1001)	0.0603 (0.0613)	0.893 (0.814)	71.88 (73.65)
[150/157]	0.1004 (0.1001)	0.0610 (0.0613)	0.971 (0.811)	71.88 (73.78)
[156/157]	0.0845 (0.1000)	0.0564 (0.0613)	0.711 (0.811)	75.00 (73.76)
 * Train Acc 73.760
 * Val Acc 73.700, Total time 0.60
 * Val loss 0.734, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0090 (0.0090)	0.914 (0.914)	75.00 (75.00)
[10/157]	0.1010 (0.0947)	0.0617 (0.0565)	0.922 (0.866)	65.62 (70.17)
[20/157]	0.1004 (0.0978)	0.0603 (0.0594)	0.905 (0.851)	78.12 (71.28)
[30/157]	0.1006 (0.0986)	0.0612 (0.0601)	0.735 (0.826)	78.12 (72.18)
[40/157]	0.1004 (0.0990)	0.0600 (0.0604)	0.836 (0.813)	78.12 (73.09)
[50/157]	0.1030 (0.0994)	0.0622 (0.0607)	0.954 (0.834)	68.75 (72.12)
[60/157]	0.1006 (0.0996)	0.0614 (0.0608)	0.738 (0.828)	78.12 (72.39)
[70/157]	0.0996 (0.0997)	0.0607 (0.0608)	0.579 (0.817)	81.25 (73.24)
[80/157]	0.1006 (0.0997)	0.0616 (0.0609)	0.840 (0.821)	65.62 (72.96)
[90/157]	0.1006 (0.0998)	0.0615 (0.0610)	0.802 (0.824)	68.75 (72.77)
[100/157]	0.1017 (0.1000)	0.0614 (0.0611)	1.066 (0.830)	59.38 (72.59)
[110/157]	0.1026 (0.1001)	0.0620 (0.0612)	0.734 (0.826)	78.12 (72.66)
[120/157]	0.0995 (0.1001)	0.0599 (0.0612)	0.751 (0.822)	81.25 (72.93)
[130/157]	0.1004 (0.1001)	0.0613 (0.0612)	0.820 (0.821)	75.00 (72.95)
[140/157]	0.0999 (0.1001)	0.0609 (0.0612)	0.562 (0.818)	75.00 (73.14)
[150/157]	0.0997 (0.1001)	0.0604 (0.0612)	0.806 (0.813)	75.00 (73.45)
[156/157]	0.0876 (0.1000)	0.0570 (0.0612)	0.642 (0.811)	87.50 (73.58)
 * Train Acc 73.580
 * Val Acc 73.300, Total time 0.60
 * Val loss 0.737, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0088 (0.0088)	0.865 (0.865)	68.75 (68.75)
[10/157]	0.1003 (0.0954)	0.0606 (0.0563)	0.899 (0.853)	68.75 (73.30)
[20/157]	0.1002 (0.0977)	0.0606 (0.0587)	0.590 (0.820)	87.50 (74.11)
[30/157]	0.1008 (0.0985)	0.0613 (0.0595)	0.734 (0.823)	75.00 (74.09)
[40/157]	0.1012 (0.0993)	0.0603 (0.0601)	0.757 (0.823)	75.00 (74.09)
[50/157]	0.1002 (0.0995)	0.0601 (0.0602)	0.829 (0.821)	75.00 (74.08)
[60/157]	0.0998 (0.0996)	0.0604 (0.0603)	0.888 (0.814)	65.62 (74.08)
[70/157]	0.1009 (0.0997)	0.0617 (0.0605)	0.686 (0.803)	78.12 (74.38)
[80/157]	0.1018 (0.0999)	0.0617 (0.0607)	0.727 (0.809)	78.12 (74.23)
[90/157]	0.0996 (0.0999)	0.0603 (0.0607)	0.722 (0.809)	78.12 (74.52)
[100/157]	0.1012 (0.0999)	0.0618 (0.0608)	0.930 (0.811)	75.00 (74.32)
[110/157]	0.1004 (0.0999)	0.0605 (0.0609)	0.661 (0.808)	81.25 (74.13)
[120/157]	0.1004 (0.0999)	0.0609 (0.0609)	0.804 (0.809)	78.12 (74.10)
[130/157]	0.1009 (0.0999)	0.0613 (0.0609)	1.117 (0.814)	62.50 (73.81)
[140/157]	0.0998 (0.0999)	0.0609 (0.0610)	0.828 (0.817)	68.75 (73.65)
[150/157]	0.1004 (0.0999)	0.0609 (0.0610)	0.799 (0.817)	62.50 (73.70)
[156/157]	0.0844 (0.0998)	0.0566 (0.0610)	0.954 (0.818)	62.50 (73.70)
 * Train Acc 73.700
 * Val Acc 73.300, Total time 0.59
 * Val loss 0.737, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0088 (0.0088)	0.787 (0.787)	71.88 (71.88)
[10/157]	0.1019 (0.0953)	0.0614 (0.0569)	1.051 (0.822)	65.62 (71.88)
[20/157]	0.1014 (0.0977)	0.0613 (0.0588)	0.599 (0.854)	84.38 (71.88)
[30/157]	0.1002 (0.0985)	0.0612 (0.0596)	0.821 (0.856)	75.00 (71.98)
[40/157]	0.1013 (0.0990)	0.0619 (0.0600)	0.810 (0.855)	65.62 (72.18)
[50/157]	0.1007 (0.0994)	0.0612 (0.0603)	0.863 (0.844)	71.88 (72.12)
[60/157]	0.1001 (0.0995)	0.0609 (0.0605)	0.839 (0.840)	81.25 (72.08)
[70/157]	0.1008 (0.0997)	0.0618 (0.0607)	1.039 (0.840)	62.50 (72.40)
[80/157]	0.1010 (0.0998)	0.0613 (0.0609)	1.241 (0.840)	62.50 (72.57)
[90/157]	0.1000 (0.0999)	0.0608 (0.0609)	0.701 (0.824)	78.12 (73.21)
[100/157]	0.1007 (0.1000)	0.0615 (0.0610)	0.799 (0.818)	68.75 (73.30)
[110/157]	0.1019 (0.1001)	0.0618 (0.0611)	0.667 (0.814)	81.25 (73.70)
[120/157]	0.0996 (0.1001)	0.0607 (0.0611)	0.704 (0.808)	78.12 (74.07)
[130/157]	0.1014 (0.1001)	0.0620 (0.0612)	0.994 (0.814)	65.62 (73.85)
[140/157]	0.1004 (0.1002)	0.0604 (0.0613)	0.803 (0.815)	75.00 (73.85)
[150/157]	0.1004 (0.1002)	0.0608 (0.0613)	0.638 (0.816)	81.25 (73.76)
[156/157]	0.0874 (0.1001)	0.0568 (0.0613)	1.225 (0.814)	75.00 (73.84)
 * Train Acc 73.840
 * Val Acc 72.600, Total time 0.59
 * Val loss 0.738, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0087 (0.0087)	0.729 (0.729)	75.00 (75.00)
[10/157]	0.1016 (0.0953)	0.0614 (0.0564)	0.708 (0.846)	78.12 (73.30)
[20/157]	0.0998 (0.0975)	0.0607 (0.0588)	0.883 (0.843)	59.38 (71.43)
[30/157]	0.1003 (0.0983)	0.0610 (0.0598)	0.999 (0.841)	65.62 (71.37)
[40/157]	0.1006 (0.0988)	0.0613 (0.0602)	0.565 (0.839)	87.50 (72.10)
[50/157]	0.1012 (0.0992)	0.0618 (0.0606)	0.543 (0.831)	84.38 (72.24)
[60/157]	0.0996 (0.0994)	0.0601 (0.0607)	0.605 (0.823)	87.50 (72.54)
[70/157]	0.0994 (0.0995)	0.0597 (0.0609)	0.761 (0.817)	65.62 (72.67)
[80/157]	0.1019 (0.0996)	0.0630 (0.0610)	0.669 (0.817)	87.50 (73.03)
[90/157]	0.1002 (0.0998)	0.0612 (0.0611)	0.613 (0.810)	81.25 (73.28)
[100/157]	0.1001 (0.0998)	0.0598 (0.0611)	0.677 (0.810)	81.25 (73.24)
[110/157]	0.1000 (0.0998)	0.0611 (0.0612)	0.766 (0.805)	68.75 (73.23)
[120/157]	0.1008 (0.0998)	0.0609 (0.0612)	0.799 (0.808)	68.75 (72.99)
[130/157]	0.0996 (0.0999)	0.0606 (0.0612)	0.916 (0.809)	71.88 (73.09)
[140/157]	0.1007 (0.0999)	0.0607 (0.0612)	0.561 (0.803)	84.38 (73.36)
[150/157]	0.0999 (0.0999)	0.0606 (0.0613)	0.647 (0.803)	75.00 (73.47)
[156/157]	0.0853 (0.0999)	0.0559 (0.0613)	0.580 (0.800)	75.00 (73.60)
 * Train Acc 73.600
 * Val Acc 73.100, Total time 0.60
 * Val loss 0.745, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0459 (0.0459)	0.0092 (0.0092)	0.777 (0.777)	84.38 (84.38)
[10/157]	0.1017 (0.0954)	0.0619 (0.0566)	0.714 (0.747)	81.25 (78.12)
[20/157]	0.1009 (0.0978)	0.0614 (0.0589)	0.483 (0.747)	87.50 (76.93)
[30/157]	0.0997 (0.0985)	0.0604 (0.0597)	0.710 (0.777)	81.25 (75.91)
[40/157]	0.1001 (0.0988)	0.0602 (0.0600)	0.730 (0.800)	78.12 (75.00)
[50/157]	0.0998 (0.0990)	0.0610 (0.0602)	0.721 (0.803)	78.12 (74.57)
[60/157]	0.1018 (0.0992)	0.0616 (0.0604)	0.954 (0.817)	68.75 (74.13)
[70/157]	0.1005 (0.0994)	0.0607 (0.0606)	0.898 (0.807)	71.88 (74.82)
[80/157]	0.1022 (0.0996)	0.0619 (0.0608)	0.776 (0.809)	68.75 (74.34)
[90/157]	0.1010 (0.0997)	0.0616 (0.0609)	1.000 (0.811)	65.62 (73.90)
[100/157]	0.0996 (0.0997)	0.0609 (0.0610)	0.726 (0.810)	75.00 (73.98)
[110/157]	0.1003 (0.0997)	0.0617 (0.0610)	1.212 (0.813)	56.25 (73.76)
[120/157]	0.1011 (0.0998)	0.0620 (0.0611)	1.000 (0.810)	59.38 (73.58)
[130/157]	0.1004 (0.0998)	0.0614 (0.0611)	0.829 (0.813)	78.12 (73.28)
[140/157]	0.1008 (0.0999)	0.0617 (0.0612)	0.706 (0.812)	78.12 (73.47)
[150/157]	0.1009 (0.0999)	0.0620 (0.0613)	0.707 (0.811)	71.88 (73.53)
[156/157]	0.0851 (0.0998)	0.0577 (0.0612)	1.030 (0.808)	87.50 (73.76)
 * Train Acc 73.760
 * Val Acc 73.400, Total time 0.60
 * Val loss 0.737, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0088 (0.0088)	0.846 (0.846)	65.62 (65.62)
[10/157]	0.1005 (0.0949)	0.0619 (0.0568)	0.672 (0.822)	81.25 (72.73)
[20/157]	0.0985 (0.0974)	0.0603 (0.0592)	0.661 (0.790)	84.38 (74.55)
[30/157]	0.0999 (0.0982)	0.0612 (0.0599)	0.798 (0.819)	81.25 (74.19)
[40/157]	0.1013 (0.0988)	0.0619 (0.0605)	0.874 (0.825)	65.62 (73.63)
[50/157]	0.1023 (0.0993)	0.0618 (0.0609)	0.716 (0.824)	78.12 (73.77)
[60/157]	0.1005 (0.0994)	0.0614 (0.0609)	0.725 (0.807)	68.75 (74.44)
[70/157]	0.1014 (0.0995)	0.0621 (0.0611)	0.753 (0.817)	71.88 (73.68)
[80/157]	0.0995 (0.0997)	0.0593 (0.0612)	0.731 (0.816)	71.88 (73.53)
[90/157]	0.1010 (0.0998)	0.0613 (0.0612)	0.880 (0.811)	65.62 (73.70)
[100/157]	0.0998 (0.0998)	0.0606 (0.0612)	1.057 (0.814)	59.38 (73.61)
[110/157]	0.0999 (0.0998)	0.0607 (0.0612)	0.713 (0.804)	78.12 (74.04)
[120/157]	0.1017 (0.0999)	0.0621 (0.0613)	0.849 (0.806)	65.62 (73.99)
[130/157]	0.1009 (0.1000)	0.0608 (0.0613)	0.880 (0.804)	71.88 (73.95)
[140/157]	0.0999 (0.1000)	0.0600 (0.0613)	0.804 (0.804)	65.62 (73.89)
[150/157]	0.1000 (0.1000)	0.0611 (0.0613)	0.790 (0.808)	62.50 (73.84)
[156/157]	0.0860 (0.1000)	0.0588 (0.0613)	0.954 (0.806)	62.50 (73.80)
 * Train Acc 73.800
 * Val Acc 73.300, Total time 0.61
 * Val loss 0.733, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0088 (0.0088)	0.940 (0.940)	71.88 (71.88)
[10/157]	0.1022 (0.0953)	0.0614 (0.0566)	0.736 (0.828)	75.00 (72.73)
[20/157]	0.1004 (0.0976)	0.0605 (0.0585)	0.866 (0.829)	68.75 (72.02)
[30/157]	0.1021 (0.0985)	0.0623 (0.0594)	0.957 (0.806)	78.12 (74.19)
[40/157]	0.1010 (0.0992)	0.0605 (0.0601)	0.628 (0.813)	81.25 (73.86)
[50/157]	0.0997 (0.0994)	0.0605 (0.0603)	1.080 (0.816)	65.62 (74.02)
[60/157]	0.1010 (0.0995)	0.0614 (0.0604)	0.655 (0.792)	75.00 (75.10)
[70/157]	0.1028 (0.0997)	0.0620 (0.0607)	0.841 (0.793)	71.88 (75.09)
[80/157]	0.0999 (0.0998)	0.0606 (0.0607)	0.850 (0.796)	71.88 (74.88)
[90/157]	0.1004 (0.0998)	0.0612 (0.0608)	0.605 (0.792)	78.12 (74.62)
[100/157]	0.1005 (0.0999)	0.0613 (0.0609)	0.692 (0.797)	84.38 (74.26)
[110/157]	0.1006 (0.0999)	0.0615 (0.0610)	0.891 (0.802)	68.75 (73.96)
[120/157]	0.1001 (0.1000)	0.0602 (0.0610)	0.893 (0.805)	68.75 (73.66)
[130/157]	0.1009 (0.1000)	0.0617 (0.0610)	0.587 (0.803)	84.38 (73.90)
[140/157]	0.1004 (0.1000)	0.0612 (0.0611)	0.571 (0.802)	84.38 (73.89)
[150/157]	0.1017 (0.1001)	0.0625 (0.0612)	0.800 (0.805)	78.12 (73.80)
[156/157]	0.0844 (0.1000)	0.0571 (0.0612)	0.616 (0.804)	75.00 (73.80)
 * Train Acc 73.800
 * Val Acc 73.300, Total time 0.60
 * Val loss 0.737, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0088 (0.0088)	0.767 (0.767)	75.00 (75.00)
[10/157]	0.1016 (0.0953)	0.0617 (0.0571)	0.828 (0.837)	84.38 (75.00)
[20/157]	0.1027 (0.0978)	0.0596 (0.0592)	0.760 (0.808)	75.00 (75.15)
[30/157]	0.0999 (0.0984)	0.0607 (0.0598)	0.680 (0.768)	78.12 (76.31)
[40/157]	0.1001 (0.0988)	0.0612 (0.0603)	1.270 (0.791)	62.50 (75.53)
[50/157]	0.1024 (0.0992)	0.0632 (0.0606)	0.789 (0.801)	71.88 (74.57)
[60/157]	0.0994 (0.0995)	0.0602 (0.0608)	0.900 (0.790)	68.75 (74.95)
[70/157]	0.0957 (0.0996)	0.0565 (0.0608)	0.621 (0.785)	78.12 (74.69)
[80/157]	0.1019 (0.0997)	0.0621 (0.0610)	0.701 (0.786)	78.12 (74.54)
[90/157]	0.1009 (0.0999)	0.0616 (0.0611)	0.650 (0.785)	84.38 (74.73)
[100/157]	0.1001 (0.0999)	0.0613 (0.0611)	0.806 (0.785)	75.00 (75.06)
[110/157]	0.1005 (0.1000)	0.0613 (0.0612)	0.766 (0.787)	68.75 (74.83)
[120/157]	0.1007 (0.1001)	0.0613 (0.0613)	0.771 (0.780)	65.62 (75.05)
[130/157]	0.1000 (0.1001)	0.0602 (0.0612)	0.752 (0.773)	78.12 (75.21)
[140/157]	0.1005 (0.1000)	0.0617 (0.0613)	0.700 (0.783)	84.38 (75.13)
[150/157]	0.0980 (0.1001)	0.0576 (0.0613)	0.607 (0.788)	78.12 (74.86)
[156/157]	0.0831 (0.0999)	0.0560 (0.0612)	1.117 (0.787)	75.00 (74.88)
 * Train Acc 74.880
 * Val Acc 73.900, Total time 0.60
 * Val loss 0.734, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0087 (0.0087)	0.835 (0.835)	71.88 (71.88)
[10/157]	0.1013 (0.0954)	0.0613 (0.0569)	0.826 (0.768)	75.00 (76.14)
[20/157]	0.1000 (0.0977)	0.0607 (0.0591)	0.638 (0.754)	75.00 (75.60)
[30/157]	0.0998 (0.0984)	0.0610 (0.0599)	0.910 (0.782)	59.38 (73.89)
[40/157]	0.0994 (0.0988)	0.0604 (0.0602)	0.752 (0.786)	78.12 (73.40)
[50/157]	0.1000 (0.0991)	0.0609 (0.0604)	0.707 (0.788)	78.12 (73.10)
[60/157]	0.1006 (0.0992)	0.0612 (0.0606)	0.993 (0.803)	68.75 (72.54)
[70/157]	0.1009 (0.0993)	0.0616 (0.0607)	0.546 (0.797)	87.50 (72.89)
[80/157]	0.1002 (0.0994)	0.0609 (0.0608)	1.229 (0.808)	59.38 (72.57)
[90/157]	0.1009 (0.0995)	0.0617 (0.0609)	0.678 (0.806)	78.12 (72.87)
[100/157]	0.1006 (0.0995)	0.0616 (0.0610)	0.853 (0.807)	71.88 (72.80)
[110/157]	0.1001 (0.0996)	0.0609 (0.0611)	0.956 (0.804)	68.75 (73.11)
[120/157]	0.1016 (0.0997)	0.0610 (0.0612)	0.658 (0.802)	78.12 (73.30)
[130/157]	0.1007 (0.0998)	0.0616 (0.0612)	0.635 (0.805)	84.38 (73.09)
[140/157]	0.1007 (0.0998)	0.0616 (0.0612)	0.804 (0.807)	68.75 (73.01)
[150/157]	0.1015 (0.0999)	0.0626 (0.0613)	0.832 (0.802)	78.12 (73.32)
[156/157]	0.0841 (0.0998)	0.0569 (0.0613)	0.561 (0.801)	87.50 (73.36)
 * Train Acc 73.360
 * Val Acc 74.000, Total time 0.58
 * Val loss 0.736, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0480 (0.0480)	0.0086 (0.0086)	0.679 (0.679)	78.12 (78.12)
[10/157]	0.0992 (0.0966)	0.0600 (0.0585)	0.680 (0.841)	75.00 (73.58)
[20/157]	0.0964 (0.0962)	0.0584 (0.0585)	0.526 (0.823)	78.12 (73.96)
[30/157]	0.1005 (0.0962)	0.0610 (0.0587)	0.667 (0.825)	71.88 (72.88)
[40/157]	0.1019 (0.0973)	0.0625 (0.0595)	0.663 (0.817)	68.75 (72.94)
[50/157]	0.1009 (0.0981)	0.0609 (0.0599)	0.690 (0.794)	75.00 (73.96)
[60/157]	0.0997 (0.0984)	0.0606 (0.0601)	0.843 (0.797)	78.12 (74.08)
[70/157]	0.1001 (0.0986)	0.0589 (0.0602)	0.840 (0.800)	78.12 (74.34)
[80/157]	0.1016 (0.0989)	0.0619 (0.0604)	0.744 (0.801)	75.00 (74.07)
[90/157]	0.1021 (0.0991)	0.0613 (0.0605)	0.910 (0.796)	68.75 (74.07)
[100/157]	0.1004 (0.0992)	0.0614 (0.0606)	1.003 (0.804)	68.75 (73.98)
[110/157]	0.1006 (0.0993)	0.0619 (0.0607)	0.598 (0.807)	87.50 (74.24)
[120/157]	0.1007 (0.0994)	0.0622 (0.0608)	0.902 (0.810)	68.75 (74.10)
[130/157]	0.1016 (0.0995)	0.0616 (0.0609)	0.864 (0.812)	75.00 (73.88)
[140/157]	0.1005 (0.0996)	0.0602 (0.0610)	0.797 (0.813)	71.88 (73.83)
[150/157]	0.1011 (0.0997)	0.0615 (0.0610)	0.765 (0.812)	71.88 (73.97)
[156/157]	0.0847 (0.0996)	0.0560 (0.0610)	0.685 (0.810)	87.50 (74.10)
 * Train Acc 74.100
 * Val Acc 73.800, Total time 0.60
 * Val loss 0.740, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0088 (0.0088)	0.831 (0.831)	75.00 (75.00)
[10/157]	0.0969 (0.0950)	0.0584 (0.0560)	0.860 (0.849)	71.88 (72.44)
[20/157]	0.1009 (0.0976)	0.0619 (0.0590)	0.945 (0.806)	62.50 (72.62)
[30/157]	0.1013 (0.0985)	0.0622 (0.0599)	0.736 (0.807)	75.00 (72.28)
[40/157]	0.0998 (0.0991)	0.0610 (0.0604)	0.652 (0.812)	75.00 (72.10)
[50/157]	0.1015 (0.0993)	0.0615 (0.0607)	0.824 (0.834)	81.25 (72.00)
[60/157]	0.1003 (0.0995)	0.0610 (0.0608)	0.698 (0.827)	71.88 (72.23)
[70/157]	0.0995 (0.0995)	0.0606 (0.0609)	0.983 (0.819)	59.38 (72.45)
[80/157]	0.0998 (0.0996)	0.0607 (0.0611)	0.966 (0.816)	71.88 (72.49)
[90/157]	0.1028 (0.0998)	0.0619 (0.0612)	0.953 (0.817)	75.00 (72.73)
[100/157]	0.1019 (0.0999)	0.0619 (0.0613)	0.840 (0.813)	62.50 (72.65)
[110/157]	0.1033 (0.1000)	0.0620 (0.0614)	0.672 (0.811)	81.25 (72.94)
[120/157]	0.0998 (0.1000)	0.0606 (0.0614)	0.728 (0.808)	78.12 (73.19)
[130/157]	0.1005 (0.1000)	0.0610 (0.0614)	0.696 (0.805)	84.38 (73.43)
[140/157]	0.0999 (0.1000)	0.0608 (0.0614)	0.808 (0.805)	68.75 (73.49)
[150/157]	0.0991 (0.1000)	0.0608 (0.0614)	0.998 (0.810)	68.75 (73.30)
[156/157]	0.0829 (0.0999)	0.0562 (0.0614)	0.600 (0.812)	75.00 (73.34)
 * Train Acc 73.340
 * Val Acc 73.600, Total time 0.60
 * Val loss 0.731, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0086 (0.0086)	0.724 (0.724)	78.12 (78.12)
[10/157]	0.1016 (0.0947)	0.0623 (0.0572)	0.845 (0.753)	75.00 (75.57)
[20/157]	0.1003 (0.0975)	0.0610 (0.0595)	0.880 (0.807)	68.75 (73.21)
[30/157]	0.0997 (0.0983)	0.0602 (0.0601)	0.640 (0.800)	84.38 (73.99)
[40/157]	0.1009 (0.0988)	0.0617 (0.0605)	0.837 (0.797)	68.75 (73.55)
[50/157]	0.0991 (0.0990)	0.0607 (0.0606)	0.736 (0.806)	75.00 (73.35)
[60/157]	0.1010 (0.0992)	0.0626 (0.0608)	0.786 (0.798)	78.12 (73.82)
[70/157]	0.1016 (0.0994)	0.0621 (0.0611)	1.076 (0.790)	62.50 (74.21)
[80/157]	0.0997 (0.0996)	0.0602 (0.0611)	0.873 (0.804)	75.00 (73.88)
[90/157]	0.1006 (0.0997)	0.0613 (0.0612)	0.667 (0.803)	84.38 (74.11)
[100/157]	0.1016 (0.0997)	0.0625 (0.0612)	1.118 (0.811)	56.25 (74.01)
[110/157]	0.0997 (0.0999)	0.0601 (0.0613)	0.856 (0.812)	65.62 (73.79)
[120/157]	0.0996 (0.0999)	0.0602 (0.0613)	0.896 (0.809)	68.75 (73.97)
[130/157]	0.0997 (0.0999)	0.0605 (0.0613)	0.845 (0.813)	78.12 (73.81)
[140/157]	0.0992 (0.0999)	0.0602 (0.0612)	0.582 (0.809)	84.38 (73.98)
[150/157]	0.1006 (0.0999)	0.0608 (0.0612)	0.646 (0.808)	81.25 (74.11)
[156/157]	0.0821 (0.0998)	0.0556 (0.0612)	0.844 (0.808)	50.00 (73.92)
 * Train Acc 73.920
 * Val Acc 73.400, Total time 0.61
 * Val loss 0.727, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 5/27; cond: 434294.40625, radio:3.5719429433811456e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0041,  0.1073, -0.1580],
          [-0.1380, -0.0746,  0.0500],
          [ 0.0051,  0.1572, -0.0149]],

         [[ 0.0549, -0.0551, -0.0376],
          [-0.1812, -0.1278, -0.0789],
          [ 0.0137,  0.0788,  0.1180]],

         [[-0.1300, -0.0833,  0.0677],
          [ 0.1596, -0.0409,  0.1446],
          [-0.0281,  0.0217,  0.1758]]],


        [[[-0.1679, -0.1050, -0.0260],
          [-0.0666,  0.1735, -0.1108],
          [-0.0792, -0.1350, -0.1715]],

         [[-0.1062,  0.1791,  0.1032],
          [ 0.0969,  0.0175, -0.0889],
          [ 0.0345, -0.1829, -0.1367]],

         [[-0.0979,  0.1279,  0.1212],
          [-0.0829, -0.0013,  0.1282],
          [ 0.1914,  0.0732,  0.0267]]],


        [[[ 0.1332, -0.1135,  0.0363],
          [-0.1476, -0.1350, -0.0988],
          [ 0.0870,  0.0754, -0.1128]],

         [[ 0.0592,  0.1010, -0.0279],
          [ 0.0059,  0.0401,  0.1155],
          [ 0.1801, -0.1533, -0.0733]],

         [[ 0.0775,  0.1566,  0.1640],
          [ 0.1684,  0.0345, -0.1718],
          [ 0.0118, -0.1273, -0.1839]]],


        ...,


        [[[ 0.0684, -0.0765, -0.1206],
          [ 0.0760,  0.0337, -0.1410],
          [-0.1322,  0.0945,  0.1444]],

         [[ 0.0182, -0.0974,  0.1002],
          [ 0.0360, -0.1679, -0.1243],
          [ 0.0343,  0.1229, -0.0628]],

         [[ 0.0806, -0.1307,  0.1823],
          [ 0.1277,  0.0267,  0.0758],
          [ 0.0451, -0.1180, -0.0130]]],


        [[[ 0.0626, -0.0405,  0.1923],
          [-0.0631, -0.1656, -0.0866],
          [-0.1704,  0.1684, -0.0840]],

         [[-0.1290, -0.0213, -0.1254],
          [-0.0925,  0.1033, -0.1426],
          [ 0.0977,  0.1513,  0.0006]],

         [[ 0.0848, -0.1124,  0.1497],
          [ 0.1870,  0.0453,  0.1722],
          [-0.1953, -0.0194,  0.0318]]],


        [[[-0.1668,  0.0944, -0.0725],
          [-0.0708, -0.1545,  0.0821],
          [-0.1406, -0.0457,  0.0852]],

         [[ 0.1451,  0.0467, -0.0710],
          [-0.0833,  0.1101,  0.0388],
          [ 0.1594,  0.0673,  0.1517]],

         [[-0.1559,  0.1529, -0.0940],
          [-0.0556, -0.0040, -0.0430],
          [ 0.0337,  0.1604, -0.1261]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.2213e+06, 2.9764e+05, 2.9382e+05, 1.2686e+05, 1.0915e+05, 9.1043e+04,
        1.7085e+04, 1.1523e+04, 3.6861e+03, 3.5724e+03, 3.4909e+03, 3.2981e+03,
        1.2588e+03, 7.3341e+02, 7.1349e+02, 6.1338e+02, 5.6137e+02, 3.6034e+02,
        1.4666e+02, 1.3302e+02, 1.1457e+02, 7.7523e+01, 7.0506e+01, 2.7138e+01,
        2.4962e+01, 1.9545e+01, 7.4172e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 5]) 

NULL SPACE BASIS :  tensor([[-1.9989e-01,  1.3604e-01,  1.2771e-01, -1.2200e-01,  8.6830e-02],
        [-3.7876e-03, -2.3861e-01,  8.4930e-03,  2.2743e-01, -1.5722e-01],
        [ 2.0530e-01,  1.2687e-01, -1.3709e-01, -1.2156e-01,  8.7183e-02],
        [ 3.8329e-01, -4.3477e-03, -2.3840e-01,  2.2627e-01, -1.5489e-01],
        [-9.1278e-04, -8.2942e-03, -1.0943e-03, -4.1825e-01,  2.7879e-01],
        [-3.8427e-01,  1.2410e-02,  2.3939e-01,  2.2499e-01, -1.5571e-01],
        [-2.1097e-01, -1.3059e-01,  1.3570e-01, -1.2377e-01,  8.6261e-02],
        [ 5.2546e-03,  2.4651e-01, -7.5497e-03,  2.2879e-01, -1.5658e-01],
        [ 2.0595e-01, -1.4001e-01, -1.2717e-01, -1.2299e-01,  8.6640e-02],
        [ 9.7614e-03, -2.5304e-01, -2.3626e-01,  4.8153e-03, -1.5315e-01],
        [-1.5734e-03,  4.4066e-01, -1.5988e-02, -7.8655e-03,  2.7663e-01],
        [-1.0564e-02, -2.3422e-01,  2.5401e-01,  4.8533e-03, -1.5285e-01],
        [-1.6820e-02,  1.5844e-02,  4.4116e-01, -2.6509e-03,  2.7203e-01],
        [ 1.2157e-03,  2.8484e-03,  1.1956e-04, -5.4964e-05, -4.8876e-01],
        [ 1.8944e-02, -1.7083e-02, -4.4141e-01, -1.5650e-04,  2.7213e-01],
        [ 6.1576e-03,  2.3518e-01, -2.5325e-01, -1.7579e-03, -1.5097e-01],
        [-4.0724e-04, -4.4321e-01,  1.6894e-02,  6.5252e-03,  2.7365e-01],
        [-6.6762e-03,  2.5295e-01,  2.3465e-01, -4.1437e-03, -1.5110e-01],
        [ 2.1453e-01,  1.3886e-01,  1.2865e-01,  1.3300e-01,  7.8621e-02],
        [ 6.2175e-03, -2.4010e-01,  8.9836e-03, -2.4877e-01, -1.4169e-01],
        [-2.1989e-01,  1.2779e-01, -1.3869e-01,  1.3266e-01,  7.7971e-02],
        [-4.1134e-01, -1.3438e-02, -2.4006e-01, -2.5242e-01, -1.3886e-01],
        [-7.3150e-04,  6.4911e-03,  8.4006e-04,  4.7139e-01,  2.4931e-01],
        [ 4.1047e-01,  5.0605e-03,  2.3956e-01, -2.5415e-01, -1.3831e-01],
        [ 2.3018e-01, -1.2428e-01,  1.3883e-01,  1.4166e-01,  7.6636e-02],
        [-5.1585e-03,  2.3367e-01, -1.0681e-02, -2.6552e-01, -1.3888e-01],
        [-2.2429e-01, -1.3380e-01, -1.2736e-01,  1.4368e-01,  7.6545e-02]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0435, -0.0322, -0.0084, -0.0665,  0.0332,  0.0305,  0.0288, -0.0045,
         -0.0241, -0.0360,  0.0372, -0.0050,  0.0384, -0.0189, -0.0174, -0.0065,
         -0.0157,  0.0237, -0.0076, -0.0066,  0.0153,  0.0306, -0.0155, -0.0142,
         -0.0250,  0.0232, -0.0002],
        [-0.0322,  0.0597, -0.0329,  0.0328, -0.0613,  0.0341, -0.0038,  0.0079,
         -0.0045,  0.0373, -0.0673,  0.0372, -0.0194,  0.0341, -0.0190, -0.0156,
          0.0288, -0.0159, -0.0067,  0.0103, -0.0058, -0.0147,  0.0297, -0.0164,
          0.0224, -0.0422,  0.0234],
        [-0.0084, -0.0329,  0.0445,  0.0312,  0.0331, -0.0676, -0.0250, -0.0036,
          0.0288, -0.0052,  0.0371, -0.0361, -0.0169, -0.0188,  0.0385,  0.0236,
         -0.0159, -0.0063,  0.0155, -0.0056, -0.0086, -0.0155, -0.0157,  0.0317,
          0.0009,  0.0225, -0.0252],
        [-0.0665,  0.0328,  0.0312,  0.1248, -0.0617, -0.0579, -0.0689,  0.0352,
          0.0307,  0.0385, -0.0194, -0.0174, -0.0691,  0.0340,  0.0315,  0.0379,
         -0.0193, -0.0166,  0.0308, -0.0148, -0.0151, -0.0608,  0.0302,  0.0287,
          0.0339, -0.0174, -0.0154],
        [ 0.0332, -0.0613,  0.0331, -0.0617,  0.1130, -0.0615,  0.0344, -0.0632,
          0.0343, -0.0189,  0.0343, -0.0192,  0.0341, -0.0609,  0.0342, -0.0192,
          0.0345, -0.0191, -0.0157,  0.0297, -0.0154,  0.0302, -0.0571,  0.0300,
         -0.0166,  0.0315, -0.0167],
        [ 0.0305,  0.0341, -0.0676, -0.0579, -0.0615,  0.1252,  0.0316,  0.0336,
         -0.0682, -0.0172, -0.0190,  0.0388,  0.0310,  0.0338, -0.0696, -0.0165,
         -0.0190,  0.0378, -0.0144, -0.0166,  0.0316,  0.0292,  0.0303, -0.0608,
         -0.0165, -0.0160,  0.0333],
        [ 0.0288, -0.0038, -0.0250, -0.0689,  0.0344,  0.0316,  0.0459, -0.0341,
         -0.0088, -0.0067, -0.0154,  0.0239,  0.0381, -0.0191, -0.0171, -0.0354,
          0.0371, -0.0055, -0.0249,  0.0223,  0.0005,  0.0336, -0.0167, -0.0158,
         -0.0109, -0.0045,  0.0162],
        [-0.0045,  0.0079, -0.0036,  0.0352, -0.0632,  0.0336, -0.0341,  0.0616,
         -0.0332, -0.0159,  0.0285, -0.0155, -0.0191,  0.0345, -0.0194,  0.0372,
         -0.0674,  0.0372,  0.0235, -0.0420,  0.0222, -0.0177,  0.0315, -0.0156,
         -0.0045,  0.0083, -0.0055],
        [-0.0241, -0.0045,  0.0288,  0.0307,  0.0343, -0.0682, -0.0088, -0.0332,
          0.0451,  0.0240, -0.0157, -0.0069, -0.0169, -0.0190,  0.0385, -0.0055,
          0.0370, -0.0354, -0.0005,  0.0233, -0.0246, -0.0149, -0.0168,  0.0325,
          0.0163, -0.0053, -0.0100],
        [-0.0360,  0.0373, -0.0052,  0.0385, -0.0189, -0.0172, -0.0067, -0.0159,
          0.0240,  0.0641, -0.0671,  0.0101, -0.0671,  0.0331,  0.0300,  0.0105,
          0.0296, -0.0431, -0.0335,  0.0354, -0.0058,  0.0341, -0.0169, -0.0152,
         -0.0045, -0.0164,  0.0227],
        [ 0.0372, -0.0673,  0.0371, -0.0194,  0.0343, -0.0190, -0.0154,  0.0285,
         -0.0157, -0.0671,  0.1212, -0.0669,  0.0336, -0.0599,  0.0334,  0.0295,
         -0.0536,  0.0295,  0.0356, -0.0640,  0.0355, -0.0169,  0.0305, -0.0172,
         -0.0167,  0.0299, -0.0163],
        [-0.0050,  0.0372, -0.0361, -0.0174, -0.0192,  0.0388,  0.0239, -0.0155,
         -0.0069,  0.0101, -0.0669,  0.0639,  0.0299,  0.0331, -0.0670, -0.0431,
          0.0297,  0.0105, -0.0060,  0.0353, -0.0331, -0.0150, -0.0166,  0.0336,
          0.0228, -0.0167, -0.0043],
        [ 0.0384, -0.0194, -0.0169, -0.0691,  0.0341,  0.0310,  0.0381, -0.0191,
         -0.0169, -0.0671,  0.0336,  0.0299,  0.1204, -0.0594, -0.0542, -0.0667,
          0.0335,  0.0298,  0.0342, -0.0169, -0.0155, -0.0610,  0.0300,  0.0277,
          0.0339, -0.0170, -0.0152],
        [-0.0189,  0.0341, -0.0188,  0.0340, -0.0609,  0.0338, -0.0191,  0.0345,
         -0.0190,  0.0331, -0.0599,  0.0331, -0.0594,  0.1068, -0.0595,  0.0333,
         -0.0604,  0.0334, -0.0169,  0.0307, -0.0170,  0.0301, -0.0545,  0.0305,
         -0.0168,  0.0307, -0.0170],
        [-0.0174, -0.0190,  0.0385,  0.0315,  0.0342, -0.0696, -0.0171, -0.0194,
          0.0385,  0.0300,  0.0334, -0.0670, -0.0542, -0.0595,  0.1205,  0.0299,
          0.0333, -0.0667, -0.0151, -0.0171,  0.0340,  0.0271,  0.0301, -0.0607,
         -0.0152, -0.0166,  0.0336],
        [-0.0065, -0.0156,  0.0236,  0.0379, -0.0192, -0.0165, -0.0354,  0.0372,
         -0.0055,  0.0105,  0.0295, -0.0431, -0.0667,  0.0333,  0.0299,  0.0636,
         -0.0670,  0.0102, -0.0048, -0.0165,  0.0232,  0.0342, -0.0166, -0.0159,
         -0.0334,  0.0354, -0.0055],
        [-0.0157,  0.0288, -0.0159, -0.0193,  0.0345, -0.0190,  0.0371, -0.0674,
          0.0370,  0.0296, -0.0536,  0.0297,  0.0335, -0.0604,  0.0333, -0.0670,
          0.1215, -0.0669, -0.0166,  0.0296, -0.0164, -0.0168,  0.0306, -0.0169,
          0.0354, -0.0642,  0.0354],
        [ 0.0237, -0.0159, -0.0063, -0.0166, -0.0191,  0.0378, -0.0055,  0.0372,
         -0.0354, -0.0431,  0.0295,  0.0105,  0.0298,  0.0334, -0.0667,  0.0102,
         -0.0669,  0.0635,  0.0230, -0.0162, -0.0050, -0.0156, -0.0169,  0.0343,
         -0.0056,  0.0352, -0.0333],
        [-0.0076, -0.0067,  0.0155,  0.0308, -0.0157, -0.0144, -0.0249,  0.0235,
         -0.0005, -0.0335,  0.0356, -0.0060,  0.0342, -0.0169, -0.0151, -0.0048,
         -0.0166,  0.0230,  0.0473, -0.0336, -0.0105, -0.0740,  0.0372,  0.0335,
          0.0335, -0.0073, -0.0259],
        [-0.0066,  0.0103, -0.0056, -0.0148,  0.0297, -0.0166,  0.0223, -0.0420,
          0.0233,  0.0354, -0.0640,  0.0353, -0.0169,  0.0307, -0.0171, -0.0165,
          0.0296, -0.0162, -0.0336,  0.0625, -0.0346,  0.0362, -0.0689,  0.0386,
         -0.0061,  0.0132, -0.0076],
        [ 0.0153, -0.0058, -0.0086, -0.0151, -0.0154,  0.0316,  0.0005,  0.0222,
         -0.0246, -0.0058,  0.0355, -0.0331, -0.0155, -0.0170,  0.0340,  0.0232,
         -0.0164, -0.0050, -0.0105, -0.0346,  0.0481,  0.0348,  0.0371, -0.0748,
         -0.0273, -0.0061,  0.0335],
        [ 0.0306, -0.0147, -0.0155, -0.0608,  0.0302,  0.0292,  0.0336, -0.0177,
         -0.0149,  0.0341, -0.0169, -0.0150, -0.0610,  0.0301,  0.0271,  0.0342,
         -0.0168, -0.0156, -0.0740,  0.0362,  0.0348,  0.1386, -0.0687, -0.0640,
         -0.0773,  0.0393,  0.0348],
        [-0.0155,  0.0297, -0.0157,  0.0302, -0.0571,  0.0303, -0.0167,  0.0315,
         -0.0168, -0.0169,  0.0305, -0.0166,  0.0300, -0.0545,  0.0301, -0.0166,
          0.0306, -0.0169,  0.0372, -0.0689,  0.0371, -0.0687,  0.1272, -0.0690,
          0.0380, -0.0708,  0.0385],
        [-0.0142, -0.0164,  0.0317,  0.0287,  0.0300, -0.0608, -0.0158, -0.0156,
          0.0325, -0.0152, -0.0172,  0.0336,  0.0277,  0.0305, -0.0607, -0.0159,
         -0.0169,  0.0343,  0.0335,  0.0386, -0.0748, -0.0640, -0.0690,  0.1385,
          0.0360,  0.0372, -0.0762],
        [-0.0250,  0.0224,  0.0009,  0.0339, -0.0166, -0.0165, -0.0109, -0.0045,
          0.0163, -0.0045, -0.0167,  0.0228,  0.0339, -0.0168, -0.0152, -0.0334,
          0.0354, -0.0056,  0.0335, -0.0061, -0.0273, -0.0773,  0.0380,  0.0360,
          0.0508, -0.0358, -0.0118],
        [ 0.0232, -0.0422,  0.0225, -0.0174,  0.0315, -0.0160, -0.0045,  0.0083,
         -0.0053, -0.0164,  0.0299, -0.0167, -0.0170,  0.0307, -0.0166,  0.0354,
         -0.0642,  0.0352, -0.0073,  0.0132, -0.0061,  0.0393, -0.0708,  0.0372,
         -0.0358,  0.0646, -0.0347],
        [-0.0002,  0.0234, -0.0252, -0.0154, -0.0167,  0.0333,  0.0162, -0.0055,
         -0.0100,  0.0227, -0.0163, -0.0043, -0.0152, -0.0170,  0.0336, -0.0055,
          0.0354, -0.0333, -0.0259, -0.0076,  0.0335,  0.0348,  0.0385, -0.0762,
         -0.0118, -0.0347,  0.0496]], device='cuda:0') 

reserving basis 78/576; cond: 9522207.0, radio:3.7706860894104466e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1470e-02, -3.9751e-02, -7.3393e-03],
          [ 3.8356e-02,  2.6538e-02,  3.1704e-02],
          [ 2.2170e-02, -1.7911e-03,  8.7430e-03]],

         [[-1.4034e-02,  3.7932e-02,  1.8368e-02],
          [-2.9987e-02,  3.6750e-02, -3.0590e-03],
          [-6.9889e-03, -1.7013e-02, -2.0458e-02]],

         [[-3.1128e-03, -2.0744e-02, -3.6706e-02],
          [-2.9166e-02, -1.0369e-02,  3.3255e-02],
          [ 2.0488e-02, -3.1916e-02, -9.4249e-03]],

         ...,

         [[-2.5514e-02,  1.1649e-02,  2.1047e-06],
          [ 1.4927e-02, -6.5021e-03, -9.4610e-03],
          [-2.0662e-03,  8.6866e-03, -3.1102e-02]],

         [[-4.1521e-02,  1.3756e-02, -3.4946e-02],
          [-6.7285e-03,  1.2064e-02,  1.1310e-03],
          [ 2.0813e-02,  1.0999e-04,  4.0997e-03]],

         [[-1.5963e-02,  1.9522e-03,  1.1394e-02],
          [ 2.3136e-02, -3.9655e-02, -1.6125e-02],
          [-4.1584e-02, -1.1385e-02,  2.9618e-02]]],


        [[[-4.2301e-02, -4.7068e-02,  1.4010e-02],
          [-2.3697e-02,  1.9920e-02,  2.9054e-03],
          [ 8.1353e-03,  2.3303e-02, -4.2275e-02]],

         [[ 3.2902e-02,  7.9223e-03, -3.0202e-02],
          [ 1.1907e-02, -1.1357e-02, -1.7423e-02],
          [ 3.3719e-03, -1.1668e-02,  4.0720e-02]],

         [[ 2.6473e-02, -1.6381e-02, -4.4828e-02],
          [-1.7188e-02, -1.0975e-02,  9.9446e-03],
          [-1.2342e-02, -2.4138e-03, -6.2627e-03]],

         ...,

         [[-5.4665e-03, -3.8236e-03,  8.7100e-03],
          [ 1.5790e-02, -3.4068e-02,  1.7220e-02],
          [ 3.3110e-02,  7.4831e-03,  3.0862e-02]],

         [[-2.1730e-02,  6.7375e-03, -4.5015e-04],
          [-1.1101e-02,  2.3821e-02, -4.6487e-02],
          [-1.0650e-03, -1.4422e-02, -2.0079e-02]],

         [[-1.4756e-02, -3.2087e-02,  4.2838e-03],
          [-6.8548e-03, -3.1439e-02, -8.1831e-03],
          [-8.7285e-03,  1.5912e-02, -4.1621e-02]]],


        [[[-3.0515e-03,  1.8503e-02,  2.0616e-02],
          [ 7.9597e-03,  1.3000e-02,  4.1717e-02],
          [-1.3424e-02,  2.5615e-02,  3.2397e-02]],

         [[-3.1186e-02,  2.0045e-02,  2.8961e-02],
          [-2.5198e-03,  3.2264e-02,  6.3236e-03],
          [ 3.5429e-02,  6.7433e-03, -2.1020e-02]],

         [[ 3.8704e-02,  1.5378e-02,  3.4313e-02],
          [ 1.8282e-02,  2.5582e-02,  2.0298e-02],
          [ 1.6789e-02, -5.9237e-03,  9.3766e-03]],

         ...,

         [[ 2.1064e-02, -1.6893e-02,  4.4114e-02],
          [-3.1996e-03,  3.8404e-02, -3.4332e-03],
          [ 2.4408e-02,  3.3264e-02, -3.6221e-02]],

         [[-2.0657e-02, -3.6394e-02,  3.6799e-02],
          [ 4.5218e-02, -1.3460e-02, -1.8913e-02],
          [ 1.2008e-02,  3.0950e-02,  1.3903e-02]],

         [[-1.4898e-02, -2.3206e-02,  3.0072e-02],
          [-2.9711e-02, -4.1816e-02, -2.1339e-02],
          [ 3.6273e-03,  5.9712e-03,  6.1281e-03]]],


        ...,


        [[[-4.4253e-02, -1.8281e-02,  9.4777e-03],
          [ 2.4323e-02, -1.7187e-02, -8.6761e-03],
          [-4.2825e-03,  2.1071e-02, -3.4668e-02]],

         [[-4.1681e-02,  2.7621e-02,  5.8209e-03],
          [-1.3612e-02, -1.1778e-02,  8.7147e-03],
          [-1.9309e-02,  2.8354e-02,  1.2275e-02]],

         [[ 2.3958e-02,  3.2063e-02,  9.9502e-04],
          [ 4.7835e-02, -7.8176e-03, -3.3999e-02],
          [ 3.8590e-02, -1.4927e-02, -2.9695e-03]],

         ...,

         [[-2.5753e-02, -5.3307e-02,  2.3094e-02],
          [-2.0502e-02,  2.2718e-02, -4.7231e-02],
          [ 1.6410e-02,  4.0667e-02,  2.7438e-02]],

         [[-4.6411e-02, -5.5611e-02,  1.6240e-02],
          [-1.3072e-02, -1.4838e-02, -6.4537e-03],
          [-2.0929e-02,  9.7926e-03, -9.3250e-03]],

         [[ 3.6325e-02, -2.2441e-02,  4.1213e-02],
          [ 9.5250e-03, -5.7607e-03,  4.7851e-02],
          [ 3.1223e-02,  2.7964e-02,  4.8078e-02]]],


        [[[ 2.5378e-02,  1.3967e-02, -1.3452e-02],
          [-1.3484e-02, -3.1927e-02, -2.0802e-02],
          [ 1.5863e-02, -5.8616e-02, -4.1958e-02]],

         [[-8.1855e-03,  2.0879e-02,  8.9126e-03],
          [ 9.8050e-03,  1.0815e-03, -4.0535e-02],
          [-9.3705e-03,  3.7749e-02,  1.2304e-02]],

         [[-9.4087e-03,  8.4760e-03,  4.0391e-02],
          [ 3.5074e-04,  5.5188e-03,  3.0383e-02],
          [-3.1427e-02,  2.5749e-02,  3.2505e-02]],

         ...,

         [[ 4.0161e-02,  1.9596e-02,  3.8772e-02],
          [-3.5786e-02,  1.0027e-02,  7.8335e-04],
          [-1.1879e-03, -4.1748e-02, -3.9173e-02]],

         [[ 1.6741e-02,  2.3275e-02, -9.1442e-03],
          [ 2.6072e-02,  8.0740e-03,  1.8171e-02],
          [-4.8339e-02, -2.4953e-02, -5.2720e-03]],

         [[ 2.7860e-02,  2.5250e-02, -3.1905e-02],
          [-3.3074e-02, -2.8953e-02, -1.1961e-02],
          [-8.0929e-03, -3.9822e-02,  2.2245e-02]]],


        [[[ 2.7623e-02,  1.0085e-02, -1.6757e-02],
          [-6.0883e-03,  6.5481e-03, -1.2631e-02],
          [-1.7728e-03, -3.5107e-02, -7.7215e-03]],

         [[-4.3880e-02, -2.8683e-02, -2.9439e-02],
          [ 1.0807e-02, -5.6322e-02, -1.2872e-03],
          [ 2.6674e-02, -3.5896e-02, -6.8629e-03]],

         [[-1.7023e-02,  1.3549e-02,  2.2261e-02],
          [ 1.5363e-02, -1.3423e-02,  1.0305e-02],
          [-2.2117e-02,  3.6024e-02,  3.7931e-03]],

         ...,

         [[-2.1281e-02, -9.0961e-03,  9.9997e-03],
          [ 3.9385e-03,  3.7287e-02, -4.2783e-02],
          [-2.2904e-02,  1.5711e-02,  2.4672e-02]],

         [[-3.1370e-02, -3.8699e-02,  5.5225e-03],
          [-1.9639e-02, -1.0659e-02, -4.8758e-02],
          [-2.8329e-02, -5.1243e-02, -3.9121e-02]],

         [[-4.0341e-03, -1.0382e-02,  3.1120e-02],
          [ 3.3923e-02,  4.5829e-03,  3.5783e-02],
          [-2.8806e-02,  4.5798e-02, -3.2631e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([6.8095e+07, 2.9816e+06, 2.3944e+06, 2.2524e+06, 1.7104e+06, 1.2301e+06,
        1.1550e+06, 1.0765e+06, 9.1772e+05, 7.4737e+05, 6.2681e+05, 5.5900e+05,
        4.7203e+05, 3.4564e+05, 2.1739e+05, 1.8457e+05, 1.7742e+05, 1.5688e+05,
        1.3249e+05, 1.2216e+05, 1.0843e+05, 9.7858e+04, 8.0996e+04, 7.1496e+04,
        6.7719e+04, 6.5692e+04, 6.0904e+04, 5.8272e+04, 5.5810e+04, 5.2680e+04,
        5.1841e+04, 4.3567e+04, 4.0862e+04, 3.8543e+04, 3.6605e+04, 3.4132e+04,
        3.1435e+04, 2.9414e+04, 2.7713e+04, 2.5385e+04, 2.3920e+04, 2.3062e+04,
        2.2451e+04, 2.1357e+04, 2.0626e+04, 1.9474e+04, 1.7697e+04, 1.7537e+04,
        1.6330e+04, 1.6074e+04, 1.5489e+04, 1.4996e+04, 1.4242e+04, 1.4146e+04,
        1.3525e+04, 1.2638e+04, 1.2098e+04, 1.1785e+04, 1.1405e+04, 1.1260e+04,
        1.0753e+04, 1.0412e+04, 1.0198e+04, 9.9218e+03, 9.7124e+03, 9.5818e+03,
        9.3116e+03, 9.1783e+03, 8.9786e+03, 8.6194e+03, 8.3355e+03, 8.0447e+03,
        7.8603e+03, 7.7745e+03, 7.6235e+03, 7.3231e+03, 7.1450e+03, 6.8330e+03,
        6.7787e+03, 6.4806e+03, 6.3390e+03, 6.1714e+03, 6.0980e+03, 6.0835e+03,
        5.7844e+03, 5.6736e+03, 5.4935e+03, 5.3685e+03, 5.2159e+03, 5.1729e+03,
        5.1438e+03, 4.8902e+03, 4.7260e+03, 4.6726e+03, 4.5559e+03, 4.4015e+03,
        4.2924e+03, 4.1542e+03, 4.0717e+03, 3.9479e+03, 3.9229e+03, 3.7694e+03,
        3.7320e+03, 3.6065e+03, 3.5655e+03, 3.4616e+03, 3.4404e+03, 3.3701e+03,
        3.2954e+03, 3.2630e+03, 3.1761e+03, 3.1053e+03, 3.0225e+03, 2.9704e+03,
        2.9409e+03, 2.8928e+03, 2.8722e+03, 2.7718e+03, 2.7259e+03, 2.7123e+03,
        2.6471e+03, 2.6192e+03, 2.5828e+03, 2.4817e+03, 2.4285e+03, 2.4107e+03,
        2.3691e+03, 2.2842e+03, 2.2663e+03, 2.2402e+03, 2.2269e+03, 2.1733e+03,
        2.1495e+03, 2.1412e+03, 2.1169e+03, 2.0630e+03, 2.0361e+03, 2.0044e+03,
        1.9672e+03, 1.9227e+03, 1.8943e+03, 1.8705e+03, 1.8550e+03, 1.8302e+03,
        1.8032e+03, 1.7937e+03, 1.7493e+03, 1.7213e+03, 1.6997e+03, 1.6881e+03,
        1.6490e+03, 1.6263e+03, 1.6186e+03, 1.5929e+03, 1.5753e+03, 1.5297e+03,
        1.5188e+03, 1.4957e+03, 1.4899e+03, 1.4559e+03, 1.4501e+03, 1.4249e+03,
        1.4112e+03, 1.4001e+03, 1.3798e+03, 1.3273e+03, 1.3214e+03, 1.3090e+03,
        1.2900e+03, 1.2798e+03, 1.2571e+03, 1.2402e+03, 1.2212e+03, 1.1993e+03,
        1.1809e+03, 1.1716e+03, 1.1550e+03, 1.1401e+03, 1.1303e+03, 1.1167e+03,
        1.1098e+03, 1.1032e+03, 1.0732e+03, 1.0650e+03, 1.0379e+03, 1.0261e+03,
        1.0132e+03, 9.9951e+02, 9.9077e+02, 9.8661e+02, 9.6783e+02, 9.6291e+02,
        9.4694e+02, 9.4344e+02, 9.2560e+02, 9.2356e+02, 9.1912e+02, 9.0019e+02,
        8.9399e+02, 8.8622e+02, 8.8012e+02, 8.6311e+02, 8.5484e+02, 8.4311e+02,
        8.3115e+02, 8.2498e+02, 8.1567e+02, 8.0859e+02, 7.9574e+02, 7.8724e+02,
        7.8444e+02, 7.7833e+02, 7.6843e+02, 7.6123e+02, 7.5123e+02, 7.4189e+02,
        7.2710e+02, 7.2088e+02, 7.1466e+02, 7.0740e+02, 6.9848e+02, 6.9031e+02,
        6.8700e+02, 6.8253e+02, 6.7517e+02, 6.6614e+02, 6.6174e+02, 6.5693e+02,
        6.4350e+02, 6.3062e+02, 6.2612e+02, 6.2217e+02, 6.1919e+02, 6.0954e+02,
        6.0820e+02, 5.9911e+02, 5.9208e+02, 5.8910e+02, 5.8129e+02, 5.8107e+02,
        5.6635e+02, 5.6333e+02, 5.5918e+02, 5.5393e+02, 5.4970e+02, 5.4227e+02,
        5.3821e+02, 5.3445e+02, 5.2561e+02, 5.2485e+02, 5.2008e+02, 5.1113e+02,
        5.1032e+02, 5.0549e+02, 5.0372e+02, 4.9806e+02, 4.8782e+02, 4.8631e+02,
        4.8344e+02, 4.7741e+02, 4.7493e+02, 4.6918e+02, 4.6539e+02, 4.6088e+02,
        4.5551e+02, 4.4966e+02, 4.4587e+02, 4.4049e+02, 4.3802e+02, 4.3365e+02,
        4.2976e+02, 4.2832e+02, 4.2645e+02, 4.2046e+02, 4.1950e+02, 4.1215e+02,
        4.0963e+02, 4.0633e+02, 4.0144e+02, 3.9937e+02, 3.9484e+02, 3.9055e+02,
        3.8534e+02, 3.8303e+02, 3.7915e+02, 3.7778e+02, 3.7632e+02, 3.7225e+02,
        3.7067e+02, 3.6364e+02, 3.5979e+02, 3.5757e+02, 3.5413e+02, 3.5242e+02,
        3.5047e+02, 3.4715e+02, 3.4555e+02, 3.4072e+02, 3.3894e+02, 3.3596e+02,
        3.3035e+02, 3.2808e+02, 3.2561e+02, 3.2330e+02, 3.2192e+02, 3.1736e+02,
        3.1622e+02, 3.1193e+02, 3.1153e+02, 3.0800e+02, 3.0642e+02, 3.0426e+02,
        3.0145e+02, 2.9824e+02, 2.9722e+02, 2.9370e+02, 2.9221e+02, 2.9025e+02,
        2.8662e+02, 2.8498e+02, 2.8324e+02, 2.8097e+02, 2.7582e+02, 2.7508e+02,
        2.7169e+02, 2.7115e+02, 2.6870e+02, 2.6781e+02, 2.6513e+02, 2.6278e+02,
        2.6084e+02, 2.5859e+02, 2.5561e+02, 2.5461e+02, 2.5338e+02, 2.5094e+02,
        2.4927e+02, 2.4667e+02, 2.4480e+02, 2.4133e+02, 2.3947e+02, 2.3750e+02,
        2.3497e+02, 2.3353e+02, 2.3170e+02, 2.3126e+02, 2.2913e+02, 2.2699e+02,
        2.2466e+02, 2.2344e+02, 2.2205e+02, 2.2184e+02, 2.1983e+02, 2.1922e+02,
        2.1720e+02, 2.1551e+02, 2.1405e+02, 2.1157e+02, 2.1007e+02, 2.0872e+02,
        2.0826e+02, 2.0767e+02, 2.0449e+02, 2.0249e+02, 2.0135e+02, 1.9903e+02,
        1.9753e+02, 1.9727e+02, 1.9493e+02, 1.9433e+02, 1.9319e+02, 1.9172e+02,
        1.9060e+02, 1.8712e+02, 1.8611e+02, 1.8495e+02, 1.8287e+02, 1.8077e+02,
        1.7987e+02, 1.7945e+02, 1.7894e+02, 1.7743e+02, 1.7520e+02, 1.7504e+02,
        1.7408e+02, 1.7310e+02, 1.7270e+02, 1.7142e+02, 1.7076e+02, 1.6763e+02,
        1.6719e+02, 1.6637e+02, 1.6446e+02, 1.6292e+02, 1.6150e+02, 1.6034e+02,
        1.5975e+02, 1.5833e+02, 1.5761e+02, 1.5685e+02, 1.5506e+02, 1.5448e+02,
        1.5283e+02, 1.5169e+02, 1.5088e+02, 1.4993e+02, 1.4871e+02, 1.4731e+02,
        1.4643e+02, 1.4615e+02, 1.4478e+02, 1.4408e+02, 1.4360e+02, 1.4163e+02,
        1.4099e+02, 1.4000e+02, 1.3853e+02, 1.3826e+02, 1.3655e+02, 1.3603e+02,
        1.3521e+02, 1.3422e+02, 1.3293e+02, 1.3196e+02, 1.3170e+02, 1.3074e+02,
        1.2950e+02, 1.2748e+02, 1.2659e+02, 1.2621e+02, 1.2560e+02, 1.2417e+02,
        1.2317e+02, 1.2257e+02, 1.2129e+02, 1.2058e+02, 1.1834e+02, 1.1827e+02,
        1.1719e+02, 1.1614e+02, 1.1584e+02, 1.1517e+02, 1.1454e+02, 1.1383e+02,
        1.1284e+02, 1.1175e+02, 1.1045e+02, 1.1019e+02, 1.0897e+02, 1.0840e+02,
        1.0698e+02, 1.0578e+02, 1.0523e+02, 1.0492e+02, 1.0354e+02, 1.0313e+02,
        1.0146e+02, 1.0034e+02, 1.0006e+02, 9.9526e+01, 9.9021e+01, 9.8123e+01,
        9.7499e+01, 9.6513e+01, 9.5712e+01, 9.5623e+01, 9.4945e+01, 9.3042e+01,
        9.2534e+01, 9.1429e+01, 9.1349e+01, 9.0110e+01, 8.9577e+01, 8.8970e+01,
        8.7839e+01, 8.6854e+01, 8.6799e+01, 8.5106e+01, 8.4840e+01, 8.3779e+01,
        8.3055e+01, 8.1695e+01, 8.1392e+01, 8.0887e+01, 7.9793e+01, 7.9385e+01,
        7.8752e+01, 7.8553e+01, 7.7551e+01, 7.6684e+01, 7.6040e+01, 7.5603e+01,
        7.4005e+01, 7.3451e+01, 7.2755e+01, 7.2466e+01, 7.2089e+01, 7.1538e+01,
        7.0583e+01, 6.9947e+01, 6.9551e+01, 6.8446e+01, 6.7777e+01, 6.7175e+01,
        6.6928e+01, 6.6147e+01, 6.5292e+01, 6.4798e+01, 6.4366e+01, 6.3434e+01,
        6.3148e+01, 6.2431e+01, 6.1876e+01, 6.1002e+01, 5.9937e+01, 5.9093e+01,
        5.8995e+01, 5.8082e+01, 5.7354e+01, 5.6432e+01, 5.5655e+01, 5.5086e+01,
        5.4705e+01, 5.3818e+01, 5.3358e+01, 5.2345e+01, 5.1994e+01, 5.1383e+01,
        4.9752e+01, 4.9401e+01, 4.8812e+01, 4.7008e+01, 4.6151e+01, 4.5976e+01,
        4.5097e+01, 4.4747e+01, 4.3986e+01, 4.3140e+01, 4.2597e+01, 4.1876e+01,
        4.1455e+01, 4.0440e+01, 3.9776e+01, 3.8310e+01, 3.8283e+01, 3.7676e+01,
        3.6780e+01, 3.6305e+01, 3.5455e+01, 3.3908e+01, 3.3325e+01, 3.1901e+01,
        3.1536e+01, 3.0133e+01, 2.9078e+01, 2.7607e+01, 2.6767e+01, 2.5155e+01,
        2.4617e+01, 2.3770e+01, 2.3208e+01, 2.2118e+01, 2.1511e+01, 2.1493e+01,
        2.0475e+01, 2.0245e+01, 1.8539e+01, 1.7925e+01, 1.6183e+01, 1.4907e+01,
        1.4421e+01, 1.1390e+01, 1.0172e+01, 9.6286e+00, 8.6828e+00, 7.1512e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 78]) 

NULL SPACE BASIS :  tensor([[-8.1011e-03,  7.5132e-02, -4.3400e-02,  ...,  1.3518e-02,
         -4.8657e-03,  6.0343e-03],
        [-2.1272e-02,  1.7626e-02,  1.6696e-02,  ..., -4.0116e-03,
          3.4164e-03, -6.1179e-03],
        [ 1.1453e-02, -8.8111e-02, -4.9990e-03,  ..., -4.0287e-03,
         -1.9621e-03,  2.3359e-03],
        ...,
        [ 1.6776e-02,  5.1915e-03,  3.2511e-02,  ...,  3.5196e-03,
         -6.3920e-05, -2.2441e-03],
        [-2.8452e-02, -3.8985e-02,  9.6214e-03,  ..., -1.2997e-03,
          2.6694e-04,  1.5330e-03],
        [-5.9271e-03,  4.1736e-02, -8.8236e-03,  ..., -4.1595e-03,
         -2.5564e-04,  2.9841e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.0047e-03, -6.2684e-03,  3.0870e-04,  ..., -5.6427e-04,
          4.1197e-04, -4.2518e-05],
        [-6.2684e-03,  1.3729e-02, -7.9214e-03,  ...,  1.2634e-03,
         -4.9122e-05, -4.1304e-04],
        [ 3.0870e-04, -7.9214e-03,  8.9534e-03,  ..., -9.4569e-04,
          1.9547e-04,  3.6386e-04],
        ...,
        [-5.6427e-04,  1.2634e-03, -9.4569e-04,  ...,  2.0989e-03,
         -1.3862e-03, -6.7062e-05],
        [ 4.1197e-04, -4.9122e-05,  1.9547e-04,  ..., -1.3862e-03,
          2.8736e-03, -1.2264e-03],
        [-4.2518e-05, -4.1304e-04,  3.6386e-04,  ..., -6.7062e-05,
         -1.2264e-03,  1.5809e-03]], device='cuda:0') 

reserving basis 150/576; cond: 1194309.125, radio:0.0005573405651375651
PARAMETER       :  Parameter containing:
tensor([[[[-0.0141,  0.0041,  0.0140],
          [ 0.0238,  0.0078,  0.0076],
          [-0.0525, -0.0493, -0.0112]],

         [[ 0.0031, -0.0116, -0.0064],
          [ 0.0270, -0.0292,  0.0261],
          [-0.0625, -0.0076, -0.0567]],

         [[-0.0002,  0.0193,  0.0255],
          [-0.0141, -0.0342,  0.0466],
          [-0.0167, -0.0081,  0.0516]],

         ...,

         [[-0.0400,  0.0276,  0.0216],
          [-0.0109,  0.0077, -0.0069],
          [-0.0351,  0.0124,  0.0237]],

         [[-0.0177,  0.0128,  0.0015],
          [-0.0335,  0.0200, -0.0048],
          [ 0.0174, -0.0391,  0.0345]],

         [[ 0.0212, -0.0012,  0.0240],
          [ 0.0103,  0.0111,  0.0405],
          [-0.0296,  0.0206, -0.0059]]],


        [[[ 0.0326,  0.0110, -0.0227],
          [-0.0088,  0.0003,  0.0105],
          [ 0.0036, -0.0467,  0.0090]],

         [[ 0.0346,  0.0246,  0.0223],
          [-0.0204, -0.0089, -0.0238],
          [ 0.0083, -0.0294,  0.0153]],

         [[-0.0306, -0.0146, -0.0278],
          [-0.0039,  0.0133,  0.0100],
          [ 0.0349,  0.0011, -0.0428]],

         ...,

         [[ 0.0169,  0.0328,  0.0060],
          [ 0.0360,  0.0391, -0.0079],
          [-0.0081, -0.0297,  0.0360]],

         [[-0.0050,  0.0127, -0.0181],
          [-0.0054, -0.0208,  0.0153],
          [-0.0404, -0.0235, -0.0343]],

         [[ 0.0347, -0.0178,  0.0096],
          [ 0.0104,  0.0156,  0.0036],
          [ 0.0233, -0.0135,  0.0233]]],


        [[[ 0.0306, -0.0054, -0.0453],
          [ 0.0314,  0.0373,  0.0301],
          [-0.0148, -0.0070,  0.0390]],

         [[ 0.0142,  0.0035,  0.0091],
          [ 0.0190, -0.0290, -0.0028],
          [ 0.0299,  0.0279, -0.0158]],

         [[ 0.0416,  0.0023,  0.0094],
          [ 0.0142, -0.0183,  0.0391],
          [-0.0337, -0.0385,  0.0081]],

         ...,

         [[-0.0004,  0.0308, -0.0125],
          [-0.0160, -0.0450,  0.0061],
          [-0.0102, -0.0251, -0.0254]],

         [[-0.0086,  0.0244, -0.0458],
          [ 0.0328,  0.0239, -0.0225],
          [-0.0272, -0.0241,  0.0111]],

         [[ 0.0382, -0.0037, -0.0087],
          [-0.0254, -0.0262,  0.0094],
          [ 0.0029,  0.0416,  0.0344]]],


        ...,


        [[[ 0.0372, -0.0314, -0.0343],
          [ 0.0364, -0.0354, -0.0469],
          [-0.0149, -0.0221,  0.0144]],

         [[ 0.0383,  0.0164,  0.0282],
          [-0.0417, -0.0428,  0.0035],
          [-0.0476,  0.0244, -0.0389]],

         [[ 0.0126,  0.0450,  0.0052],
          [-0.0448, -0.0420,  0.0209],
          [-0.0213,  0.0176,  0.0276]],

         ...,

         [[-0.0250, -0.0244,  0.0183],
          [ 0.0024,  0.0295,  0.0367],
          [ 0.0343, -0.0484,  0.0118]],

         [[ 0.0009, -0.0233,  0.0336],
          [-0.0382, -0.0061, -0.0027],
          [-0.0082, -0.0446,  0.0065]],

         [[-0.0039, -0.0201, -0.0075],
          [ 0.0312,  0.0480, -0.0024],
          [-0.0023,  0.0310,  0.0077]]],


        [[[-0.0005, -0.0415,  0.0143],
          [-0.0409,  0.0211,  0.0336],
          [ 0.0132,  0.0279, -0.0120]],

         [[ 0.0130, -0.0208, -0.0190],
          [ 0.0071, -0.0448,  0.0018],
          [-0.0219, -0.0202, -0.0052]],

         [[ 0.0266,  0.0252,  0.0151],
          [-0.0049,  0.0058, -0.0200],
          [ 0.0332, -0.0351, -0.0012]],

         ...,

         [[-0.0216,  0.0429,  0.0049],
          [ 0.0250,  0.0237, -0.0307],
          [-0.0043, -0.0388,  0.0145]],

         [[-0.0232, -0.0142, -0.0359],
          [-0.0039,  0.0210,  0.0028],
          [ 0.0100, -0.0313,  0.0332]],

         [[ 0.0103,  0.0429,  0.0124],
          [-0.0026,  0.0221,  0.0185],
          [-0.0126,  0.0048,  0.0227]]],


        [[[ 0.0086, -0.0093, -0.0075],
          [-0.0160,  0.0164, -0.0079],
          [-0.0395, -0.0433,  0.0427]],

         [[-0.0252,  0.0047, -0.0185],
          [-0.0318, -0.0284,  0.0007],
          [-0.0333,  0.0119,  0.0107]],

         [[ 0.0004,  0.0141, -0.0149],
          [ 0.0068, -0.0373, -0.0083],
          [-0.0234,  0.0278, -0.0473]],

         ...,

         [[ 0.0401, -0.0362, -0.0335],
          [-0.0110,  0.0118,  0.0401],
          [-0.0143,  0.0307, -0.0370]],

         [[-0.0122, -0.0310, -0.0109],
          [ 0.0160,  0.0214, -0.0296],
          [ 0.0312, -0.0314,  0.0295]],

         [[-0.0029, -0.0316, -0.0279],
          [-0.0211,  0.0326,  0.0005],
          [-0.0158,  0.0394,  0.0060]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([6.0005e+07, 2.2740e+06, 1.9124e+06, 1.7919e+06, 1.7351e+06, 1.1092e+06,
        9.0262e+05, 8.2069e+05, 7.4614e+05, 5.5161e+05, 5.0695e+05, 4.9771e+05,
        4.4571e+05, 4.0138e+05, 3.2559e+05, 2.3311e+05, 2.1032e+05, 1.9804e+05,
        1.7942e+05, 1.5762e+05, 1.4820e+05, 1.1597e+05, 1.1436e+05, 1.0873e+05,
        1.0429e+05, 9.1355e+04, 8.8176e+04, 8.1468e+04, 7.8269e+04, 6.8933e+04,
        6.5593e+04, 6.1044e+04, 5.9218e+04, 5.6795e+04, 5.0706e+04, 4.8442e+04,
        4.7846e+04, 4.4819e+04, 3.9152e+04, 3.7957e+04, 3.6176e+04, 3.2661e+04,
        3.1874e+04, 3.1057e+04, 2.9125e+04, 2.8245e+04, 2.7021e+04, 2.5832e+04,
        2.4587e+04, 2.3505e+04, 2.3306e+04, 2.2708e+04, 2.2476e+04, 2.1770e+04,
        2.0945e+04, 2.0044e+04, 1.9924e+04, 1.9351e+04, 1.8483e+04, 1.8009e+04,
        1.7617e+04, 1.7040e+04, 1.6603e+04, 1.6111e+04, 1.5841e+04, 1.5104e+04,
        1.4574e+04, 1.4164e+04, 1.3836e+04, 1.3645e+04, 1.3337e+04, 1.2965e+04,
        1.2813e+04, 1.2783e+04, 1.2575e+04, 1.2029e+04, 1.1827e+04, 1.1497e+04,
        1.1375e+04, 1.1145e+04, 1.0897e+04, 1.0723e+04, 1.0554e+04, 1.0323e+04,
        1.0010e+04, 9.9147e+03, 9.7707e+03, 9.4275e+03, 9.4020e+03, 9.3214e+03,
        8.9870e+03, 8.9434e+03, 8.8492e+03, 8.5595e+03, 8.4770e+03, 8.4574e+03,
        8.2089e+03, 8.1154e+03, 7.9658e+03, 7.8865e+03, 7.6985e+03, 7.6371e+03,
        7.5434e+03, 7.4161e+03, 7.2899e+03, 7.1424e+03, 7.0228e+03, 6.9186e+03,
        6.8252e+03, 6.7562e+03, 6.6947e+03, 6.6555e+03, 6.4619e+03, 6.4177e+03,
        6.3847e+03, 6.2198e+03, 6.1650e+03, 5.9717e+03, 5.8939e+03, 5.7949e+03,
        5.7665e+03, 5.7305e+03, 5.6435e+03, 5.5474e+03, 5.5072e+03, 5.4595e+03,
        5.4005e+03, 5.3219e+03, 5.2069e+03, 5.1396e+03, 5.1071e+03, 5.0598e+03,
        4.9554e+03, 4.9499e+03, 4.8930e+03, 4.8313e+03, 4.6831e+03, 4.6399e+03,
        4.6004e+03, 4.4936e+03, 4.4781e+03, 4.4223e+03, 4.3426e+03, 4.3250e+03,
        4.2727e+03, 4.1924e+03, 4.1131e+03, 4.1051e+03, 4.0361e+03, 4.0156e+03,
        3.9675e+03, 3.9240e+03, 3.9100e+03, 3.8006e+03, 3.7644e+03, 3.7580e+03,
        3.7251e+03, 3.6661e+03, 3.6226e+03, 3.5580e+03, 3.5300e+03, 3.5171e+03,
        3.4906e+03, 3.4012e+03, 3.3841e+03, 3.3555e+03, 3.3338e+03, 3.3065e+03,
        3.2420e+03, 3.2000e+03, 3.1785e+03, 3.1708e+03, 3.1414e+03, 3.1230e+03,
        3.0918e+03, 3.0782e+03, 3.0559e+03, 3.0061e+03, 2.9484e+03, 2.9358e+03,
        2.9009e+03, 2.8712e+03, 2.8373e+03, 2.8200e+03, 2.7928e+03, 2.7682e+03,
        2.7394e+03, 2.6969e+03, 2.6848e+03, 2.6786e+03, 2.6571e+03, 2.6449e+03,
        2.5926e+03, 2.5725e+03, 2.5355e+03, 2.5189e+03, 2.5027e+03, 2.4959e+03,
        2.4761e+03, 2.4378e+03, 2.4316e+03, 2.3986e+03, 2.3836e+03, 2.3429e+03,
        2.3145e+03, 2.2896e+03, 2.2844e+03, 2.2692e+03, 2.2526e+03, 2.2368e+03,
        2.2134e+03, 2.2024e+03, 2.1767e+03, 2.1630e+03, 2.1498e+03, 2.1006e+03,
        2.0824e+03, 2.0765e+03, 2.0669e+03, 2.0530e+03, 2.0154e+03, 1.9999e+03,
        1.9931e+03, 1.9766e+03, 1.9715e+03, 1.9575e+03, 1.9332e+03, 1.9181e+03,
        1.8888e+03, 1.8679e+03, 1.8614e+03, 1.8355e+03, 1.8333e+03, 1.8252e+03,
        1.8002e+03, 1.7977e+03, 1.7769e+03, 1.7674e+03, 1.7470e+03, 1.7428e+03,
        1.7329e+03, 1.7169e+03, 1.7088e+03, 1.6875e+03, 1.6832e+03, 1.6599e+03,
        1.6514e+03, 1.6335e+03, 1.6156e+03, 1.6095e+03, 1.5926e+03, 1.5822e+03,
        1.5762e+03, 1.5610e+03, 1.5489e+03, 1.5396e+03, 1.5239e+03, 1.5177e+03,
        1.5121e+03, 1.5004e+03, 1.4803e+03, 1.4745e+03, 1.4580e+03, 1.4513e+03,
        1.4435e+03, 1.4357e+03, 1.4214e+03, 1.4126e+03, 1.3954e+03, 1.3773e+03,
        1.3743e+03, 1.3669e+03, 1.3563e+03, 1.3460e+03, 1.3303e+03, 1.3266e+03,
        1.3177e+03, 1.3124e+03, 1.2993e+03, 1.2914e+03, 1.2790e+03, 1.2725e+03,
        1.2649e+03, 1.2590e+03, 1.2499e+03, 1.2351e+03, 1.2326e+03, 1.2265e+03,
        1.2172e+03, 1.2056e+03, 1.1975e+03, 1.1782e+03, 1.1672e+03, 1.1636e+03,
        1.1556e+03, 1.1532e+03, 1.1455e+03, 1.1370e+03, 1.1267e+03, 1.1208e+03,
        1.1181e+03, 1.1095e+03, 1.1049e+03, 1.0853e+03, 1.0842e+03, 1.0787e+03,
        1.0751e+03, 1.0704e+03, 1.0606e+03, 1.0516e+03, 1.0447e+03, 1.0374e+03,
        1.0340e+03, 1.0268e+03, 1.0210e+03, 1.0153e+03, 1.0041e+03, 9.9081e+02,
        9.8399e+02, 9.7971e+02, 9.7837e+02, 9.6913e+02, 9.6580e+02, 9.6398e+02,
        9.5490e+02, 9.4971e+02, 9.3856e+02, 9.3785e+02, 9.2900e+02, 9.2356e+02,
        9.1993e+02, 9.0974e+02, 9.0460e+02, 9.0274e+02, 8.9584e+02, 8.8833e+02,
        8.8384e+02, 8.8045e+02, 8.7085e+02, 8.6343e+02, 8.5927e+02, 8.5032e+02,
        8.4962e+02, 8.4371e+02, 8.3781e+02, 8.2711e+02, 8.2651e+02, 8.2066e+02,
        8.1811e+02, 8.1297e+02, 8.0802e+02, 8.0171e+02, 7.9616e+02, 7.9492e+02,
        7.9089e+02, 7.8567e+02, 7.7519e+02, 7.7065e+02, 7.6286e+02, 7.5784e+02,
        7.5498e+02, 7.4857e+02, 7.4491e+02, 7.4285e+02, 7.3964e+02, 7.3295e+02,
        7.3201e+02, 7.2653e+02, 7.2460e+02, 7.1885e+02, 7.1442e+02, 7.1147e+02,
        7.0724e+02, 7.0035e+02, 6.9588e+02, 6.9314e+02, 6.8960e+02, 6.8460e+02,
        6.8289e+02, 6.8154e+02, 6.7412e+02, 6.6708e+02, 6.6123e+02, 6.5859e+02,
        6.5373e+02, 6.4983e+02, 6.4916e+02, 6.4400e+02, 6.3723e+02, 6.3260e+02,
        6.2841e+02, 6.2414e+02, 6.1984e+02, 6.1569e+02, 6.1322e+02, 6.0508e+02,
        6.0011e+02, 5.9897e+02, 5.9624e+02, 5.9117e+02, 5.9008e+02, 5.8427e+02,
        5.8075e+02, 5.7609e+02, 5.7349e+02, 5.6917e+02, 5.6507e+02, 5.6069e+02,
        5.5916e+02, 5.5375e+02, 5.5173e+02, 5.4804e+02, 5.4495e+02, 5.3846e+02,
        5.3704e+02, 5.3501e+02, 5.3300e+02, 5.2909e+02, 5.2828e+02, 5.2494e+02,
        5.2100e+02, 5.1855e+02, 5.1433e+02, 5.1001e+02, 5.0677e+02, 5.0516e+02,
        5.0036e+02, 4.9631e+02, 4.9474e+02, 4.9143e+02, 4.8983e+02, 4.8518e+02,
        4.8175e+02, 4.7820e+02, 4.7103e+02, 4.7016e+02, 4.6647e+02, 4.6470e+02,
        4.6230e+02, 4.5654e+02, 4.5333e+02, 4.5154e+02, 4.4837e+02, 4.4701e+02,
        4.4379e+02, 4.3954e+02, 4.3409e+02, 4.3364e+02, 4.2969e+02, 4.2876e+02,
        4.2520e+02, 4.2381e+02, 4.2034e+02, 4.1740e+02, 4.1373e+02, 4.1310e+02,
        4.1181e+02, 4.0965e+02, 4.0740e+02, 4.0272e+02, 3.9563e+02, 3.9376e+02,
        3.8878e+02, 3.8679e+02, 3.8419e+02, 3.8287e+02, 3.8028e+02, 3.7743e+02,
        3.7605e+02, 3.7432e+02, 3.7216e+02, 3.6770e+02, 3.6715e+02, 3.6245e+02,
        3.5923e+02, 3.5633e+02, 3.5414e+02, 3.5248e+02, 3.5153e+02, 3.4967e+02,
        3.4756e+02, 3.4479e+02, 3.4128e+02, 3.3940e+02, 3.3449e+02, 3.3356e+02,
        3.2778e+02, 3.2679e+02, 3.2422e+02, 3.2278e+02, 3.2187e+02, 3.1731e+02,
        3.1641e+02, 3.1126e+02, 3.0864e+02, 3.0643e+02, 3.0462e+02, 3.0015e+02,
        2.9810e+02, 2.9515e+02, 2.9179e+02, 2.8840e+02, 2.8626e+02, 2.8121e+02,
        2.7927e+02, 2.7766e+02, 2.7316e+02, 2.7187e+02, 2.6970e+02, 2.6729e+02,
        2.6578e+02, 2.6433e+02, 2.6104e+02, 2.5947e+02, 2.5821e+02, 2.5611e+02,
        2.5206e+02, 2.4707e+02, 2.4565e+02, 2.4437e+02, 2.4072e+02, 2.3730e+02,
        2.3540e+02, 2.3381e+02, 2.3030e+02, 2.2939e+02, 2.2591e+02, 2.2149e+02,
        2.2051e+02, 2.1786e+02, 2.1738e+02, 2.0895e+02, 2.0707e+02, 2.0412e+02,
        2.0317e+02, 1.9918e+02, 1.9807e+02, 1.9515e+02, 1.9433e+02, 1.9341e+02,
        1.8788e+02, 1.8423e+02, 1.8287e+02, 1.8086e+02, 1.7844e+02, 1.7604e+02,
        1.7295e+02, 1.7003e+02, 1.6425e+02, 1.6413e+02, 1.5975e+02, 1.5756e+02,
        1.5475e+02, 1.5300e+02, 1.5019e+02, 1.4682e+02, 1.4513e+02, 1.4163e+02,
        1.3880e+02, 1.3446e+02, 1.3021e+02, 1.2849e+02, 1.2507e+02, 1.1920e+02,
        1.1263e+02, 1.0645e+02, 1.0498e+02, 1.0080e+02, 9.5879e+01, 9.3678e+01,
        9.2851e+01, 8.7365e+01, 8.1715e+01, 7.9977e+01, 6.5989e+01, 5.0242e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 150]) 

NULL SPACE BASIS :  tensor([[-0.0497, -0.0213,  0.0376,  ..., -0.0048,  0.0050, -0.0143],
        [-0.0349,  0.0279,  0.0197,  ..., -0.0056,  0.0037,  0.0272],
        [ 0.0658, -0.0224,  0.0031,  ...,  0.0127,  0.0026, -0.0163],
        ...,
        [-0.0560, -0.0177,  0.0363,  ..., -0.0269, -0.2040, -0.0440],
        [-0.0008, -0.0358,  0.0097,  ..., -0.0109,  0.3329,  0.0477],
        [-0.0043, -0.0131,  0.0261,  ...,  0.0320, -0.1553, -0.0180]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0361, -0.0207, -0.0068,  ...,  0.0009,  0.0005,  0.0003],
        [-0.0207,  0.0427, -0.0207,  ..., -0.0010, -0.0001,  0.0002],
        [-0.0068, -0.0207,  0.0348,  ..., -0.0007,  0.0005,  0.0004],
        ...,
        [ 0.0009, -0.0010, -0.0007,  ...,  0.0478, -0.0225, -0.0098],
        [ 0.0005, -0.0001,  0.0005,  ..., -0.0225,  0.0525, -0.0207],
        [ 0.0003,  0.0002,  0.0004,  ..., -0.0098, -0.0207,  0.0458]],
       device='cuda:0') 

reserving basis 228/576; cond: 1057663.875, radio:0.0008660195162519813
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0190,  0.0401,  0.0318],
          [ 0.0035, -0.0028, -0.0087],
          [-0.0349,  0.0284,  0.0166]],

         [[-0.0427, -0.0197, -0.0223],
          [-0.0473, -0.0462, -0.0274],
          [-0.0237,  0.0054, -0.0060]],

         [[ 0.0175,  0.0237, -0.0504],
          [-0.0146, -0.0302, -0.0191],
          [ 0.0116,  0.0107,  0.0194]],

         ...,

         [[-0.0555,  0.0320,  0.0450],
          [-0.0298, -0.0068, -0.0152],
          [ 0.0238, -0.0233, -0.0337]],

         [[ 0.0110, -0.0287,  0.0348],
          [ 0.0404, -0.0259, -0.0034],
          [-0.0190, -0.0329,  0.0198]],

         [[-0.0266, -0.0207,  0.0057],
          [ 0.0169,  0.0148, -0.0237],
          [-0.0259,  0.0036,  0.0040]]],


        [[[-0.0305, -0.0202, -0.0116],
          [ 0.0362,  0.0210,  0.0292],
          [-0.0137,  0.0463,  0.0012]],

         [[-0.0387, -0.0330, -0.0311],
          [ 0.0001, -0.0404, -0.0242],
          [-0.0145,  0.0200,  0.0219]],

         [[-0.0091,  0.0161, -0.0233],
          [-0.0353, -0.0300,  0.0141],
          [ 0.0298,  0.0117, -0.0191]],

         ...,

         [[ 0.0246, -0.0459,  0.0175],
          [-0.0125,  0.0121, -0.0204],
          [ 0.0270,  0.0253, -0.0071]],

         [[ 0.0119, -0.0403, -0.0077],
          [-0.0135, -0.0188, -0.0411],
          [-0.0041,  0.0054, -0.0005]],

         [[-0.0014, -0.0270, -0.0107],
          [-0.0311, -0.0277, -0.0162],
          [ 0.0343,  0.0214, -0.0178]]],


        [[[-0.0405, -0.0088, -0.0022],
          [-0.0574, -0.0426, -0.0350],
          [-0.0013,  0.0348,  0.0385]],

         [[-0.0184, -0.0094, -0.0041],
          [-0.0237, -0.0405, -0.0387],
          [-0.0174, -0.0177, -0.0096]],

         [[ 0.0077, -0.0145,  0.0051],
          [-0.0359,  0.0503, -0.0085],
          [-0.0056,  0.0266,  0.0379]],

         ...,

         [[-0.0450,  0.0229,  0.0366],
          [-0.0187, -0.0406,  0.0336],
          [-0.0116, -0.0295,  0.0132]],

         [[ 0.0269,  0.0184,  0.0401],
          [ 0.0232, -0.0299, -0.0221],
          [-0.0343, -0.0501,  0.0197]],

         [[-0.0290,  0.0499,  0.0323],
          [ 0.0201,  0.0085, -0.0119],
          [-0.0150, -0.0009, -0.0377]]],


        ...,


        [[[ 0.0346,  0.0054, -0.0066],
          [ 0.0393,  0.0450,  0.0016],
          [-0.0157,  0.0125, -0.0379]],

         [[-0.0312,  0.0013, -0.0282],
          [-0.0105,  0.0342,  0.0140],
          [-0.0047, -0.0174,  0.0211]],

         [[-0.0419, -0.0408, -0.0357],
          [-0.0276,  0.0353,  0.0429],
          [-0.0115, -0.0346, -0.0275]],

         ...,

         [[ 0.0338, -0.0065,  0.0339],
          [-0.0116,  0.0431, -0.0124],
          [-0.0393,  0.0260, -0.0237]],

         [[-0.0184, -0.0424, -0.0143],
          [-0.0114,  0.0187, -0.0350],
          [ 0.0244,  0.0225,  0.0134]],

         [[ 0.0218, -0.0157,  0.0106],
          [ 0.0330, -0.0042, -0.0268],
          [-0.0403,  0.0146, -0.0117]]],


        [[[-0.0378,  0.0239, -0.0476],
          [ 0.0035, -0.0419,  0.0204],
          [-0.0164, -0.0071,  0.0368]],

         [[ 0.0185, -0.0305,  0.0074],
          [-0.0081,  0.0272,  0.0055],
          [ 0.0242, -0.0116,  0.0164]],

         [[-0.0348, -0.0420, -0.0246],
          [-0.0237, -0.0175,  0.0183],
          [ 0.0085, -0.0334, -0.0370]],

         ...,

         [[-0.0092, -0.0163, -0.0306],
          [-0.0204,  0.0284, -0.0232],
          [-0.0409,  0.0343,  0.0317]],

         [[-0.0301, -0.0107, -0.0292],
          [ 0.0367,  0.0025, -0.0163],
          [ 0.0300,  0.0324, -0.0208]],

         [[ 0.0147, -0.0266, -0.0073],
          [-0.0401, -0.0343,  0.0387],
          [ 0.0386,  0.0223,  0.0386]]],


        [[[-0.0385,  0.0323, -0.0044],
          [-0.0339,  0.0353, -0.0078],
          [-0.0381, -0.0228,  0.0299]],

         [[-0.0037, -0.0253,  0.0088],
          [ 0.0185,  0.0021, -0.0097],
          [-0.0147, -0.0025,  0.0312]],

         [[-0.0221, -0.0370, -0.0063],
          [-0.0016, -0.0012,  0.0025],
          [ 0.0349, -0.0265,  0.0024]],

         ...,

         [[ 0.0124, -0.0167, -0.0195],
          [-0.0201,  0.0309, -0.0444],
          [ 0.0002,  0.0155, -0.0300]],

         [[ 0.0358, -0.0064,  0.0316],
          [ 0.0050, -0.0210, -0.0384],
          [-0.0206,  0.0085, -0.0362]],

         [[ 0.0102,  0.0415, -0.0179],
          [-0.0317,  0.0421,  0.0361],
          [ 0.0268,  0.0156,  0.0076]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([6.1449e+07, 2.3146e+06, 2.0351e+06, 1.6909e+06, 1.4057e+06, 9.9268e+05,
        9.1102e+05, 7.2910e+05, 6.8283e+05, 5.3963e+05, 4.8927e+05, 4.5365e+05,
        3.5941e+05, 2.8187e+05, 1.4960e+05, 1.3523e+05, 1.2728e+05, 1.2076e+05,
        1.1546e+05, 1.0221e+05, 9.8195e+04, 8.6411e+04, 8.0168e+04, 7.5532e+04,
        7.0762e+04, 6.3988e+04, 6.2253e+04, 5.8058e+04, 5.6632e+04, 5.4799e+04,
        5.0704e+04, 4.6662e+04, 4.5852e+04, 4.5321e+04, 4.0445e+04, 3.7965e+04,
        3.6486e+04, 3.5029e+04, 3.3967e+04, 3.2684e+04, 3.1895e+04, 3.0547e+04,
        3.0082e+04, 2.8733e+04, 2.7885e+04, 2.6070e+04, 2.5638e+04, 2.4448e+04,
        2.4089e+04, 2.3095e+04, 2.1646e+04, 2.1196e+04, 2.0118e+04, 1.8982e+04,
        1.8762e+04, 1.7469e+04, 1.6866e+04, 1.6615e+04, 1.6097e+04, 1.5743e+04,
        1.4704e+04, 1.4514e+04, 1.4092e+04, 1.3837e+04, 1.3437e+04, 1.3362e+04,
        1.2825e+04, 1.2729e+04, 1.2362e+04, 1.2234e+04, 1.2090e+04, 1.1348e+04,
        1.1094e+04, 1.0899e+04, 1.0428e+04, 1.0069e+04, 9.9399e+03, 9.8775e+03,
        9.7716e+03, 9.5151e+03, 9.3835e+03, 9.2062e+03, 8.9534e+03, 8.7996e+03,
        8.5001e+03, 8.4063e+03, 8.2487e+03, 8.0240e+03, 7.9834e+03, 7.8445e+03,
        7.7234e+03, 7.6792e+03, 7.4844e+03, 7.3718e+03, 7.0836e+03, 7.0301e+03,
        6.7084e+03, 6.6742e+03, 6.5024e+03, 6.4756e+03, 6.3048e+03, 6.2196e+03,
        6.0993e+03, 5.9883e+03, 5.9382e+03, 5.8974e+03, 5.7771e+03, 5.6785e+03,
        5.5568e+03, 5.4877e+03, 5.3453e+03, 5.2966e+03, 5.1326e+03, 5.0807e+03,
        5.0593e+03, 4.9800e+03, 4.9381e+03, 4.7577e+03, 4.7304e+03, 4.6283e+03,
        4.5658e+03, 4.4092e+03, 4.3982e+03, 4.3518e+03, 4.2695e+03, 4.2603e+03,
        4.1904e+03, 4.1777e+03, 4.1109e+03, 4.0174e+03, 3.9549e+03, 3.8974e+03,
        3.8568e+03, 3.7664e+03, 3.7325e+03, 3.6693e+03, 3.6105e+03, 3.5170e+03,
        3.5040e+03, 3.4428e+03, 3.3894e+03, 3.3656e+03, 3.3079e+03, 3.2893e+03,
        3.2795e+03, 3.2533e+03, 3.1914e+03, 3.1474e+03, 3.1039e+03, 3.0806e+03,
        3.0479e+03, 2.9942e+03, 2.9186e+03, 2.9072e+03, 2.8728e+03, 2.8380e+03,
        2.7929e+03, 2.7887e+03, 2.7476e+03, 2.7315e+03, 2.6966e+03, 2.6376e+03,
        2.6369e+03, 2.6191e+03, 2.6147e+03, 2.5910e+03, 2.5588e+03, 2.5482e+03,
        2.4731e+03, 2.4657e+03, 2.4333e+03, 2.4206e+03, 2.4152e+03, 2.3916e+03,
        2.3589e+03, 2.3307e+03, 2.3055e+03, 2.2760e+03, 2.2474e+03, 2.2199e+03,
        2.2156e+03, 2.1895e+03, 2.1654e+03, 2.1275e+03, 2.1017e+03, 2.0886e+03,
        2.0714e+03, 2.0563e+03, 2.0488e+03, 2.0416e+03, 2.0251e+03, 1.9946e+03,
        1.9690e+03, 1.9547e+03, 1.8972e+03, 1.8893e+03, 1.8746e+03, 1.8493e+03,
        1.8374e+03, 1.8082e+03, 1.7998e+03, 1.7718e+03, 1.7666e+03, 1.7356e+03,
        1.7153e+03, 1.7131e+03, 1.6964e+03, 1.6798e+03, 1.6661e+03, 1.6580e+03,
        1.6349e+03, 1.6291e+03, 1.6068e+03, 1.6000e+03, 1.5941e+03, 1.5788e+03,
        1.5662e+03, 1.5546e+03, 1.5467e+03, 1.5291e+03, 1.5128e+03, 1.5033e+03,
        1.4886e+03, 1.4834e+03, 1.4528e+03, 1.4510e+03, 1.4326e+03, 1.4180e+03,
        1.4068e+03, 1.3921e+03, 1.3853e+03, 1.3736e+03, 1.3678e+03, 1.3540e+03,
        1.3476e+03, 1.3281e+03, 1.3189e+03, 1.3170e+03, 1.3086e+03, 1.3027e+03,
        1.2867e+03, 1.2770e+03, 1.2549e+03, 1.2407e+03, 1.2344e+03, 1.2223e+03,
        1.2186e+03, 1.2068e+03, 1.1984e+03, 1.1901e+03, 1.1783e+03, 1.1608e+03,
        1.1567e+03, 1.1480e+03, 1.1406e+03, 1.1325e+03, 1.1247e+03, 1.1158e+03,
        1.0995e+03, 1.0917e+03, 1.0876e+03, 1.0849e+03, 1.0751e+03, 1.0645e+03,
        1.0533e+03, 1.0480e+03, 1.0350e+03, 1.0318e+03, 1.0266e+03, 1.0242e+03,
        1.0063e+03, 1.0012e+03, 9.9990e+02, 9.8746e+02, 9.8246e+02, 9.7318e+02,
        9.6669e+02, 9.6004e+02, 9.5071e+02, 9.4326e+02, 9.3943e+02, 9.3168e+02,
        9.2558e+02, 9.1929e+02, 9.0979e+02, 9.0575e+02, 9.0232e+02, 8.9751e+02,
        8.8871e+02, 8.8649e+02, 8.7590e+02, 8.7286e+02, 8.6680e+02, 8.5383e+02,
        8.5167e+02, 8.5053e+02, 8.4464e+02, 8.3984e+02, 8.3007e+02, 8.2337e+02,
        8.1846e+02, 8.1588e+02, 8.0652e+02, 8.0038e+02, 7.9502e+02, 7.9142e+02,
        7.8610e+02, 7.7862e+02, 7.7247e+02, 7.6413e+02, 7.5976e+02, 7.5530e+02,
        7.5139e+02, 7.4378e+02, 7.4021e+02, 7.3447e+02, 7.2950e+02, 7.2547e+02,
        7.2272e+02, 7.1182e+02, 7.0561e+02, 6.9984e+02, 6.9248e+02, 6.8641e+02,
        6.8254e+02, 6.7809e+02, 6.7490e+02, 6.6743e+02, 6.6335e+02, 6.5952e+02,
        6.5625e+02, 6.5180e+02, 6.4990e+02, 6.4276e+02, 6.3886e+02, 6.3533e+02,
        6.2843e+02, 6.2601e+02, 6.2114e+02, 6.1430e+02, 6.1146e+02, 6.0825e+02,
        6.0403e+02, 5.9977e+02, 5.9772e+02, 5.9714e+02, 5.9408e+02, 5.8811e+02,
        5.7742e+02, 5.7486e+02, 5.7263e+02, 5.7104e+02, 5.6809e+02, 5.6460e+02,
        5.6159e+02, 5.5798e+02, 5.5478e+02, 5.5255e+02, 5.4958e+02, 5.4546e+02,
        5.4055e+02, 5.3756e+02, 5.3588e+02, 5.2736e+02, 5.2467e+02, 5.2249e+02,
        5.1579e+02, 5.1352e+02, 5.0672e+02, 5.0546e+02, 5.0081e+02, 4.9722e+02,
        4.9435e+02, 4.9263e+02, 4.8992e+02, 4.8549e+02, 4.8404e+02, 4.7835e+02,
        4.7599e+02, 4.7473e+02, 4.7358e+02, 4.6811e+02, 4.6529e+02, 4.6323e+02,
        4.5940e+02, 4.5579e+02, 4.5094e+02, 4.5048e+02, 4.4786e+02, 4.4493e+02,
        4.4165e+02, 4.3686e+02, 4.3467e+02, 4.3134e+02, 4.2999e+02, 4.2949e+02,
        4.2446e+02, 4.2237e+02, 4.1799e+02, 4.1623e+02, 4.1485e+02, 4.1346e+02,
        4.0817e+02, 4.0792e+02, 4.0598e+02, 4.0225e+02, 3.9889e+02, 3.9675e+02,
        3.9447e+02, 3.9379e+02, 3.9171e+02, 3.8753e+02, 3.8477e+02, 3.8291e+02,
        3.8183e+02, 3.7911e+02, 3.7711e+02, 3.7366e+02, 3.7175e+02, 3.6737e+02,
        3.6463e+02, 3.5957e+02, 3.5809e+02, 3.5644e+02, 3.5378e+02, 3.5129e+02,
        3.4899e+02, 3.4870e+02, 3.4606e+02, 3.4465e+02, 3.4195e+02, 3.4157e+02,
        3.3948e+02, 3.3639e+02, 3.3367e+02, 3.3179e+02, 3.3052e+02, 3.2658e+02,
        3.2579e+02, 3.2211e+02, 3.2071e+02, 3.1850e+02, 3.1754e+02, 3.1534e+02,
        3.1330e+02, 3.1156e+02, 3.0829e+02, 3.0561e+02, 3.0409e+02, 3.0269e+02,
        3.0119e+02, 2.9997e+02, 2.9958e+02, 2.9616e+02, 2.9519e+02, 2.9220e+02,
        2.9040e+02, 2.8825e+02, 2.8549e+02, 2.8354e+02, 2.8113e+02, 2.8079e+02,
        2.7589e+02, 2.7485e+02, 2.7225e+02, 2.7014e+02, 2.6829e+02, 2.6479e+02,
        2.6412e+02, 2.6232e+02, 2.6005e+02, 2.5855e+02, 2.5575e+02, 2.5421e+02,
        2.5377e+02, 2.5245e+02, 2.5010e+02, 2.4826e+02, 2.4577e+02, 2.4418e+02,
        2.4026e+02, 2.3996e+02, 2.3798e+02, 2.3712e+02, 2.3538e+02, 2.3290e+02,
        2.3193e+02, 2.3041e+02, 2.2911e+02, 2.2678e+02, 2.2543e+02, 2.2400e+02,
        2.1999e+02, 2.1868e+02, 2.1815e+02, 2.1636e+02, 2.1532e+02, 2.1281e+02,
        2.1118e+02, 2.0949e+02, 2.0842e+02, 2.0558e+02, 2.0361e+02, 2.0237e+02,
        2.0116e+02, 1.9850e+02, 1.9780e+02, 1.9481e+02, 1.9212e+02, 1.9114e+02,
        1.8899e+02, 1.8881e+02, 1.8612e+02, 1.8579e+02, 1.8448e+02, 1.8290e+02,
        1.8000e+02, 1.7826e+02, 1.7708e+02, 1.7559e+02, 1.7361e+02, 1.7150e+02,
        1.6940e+02, 1.6736e+02, 1.6589e+02, 1.6502e+02, 1.6434e+02, 1.6352e+02,
        1.6302e+02, 1.6150e+02, 1.5911e+02, 1.5743e+02, 1.5612e+02, 1.5540e+02,
        1.5206e+02, 1.5010e+02, 1.4857e+02, 1.4641e+02, 1.4479e+02, 1.4414e+02,
        1.4276e+02, 1.4132e+02, 1.3908e+02, 1.3842e+02, 1.3588e+02, 1.3444e+02,
        1.3385e+02, 1.3314e+02, 1.3175e+02, 1.3008e+02, 1.2878e+02, 1.2744e+02,
        1.2473e+02, 1.2332e+02, 1.2237e+02, 1.2145e+02, 1.1876e+02, 1.1693e+02,
        1.1474e+02, 1.1100e+02, 1.1053e+02, 1.0740e+02, 1.0379e+02, 1.0204e+02,
        1.0079e+02, 9.8657e+01, 9.4263e+01, 9.1927e+01, 8.9123e+01, 8.5960e+01,
        8.2890e+01, 8.0950e+01, 7.9979e+01, 7.6533e+01, 6.0368e+01, 5.8098e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 228]) 

NULL SPACE BASIS :  tensor([[ 0.0648,  0.0130, -0.0235,  ..., -0.0007,  0.0199, -0.0052],
        [ 0.0103, -0.0311,  0.0036,  ...,  0.0030, -0.0282,  0.0065],
        [ 0.0417, -0.0072,  0.0761,  ..., -0.0026,  0.0095, -0.0031],
        ...,
        [-0.0062, -0.0112,  0.0090,  ..., -0.0024, -0.0027, -0.0143],
        [ 0.0247,  0.0599, -0.0086,  ...,  0.0228,  0.0022,  0.0175],
        [ 0.0579,  0.0432,  0.0015,  ..., -0.0216, -0.0075, -0.0118]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2108e-02, -1.6683e-02, -8.9819e-04,  ..., -6.4314e-05,
          2.8715e-04,  8.0117e-04],
        [-1.6683e-02,  3.2950e-02, -1.3485e-02,  ..., -1.3588e-03,
          7.0325e-04, -1.5515e-03],
        [-8.9819e-04, -1.3485e-02,  1.8558e-02,  ...,  7.8169e-04,
         -1.3802e-03,  4.0635e-04],
        ...,
        [-6.4314e-05, -1.3588e-03,  7.8169e-04,  ...,  1.9649e-02,
         -1.0074e-02, -2.3380e-03],
        [ 2.8715e-04,  7.0325e-04, -1.3802e-03,  ..., -1.0074e-02,
          2.5853e-02, -1.0422e-02],
        [ 8.0117e-04, -1.5515e-03,  4.0635e-04,  ..., -2.3380e-03,
         -1.0422e-02,  2.2022e-02]], device='cuda:0') 

reserving basis 309/576; cond: 282920.71875, radio:0.004108773544430733
PARAMETER       :  Parameter containing:
tensor([[[[-2.7208e-02, -4.4228e-03,  1.8735e-02],
          [-2.3648e-02, -2.7086e-02, -1.2766e-02],
          [ 1.0684e-02,  1.1861e-02,  8.2110e-03]],

         [[ 6.0643e-04,  1.5249e-02,  5.4408e-03],
          [-4.7720e-03,  8.0238e-04,  1.8182e-02],
          [-1.9153e-02, -1.6352e-02, -4.7564e-02]],

         [[ 2.3259e-03,  4.3719e-02,  4.3874e-02],
          [-3.8565e-03, -3.7358e-02, -4.3941e-03],
          [-3.5312e-02, -5.3236e-03, -2.5679e-02]],

         ...,

         [[-1.8552e-02,  4.3980e-02,  3.3357e-03],
          [-2.7264e-02, -9.5358e-03,  4.1621e-02],
          [-1.4099e-02, -3.9088e-02, -1.7054e-02]],

         [[ 2.8838e-02,  1.6141e-02, -4.4807e-02],
          [ 2.8130e-02,  6.1813e-03,  4.5754e-02],
          [-3.2966e-02,  2.1876e-02, -2.0905e-02]],

         [[ 3.4894e-02,  2.2000e-02,  2.1680e-02],
          [-3.5994e-02, -9.5775e-03,  2.5429e-02],
          [ 3.0909e-02, -1.5075e-02, -2.6159e-02]]],


        [[[-2.7432e-02, -2.3798e-02, -4.6439e-02],
          [ 9.1801e-03, -9.6370e-03,  8.4953e-03],
          [-5.2529e-02,  7.2362e-03,  1.6425e-03]],

         [[-3.2731e-02,  1.9069e-02,  5.2497e-02],
          [ 1.4742e-03,  2.2136e-02, -1.2674e-02],
          [-1.3374e-02,  2.9569e-02,  4.6139e-02]],

         [[-2.1936e-03, -1.7038e-02,  3.0787e-02],
          [-1.1620e-03, -3.1713e-02,  3.6342e-02],
          [ 3.4240e-02,  3.3383e-02, -2.8854e-03]],

         ...,

         [[-7.0714e-03,  2.1199e-02, -2.2315e-02],
          [-2.7630e-02,  2.2392e-02,  1.3925e-02],
          [-3.8972e-02,  2.8070e-02, -4.3611e-02]],

         [[-8.8344e-03,  2.1636e-02, -2.1723e-02],
          [-1.9251e-02, -2.9948e-02, -3.5991e-02],
          [-4.4784e-02, -1.0358e-02, -3.8948e-02]],

         [[ 2.3102e-02, -3.7857e-02, -3.5139e-02],
          [-2.0477e-02, -3.4014e-02, -1.6389e-02],
          [-2.3557e-02,  1.2756e-03,  9.8034e-03]]],


        [[[-5.8526e-03, -5.9155e-03, -1.6923e-02],
          [ 8.6678e-05, -4.4302e-02, -3.8975e-02],
          [-4.0933e-02, -1.9200e-03, -1.3376e-02]],

         [[-2.5309e-02, -2.2396e-02, -1.9857e-02],
          [-1.0459e-04, -2.7425e-02, -1.2625e-02],
          [ 1.6653e-02, -2.5943e-02,  1.5650e-03]],

         [[ 4.7884e-02,  4.1682e-02,  2.4761e-03],
          [ 2.9377e-02,  4.7506e-03, -3.9307e-02],
          [ 1.1457e-02,  8.0516e-03, -6.7775e-04]],

         ...,

         [[ 1.3627e-02, -1.3323e-02, -2.0349e-03],
          [ 1.5012e-02,  1.6924e-02, -4.0373e-02],
          [-3.4812e-02,  9.7152e-03, -1.3176e-02]],

         [[ 1.1866e-02,  6.0023e-03, -2.5185e-02],
          [-2.7965e-02,  9.2784e-03,  2.5989e-02],
          [ 4.8388e-03, -2.1806e-02,  4.1545e-04]],

         [[ 1.8244e-02, -1.1804e-02, -3.1525e-02],
          [ 1.1213e-02, -3.3629e-02, -3.3392e-02],
          [ 3.1002e-02, -1.5530e-02,  7.9641e-04]]],


        ...,


        [[[ 1.2573e-02,  3.2226e-02,  1.4911e-02],
          [-1.3347e-03, -1.6988e-02, -1.8282e-02],
          [ 4.8926e-02,  2.3613e-02, -5.4580e-02]],

         [[ 4.7669e-03, -2.5924e-02,  2.8694e-03],
          [-3.1331e-02,  2.6279e-02, -2.0768e-02],
          [ 1.6403e-02, -2.5550e-02,  4.3945e-02]],

         [[ 2.5102e-02, -2.0150e-02,  4.3818e-02],
          [ 3.3187e-02, -3.9157e-02, -3.9171e-02],
          [ 1.6830e-02, -1.1588e-02, -3.3383e-03]],

         ...,

         [[-1.5268e-02, -1.3487e-02,  3.2553e-03],
          [ 1.3703e-02,  9.0057e-03,  1.3534e-02],
          [ 2.4600e-03, -1.0672e-02,  2.7478e-03]],

         [[ 1.4783e-02,  2.1468e-02, -4.6389e-02],
          [-3.1145e-02, -1.9774e-02, -3.4167e-03],
          [ 9.9654e-03,  8.3509e-03,  1.4153e-02]],

         [[ 3.0857e-02,  6.2937e-03,  2.9482e-02],
          [ 1.6990e-02,  4.3512e-02,  2.9861e-03],
          [ 3.9979e-02, -9.9074e-03,  2.0472e-02]]],


        [[[-1.2435e-02,  2.0278e-02,  3.4761e-02],
          [ 1.5721e-02,  2.4093e-02,  7.1609e-03],
          [-4.0394e-03,  2.7804e-02, -5.1456e-02]],

         [[ 3.5670e-02, -1.4534e-02, -2.7233e-02],
          [-3.7560e-02,  5.9898e-03, -3.5519e-02],
          [-4.0003e-02, -3.8817e-03,  4.0400e-02]],

         [[ 2.0155e-02, -4.2208e-02, -2.3566e-02],
          [-3.4949e-02,  2.7578e-02, -1.6705e-02],
          [-3.4672e-02,  1.4863e-02, -4.0422e-03]],

         ...,

         [[-1.7057e-02, -2.8145e-02,  1.4363e-02],
          [ 1.3232e-02,  3.7751e-02,  8.8543e-03],
          [-2.0456e-02, -3.4130e-02, -6.2390e-03]],

         [[-2.0716e-02,  3.7415e-02, -2.4290e-02],
          [ 6.0504e-03, -1.5221e-02,  3.8175e-02],
          [ 4.6012e-02, -1.7218e-03, -1.0564e-02]],

         [[-2.2796e-02,  6.1035e-03,  2.9257e-02],
          [-3.3026e-02, -2.2575e-02,  1.0043e-02],
          [ 1.1256e-02,  1.0456e-02, -3.1217e-03]]],


        [[[ 3.8388e-02,  4.3077e-02,  7.3729e-05],
          [ 2.0782e-02, -1.2279e-02,  2.4819e-02],
          [ 6.9667e-03, -3.5329e-02, -5.0133e-02]],

         [[ 4.6350e-02, -4.9514e-03,  3.2091e-02],
          [ 1.9567e-02,  3.4501e-02,  8.8333e-03],
          [-4.9207e-04, -8.7257e-03,  3.7736e-02]],

         [[-2.3226e-02,  8.4909e-04, -4.2523e-02],
          [-2.3924e-03, -8.8639e-03,  3.9836e-02],
          [ 2.1632e-02, -1.8282e-02,  2.7242e-02]],

         ...,

         [[-8.3456e-03,  3.7708e-02, -3.3972e-02],
          [-9.3216e-03, -3.8028e-02,  2.7611e-02],
          [-3.7433e-02,  6.9251e-03,  7.1543e-04]],

         [[ 3.2469e-03, -3.5590e-02, -1.6053e-02],
          [-3.6481e-02,  2.6208e-02, -2.1532e-02],
          [-2.3733e-02, -3.9880e-02,  7.1170e-03]],

         [[ 4.1575e-02, -5.4252e-03,  3.4860e-02],
          [ 3.4250e-02, -1.4618e-02,  2.4149e-02],
          [ 2.5972e-04,  2.5441e-02,  2.4094e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([7.3246e+07, 1.9798e+06, 1.7085e+06, 1.5006e+06, 1.3164e+06, 9.0901e+05,
        8.8113e+05, 7.5223e+05, 6.6760e+05, 5.8141e+05, 5.4502e+05, 4.6637e+05,
        4.3603e+05, 3.3706e+05, 2.7879e+05, 2.2944e+05, 2.0496e+05, 1.7221e+05,
        1.5906e+05, 1.4726e+05, 1.3887e+05, 1.3387e+05, 1.3052e+05, 1.1685e+05,
        1.1038e+05, 1.0434e+05, 9.8952e+04, 9.3633e+04, 8.9321e+04, 8.1308e+04,
        7.7712e+04, 7.3782e+04, 6.9009e+04, 6.7358e+04, 5.9551e+04, 5.6986e+04,
        5.5813e+04, 5.2760e+04, 5.0083e+04, 4.9139e+04, 4.8645e+04, 4.5677e+04,
        4.4385e+04, 4.0829e+04, 3.9596e+04, 3.8245e+04, 3.6582e+04, 3.5825e+04,
        3.5206e+04, 3.4156e+04, 3.3298e+04, 3.1846e+04, 2.9879e+04, 2.9462e+04,
        2.8390e+04, 2.8037e+04, 2.7759e+04, 2.7302e+04, 2.6385e+04, 2.5823e+04,
        2.5415e+04, 2.4698e+04, 2.4224e+04, 2.2372e+04, 2.2201e+04, 2.1764e+04,
        2.1064e+04, 2.0791e+04, 2.0614e+04, 1.9764e+04, 1.9568e+04, 1.9231e+04,
        1.8794e+04, 1.8705e+04, 1.8067e+04, 1.7835e+04, 1.7691e+04, 1.7261e+04,
        1.6966e+04, 1.6367e+04, 1.6196e+04, 1.6068e+04, 1.5463e+04, 1.5270e+04,
        1.5149e+04, 1.4861e+04, 1.4624e+04, 1.4149e+04, 1.3956e+04, 1.3869e+04,
        1.3641e+04, 1.3273e+04, 1.3094e+04, 1.2882e+04, 1.2373e+04, 1.2201e+04,
        1.2162e+04, 1.2032e+04, 1.1676e+04, 1.1522e+04, 1.1308e+04, 1.1199e+04,
        1.1080e+04, 1.0988e+04, 1.0872e+04, 1.0796e+04, 1.0562e+04, 1.0477e+04,
        1.0345e+04, 1.0285e+04, 9.8594e+03, 9.8034e+03, 9.6375e+03, 9.4490e+03,
        9.3459e+03, 9.2810e+03, 9.1616e+03, 9.1167e+03, 9.0535e+03, 8.9481e+03,
        8.7912e+03, 8.7040e+03, 8.6366e+03, 8.5805e+03, 8.4698e+03, 8.3731e+03,
        8.3175e+03, 8.1555e+03, 7.9937e+03, 7.9265e+03, 7.8090e+03, 7.7516e+03,
        7.7120e+03, 7.5715e+03, 7.4681e+03, 7.4446e+03, 7.3366e+03, 7.1937e+03,
        7.1550e+03, 7.1237e+03, 7.0334e+03, 6.9711e+03, 6.9026e+03, 6.8521e+03,
        6.7534e+03, 6.6235e+03, 6.5626e+03, 6.5367e+03, 6.4902e+03, 6.4431e+03,
        6.3635e+03, 6.2682e+03, 6.2091e+03, 6.1226e+03, 6.0444e+03, 6.0311e+03,
        5.9639e+03, 5.8634e+03, 5.7994e+03, 5.7302e+03, 5.6834e+03, 5.6823e+03,
        5.6013e+03, 5.5870e+03, 5.5504e+03, 5.4934e+03, 5.4581e+03, 5.3917e+03,
        5.3542e+03, 5.3178e+03, 5.2705e+03, 5.2131e+03, 5.1758e+03, 5.1627e+03,
        5.1499e+03, 5.0999e+03, 5.0677e+03, 5.0196e+03, 4.9680e+03, 4.9328e+03,
        4.9074e+03, 4.8475e+03, 4.8363e+03, 4.7893e+03, 4.7748e+03, 4.7248e+03,
        4.6522e+03, 4.6226e+03, 4.5886e+03, 4.5475e+03, 4.4793e+03, 4.4533e+03,
        4.3974e+03, 4.3836e+03, 4.3564e+03, 4.3347e+03, 4.2703e+03, 4.2272e+03,
        4.2117e+03, 4.1937e+03, 4.1287e+03, 4.1079e+03, 4.0826e+03, 4.0517e+03,
        4.0467e+03, 4.0340e+03, 3.9832e+03, 3.9397e+03, 3.9300e+03, 3.8701e+03,
        3.8603e+03, 3.8160e+03, 3.7747e+03, 3.7390e+03, 3.7185e+03, 3.6772e+03,
        3.6735e+03, 3.6556e+03, 3.6173e+03, 3.6162e+03, 3.5661e+03, 3.5340e+03,
        3.5013e+03, 3.4641e+03, 3.4507e+03, 3.4196e+03, 3.4057e+03, 3.3911e+03,
        3.3415e+03, 3.3271e+03, 3.3028e+03, 3.2895e+03, 3.2761e+03, 3.2477e+03,
        3.2174e+03, 3.1964e+03, 3.1575e+03, 3.1446e+03, 3.1333e+03, 3.1074e+03,
        3.0842e+03, 3.0818e+03, 3.0617e+03, 3.0532e+03, 3.0278e+03, 2.9832e+03,
        2.9615e+03, 2.9551e+03, 2.9281e+03, 2.9183e+03, 2.8895e+03, 2.8537e+03,
        2.8448e+03, 2.8291e+03, 2.8241e+03, 2.7996e+03, 2.7919e+03, 2.7600e+03,
        2.7486e+03, 2.7201e+03, 2.7133e+03, 2.6876e+03, 2.6663e+03, 2.6603e+03,
        2.6379e+03, 2.6266e+03, 2.6033e+03, 2.5823e+03, 2.5755e+03, 2.5661e+03,
        2.5316e+03, 2.5207e+03, 2.5068e+03, 2.4960e+03, 2.4750e+03, 2.4598e+03,
        2.4492e+03, 2.4354e+03, 2.4233e+03, 2.3968e+03, 2.3801e+03, 2.3669e+03,
        2.3581e+03, 2.3325e+03, 2.3201e+03, 2.3110e+03, 2.2975e+03, 2.2902e+03,
        2.2698e+03, 2.2640e+03, 2.2454e+03, 2.2324e+03, 2.2292e+03, 2.2226e+03,
        2.2052e+03, 2.1873e+03, 2.1722e+03, 2.1663e+03, 2.1581e+03, 2.1312e+03,
        2.1158e+03, 2.1038e+03, 2.0821e+03, 2.0739e+03, 2.0630e+03, 2.0531e+03,
        2.0431e+03, 2.0360e+03, 2.0283e+03, 2.0079e+03, 1.9993e+03, 1.9853e+03,
        1.9812e+03, 1.9701e+03, 1.9527e+03, 1.9330e+03, 1.9238e+03, 1.9221e+03,
        1.9156e+03, 1.8933e+03, 1.8845e+03, 1.8759e+03, 1.8676e+03, 1.8620e+03,
        1.8450e+03, 1.8330e+03, 1.8291e+03, 1.8178e+03, 1.8092e+03, 1.8017e+03,
        1.7960e+03, 1.7808e+03, 1.7657e+03, 1.7631e+03, 1.7541e+03, 1.7378e+03,
        1.7322e+03, 1.7251e+03, 1.7063e+03, 1.7003e+03, 1.6890e+03, 1.6838e+03,
        1.6707e+03, 1.6634e+03, 1.6590e+03, 1.6573e+03, 1.6524e+03, 1.6272e+03,
        1.6214e+03, 1.6097e+03, 1.6066e+03, 1.6015e+03, 1.5907e+03, 1.5836e+03,
        1.5731e+03, 1.5624e+03, 1.5569e+03, 1.5479e+03, 1.5413e+03, 1.5309e+03,
        1.5231e+03, 1.5110e+03, 1.5014e+03, 1.4968e+03, 1.4953e+03, 1.4780e+03,
        1.4748e+03, 1.4683e+03, 1.4535e+03, 1.4508e+03, 1.4449e+03, 1.4291e+03,
        1.4276e+03, 1.4189e+03, 1.4143e+03, 1.4125e+03, 1.3961e+03, 1.3912e+03,
        1.3820e+03, 1.3812e+03, 1.3665e+03, 1.3629e+03, 1.3568e+03, 1.3446e+03,
        1.3390e+03, 1.3354e+03, 1.3331e+03, 1.3238e+03, 1.3154e+03, 1.3011e+03,
        1.2956e+03, 1.2909e+03, 1.2839e+03, 1.2791e+03, 1.2727e+03, 1.2646e+03,
        1.2526e+03, 1.2489e+03, 1.2457e+03, 1.2405e+03, 1.2369e+03, 1.2287e+03,
        1.2251e+03, 1.2161e+03, 1.2151e+03, 1.2070e+03, 1.1976e+03, 1.1854e+03,
        1.1804e+03, 1.1762e+03, 1.1722e+03, 1.1630e+03, 1.1580e+03, 1.1508e+03,
        1.1430e+03, 1.1386e+03, 1.1320e+03, 1.1247e+03, 1.1225e+03, 1.1132e+03,
        1.1070e+03, 1.1026e+03, 1.0895e+03, 1.0853e+03, 1.0754e+03, 1.0740e+03,
        1.0667e+03, 1.0634e+03, 1.0599e+03, 1.0578e+03, 1.0540e+03, 1.0464e+03,
        1.0310e+03, 1.0297e+03, 1.0258e+03, 1.0204e+03, 1.0135e+03, 1.0088e+03,
        1.0067e+03, 9.9301e+02, 9.9091e+02, 9.8580e+02, 9.7944e+02, 9.7217e+02,
        9.6560e+02, 9.5881e+02, 9.5428e+02, 9.5076e+02, 9.4816e+02, 9.3840e+02,
        9.3112e+02, 9.2832e+02, 9.2275e+02, 9.1975e+02, 9.1694e+02, 9.1378e+02,
        9.0852e+02, 9.0694e+02, 9.0143e+02, 8.9194e+02, 8.8743e+02, 8.7789e+02,
        8.7569e+02, 8.6981e+02, 8.6746e+02, 8.5919e+02, 8.5409e+02, 8.4836e+02,
        8.4473e+02, 8.3880e+02, 8.3367e+02, 8.2782e+02, 8.2152e+02, 8.2106e+02,
        8.1109e+02, 8.0970e+02, 8.0456e+02, 8.0114e+02, 7.9138e+02, 7.8768e+02,
        7.8606e+02, 7.8192e+02, 7.7529e+02, 7.7432e+02, 7.6326e+02, 7.6024e+02,
        7.5597e+02, 7.4680e+02, 7.4490e+02, 7.4048e+02, 7.3411e+02, 7.2775e+02,
        7.1844e+02, 7.1361e+02, 7.1217e+02, 7.0875e+02, 7.0467e+02, 7.0042e+02,
        6.8708e+02, 6.8490e+02, 6.8037e+02, 6.7930e+02, 6.7452e+02, 6.7284e+02,
        6.7043e+02, 6.6151e+02, 6.5915e+02, 6.5407e+02, 6.4871e+02, 6.4051e+02,
        6.3809e+02, 6.3420e+02, 6.2545e+02, 6.2308e+02, 6.1732e+02, 6.1358e+02,
        6.1052e+02, 6.0455e+02, 5.9658e+02, 5.9189e+02, 5.9049e+02, 5.8265e+02,
        5.7709e+02, 5.7094e+02, 5.6833e+02, 5.5984e+02, 5.5703e+02, 5.5153e+02,
        5.4408e+02, 5.4044e+02, 5.3768e+02, 5.3264e+02, 5.2949e+02, 5.2383e+02,
        5.2253e+02, 5.1811e+02, 5.0893e+02, 5.0706e+02, 5.0565e+02, 4.9619e+02,
        4.9245e+02, 4.9123e+02, 4.8373e+02, 4.8245e+02, 4.7279e+02, 4.7257e+02,
        4.6615e+02, 4.6459e+02, 4.5784e+02, 4.5422e+02, 4.4398e+02, 4.3622e+02,
        4.3320e+02, 4.2587e+02, 4.2139e+02, 4.1594e+02, 4.1311e+02, 4.0814e+02,
        3.9795e+02, 3.9110e+02, 3.8760e+02, 3.8059e+02, 3.7749e+02, 3.7075e+02,
        3.6328e+02, 3.5924e+02, 3.5020e+02, 3.4039e+02, 3.3254e+02, 3.2629e+02,
        3.1726e+02, 3.0623e+02, 3.0042e+02, 2.8998e+02, 2.6887e+02, 2.5889e+02],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 309]) 

NULL SPACE BASIS :  tensor([[-0.0126,  0.0130, -0.0105,  ...,  0.0108, -0.0045,  0.0110],
        [-0.0034, -0.0244,  0.0202,  ..., -0.0115,  0.0227, -0.0170],
        [-0.0833,  0.0005,  0.0134,  ...,  0.0091, -0.0152,  0.0102],
        ...,
        [ 0.0399, -0.0399, -0.0391,  ..., -0.0156,  0.0338, -0.0021],
        [ 0.0222,  0.0432,  0.0332,  ...,  0.0124, -0.0530, -0.0149],
        [ 0.0277,  0.0166,  0.0311,  ..., -0.0032,  0.0163,  0.0103]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0271, -0.0093, -0.0021,  ...,  0.0011, -0.0005, -0.0007],
        [-0.0093,  0.0319, -0.0097,  ..., -0.0009, -0.0006, -0.0003],
        [-0.0021, -0.0097,  0.0231,  ...,  0.0005, -0.0010, -0.0012],
        ...,
        [ 0.0011, -0.0009,  0.0005,  ...,  0.0323, -0.0088, -0.0044],
        [-0.0005, -0.0006, -0.0010,  ..., -0.0088,  0.0369, -0.0084],
        [-0.0007, -0.0003, -0.0012,  ..., -0.0044, -0.0084,  0.0296]],
       device='cuda:0') 

reserving basis 276/576; cond: 503517.25, radio:0.0021641673520207405
PARAMETER       :  Parameter containing:
tensor([[[[-2.7988e-02, -1.2289e-02, -2.4619e-02],
          [ 3.3034e-02, -1.6332e-02, -2.4109e-02],
          [ 1.6397e-02, -6.6576e-03,  4.5162e-02]],

         [[ 3.5368e-02, -7.9230e-03, -2.6632e-02],
          [ 3.2146e-02,  6.7774e-03,  3.6179e-02],
          [ 8.4004e-03, -2.0456e-02,  2.9004e-03]],

         [[-8.0566e-03, -2.2450e-02, -1.8303e-02],
          [ 2.5491e-02, -4.5672e-02,  2.1365e-02],
          [-3.2400e-02,  3.3589e-03, -1.1778e-02]],

         ...,

         [[-1.7241e-02,  1.5368e-02,  2.5235e-02],
          [ 1.4678e-02, -1.3749e-02, -1.0187e-02],
          [ 1.5108e-02, -1.4511e-02, -3.2840e-02]],

         [[ 2.5435e-03,  3.1917e-02,  4.2615e-02],
          [ 3.7724e-03,  2.4929e-02,  2.3992e-02],
          [ 6.1019e-03,  1.0043e-02,  3.4150e-02]],

         [[-4.0319e-02, -2.2861e-02, -1.3530e-02],
          [-2.1308e-02, -2.2466e-02,  2.3763e-02],
          [-3.5757e-02, -2.5066e-03, -3.7794e-02]]],


        [[[ 5.6554e-03, -1.7578e-02, -5.0421e-02],
          [-1.1756e-02, -2.3397e-02, -4.5182e-02],
          [-5.6269e-03, -1.8203e-02,  1.2189e-02]],

         [[-1.4073e-02, -5.6729e-03,  9.4261e-03],
          [ 1.9906e-02,  3.9793e-02,  2.5630e-02],
          [-1.2281e-02, -3.9329e-03,  8.1543e-03]],

         [[-2.0123e-02,  1.0438e-02, -1.8150e-02],
          [ 2.8325e-02, -3.4214e-02,  3.8448e-02],
          [-1.2593e-02,  7.4428e-03, -6.8485e-04]],

         ...,

         [[ 4.0862e-03,  4.2345e-02, -1.3811e-02],
          [-1.4168e-03, -8.3221e-04, -2.0557e-02],
          [-2.6043e-02, -1.0454e-02, -1.2162e-02]],

         [[-5.1396e-02,  3.4281e-03,  1.4863e-02],
          [ 1.3892e-02, -1.6099e-03, -1.1187e-02],
          [ 4.8194e-02, -9.0299e-03,  4.8718e-04]],

         [[-4.2012e-02,  8.7773e-03,  1.4668e-02],
          [-1.4563e-02,  2.7586e-02, -9.7032e-03],
          [ 2.3015e-02,  3.5992e-02,  3.7432e-02]]],


        [[[-3.8744e-02,  2.2923e-02,  1.3289e-02],
          [ 3.0158e-02,  1.7682e-02,  3.0552e-02],
          [ 3.3237e-02,  1.5881e-02,  2.2784e-02]],

         [[-5.9424e-03,  3.8525e-03, -7.6548e-03],
          [ 2.3972e-02, -2.1525e-02,  2.0312e-02],
          [-8.0732e-03, -3.6930e-02, -2.6338e-02]],

         [[ 4.2469e-02, -1.5583e-02, -9.8193e-03],
          [ 1.0948e-02,  3.6474e-02, -5.3984e-03],
          [-2.1676e-02,  1.2814e-02,  3.8192e-02]],

         ...,

         [[ 1.9409e-02, -2.4591e-02,  2.3634e-02],
          [ 4.3225e-02,  1.2361e-02, -2.0053e-02],
          [ 3.4286e-02,  2.4928e-02, -1.3674e-02]],

         [[ 2.0438e-02, -3.9915e-02, -1.7821e-02],
          [-2.9772e-02,  2.7813e-02,  4.4428e-03],
          [ 3.1178e-03,  2.9809e-02,  2.1539e-02]],

         [[-2.3702e-02,  7.9655e-03,  7.6285e-03],
          [-2.4603e-02,  3.7222e-03, -3.3774e-02],
          [ 4.0458e-03,  1.6742e-02,  4.8584e-03]]],


        ...,


        [[[-1.5105e-02, -2.7496e-02, -3.5584e-02],
          [-8.6463e-04,  1.2106e-02, -4.9529e-02],
          [-1.0408e-02, -9.7157e-03, -3.8554e-02]],

         [[-2.1476e-02,  2.6504e-02,  3.8501e-02],
          [-1.5134e-02, -2.9835e-02, -3.2680e-02],
          [-1.2480e-02,  3.8143e-02,  4.2529e-02]],

         [[-2.5770e-03, -1.8569e-02,  3.9698e-02],
          [-5.7793e-03,  3.0381e-02,  3.9511e-02],
          [ 1.2564e-03,  4.2488e-02,  1.7943e-02]],

         ...,

         [[ 3.0630e-02, -4.3421e-02, -3.5876e-02],
          [ 1.5908e-02,  2.1080e-02, -9.3605e-03],
          [ 1.6725e-03, -4.7600e-03,  8.6768e-03]],

         [[ 2.2621e-02, -8.4324e-05,  8.6956e-03],
          [ 2.6684e-02,  9.6790e-03, -4.0803e-02],
          [ 4.5112e-02,  3.7130e-02, -3.8612e-02]],

         [[ 2.6019e-03,  4.2346e-02, -4.5649e-04],
          [ 3.0765e-02, -3.4143e-02, -6.6407e-03],
          [ 3.1515e-03, -3.3169e-02,  1.8456e-02]]],


        [[[ 3.6684e-02, -1.1720e-02,  4.5733e-02],
          [-4.8078e-02,  1.9770e-02, -1.3985e-02],
          [ 3.0972e-02, -1.3554e-02, -2.0662e-02]],

         [[ 2.6515e-02, -2.4015e-02, -1.2658e-02],
          [ 2.4339e-02,  9.1644e-03, -2.2047e-02],
          [ 5.1698e-02, -2.3735e-02,  4.5795e-02]],

         [[-3.0794e-02, -3.6189e-02,  2.7743e-02],
          [ 1.7066e-02, -3.6717e-02, -5.3098e-02],
          [-3.9029e-02, -7.3009e-04, -1.2157e-02]],

         ...,

         [[-1.9825e-02,  1.9040e-02,  3.2014e-02],
          [-3.5222e-02,  9.5595e-03, -9.0831e-03],
          [ 3.1371e-02, -3.3940e-02,  3.6275e-02]],

         [[-4.8545e-03, -1.6847e-02, -3.2146e-02],
          [ 2.3458e-02, -1.0517e-02,  1.2659e-02],
          [-4.1400e-04,  3.4511e-02,  1.2953e-02]],

         [[-3.0848e-02, -1.4730e-02, -4.1663e-02],
          [ 2.7151e-02, -4.2123e-02, -2.9621e-02],
          [-3.6240e-02,  4.3766e-02,  1.6902e-02]]],


        [[[-1.3063e-03, -4.7577e-03,  1.5697e-02],
          [-3.8799e-02, -5.6709e-03, -7.8829e-03],
          [ 2.4678e-02,  4.5351e-03,  3.6642e-02]],

         [[-2.3358e-02,  2.8057e-02,  1.4945e-02],
          [ 3.9177e-02,  2.6403e-02,  1.8814e-02],
          [-4.9202e-03,  2.3716e-02,  1.2296e-02]],

         [[-1.4590e-02, -2.6695e-02,  3.4576e-02],
          [-3.7009e-02,  2.0048e-02,  1.7003e-02],
          [ 3.0381e-03, -2.7019e-02, -3.3241e-02]],

         ...,

         [[ 5.1429e-02, -2.0040e-02,  2.7442e-02],
          [ 3.3197e-02, -3.9018e-02,  2.7457e-02],
          [-8.1176e-03, -4.1769e-02, -1.9719e-02]],

         [[-1.6071e-02, -8.3414e-04, -1.8528e-02],
          [-3.0948e-02, -4.8425e-02, -3.8918e-02],
          [ 2.2207e-02,  1.8919e-02, -1.4420e-02]],

         [[-8.8199e-03, -1.3709e-02,  4.7393e-02],
          [ 2.3533e-02,  1.8204e-02,  2.3968e-03],
          [ 1.1769e-02,  3.0732e-02, -1.2547e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.4252e+07, 6.9678e+05, 6.3714e+05, 4.0745e+05, 2.9214e+05, 1.6935e+05,
        1.5779e+05, 1.0652e+05, 9.0452e+04, 7.4590e+04, 4.7702e+04, 4.1350e+04,
        3.8296e+04, 3.6172e+04, 3.2610e+04, 3.0245e+04, 2.9509e+04, 2.8049e+04,
        2.5926e+04, 2.3421e+04, 2.2727e+04, 2.0349e+04, 1.9731e+04, 1.7758e+04,
        1.7211e+04, 1.6372e+04, 1.5243e+04, 1.4652e+04, 1.3544e+04, 1.2896e+04,
        1.2164e+04, 1.1768e+04, 1.1421e+04, 1.0459e+04, 9.8111e+03, 9.5722e+03,
        8.8378e+03, 8.4393e+03, 7.9027e+03, 7.5394e+03, 6.8876e+03, 6.6182e+03,
        6.4935e+03, 6.4271e+03, 6.2622e+03, 6.0782e+03, 5.8078e+03, 5.4419e+03,
        5.3218e+03, 5.2392e+03, 5.1159e+03, 4.7183e+03, 4.5050e+03, 4.4142e+03,
        4.3381e+03, 4.2418e+03, 4.0295e+03, 3.9484e+03, 3.8928e+03, 3.7993e+03,
        3.6976e+03, 3.6596e+03, 3.6120e+03, 3.5560e+03, 3.5016e+03, 3.3257e+03,
        3.2743e+03, 3.1966e+03, 3.1382e+03, 3.0436e+03, 2.9404e+03, 2.8918e+03,
        2.8374e+03, 2.7854e+03, 2.7392e+03, 2.6844e+03, 2.6356e+03, 2.5542e+03,
        2.5030e+03, 2.4653e+03, 2.4277e+03, 2.3804e+03, 2.3514e+03, 2.2298e+03,
        2.1877e+03, 2.1710e+03, 2.1047e+03, 2.0867e+03, 2.0438e+03, 2.0100e+03,
        1.9839e+03, 1.9803e+03, 1.9374e+03, 1.9189e+03, 1.8990e+03, 1.8553e+03,
        1.8120e+03, 1.7839e+03, 1.7421e+03, 1.7108e+03, 1.7065e+03, 1.6774e+03,
        1.6079e+03, 1.5899e+03, 1.5768e+03, 1.5468e+03, 1.5455e+03, 1.5094e+03,
        1.4906e+03, 1.4771e+03, 1.4659e+03, 1.4434e+03, 1.4088e+03, 1.3948e+03,
        1.3729e+03, 1.3471e+03, 1.3328e+03, 1.3119e+03, 1.2947e+03, 1.2717e+03,
        1.2648e+03, 1.2562e+03, 1.2396e+03, 1.2214e+03, 1.1995e+03, 1.1943e+03,
        1.1736e+03, 1.1688e+03, 1.1654e+03, 1.1486e+03, 1.1291e+03, 1.1251e+03,
        1.1046e+03, 1.0884e+03, 1.0859e+03, 1.0703e+03, 1.0638e+03, 1.0433e+03,
        1.0291e+03, 1.0183e+03, 1.0120e+03, 1.0042e+03, 9.8166e+02, 9.6554e+02,
        9.6133e+02, 9.5268e+02, 9.3874e+02, 9.2580e+02, 9.2511e+02, 9.1767e+02,
        9.0182e+02, 8.8988e+02, 8.7718e+02, 8.7175e+02, 8.6186e+02, 8.5476e+02,
        8.4488e+02, 8.3976e+02, 8.3188e+02, 8.2848e+02, 8.0896e+02, 8.0769e+02,
        8.0153e+02, 7.9112e+02, 7.8800e+02, 7.7164e+02, 7.6173e+02, 7.5549e+02,
        7.5367e+02, 7.4561e+02, 7.4258e+02, 7.3571e+02, 7.2920e+02, 7.2112e+02,
        7.1291e+02, 7.0415e+02, 7.0068e+02, 6.9290e+02, 6.8902e+02, 6.7944e+02,
        6.7495e+02, 6.6730e+02, 6.6345e+02, 6.5733e+02, 6.4900e+02, 6.4643e+02,
        6.4061e+02, 6.3348e+02, 6.2763e+02, 6.2623e+02, 6.1304e+02, 6.1055e+02,
        6.0425e+02, 5.9889e+02, 5.9287e+02, 5.9182e+02, 5.8437e+02, 5.7731e+02,
        5.6917e+02, 5.6559e+02, 5.6500e+02, 5.6076e+02, 5.5878e+02, 5.5629e+02,
        5.4562e+02, 5.4385e+02, 5.4151e+02, 5.3783e+02, 5.3038e+02, 5.2724e+02,
        5.1912e+02, 5.1454e+02, 5.1352e+02, 5.1162e+02, 5.0877e+02, 5.0418e+02,
        4.9969e+02, 4.9745e+02, 4.9474e+02, 4.9043e+02, 4.8975e+02, 4.8831e+02,
        4.8503e+02, 4.8156e+02, 4.7560e+02, 4.7231e+02, 4.6937e+02, 4.6401e+02,
        4.5956e+02, 4.5720e+02, 4.5055e+02, 4.4791e+02, 4.4576e+02, 4.4331e+02,
        4.3656e+02, 4.3311e+02, 4.3254e+02, 4.3009e+02, 4.2605e+02, 4.2495e+02,
        4.2128e+02, 4.2107e+02, 4.1632e+02, 4.1368e+02, 4.1059e+02, 4.0807e+02,
        4.0190e+02, 3.9846e+02, 3.9690e+02, 3.9457e+02, 3.9083e+02, 3.9020e+02,
        3.8408e+02, 3.8209e+02, 3.7932e+02, 3.7766e+02, 3.7721e+02, 3.7194e+02,
        3.6961e+02, 3.6693e+02, 3.6295e+02, 3.6130e+02, 3.5662e+02, 3.5580e+02,
        3.5405e+02, 3.4993e+02, 3.4807e+02, 3.4488e+02, 3.4405e+02, 3.4210e+02,
        3.4096e+02, 3.4004e+02, 3.3838e+02, 3.3511e+02, 3.3356e+02, 3.3196e+02,
        3.2887e+02, 3.2633e+02, 3.2447e+02, 3.2275e+02, 3.2079e+02, 3.1861e+02,
        3.1716e+02, 3.1545e+02, 3.1417e+02, 3.0954e+02, 3.0854e+02, 3.0718e+02,
        3.0562e+02, 3.0373e+02, 3.0272e+02, 2.9973e+02, 2.9814e+02, 2.9686e+02,
        2.9342e+02, 2.9175e+02, 2.8991e+02, 2.8729e+02, 2.8583e+02, 2.8435e+02,
        2.8237e+02, 2.8195e+02, 2.8035e+02, 2.7902e+02, 2.7685e+02, 2.7439e+02,
        2.7317e+02, 2.7147e+02, 2.7096e+02, 2.6921e+02, 2.6738e+02, 2.6577e+02,
        2.6532e+02, 2.6270e+02, 2.6119e+02, 2.5957e+02, 2.5771e+02, 2.5690e+02,
        2.5401e+02, 2.5362e+02, 2.5228e+02, 2.5147e+02, 2.5037e+02, 2.4857e+02,
        2.4748e+02, 2.4602e+02, 2.4515e+02, 2.4101e+02, 2.3988e+02, 2.3787e+02,
        2.3741e+02, 2.3630e+02, 2.3489e+02, 2.3333e+02, 2.3287e+02, 2.3150e+02,
        2.2926e+02, 2.2866e+02, 2.2599e+02, 2.2534e+02, 2.2492e+02, 2.2376e+02,
        2.2103e+02, 2.1943e+02, 2.1856e+02, 2.1688e+02, 2.1501e+02, 2.1472e+02,
        2.1310e+02, 2.1194e+02, 2.1116e+02, 2.1084e+02, 2.0765e+02, 2.0566e+02,
        2.0488e+02, 2.0373e+02, 2.0294e+02, 2.0283e+02, 2.0138e+02, 2.0068e+02,
        1.9950e+02, 1.9844e+02, 1.9779e+02, 1.9649e+02, 1.9572e+02, 1.9454e+02,
        1.9328e+02, 1.9209e+02, 1.9098e+02, 1.9027e+02, 1.8898e+02, 1.8840e+02,
        1.8646e+02, 1.8560e+02, 1.8439e+02, 1.8205e+02, 1.8094e+02, 1.7987e+02,
        1.7976e+02, 1.7853e+02, 1.7814e+02, 1.7757e+02, 1.7632e+02, 1.7518e+02,
        1.7440e+02, 1.7369e+02, 1.7323e+02, 1.7241e+02, 1.7159e+02, 1.7084e+02,
        1.6986e+02, 1.6778e+02, 1.6741e+02, 1.6603e+02, 1.6581e+02, 1.6481e+02,
        1.6417e+02, 1.6276e+02, 1.6214e+02, 1.6142e+02, 1.6082e+02, 1.5911e+02,
        1.5790e+02, 1.5703e+02, 1.5662e+02, 1.5565e+02, 1.5355e+02, 1.5316e+02,
        1.5310e+02, 1.5116e+02, 1.5042e+02, 1.4969e+02, 1.4937e+02, 1.4903e+02,
        1.4830e+02, 1.4756e+02, 1.4709e+02, 1.4593e+02, 1.4518e+02, 1.4446e+02,
        1.4353e+02, 1.4300e+02, 1.4180e+02, 1.4123e+02, 1.4098e+02, 1.4056e+02,
        1.3846e+02, 1.3829e+02, 1.3717e+02, 1.3703e+02, 1.3587e+02, 1.3433e+02,
        1.3381e+02, 1.3354e+02, 1.3311e+02, 1.3284e+02, 1.3201e+02, 1.3129e+02,
        1.3032e+02, 1.2981e+02, 1.2878e+02, 1.2867e+02, 1.2799e+02, 1.2740e+02,
        1.2657e+02, 1.2585e+02, 1.2573e+02, 1.2472e+02, 1.2441e+02, 1.2245e+02,
        1.2193e+02, 1.2136e+02, 1.2060e+02, 1.2033e+02, 1.1871e+02, 1.1829e+02,
        1.1784e+02, 1.1608e+02, 1.1599e+02, 1.1562e+02, 1.1489e+02, 1.1398e+02,
        1.1342e+02, 1.1297e+02, 1.1212e+02, 1.1110e+02, 1.1024e+02, 1.1012e+02,
        1.0959e+02, 1.0887e+02, 1.0846e+02, 1.0761e+02, 1.0703e+02, 1.0619e+02,
        1.0557e+02, 1.0439e+02, 1.0409e+02, 1.0311e+02, 1.0289e+02, 1.0232e+02,
        1.0168e+02, 1.0074e+02, 1.0014e+02, 9.9157e+01, 9.8852e+01, 9.7887e+01,
        9.7339e+01, 9.6127e+01, 9.5456e+01, 9.4626e+01, 9.4562e+01, 9.3746e+01,
        9.2799e+01, 9.1661e+01, 9.1329e+01, 9.0793e+01, 9.0260e+01, 8.9300e+01,
        8.8628e+01, 8.8035e+01, 8.7461e+01, 8.6888e+01, 8.6136e+01, 8.5916e+01,
        8.4876e+01, 8.4668e+01, 8.3864e+01, 8.3027e+01, 8.2109e+01, 8.1818e+01,
        8.1303e+01, 8.1181e+01, 8.0534e+01, 7.9669e+01, 7.8563e+01, 7.8389e+01,
        7.8082e+01, 7.7253e+01, 7.6768e+01, 7.6518e+01, 7.5196e+01, 7.4921e+01,
        7.4603e+01, 7.4056e+01, 7.3215e+01, 7.2970e+01, 7.2238e+01, 7.1505e+01,
        7.0981e+01, 7.0307e+01, 6.9495e+01, 6.9123e+01, 6.8621e+01, 6.8229e+01,
        6.8035e+01, 6.7512e+01, 6.6788e+01, 6.5629e+01, 6.5084e+01, 6.4921e+01,
        6.4164e+01, 6.3048e+01, 6.2856e+01, 6.2368e+01, 6.1253e+01, 6.0614e+01,
        5.9921e+01, 5.9234e+01, 5.8541e+01, 5.8474e+01, 5.8197e+01, 5.5878e+01,
        5.5514e+01, 5.5138e+01, 5.4441e+01, 5.2823e+01, 5.2186e+01, 5.1791e+01,
        5.1362e+01, 5.0121e+01, 4.8574e+01, 4.8241e+01, 4.6381e+01, 4.5469e+01,
        4.5325e+01, 4.3672e+01, 4.2674e+01, 4.2386e+01, 4.1928e+01, 4.0217e+01,
        3.9026e+01, 3.8754e+01, 3.7491e+01, 3.6599e+01, 3.2048e+01, 2.8305e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 276]) 

NULL SPACE BASIS :  tensor([[ 0.0206,  0.0388,  0.0307,  ..., -0.0033,  0.0004,  0.0121],
        [ 0.0098, -0.0222, -0.0177,  ...,  0.0047,  0.0027, -0.0130],
        [-0.0018, -0.0338, -0.0020,  ..., -0.0044, -0.0061,  0.0026],
        ...,
        [-0.0672,  0.1020,  0.0307,  ..., -0.0073,  0.0044,  0.0021],
        [ 0.0074, -0.0299,  0.0425,  ...,  0.0027, -0.0003,  0.0147],
        [-0.0833, -0.0436, -0.0633,  ..., -0.0022,  0.0011, -0.0129]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.9592e-02, -1.6873e-02,  2.5107e-03,  ..., -1.3039e-03,
          1.3113e-03, -2.4036e-04],
        [-1.6873e-02,  3.0937e-02, -1.4355e-02,  ...,  3.1922e-05,
         -5.7870e-04, -4.8639e-04],
        [ 2.5107e-03, -1.4355e-02,  1.6418e-02,  ...,  6.7390e-04,
         -1.4863e-05,  2.8015e-04],
        ...,
        [-1.3039e-03,  3.1922e-05,  6.7390e-04,  ...,  1.3430e-02,
         -7.5740e-03, -6.1746e-04],
        [ 1.3113e-03, -5.7870e-04, -1.4863e-05,  ..., -7.5740e-03,
          1.7049e-02, -6.4945e-03],
        [-2.4036e-04, -4.8639e-04,  2.8015e-04,  ..., -6.1746e-04,
         -6.4945e-03,  1.2823e-02]], device='cuda:0') 

reserving basis 741/1152; cond: 351047.0625, radio:0.007952868938446045
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0150,  0.0304, -0.0180],
          [-0.0003, -0.0164,  0.0101],
          [ 0.0187,  0.0189, -0.0030]],

         [[-0.0187, -0.0228, -0.0258],
          [-0.0230, -0.0229,  0.0069],
          [-0.0157,  0.0088, -0.0284]],

         [[ 0.0009, -0.0138,  0.0043],
          [ 0.0081,  0.0153, -0.0155],
          [-0.0150, -0.0258,  0.0271]],

         ...,

         [[-0.0128,  0.0018,  0.0053],
          [-0.0300, -0.0204, -0.0014],
          [ 0.0070,  0.0155,  0.0288]],

         [[ 0.0099, -0.0334, -0.0184],
          [ 0.0235,  0.0159,  0.0004],
          [ 0.0156, -0.0150,  0.0342]],

         [[ 0.0195,  0.0067,  0.0158],
          [-0.0222, -0.0331, -0.0258],
          [-0.0289,  0.0194, -0.0152]]],


        [[[ 0.0252,  0.0011,  0.0080],
          [-0.0149, -0.0119,  0.0226],
          [-0.0264, -0.0289, -0.0146]],

         [[ 0.0083, -0.0235, -0.0228],
          [ 0.0091, -0.0124,  0.0014],
          [-0.0271,  0.0061, -0.0131]],

         [[ 0.0003, -0.0180, -0.0197],
          [-0.0047,  0.0135, -0.0302],
          [ 0.0253,  0.0174,  0.0015]],

         ...,

         [[ 0.0047,  0.0137, -0.0041],
          [-0.0212,  0.0256,  0.0264],
          [ 0.0150,  0.0201,  0.0210]],

         [[ 0.0248,  0.0018,  0.0073],
          [-0.0077, -0.0213, -0.0266],
          [ 0.0420, -0.0191, -0.0294]],

         [[-0.0141, -0.0174, -0.0318],
          [-0.0221, -0.0127, -0.0075],
          [ 0.0082, -0.0103, -0.0013]]],


        [[[-0.0168,  0.0189, -0.0284],
          [-0.0046,  0.0178, -0.0023],
          [ 0.0004,  0.0281,  0.0166]],

         [[ 0.0048, -0.0122,  0.0242],
          [-0.0042, -0.0094,  0.0050],
          [-0.0211,  0.0049,  0.0092]],

         [[-0.0171, -0.0151, -0.0130],
          [-0.0257, -0.0269, -0.0310],
          [-0.0125, -0.0176,  0.0167]],

         ...,

         [[ 0.0255,  0.0081, -0.0176],
          [-0.0310,  0.0151,  0.0044],
          [ 0.0198, -0.0165, -0.0326]],

         [[-0.0405,  0.0024, -0.0114],
          [-0.0169,  0.0170,  0.0081],
          [-0.0139,  0.0077, -0.0144]],

         [[-0.0140, -0.0192,  0.0107],
          [-0.0044, -0.0100,  0.0053],
          [ 0.0106, -0.0179,  0.0306]]],


        ...,


        [[[ 0.0276,  0.0009,  0.0317],
          [ 0.0244,  0.0172, -0.0299],
          [ 0.0036,  0.0003,  0.0280]],

         [[ 0.0090, -0.0084,  0.0122],
          [ 0.0048, -0.0252,  0.0045],
          [-0.0153, -0.0076,  0.0159]],

         [[ 0.0170,  0.0127, -0.0097],
          [-0.0136,  0.0276,  0.0058],
          [ 0.0060, -0.0071, -0.0139]],

         ...,

         [[-0.0291,  0.0142, -0.0149],
          [ 0.0073, -0.0282, -0.0157],
          [-0.0192,  0.0031,  0.0080]],

         [[ 0.0193,  0.0180,  0.0282],
          [-0.0216,  0.0007,  0.0159],
          [-0.0052,  0.0104,  0.0047]],

         [[-0.0149, -0.0030,  0.0235],
          [ 0.0160,  0.0092,  0.0310],
          [ 0.0074,  0.0097,  0.0197]]],


        [[[-0.0033, -0.0117,  0.0051],
          [-0.0283, -0.0086,  0.0197],
          [ 0.0108, -0.0273, -0.0223]],

         [[-0.0129, -0.0155,  0.0243],
          [-0.0197, -0.0349, -0.0047],
          [-0.0245, -0.0006, -0.0224]],

         [[-0.0168, -0.0029, -0.0213],
          [-0.0119,  0.0147,  0.0226],
          [ 0.0084, -0.0199,  0.0199]],

         ...,

         [[-0.0138,  0.0119,  0.0053],
          [-0.0289, -0.0208, -0.0136],
          [-0.0227, -0.0499, -0.0170]],

         [[-0.0022, -0.0065, -0.0125],
          [-0.0302,  0.0310,  0.0302],
          [-0.0366,  0.0229, -0.0078]],

         [[ 0.0204, -0.0236, -0.0093],
          [ 0.0286, -0.0169,  0.0317],
          [-0.0059,  0.0172,  0.0005]]],


        [[[ 0.0045, -0.0274,  0.0283],
          [-0.0243,  0.0042, -0.0365],
          [-0.0010, -0.0275, -0.0179]],

         [[ 0.0158,  0.0152, -0.0155],
          [ 0.0045,  0.0141,  0.0094],
          [-0.0162, -0.0265,  0.0236]],

         [[-0.0095, -0.0091,  0.0066],
          [-0.0193,  0.0004,  0.0254],
          [-0.0169,  0.0248, -0.0044]],

         ...,

         [[ 0.0274,  0.0103, -0.0082],
          [-0.0168,  0.0304,  0.0144],
          [ 0.0283, -0.0109, -0.0013]],

         [[-0.0153,  0.0104, -0.0042],
          [-0.0224,  0.0319,  0.0282],
          [ 0.0165,  0.0128,  0.0073]],

         [[ 0.0106,  0.0383, -0.0186],
          [-0.0147,  0.0124,  0.0318],
          [ 0.0042, -0.0257, -0.0188]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.7693e+07, 1.0663e+06, 1.0155e+06,  ..., 1.0630e+02, 9.3497e+01,
        7.8886e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 741]) 

NULL SPACE BASIS :  tensor([[-4.1636e-02, -2.0324e-02,  1.5304e-02,  ...,  1.8505e-02,
         -1.3020e-02,  8.0534e-03],
        [ 1.9102e-02,  2.4901e-02, -1.7186e-02,  ..., -6.7785e-03,
          1.5468e-02, -2.4537e-03],
        [ 1.7617e-02,  1.3236e-02, -2.1508e-05,  ..., -1.4198e-02,
         -3.5264e-03, -2.0050e-03],
        ...,
        [ 3.5696e-02, -4.5080e-02, -2.3032e-02,  ...,  2.2770e-03,
          4.8831e-03, -5.8372e-03],
        [ 4.4117e-02,  3.0937e-02, -5.8536e-02,  ...,  5.8066e-03,
         -1.6851e-02, -7.2821e-03],
        [-1.1981e-02, -8.6031e-03,  2.2095e-02,  ..., -2.1723e-02,
          1.4980e-02,  1.0758e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.9224e-02, -1.2673e-03, -5.4767e-04,  ..., -4.3418e-04,
          4.3994e-04,  4.8292e-05],
        [-1.2673e-03,  3.0223e-02, -8.1501e-04,  ...,  6.5560e-05,
          1.7530e-04,  3.5915e-04],
        [-5.4767e-04, -8.1501e-04,  2.9793e-02,  ...,  7.1508e-05,
          3.7067e-04,  2.0705e-04],
        ...,
        [-4.3418e-04,  6.5560e-05,  7.1508e-05,  ...,  2.2691e-02,
         -2.8990e-03, -1.3007e-03],
        [ 4.3994e-04,  1.7530e-04,  3.7067e-04,  ..., -2.8990e-03,
          2.3630e-02, -2.8558e-03],
        [ 4.8292e-05,  3.5915e-04,  2.0705e-04,  ..., -1.3007e-03,
         -2.8558e-03,  2.4654e-02]], device='cuda:0') 

reserving basis 44/64; cond: 7232.775390625, radio:0.017772873863577843
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0160]],

         [[ 0.1011]],

         [[ 0.0599]],

         ...,

         [[-0.0859]],

         [[-0.0898]],

         [[-0.0441]]],


        [[[ 0.0286]],

         [[-0.0224]],

         [[-0.0908]],

         ...,

         [[-0.0435]],

         [[ 0.0219]],

         [[ 0.0282]]],


        [[[ 0.0308]],

         [[-0.0627]],

         [[-0.0045]],

         ...,

         [[-0.1411]],

         [[ 0.0931]],

         [[ 0.0229]]],


        ...,


        [[[ 0.0284]],

         [[-0.0808]],

         [[-0.1022]],

         ...,

         [[-0.0308]],

         [[ 0.0554]],

         [[-0.0131]]],


        [[[ 0.0189]],

         [[-0.0529]],

         [[ 0.1286]],

         ...,

         [[ 0.0007]],

         [[ 0.0319]],

         [[ 0.0623]]],


        [[[ 0.0041]],

         [[ 0.0019]],

         [[ 0.0902]],

         ...,

         [[ 0.0003]],

         [[ 0.0587]],

         [[ 0.0876]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.7881e+06, 9.6806e+04, 8.1098e+04, 3.7646e+04, 2.5955e+04, 2.0839e+04,
        1.4687e+04, 1.1939e+04, 7.8130e+03, 6.9102e+03, 6.2645e+03, 6.0701e+03,
        5.0790e+03, 4.5883e+03, 4.1651e+03, 3.9868e+03, 3.5227e+03, 3.4853e+03,
        3.0030e+03, 2.8824e+03, 2.3949e+03, 2.2071e+03, 1.9496e+03, 1.8376e+03,
        1.7550e+03, 1.7388e+03, 1.6208e+03, 1.3929e+03, 1.3556e+03, 1.2694e+03,
        1.1705e+03, 1.1129e+03, 1.1019e+03, 1.0368e+03, 9.7566e+02, 9.3018e+02,
        8.9456e+02, 8.6688e+02, 8.2936e+02, 7.8258e+02, 7.6766e+02, 7.3578e+02,
        6.8987e+02, 6.6543e+02, 6.0773e+02, 6.0070e+02, 5.6103e+02, 5.2482e+02,
        5.0872e+02, 5.0705e+02, 4.8689e+02, 4.7415e+02, 4.5578e+02, 4.3802e+02,
        4.1859e+02, 3.9552e+02, 3.8500e+02, 3.6316e+02, 3.4318e+02, 3.3697e+02,
        3.1368e+02, 2.9193e+02, 2.8593e+02, 2.4722e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 44]) 

NULL SPACE BASIS :  tensor([[-0.2029,  0.1081,  0.2153,  ..., -0.0034,  0.1539, -0.1007],
        [-0.0453,  0.0855, -0.0686,  ..., -0.0508, -0.0280, -0.0460],
        [-0.0254,  0.1302,  0.0218,  ...,  0.2665, -0.2237,  0.0462],
        ...,
        [-0.1274, -0.1151, -0.0271,  ...,  0.0222, -0.0145,  0.0197],
        [ 0.0541, -0.0990,  0.0681,  ...,  0.0094,  0.0313, -0.0308],
        [ 0.2300, -0.0340,  0.0098,  ...,  0.0111, -0.0008,  0.0302]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0977,  0.0035,  0.0190,  ...,  0.0012, -0.0034, -0.0131],
        [ 0.0035,  0.1228, -0.0107,  ...,  0.0110, -0.0095, -0.0038],
        [ 0.0190, -0.0107,  0.1021,  ..., -0.0064,  0.0025,  0.0054],
        ...,
        [ 0.0012,  0.0110, -0.0064,  ...,  0.0356, -0.0323, -0.0026],
        [-0.0034, -0.0095,  0.0025,  ..., -0.0323,  0.1141,  0.0036],
        [-0.0131, -0.0038,  0.0054,  ..., -0.0026,  0.0036,  0.0843]],
       device='cuda:0') 

reserving basis 787/1152; cond: 296955.0625, radio:0.009427404962480068
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1664e-02, -3.4824e-03,  9.8111e-03],
          [-2.9234e-03,  1.6726e-02,  1.1645e-02],
          [-1.0709e-02, -1.2409e-02, -1.3218e-02]],

         [[ 4.9718e-03, -1.6559e-02, -2.2089e-02],
          [ 2.6875e-02, -1.5115e-03,  5.1501e-04],
          [ 7.6700e-03,  1.9895e-02, -1.6452e-02]],

         [[ 6.2040e-03, -2.3597e-05,  6.2778e-03],
          [ 1.5542e-02,  1.7285e-02,  1.6447e-02],
          [-2.4209e-02, -1.3796e-02,  1.3209e-02]],

         ...,

         [[ 1.6252e-02,  6.2133e-03, -8.3078e-04],
          [-1.1237e-02,  2.7106e-02,  5.7630e-03],
          [ 1.5181e-02, -1.9865e-03, -7.2948e-03]],

         [[ 8.5618e-03,  1.1773e-02,  6.9503e-03],
          [ 2.3729e-03,  2.2788e-02,  2.1060e-02],
          [ 2.3655e-02,  2.7534e-02, -6.0315e-03]],

         [[ 1.2781e-03,  2.6966e-02,  4.7128e-02],
          [-1.8187e-02,  1.4232e-02,  2.0305e-02],
          [ 6.7390e-03, -2.0379e-02,  1.7300e-02]]],


        [[[-1.0540e-02,  7.6332e-03, -2.1704e-02],
          [ 1.3587e-02, -7.6000e-03,  4.3810e-03],
          [ 2.7516e-03, -2.4165e-02,  1.9415e-02]],

         [[-2.7597e-03,  1.7902e-02,  2.5340e-02],
          [ 2.1128e-02,  1.6914e-02, -1.0597e-04],
          [-9.6836e-03,  1.6587e-02,  1.8114e-02]],

         [[ 1.8676e-02,  1.6863e-02,  8.5638e-03],
          [ 2.6256e-02, -5.8121e-03,  2.0975e-02],
          [-1.6168e-02,  1.5569e-03,  9.1599e-03]],

         ...,

         [[-3.0279e-03,  1.6321e-02, -1.7791e-02],
          [-2.6795e-02, -2.2552e-02, -3.1191e-02],
          [-2.1613e-02, -2.2660e-02, -2.4177e-02]],

         [[-2.1990e-02,  1.8359e-03,  3.0738e-02],
          [-1.5337e-02, -1.7440e-02, -2.5450e-03],
          [ 1.4539e-02, -1.0598e-02, -1.7866e-02]],

         [[ 2.2848e-02, -2.3920e-02,  1.3475e-02],
          [-7.8591e-03, -1.3972e-02, -2.8366e-02],
          [ 2.0866e-03,  1.5692e-02, -3.2118e-02]]],


        [[[ 1.8383e-02,  2.0186e-02,  1.4246e-02],
          [-6.2941e-03, -2.9608e-02,  2.8470e-02],
          [-1.2099e-02, -1.1325e-03, -2.4693e-02]],

         [[-1.0032e-02,  6.2738e-03, -1.4855e-02],
          [ 3.2472e-02,  6.8079e-03, -4.8367e-03],
          [-6.7096e-03, -1.9030e-02,  1.8416e-02]],

         [[-3.1224e-02, -1.5687e-02, -2.3130e-02],
          [-1.5851e-02, -1.4394e-02,  2.1153e-03],
          [ 2.3963e-02, -1.2768e-02, -7.4174e-03]],

         ...,

         [[-3.5091e-02, -7.1110e-03, -2.1404e-02],
          [-6.6478e-03, -3.4513e-02, -2.6288e-02],
          [-3.1542e-03, -2.7333e-02, -3.9667e-04]],

         [[ 2.4686e-02, -1.3412e-02,  6.4227e-03],
          [ 1.9209e-02, -1.7762e-02, -1.9230e-02],
          [-3.0291e-03, -2.9808e-02,  1.0878e-02]],

         [[ 1.0917e-02,  1.3937e-02, -6.6874e-04],
          [ 6.6476e-03,  1.0470e-02, -1.1178e-02],
          [ 4.1312e-03,  2.3900e-02, -1.2239e-02]]],


        ...,


        [[[-1.5019e-02, -2.3979e-02, -3.4721e-02],
          [-9.9978e-03, -1.5530e-02,  1.8028e-03],
          [ 1.5701e-02,  6.9762e-03,  2.3296e-02]],

         [[-3.1490e-02, -7.0654e-03, -2.5144e-02],
          [-1.3058e-02,  1.5365e-02,  1.1299e-02],
          [-1.2671e-02,  3.1466e-02,  2.8344e-02]],

         [[ 2.9724e-02,  6.4601e-03,  1.0899e-03],
          [-1.6389e-02,  2.5061e-03, -2.7926e-02],
          [ 3.1700e-02,  2.3500e-02,  8.8698e-03]],

         ...,

         [[-2.1969e-02, -2.6847e-02,  7.4728e-03],
          [ 1.6653e-02, -1.4053e-02,  1.6623e-02],
          [-1.1134e-02,  3.4764e-02, -5.4916e-03]],

         [[ 1.1061e-02,  3.5209e-02,  1.1506e-02],
          [-1.0896e-02, -1.8798e-02,  2.4985e-02],
          [ 1.7046e-02, -1.5692e-03, -4.4568e-03]],

         [[ 3.7424e-02,  7.8961e-03,  1.7415e-02],
          [ 2.0766e-02, -1.4952e-02,  1.5973e-02],
          [-1.1361e-02, -3.3588e-02, -3.7518e-02]]],


        [[[-2.7820e-02, -1.4850e-03,  1.4260e-02],
          [ 2.1083e-02,  1.4767e-02, -2.0359e-02],
          [ 2.9301e-02,  2.5662e-02, -1.7889e-03]],

         [[ 8.0678e-03, -3.7846e-03,  1.9334e-03],
          [ 2.0516e-02,  3.9219e-02,  1.9342e-02],
          [ 2.3238e-02,  3.8459e-02, -1.0981e-02]],

         [[-1.9633e-02, -1.6910e-02, -2.8424e-02],
          [-1.7063e-02, -1.6496e-03, -9.4082e-03],
          [-1.7987e-03,  5.4621e-03, -2.7789e-02]],

         ...,

         [[-2.6655e-02,  1.5987e-02, -2.5030e-02],
          [ 1.6559e-02, -1.1063e-02, -5.8672e-03],
          [-1.0562e-02,  2.7477e-02,  1.2857e-02]],

         [[-2.7870e-02,  2.3956e-03,  1.4664e-02],
          [-3.2753e-03, -4.9662e-03,  3.4831e-03],
          [ 2.0628e-02,  2.3585e-02,  2.5594e-03]],

         [[-2.8151e-02, -7.4744e-03,  9.4258e-03],
          [-2.5961e-02,  1.2654e-02,  2.7858e-02],
          [-2.9628e-02,  9.5173e-04,  1.4863e-02]]],


        [[[-1.5794e-02,  1.5157e-02, -2.8683e-03],
          [-3.4676e-03,  1.1793e-02,  4.7024e-03],
          [-5.0973e-03, -1.9232e-03,  1.9091e-02]],

         [[-8.4211e-04, -2.1516e-02, -1.8927e-02],
          [-2.4284e-02, -1.2852e-02, -3.8667e-02],
          [ 3.8332e-02,  1.5580e-02,  3.0804e-03]],

         [[ 2.9304e-02,  6.7108e-04, -9.4331e-03],
          [-1.7442e-02, -1.3990e-02, -1.5892e-02],
          [ 2.6420e-02, -7.3506e-03, -2.0961e-02]],

         ...,

         [[ 9.9857e-03, -5.5996e-03,  9.1962e-03],
          [-2.0718e-02, -1.1640e-02,  5.5294e-03],
          [ 1.3997e-02,  1.7950e-04, -1.2520e-02]],

         [[-1.6489e-02, -1.0257e-02,  2.7123e-03],
          [ 1.6926e-02,  1.0629e-02, -3.1485e-02],
          [-1.6847e-02, -6.6245e-03, -9.6268e-03]],

         [[-2.8747e-03, -3.0133e-02, -1.5696e-02],
          [ 3.0546e-03,  2.3354e-02, -2.7215e-02],
          [ 2.5656e-02,  1.2106e-02,  2.1393e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.6803e+07, 1.0418e+06, 9.9151e+05,  ..., 1.1060e+02, 1.0874e+02,
        9.0259e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 787]) 

NULL SPACE BASIS :  tensor([[ 2.0781e-02, -3.5719e-02, -3.8742e-02,  ...,  1.1646e-02,
         -2.4573e-03, -1.8492e-02],
        [-7.2309e-03, -2.6761e-02,  3.8143e-03,  ..., -2.4273e-02,
          4.9660e-03,  1.7091e-02],
        [-1.5939e-02, -2.0543e-02,  3.4019e-02,  ...,  5.1387e-03,
         -8.4633e-03, -7.2523e-03],
        ...,
        [-3.3074e-02,  4.7498e-02,  2.8459e-03,  ..., -4.6219e-03,
          8.2886e-04,  2.7154e-03],
        [-4.6520e-02, -3.4587e-03,  1.6345e-02,  ..., -8.7134e-05,
          4.5758e-03, -4.4954e-03],
        [-6.8307e-02,  3.9382e-02,  2.8580e-02,  ...,  2.5906e-03,
         -4.6140e-03, -2.3410e-04]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2180e-02, -5.4469e-03, -2.4619e-03,  ..., -5.4583e-04,
         -5.8488e-04,  2.1941e-04],
        [-5.4469e-03,  2.3062e-02, -5.1171e-03,  ..., -8.6027e-05,
          2.9461e-05, -1.3417e-04],
        [-2.4619e-03, -5.1171e-03,  2.1738e-02,  ..., -7.1469e-05,
         -7.7070e-06,  1.3270e-04],
        ...,
        [-5.4583e-04, -8.6027e-05, -7.1469e-05,  ...,  1.9825e-02,
         -6.3792e-03, -2.4864e-03],
        [-5.8488e-04,  2.9461e-05, -7.7070e-06,  ..., -6.3792e-03,
          2.0425e-02, -6.3199e-03],
        [ 2.1941e-04, -1.3417e-04,  1.3270e-04,  ..., -2.4864e-03,
         -6.3199e-03,  2.0131e-02]], device='cuda:0') 

reserving basis 394/1152; cond: 704562.0625, radio:0.0027534249238669872
PARAMETER       :  Parameter containing:
tensor([[[[ 1.8841e-03, -1.2910e-02, -7.1675e-03],
          [ 1.7726e-02,  1.7947e-02,  3.7258e-02],
          [-1.1108e-02, -5.5119e-03,  1.6313e-02]],

         [[-1.6573e-02,  1.9479e-02, -9.6191e-03],
          [-1.1713e-02, -2.3215e-02, -2.5229e-02],
          [-6.5178e-04, -1.6745e-02, -3.6468e-03]],

         [[-1.9384e-02, -1.3273e-02,  3.2874e-02],
          [ 3.7700e-02,  2.4237e-02,  8.4229e-04],
          [ 5.3863e-03,  6.1439e-03, -6.5432e-03]],

         ...,

         [[ 3.7612e-02,  1.2316e-02, -6.2420e-03],
          [ 4.0062e-02,  1.5810e-02,  1.5977e-02],
          [-1.5710e-02,  1.1143e-03, -1.5228e-02]],

         [[ 4.3481e-02,  4.6707e-03,  2.9084e-02],
          [ 3.0800e-02,  1.2361e-02,  1.9527e-02],
          [-2.2093e-02, -3.8334e-03,  1.9504e-02]],

         [[ 1.1289e-02, -1.3511e-02, -1.4399e-02],
          [ 2.0346e-02,  1.8254e-02,  3.7665e-02],
          [-1.4673e-02,  1.5143e-02,  1.1527e-03]]],


        [[[ 2.0053e-02,  3.6379e-02, -2.6118e-02],
          [ 2.4167e-02, -3.7790e-03, -2.7364e-02],
          [ 9.1699e-03, -2.5228e-02, -1.8471e-02]],

         [[-3.1229e-02, -3.4273e-02, -4.2607e-02],
          [-1.9953e-02, -2.3192e-02, -6.9877e-03],
          [ 1.7752e-02, -1.3776e-02,  5.8357e-03]],

         [[ 1.1643e-02,  1.9945e-02,  2.9162e-02],
          [-4.5956e-03,  1.4219e-02, -5.6602e-03],
          [ 1.2026e-02, -9.6267e-03, -3.0375e-02]],

         ...,

         [[-1.0844e-02,  3.0635e-02, -1.6857e-02],
          [ 3.1800e-02, -8.0516e-03,  1.1622e-02],
          [ 9.1231e-03,  1.8369e-02, -2.0886e-02]],

         [[-2.5298e-02,  1.9265e-02, -1.7862e-02],
          [-1.5930e-02, -2.7269e-03, -1.9863e-02],
          [ 2.8385e-02, -1.0130e-02,  2.0975e-02]],

         [[ 1.2768e-02, -2.6075e-02,  3.4401e-02],
          [-1.9226e-02,  2.2834e-02,  1.3848e-03],
          [ 1.5903e-02, -3.4190e-04, -7.0978e-03]]],


        [[[-1.8055e-03, -7.5504e-04,  9.5489e-03],
          [-2.0765e-02,  5.7905e-03, -1.7542e-02],
          [-1.6085e-02,  1.9202e-03, -5.4758e-03]],

         [[ 1.4067e-02, -1.7207e-02, -1.5189e-02],
          [ 2.5677e-02, -5.5129e-04,  3.0947e-02],
          [ 3.3876e-02,  2.0035e-02, -1.4229e-02]],

         [[-1.5468e-02,  1.0064e-02, -2.5039e-02],
          [ 6.6798e-03,  1.7760e-02,  2.6221e-03],
          [-3.4145e-04, -6.6625e-03,  3.1068e-02]],

         ...,

         [[ 1.8989e-02,  2.3589e-02,  1.6190e-02],
          [-1.6464e-02, -2.5073e-02,  4.2912e-03],
          [ 3.0988e-03,  1.7067e-02, -1.0340e-02]],

         [[ 1.5793e-02,  1.6684e-02,  8.6421e-03],
          [ 8.1658e-03, -9.1909e-03, -2.0552e-02],
          [ 4.2609e-03,  1.0529e-02,  3.1870e-02]],

         [[-1.3001e-02, -1.5100e-02, -4.4334e-03],
          [-2.1797e-02,  1.8150e-03,  7.7043e-04],
          [-3.3253e-03,  6.4326e-03,  4.0532e-02]]],


        ...,


        [[[-9.8815e-03, -2.4624e-02,  1.1497e-02],
          [ 1.9103e-02, -7.2604e-03,  1.6877e-02],
          [ 3.1123e-03, -1.5702e-02,  2.4267e-02]],

         [[-2.2313e-02, -2.3744e-02, -7.9154e-03],
          [-1.9402e-02, -1.8168e-02,  1.8638e-03],
          [-3.2916e-02,  2.2902e-02, -6.1674e-03]],

         [[-5.2209e-03,  3.4158e-03, -6.4202e-03],
          [-5.6984e-03,  1.8996e-02,  3.0377e-03],
          [ 3.1599e-02,  1.0451e-02,  1.8950e-02]],

         ...,

         [[ 4.0146e-04, -1.2262e-02,  3.9144e-02],
          [-1.9068e-03,  2.0763e-02,  2.0572e-02],
          [ 1.4490e-02,  1.0193e-02,  9.2743e-03]],

         [[-2.0799e-02,  2.0917e-03, -5.2395e-03],
          [-1.7414e-02,  1.2986e-02,  5.5717e-03],
          [ 1.1664e-02, -1.3309e-03,  2.0052e-02]],

         [[-4.7590e-03, -6.4808e-03, -9.4851e-03],
          [ 5.3959e-03,  2.2628e-02, -1.7677e-02],
          [-1.2459e-03, -1.3184e-02, -5.5950e-03]]],


        [[[-8.4273e-03,  3.8793e-02,  1.1003e-02],
          [-2.4589e-03, -7.7221e-04, -8.1811e-04],
          [-2.3732e-02,  2.3580e-02,  2.2807e-02]],

         [[ 1.6046e-02, -8.8693e-03, -1.8607e-04],
          [-3.1773e-03,  3.2574e-02,  1.1935e-02],
          [ 5.4471e-03, -7.7229e-03, -2.4447e-02]],

         [[-9.2200e-03, -1.4416e-02,  6.0007e-03],
          [-1.4524e-02, -3.2028e-02, -4.2606e-03],
          [ 1.3465e-02, -1.4424e-02,  2.6760e-02]],

         ...,

         [[-2.0611e-03,  1.1969e-02, -2.0267e-02],
          [ 1.5871e-02,  6.0706e-04,  1.7329e-02],
          [ 2.9973e-02,  1.2676e-02,  2.3973e-02]],

         [[ 9.4186e-03, -2.7258e-02,  3.7867e-03],
          [-1.7274e-02,  1.0734e-03, -2.0134e-02],
          [ 1.8181e-02,  1.9936e-02,  2.4713e-02]],

         [[-3.3960e-03, -2.9332e-02, -2.0214e-02],
          [ 1.3270e-02,  1.1581e-02,  3.0059e-03],
          [ 1.6524e-02,  5.8364e-03, -2.8516e-02]]],


        [[[ 2.2266e-02, -2.5286e-02,  8.7311e-04],
          [ 1.1075e-02,  7.0592e-03, -2.8692e-02],
          [ 2.1155e-02,  2.4428e-02,  1.1698e-02]],

         [[-4.1403e-02, -1.6934e-02, -3.0697e-02],
          [ 6.4584e-05,  1.2355e-02, -2.0043e-03],
          [ 4.6014e-03, -2.6224e-02,  1.0584e-02]],

         [[-1.1597e-02,  5.2392e-05, -8.7657e-03],
          [-2.4744e-02, -1.5443e-02, -2.1976e-02],
          [ 2.7198e-03, -1.0173e-02,  1.3552e-02]],

         ...,

         [[-1.4674e-02, -2.7867e-02, -2.8217e-02],
          [ 1.1078e-02,  1.8475e-03,  9.8236e-04],
          [ 1.8837e-02, -1.5755e-02, -1.5509e-02]],

         [[-3.4174e-03, -1.7143e-02, -6.7321e-03],
          [ 2.4659e-03,  4.5226e-03,  1.8778e-02],
          [ 1.5181e-03,  2.0432e-03,  3.2116e-03]],

         [[ 2.1051e-02,  1.3203e-02,  1.5187e-03],
          [ 2.0599e-02, -7.1304e-03,  2.0051e-02],
          [-1.4426e-02, -1.4662e-02,  1.5135e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.9431e+07, 1.1071e+06, 9.8324e+05,  ..., 6.7855e+01, 4.6444e+01,
        4.1772e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 394]) 

NULL SPACE BASIS :  tensor([[ 6.9910e-03, -1.5746e-02,  2.2823e-02,  ..., -4.7904e-03,
          5.7115e-04,  6.8315e-04],
        [ 6.7123e-03, -1.8332e-02, -1.2478e-02,  ...,  1.3524e-04,
         -2.5562e-03,  1.6867e-03],
        [ 6.1854e-03,  1.1326e-02, -3.2099e-02,  ...,  1.6938e-03,
         -5.1173e-05, -2.7840e-03],
        ...,
        [-1.1849e-02,  9.6753e-03,  3.5379e-02,  ...,  1.3302e-02,
         -2.1497e-03,  5.3206e-04],
        [ 3.8402e-02, -1.4517e-02,  1.9262e-02,  ..., -6.7447e-03,
         -2.0334e-03, -2.0274e-03],
        [ 4.7917e-02,  1.5414e-02,  1.0931e-02,  ..., -1.2847e-05,
         -9.3418e-04,  2.9232e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.3975e-03, -1.0507e-03, -3.8007e-04,  ...,  4.9725e-04,
         -1.0262e-04,  1.5076e-04],
        [-1.0507e-03,  4.2567e-03, -1.0455e-03,  ..., -7.6035e-05,
          3.5088e-04, -4.5858e-04],
        [-3.8007e-04, -1.0455e-03,  3.3674e-03,  ..., -1.7007e-04,
         -4.3160e-04, -2.2199e-04],
        ...,
        [ 4.9725e-04, -7.6035e-05, -1.7007e-04,  ...,  3.1462e-02,
         -8.3257e-03, -2.3025e-03],
        [-1.0262e-04,  3.5088e-04, -4.3160e-04,  ..., -8.3257e-03,
          3.2000e-02, -7.4128e-03],
        [ 1.5076e-04, -4.5858e-04, -2.2199e-04,  ..., -2.3025e-03,
         -7.4128e-03,  3.0526e-02]], device='cuda:0') 

reserving basis 762/1152; cond: 354329.6875, radio:0.00803674291819334
PARAMETER       :  Parameter containing:
tensor([[[[-2.0144e-02, -1.4257e-02,  4.1903e-02],
          [ 2.1153e-02,  8.8128e-03, -2.2917e-03],
          [ 1.5750e-02,  2.1359e-02, -2.7495e-02]],

         [[ 6.6964e-03,  1.2988e-03,  3.3521e-02],
          [-2.7459e-03,  1.7144e-02,  2.9849e-02],
          [ 2.7409e-02, -2.3637e-02, -6.8183e-03]],

         [[ 1.7991e-02,  5.3408e-03,  7.8979e-03],
          [-6.2254e-03, -8.7392e-03,  2.5595e-02],
          [ 1.0662e-02,  2.0023e-02,  2.6439e-02]],

         ...,

         [[ 7.4570e-03,  1.5833e-02,  2.6629e-02],
          [ 3.2804e-02, -2.0579e-02,  2.3828e-02],
          [-4.1241e-04, -1.4420e-02, -1.3282e-02]],

         [[-7.4192e-03, -6.7739e-03, -1.8923e-02],
          [-2.0679e-03,  3.7904e-03,  5.1071e-03],
          [ 1.4890e-03, -1.1913e-03, -7.8906e-03]],

         [[-3.8368e-02,  8.2114e-03, -1.7852e-02],
          [ 1.0528e-02,  2.6403e-03,  1.7377e-02],
          [ 2.6364e-02, -4.5454e-03,  2.3428e-02]]],


        [[[ 5.3298e-03, -3.3872e-03,  1.5385e-02],
          [ 2.0458e-02,  1.2607e-02, -1.5429e-03],
          [-2.8700e-03, -9.9002e-03, -3.1588e-03]],

         [[ 3.4527e-03,  1.1963e-02,  1.7128e-02],
          [-2.5498e-02, -2.7932e-03, -2.5357e-02],
          [-1.3572e-02,  1.8994e-02, -2.5429e-03]],

         [[-1.8014e-02, -6.4964e-03,  2.7360e-02],
          [ 3.6615e-03, -1.1292e-02, -1.1674e-02],
          [ 7.4641e-04,  1.3357e-02, -1.3594e-02]],

         ...,

         [[-2.0780e-02, -1.6672e-02, -2.1929e-02],
          [-2.6849e-02,  7.3074e-03, -2.0613e-02],
          [ 3.2518e-02,  1.8144e-03, -3.4742e-02]],

         [[ 1.7914e-02,  2.6769e-02,  3.4514e-02],
          [-1.4301e-02,  1.9160e-02, -9.9071e-03],
          [ 2.1706e-02,  1.9309e-02,  4.0291e-02]],

         [[ 9.5335e-04,  3.9246e-03, -7.1882e-05],
          [-3.7431e-03, -8.5604e-03,  2.0631e-02],
          [ 5.6127e-03,  4.6473e-02,  4.1080e-02]]],


        [[[-2.0853e-02, -2.0990e-02,  4.2932e-03],
          [-2.3125e-02,  4.0573e-03,  8.6041e-04],
          [ 5.6394e-03,  2.9710e-03,  4.7943e-03]],

         [[-2.4691e-03,  8.0216e-03,  9.1952e-03],
          [-3.5637e-04,  2.2744e-02,  6.7528e-03],
          [ 2.0272e-03,  3.4315e-02, -1.9262e-02]],

         [[-4.7339e-03, -1.8412e-02, -2.2873e-02],
          [-2.2327e-02, -2.9828e-02, -6.1737e-03],
          [-7.1495e-03, -2.7882e-02, -1.1089e-02]],

         ...,

         [[ 1.2855e-02,  1.0717e-02,  4.1849e-02],
          [-2.2006e-02,  8.1839e-03, -1.6244e-02],
          [-1.8836e-02,  2.8527e-02, -2.3473e-02]],

         [[-3.6132e-02, -1.0768e-02, -2.2782e-03],
          [ 6.1257e-03, -1.1201e-02,  7.8605e-03],
          [-9.5396e-03,  1.7226e-02, -2.6020e-02]],

         [[ 2.5497e-03, -8.5528e-03, -1.5027e-02],
          [-2.9931e-02,  3.6298e-03,  2.3921e-02],
          [-2.1337e-02,  1.0407e-02,  1.8805e-02]]],


        ...,


        [[[-1.2823e-03,  1.0783e-02, -2.0459e-02],
          [-1.9011e-02, -1.3794e-02, -2.3683e-02],
          [-4.4247e-03, -1.5851e-02, -2.3712e-02]],

         [[ 3.1207e-02,  3.1657e-03,  7.2016e-03],
          [-8.3149e-03,  6.1606e-03, -2.3375e-02],
          [-3.5161e-02, -3.8766e-02, -1.6163e-02]],

         [[ 8.9908e-03,  1.1811e-02, -2.0067e-03],
          [-5.4242e-03, -1.6541e-02,  3.7684e-03],
          [-4.1790e-03,  3.5882e-02,  2.3068e-02]],

         ...,

         [[ 2.0353e-02, -7.0797e-03,  8.9216e-03],
          [-1.2914e-02, -2.2892e-02,  2.0533e-02],
          [ 1.2237e-02, -2.5762e-02, -1.7241e-02]],

         [[-7.8177e-03, -1.1998e-02,  2.9341e-02],
          [-3.5505e-02,  2.5845e-02,  1.6108e-02],
          [-1.8278e-02,  4.7807e-03,  1.5900e-02]],

         [[-2.1295e-02, -2.0346e-02,  4.1708e-03],
          [-2.5718e-03, -1.8713e-02, -2.4207e-02],
          [-4.8728e-03,  1.4183e-02, -1.8298e-02]]],


        [[[-3.0642e-02,  8.2491e-03,  1.0111e-02],
          [-9.1913e-03, -1.5397e-02, -2.5068e-02],
          [-2.4978e-02, -9.7822e-03,  3.3245e-04]],

         [[ 1.0058e-02,  1.8117e-02,  1.1932e-02],
          [-1.8534e-02, -2.1660e-02, -2.3491e-02],
          [-9.0359e-03, -8.7379e-03, -1.8060e-02]],

         [[-2.7431e-02, -1.9919e-02, -1.8742e-02],
          [ 1.9259e-02,  2.2440e-02,  3.0360e-02],
          [ 1.7155e-02, -1.6792e-02, -1.7957e-02]],

         ...,

         [[-3.6485e-02,  1.4621e-02,  1.0636e-02],
          [-3.2000e-02,  1.1680e-03, -5.9266e-03],
          [-2.6161e-03,  1.3106e-03,  1.0637e-02]],

         [[ 2.1145e-03, -1.2790e-02, -6.8919e-04],
          [ 4.1672e-03, -3.0191e-02, -1.4132e-03],
          [ 5.7490e-03, -4.7798e-03,  1.0266e-02]],

         [[-3.9211e-03,  6.2800e-03,  4.1219e-03],
          [-1.1279e-02,  8.9155e-04, -2.1846e-02],
          [ 5.5528e-03,  9.9533e-03, -1.7707e-02]]],


        [[[ 6.6035e-03, -7.3833e-03,  1.7845e-02],
          [ 1.2856e-02, -1.9689e-02,  9.3476e-03],
          [ 7.1426e-03,  2.0892e-02,  4.1289e-03]],

         [[ 3.1631e-02,  6.0881e-03,  9.6599e-03],
          [-9.7890e-03,  3.1201e-02,  1.7789e-02],
          [ 8.6808e-03,  1.3017e-03,  1.4238e-02]],

         [[ 6.9512e-03, -1.3001e-02,  4.3808e-03],
          [-7.0103e-03, -3.5527e-02, -1.9249e-02],
          [-3.6584e-02, -1.2475e-02,  3.9821e-03]],

         ...,

         [[ 8.4916e-03, -3.2737e-02,  1.3301e-02],
          [-3.6934e-02,  1.7124e-02,  2.0708e-03],
          [-1.6355e-02,  5.0073e-03, -1.2523e-02]],

         [[ 1.6001e-02, -2.4599e-03,  1.0550e-03],
          [ 3.1518e-03,  2.7725e-02,  2.8225e-02],
          [ 9.1508e-03,  1.4645e-02,  2.4346e-02]],

         [[ 2.0991e-02,  3.0704e-02,  2.2268e-02],
          [-6.2670e-03,  7.0306e-03, -5.4112e-03],
          [-2.1767e-02,  8.2753e-03, -1.5420e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([7.1895e+06, 4.2183e+05, 4.0097e+05,  ..., 2.6272e+01, 2.3678e+01,
        2.0290e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 762]) 

NULL SPACE BASIS :  tensor([[ 3.0871e-03,  1.5091e-02,  6.7403e-02,  ...,  1.6627e-03,
         -1.5082e-02, -6.2091e-03],
        [-4.2102e-02, -1.8287e-03,  5.4776e-03,  ..., -3.7203e-03,
          9.3806e-03,  5.2195e-03],
        [-1.0998e-02,  9.2947e-03, -1.8054e-02,  ...,  4.0748e-03,
         -1.1848e-03,  6.4441e-04],
        ...,
        [ 2.5590e-02, -5.5517e-05,  2.1663e-02,  ...,  1.1723e-02,
         -1.0272e-02,  4.2559e-03],
        [ 1.6591e-02,  2.2575e-02,  2.3747e-02,  ..., -3.8131e-03,
          2.1594e-02,  2.8575e-04],
        [-6.2532e-02,  1.6921e-02,  1.0865e-02,  ...,  1.4917e-03,
         -2.3555e-03, -5.9907e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.4110e-02, -4.7145e-03, -1.6757e-03,  ..., -1.8100e-04,
         -4.5181e-05,  3.5728e-04],
        [-4.7145e-03,  2.3893e-02, -5.1111e-03,  ..., -1.7573e-04,
         -2.1221e-04,  1.3970e-04],
        [-1.6757e-03, -5.1111e-03,  2.3434e-02,  ..., -4.2935e-04,
          1.4494e-04, -7.1354e-05],
        ...,
        [-1.8100e-04, -1.7573e-04, -4.2935e-04,  ...,  2.3522e-02,
         -6.7172e-03, -2.8248e-03],
        [-4.5181e-05, -2.1221e-04,  1.4494e-04,  ..., -6.7172e-03,
          2.2993e-02, -6.8352e-03],
        [ 3.5728e-04,  1.3970e-04, -7.1354e-05,  ..., -2.8248e-03,
         -6.8352e-03,  2.1567e-02]], device='cuda:0') 

reserving basis 1467/2304; cond: 687674.75, radio:0.007595459930598736
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0034, -0.0048, -0.0002],
          [-0.0245, -0.0076,  0.0137],
          [-0.0257, -0.0071, -0.0176]],

         [[ 0.0071,  0.0218,  0.0028],
          [-0.0058, -0.0183,  0.0205],
          [-0.0108, -0.0014,  0.0135]],

         [[-0.0173, -0.0166, -0.0119],
          [-0.0017, -0.0148, -0.0209],
          [ 0.0060,  0.0080, -0.0121]],

         ...,

         [[ 0.0149,  0.0117,  0.0158],
          [-0.0163, -0.0137,  0.0102],
          [ 0.0030, -0.0003,  0.0015]],

         [[ 0.0215,  0.0007,  0.0146],
          [-0.0137,  0.0008,  0.0236],
          [ 0.0211, -0.0018,  0.0089]],

         [[ 0.0080,  0.0083, -0.0209],
          [ 0.0059, -0.0039,  0.0025],
          [ 0.0236,  0.0053,  0.0114]]],


        [[[ 0.0087, -0.0050, -0.0020],
          [ 0.0049,  0.0076, -0.0089],
          [ 0.0235, -0.0099, -0.0138]],

         [[-0.0105,  0.0114, -0.0010],
          [-0.0188,  0.0221,  0.0129],
          [ 0.0008, -0.0055,  0.0003]],

         [[-0.0164, -0.0025,  0.0110],
          [ 0.0242,  0.0039,  0.0062],
          [-0.0073,  0.0222, -0.0125]],

         ...,

         [[-0.0123,  0.0223,  0.0016],
          [ 0.0214, -0.0138,  0.0155],
          [ 0.0028, -0.0194, -0.0126]],

         [[-0.0150, -0.0261, -0.0058],
          [-0.0084,  0.0038, -0.0056],
          [-0.0155,  0.0167,  0.0185]],

         [[ 0.0077, -0.0057, -0.0205],
          [-0.0078, -0.0141,  0.0064],
          [-0.0196,  0.0093, -0.0065]]],


        [[[ 0.0162, -0.0220, -0.0123],
          [ 0.0164,  0.0184, -0.0026],
          [ 0.0173,  0.0215, -0.0249]],

         [[-0.0014,  0.0126,  0.0024],
          [-0.0108, -0.0134, -0.0200],
          [-0.0083,  0.0233,  0.0016]],

         [[-0.0049, -0.0112,  0.0002],
          [-0.0059, -0.0055,  0.0096],
          [-0.0004, -0.0138,  0.0096]],

         ...,

         [[-0.0158,  0.0225, -0.0074],
          [ 0.0171, -0.0159, -0.0036],
          [-0.0176, -0.0140, -0.0218]],

         [[-0.0033,  0.0186, -0.0117],
          [-0.0172, -0.0211, -0.0203],
          [ 0.0187, -0.0186, -0.0100]],

         [[ 0.0084,  0.0032, -0.0196],
          [ 0.0024, -0.0046,  0.0109],
          [-0.0098, -0.0001, -0.0093]]],


        ...,


        [[[ 0.0074,  0.0184,  0.0073],
          [ 0.0196, -0.0156, -0.0159],
          [-0.0048, -0.0135, -0.0115]],

         [[-0.0152, -0.0105,  0.0294],
          [-0.0001,  0.0093, -0.0037],
          [ 0.0243, -0.0195,  0.0113]],

         [[-0.0175, -0.0007, -0.0139],
          [ 0.0042,  0.0264, -0.0242],
          [ 0.0096,  0.0318,  0.0138]],

         ...,

         [[-0.0058,  0.0120,  0.0026],
          [-0.0123, -0.0109,  0.0066],
          [ 0.0016,  0.0012, -0.0289]],

         [[-0.0118,  0.0026, -0.0112],
          [-0.0161, -0.0104, -0.0019],
          [-0.0084,  0.0037,  0.0201]],

         [[-0.0258, -0.0422, -0.0284],
          [ 0.0103, -0.0272, -0.0197],
          [ 0.0124, -0.0111,  0.0100]]],


        [[[-0.0091,  0.0163,  0.0012],
          [-0.0130, -0.0201,  0.0035],
          [ 0.0101, -0.0040, -0.0141]],

         [[-0.0034,  0.0060, -0.0041],
          [ 0.0056, -0.0147,  0.0151],
          [ 0.0092, -0.0133, -0.0154]],

         [[-0.0220, -0.0295, -0.0215],
          [ 0.0246,  0.0167, -0.0011],
          [-0.0037,  0.0088, -0.0054]],

         ...,

         [[ 0.0085, -0.0133, -0.0295],
          [ 0.0163,  0.0145, -0.0233],
          [-0.0141, -0.0233,  0.0130]],

         [[ 0.0053,  0.0014,  0.0111],
          [-0.0139, -0.0080, -0.0103],
          [-0.0082,  0.0081,  0.0048]],

         [[-0.0242,  0.0161,  0.0166],
          [-0.0269, -0.0101,  0.0255],
          [-0.0160,  0.0363,  0.0395]]],


        [[[-0.0019, -0.0307,  0.0093],
          [-0.0155, -0.0083,  0.0275],
          [-0.0200, -0.0087,  0.0096]],

         [[-0.0191, -0.0165,  0.0137],
          [-0.0100,  0.0073, -0.0065],
          [ 0.0052,  0.0119,  0.0011]],

         [[ 0.0093, -0.0063, -0.0055],
          [-0.0121,  0.0230, -0.0029],
          [-0.0215,  0.0012, -0.0055]],

         ...,

         [[-0.0027, -0.0288, -0.0019],
          [-0.0238, -0.0261,  0.0076],
          [ 0.0114, -0.0144,  0.0188]],

         [[ 0.0098,  0.0032,  0.0181],
          [ 0.0067, -0.0121, -0.0006],
          [-0.0223,  0.0145, -0.0262]],

         [[-0.0251,  0.0086, -0.0341],
          [ 0.0102, -0.0138, -0.0057],
          [ 0.0132, -0.0252, -0.0191]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.0470e+07, 6.3460e+05, 5.9438e+05,  ..., 1.6556e+01, 1.5672e+01,
        1.5225e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1467]) 

NULL SPACE BASIS :  tensor([[-2.1189e-02,  2.0579e-04, -7.3941e-03,  ..., -4.1026e-03,
         -4.8952e-03, -2.3251e-03],
        [ 1.5870e-02,  1.8244e-02, -8.1128e-04,  ..., -2.8937e-03,
         -9.9206e-04, -3.0284e-03],
        [-1.0153e-02, -1.1891e-02,  1.4011e-02,  ...,  6.4801e-03,
         -1.1577e-03,  2.1647e-03],
        ...,
        [-5.5999e-03, -3.5680e-03, -1.8175e-02,  ...,  3.5804e-03,
          2.2342e-03,  1.1338e-02],
        [-1.1212e-02, -1.0511e-03, -1.0792e-02,  ..., -7.0797e-03,
         -4.8128e-04, -6.7643e-03],
        [-1.4072e-02, -1.0109e-02, -6.4799e-03,  ..., -9.9539e-06,
          2.9838e-04,  6.8609e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.6564e-02, -5.9501e-04, -4.3317e-04,  ...,  3.9624e-05,
         -1.0945e-04,  6.5295e-05],
        [-5.9501e-04,  1.6103e-02, -8.4990e-04,  ...,  3.0800e-05,
         -4.5215e-05, -1.2521e-04],
        [-4.3317e-04, -8.4990e-04,  1.6417e-02,  ...,  7.9035e-05,
          1.0688e-04, -8.5404e-05],
        ...,
        [ 3.9624e-05,  3.0800e-05,  7.9035e-05,  ...,  2.0534e-02,
         -1.9759e-04,  1.0321e-05],
        [-1.0945e-04, -4.5215e-05,  1.0688e-04,  ..., -1.9759e-04,
          2.0161e-02, -2.4134e-04],
        [ 6.5295e-05, -1.2521e-04, -8.5404e-05,  ...,  1.0321e-05,
         -2.4134e-04,  2.0758e-02]], device='cuda:0') 

reserving basis 98/128; cond: 12979.0126953125, radio:0.021186215803027153
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0234]],

         [[-0.0335]],

         [[-0.0267]],

         ...,

         [[-0.0389]],

         [[-0.0708]],

         [[ 0.0869]]],


        [[[-0.0436]],

         [[ 0.0660]],

         [[ 0.0784]],

         ...,

         [[-0.0519]],

         [[-0.0035]],

         [[-0.0209]]],


        [[[ 0.0616]],

         [[-0.0195]],

         [[-0.0269]],

         ...,

         [[ 0.0543]],

         [[ 0.0096]],

         [[-0.0243]]],


        ...,


        [[[-0.0643]],

         [[ 0.0296]],

         [[ 0.0827]],

         ...,

         [[-0.0785]],

         [[-0.0530]],

         [[ 0.0321]]],


        [[[-0.0242]],

         [[-0.0713]],

         [[ 0.0079]],

         ...,

         [[ 0.0296]],

         [[ 0.0494]],

         [[ 0.0266]]],


        [[[ 0.0394]],

         [[-0.0382]],

         [[-0.0639]],

         ...,

         [[-0.0474]],

         [[ 0.0678]],

         [[-0.0332]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.0097e+06, 5.3394e+04, 4.4032e+04, 1.8086e+04, 1.1052e+04, 9.2852e+03,
        7.7344e+03, 5.5099e+03, 4.9311e+03, 4.7315e+03, 4.5796e+03, 3.3634e+03,
        2.9789e+03, 2.5637e+03, 2.1895e+03, 2.0862e+03, 1.7723e+03, 1.7055e+03,
        1.5575e+03, 1.4454e+03, 1.3700e+03, 1.2806e+03, 1.1813e+03, 1.1292e+03,
        1.0694e+03, 1.0525e+03, 9.4060e+02, 9.0734e+02, 8.1881e+02, 8.0628e+02,
        7.3730e+02, 7.1947e+02, 6.8572e+02, 6.5956e+02, 6.3305e+02, 6.2310e+02,
        5.8011e+02, 5.5447e+02, 5.2243e+02, 5.1938e+02, 5.0382e+02, 4.9432e+02,
        4.8675e+02, 4.6265e+02, 4.4655e+02, 4.2733e+02, 4.1742e+02, 4.1471e+02,
        4.0134e+02, 3.8618e+02, 3.7651e+02, 3.6751e+02, 3.5379e+02, 3.4437e+02,
        3.4019e+02, 3.3703e+02, 3.2091e+02, 3.1715e+02, 3.1212e+02, 3.0173e+02,
        2.9818e+02, 2.9037e+02, 2.8728e+02, 2.8033e+02, 2.7584e+02, 2.7001e+02,
        2.6733e+02, 2.5888e+02, 2.5317e+02, 2.4997e+02, 2.4698e+02, 2.4155e+02,
        2.3562e+02, 2.3097e+02, 2.3028e+02, 2.2327e+02, 2.1945e+02, 2.1373e+02,
        2.1084e+02, 2.0859e+02, 2.0382e+02, 2.0073e+02, 1.9720e+02, 1.9562e+02,
        1.9215e+02, 1.9021e+02, 1.8880e+02, 1.8296e+02, 1.8186e+02, 1.7916e+02,
        1.7664e+02, 1.7278e+02, 1.7087e+02, 1.6787e+02, 1.6685e+02, 1.6444e+02,
        1.6096e+02, 1.6003e+02, 1.5687e+02, 1.5601e+02, 1.5250e+02, 1.5091e+02,
        1.4897e+02, 1.4684e+02, 1.4530e+02, 1.4361e+02, 1.4104e+02, 1.4011e+02,
        1.3775e+02, 1.3625e+02, 1.3159e+02, 1.2860e+02, 1.2511e+02, 1.2445e+02,
        1.1933e+02, 1.1709e+02, 1.1621e+02, 1.1442e+02, 1.1348e+02, 1.1258e+02,
        1.0939e+02, 1.0647e+02, 1.0617e+02, 1.0311e+02, 1.0168e+02, 9.7752e+01,
        9.0356e+01, 7.7795e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 98]) 

NULL SPACE BASIS :  tensor([[-1.8449e-01,  1.0071e-01, -7.4679e-03,  ...,  5.9371e-02,
         -4.8250e-02,  5.1312e-02],
        [ 5.9377e-02, -1.3718e-01,  9.6853e-03,  ..., -1.4805e-01,
          3.0647e-02, -2.1190e-02],
        [ 2.1499e-01,  3.8930e-02,  3.2501e-02,  ..., -7.5138e-02,
         -1.7165e-01, -1.1260e-01],
        ...,
        [ 5.8859e-02,  2.9679e-02, -7.5637e-02,  ..., -9.0707e-03,
          1.0056e-01, -1.2109e-02],
        [-1.1177e-01, -7.7500e-03, -4.6860e-02,  ...,  7.0833e-02,
         -2.4036e-02,  4.9116e-02],
        [ 7.8560e-02,  1.7344e-04, -7.4975e-03,  ...,  8.0662e-02,
         -3.2302e-02,  8.0060e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0787,  0.0011,  0.0001,  ..., -0.0046,  0.0031, -0.0088],
        [ 0.0011,  0.0663,  0.0011,  ..., -0.0021, -0.0049,  0.0081],
        [ 0.0001,  0.0011,  0.0869,  ...,  0.0003, -0.0013, -0.0036],
        ...,
        [-0.0046, -0.0021,  0.0003,  ...,  0.0765,  0.0009, -0.0072],
        [ 0.0031, -0.0049, -0.0013,  ...,  0.0009,  0.0790, -0.0017],
        [-0.0088,  0.0081, -0.0036,  ..., -0.0072, -0.0017,  0.0704]],
       device='cuda:0') 

reserving basis 1289/2304; cond: 766898.625, radio:0.006421957165002823
PARAMETER       :  Parameter containing:
tensor([[[[ 4.9415e-03,  1.3166e-03, -1.0631e-02],
          [ 5.0304e-03, -1.9892e-02,  9.5286e-04],
          [-6.4181e-03,  2.1396e-02, -5.5497e-03]],

         [[-1.6953e-03,  1.2715e-02,  1.9026e-03],
          [ 2.6300e-02, -6.5562e-03, -2.1213e-02],
          [-3.2493e-03,  5.9045e-03,  1.1207e-02]],

         [[ 1.5034e-02,  2.7336e-02, -1.3511e-02],
          [ 1.4305e-02,  1.1007e-02,  1.2613e-02],
          [-3.8423e-03,  9.8123e-03,  6.1494e-03]],

         ...,

         [[-4.4369e-04,  2.0474e-02,  2.6847e-03],
          [-2.7295e-02, -5.5973e-03, -3.1185e-02],
          [-7.4201e-04, -1.5913e-02, -2.3416e-03]],

         [[-2.4047e-02,  1.3856e-02,  9.6560e-03],
          [ 4.1055e-03, -2.6009e-02, -1.8744e-02],
          [-1.9827e-02, -1.5065e-02, -1.5057e-02]],

         [[ 1.9868e-03, -1.2404e-03, -2.1567e-02],
          [ 8.1828e-03,  1.1358e-02,  5.4022e-03],
          [ 2.2889e-02,  2.0005e-02, -1.3836e-02]]],


        [[[-4.9787e-03,  2.2991e-02,  3.0625e-02],
          [ 1.9826e-03, -1.8948e-02,  1.5708e-02],
          [ 6.6720e-03,  1.7810e-02, -2.5136e-02]],

         [[-1.4958e-02, -2.0739e-03, -3.9027e-02],
          [-8.5983e-04, -5.8138e-03,  8.5900e-03],
          [ 1.6447e-03,  1.8681e-02,  2.4951e-02]],

         [[-7.6486e-03, -2.3346e-02,  1.4648e-02],
          [-1.0323e-02, -1.0392e-04, -1.5050e-02],
          [ 6.7567e-03,  7.1427e-03,  1.8739e-02]],

         ...,

         [[-1.1477e-02, -2.6110e-02, -1.0446e-02],
          [-8.8536e-03,  5.7274e-03,  1.7201e-02],
          [ 3.4294e-03,  1.3624e-02, -8.6122e-03]],

         [[ 1.8956e-03, -1.7407e-02, -1.3511e-03],
          [ 1.1110e-02,  1.7348e-02,  2.1969e-02],
          [-1.6404e-02, -1.9623e-02,  1.9172e-02]],

         [[ 5.9785e-03, -1.2447e-02, -2.9162e-02],
          [-6.7474e-03,  5.7226e-03,  3.8050e-03],
          [-1.3113e-02, -2.5515e-02, -1.3087e-02]]],


        [[[-1.3307e-02,  1.6583e-04,  3.4090e-03],
          [ 1.2371e-02, -2.1335e-02, -1.7060e-02],
          [-9.9407e-03,  2.1110e-02,  3.7556e-03]],

         [[-1.7116e-02,  7.5874e-03,  3.8863e-03],
          [-1.0946e-02, -3.9054e-03,  1.8682e-02],
          [-1.7892e-02,  1.5931e-02,  1.8245e-03]],

         [[ 2.1204e-02, -9.4223e-03,  1.9013e-02],
          [-1.5101e-02, -3.7005e-03,  2.5475e-02],
          [-1.8066e-02, -1.2881e-02,  2.1816e-02]],

         ...,

         [[ 1.6343e-02, -6.1903e-03, -1.7259e-02],
          [ 1.1571e-02, -2.8592e-02, -2.9871e-03],
          [ 8.7337e-03, -2.8971e-02, -3.3180e-02]],

         [[-8.9731e-03,  7.9784e-04, -1.4209e-02],
          [ 1.3762e-02,  6.5441e-03,  4.3163e-03],
          [ 2.5140e-02, -3.1733e-02, -8.7825e-03]],

         [[-2.9235e-03, -1.2528e-02, -1.2600e-02],
          [-1.0263e-02, -1.4878e-02,  2.5043e-03],
          [ 1.6863e-02,  3.0577e-02,  1.8345e-02]]],


        ...,


        [[[-2.0214e-02,  1.1496e-02, -2.7054e-02],
          [ 4.8339e-03, -1.1189e-02, -2.9606e-02],
          [ 1.0133e-02,  6.7901e-04, -5.6201e-03]],

         [[-1.0077e-02, -2.4064e-02, -2.3403e-03],
          [ 1.3999e-02,  1.6107e-02,  6.7417e-03],
          [-1.7024e-02, -2.5474e-02, -1.3018e-02]],

         [[ 4.1708e-03, -1.7097e-02,  1.2686e-02],
          [-1.9597e-02,  6.3095e-04,  2.1283e-02],
          [ 1.6486e-02,  5.4745e-03, -1.6740e-02]],

         ...,

         [[ 1.5827e-02, -3.7426e-02,  1.2309e-02],
          [-1.2956e-02, -2.1009e-03,  6.1384e-03],
          [-1.3238e-02, -2.3716e-02,  3.4969e-03]],

         [[ 1.5566e-02, -1.1569e-03,  2.7277e-03],
          [-1.5079e-02,  1.1371e-02,  8.0186e-04],
          [-2.1633e-02,  1.9410e-03,  3.7409e-03]],

         [[-2.1254e-02, -1.2518e-02, -2.3886e-02],
          [ 8.0375e-03,  1.2237e-02, -5.2491e-03],
          [-2.2365e-02,  2.5240e-02,  3.1868e-03]]],


        [[[ 7.8829e-03, -4.4690e-03,  3.6197e-02],
          [ 5.8475e-03,  1.9277e-02,  1.1747e-02],
          [ 2.6147e-02, -5.4676e-03,  3.4314e-02]],

         [[ 4.9120e-03,  2.1239e-02, -1.6474e-02],
          [-1.3101e-02,  3.9716e-03,  2.2511e-02],
          [ 4.5862e-03,  1.0570e-02,  2.7576e-02]],

         [[-1.1726e-02,  3.0945e-03,  1.4593e-02],
          [-1.3549e-02,  8.6321e-03, -2.2257e-02],
          [ 2.5833e-03,  1.8084e-02,  1.0239e-02]],

         ...,

         [[ 1.0591e-02,  1.2677e-02, -1.3862e-02],
          [ 1.3898e-02,  1.7666e-02, -1.7931e-02],
          [ 7.3666e-04,  1.8351e-02,  3.1636e-02]],

         [[ 4.0249e-03, -1.0104e-02,  1.8806e-02],
          [ 1.0226e-02,  1.5894e-02,  2.0623e-02],
          [-1.3367e-02,  7.7071e-03,  1.3982e-02]],

         [[-1.3763e-02, -1.0161e-02,  4.2434e-03],
          [ 7.2352e-03, -4.1429e-03,  2.9512e-03],
          [-1.6866e-02, -3.3705e-03,  1.8408e-02]]],


        [[[ 2.8583e-02,  4.4046e-03, -6.3118e-03],
          [ 9.3309e-03, -4.5264e-03,  8.1486e-03],
          [-2.2985e-03, -1.2604e-02, -7.6942e-03]],

         [[-9.7703e-03, -6.3724e-03,  1.4722e-02],
          [ 1.8271e-02, -1.3551e-03,  2.5078e-02],
          [-1.9329e-03,  2.5724e-02,  1.0879e-02]],

         [[-1.1991e-02, -2.1041e-02, -3.1131e-03],
          [-1.3144e-02, -2.6622e-02, -1.3194e-02],
          [ 1.9196e-03,  1.1487e-03,  4.9039e-03]],

         ...,

         [[ 9.8944e-03,  7.7752e-03,  1.7258e-02],
          [ 1.6025e-02,  1.4072e-02, -1.2318e-02],
          [-3.3690e-04, -1.3285e-02, -1.5053e-02]],

         [[ 1.3053e-02, -1.0588e-02, -1.4019e-02],
          [-7.4069e-04, -2.3345e-02, -1.5107e-02],
          [ 9.5867e-05, -2.1258e-02, -2.0072e-02]],

         [[ 4.5948e-03, -3.1724e-03, -1.1540e-02],
          [-1.5485e-03, -8.9417e-03, -1.5981e-02],
          [ 2.0642e-02,  2.3894e-03,  1.4071e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([9.0125e+06, 6.0923e+05, 5.8660e+05,  ..., 1.4438e+01, 1.4274e+01,
        1.1752e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1289]) 

NULL SPACE BASIS :  tensor([[-1.3206e-02, -2.2778e-03,  2.7055e-02,  ..., -6.0675e-03,
          1.9857e-03, -2.9014e-03],
        [-1.8408e-02,  2.6913e-02, -2.9046e-03,  ...,  5.0364e-03,
          2.0820e-03,  5.0888e-03],
        [-1.4008e-02,  3.0564e-02,  2.5396e-03,  ..., -4.4560e-03,
         -1.1969e-03, -1.9280e-03],
        ...,
        [-1.5348e-02,  2.0038e-02, -1.0570e-02,  ...,  2.7708e-02,
         -5.3330e-03, -1.9834e-03],
        [-1.1237e-02, -6.1935e-03,  9.7860e-04,  ..., -2.4344e-02,
          1.2056e-02, -6.5207e-05],
        [-5.3233e-03,  1.2769e-02, -8.4716e-03,  ...,  1.2565e-03,
         -6.1124e-03, -8.8502e-04]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.1339e-02, -4.6092e-03, -4.5468e-04,  ...,  1.0712e-04,
         -7.7961e-05, -4.7013e-06],
        [-4.6092e-03,  1.2391e-02, -4.6350e-03,  ...,  9.3879e-06,
          3.3966e-04, -2.0940e-04],
        [-4.5468e-04, -4.6350e-03,  1.1057e-02,  ..., -1.3058e-04,
         -1.9821e-04,  3.5050e-04],
        ...,
        [ 1.0712e-04,  9.3879e-06, -1.3058e-04,  ...,  2.1568e-02,
         -7.0137e-04, -5.9134e-04],
        [-7.7961e-05,  3.3966e-04, -1.9821e-04,  ..., -7.0137e-04,
          2.1035e-02, -5.6620e-04],
        [-4.7013e-06, -2.0940e-04,  3.5050e-04,  ..., -5.9134e-04,
         -5.6620e-04,  2.0808e-02]], device='cuda:0') 

reserving basis 967/2304; cond: 1217164.875, radio:0.0029582050628960133
PARAMETER       :  Parameter containing:
tensor([[[[-1.3585e-02, -6.5323e-03, -1.5092e-02],
          [-4.4658e-03, -2.9625e-02,  6.1806e-03],
          [-1.5441e-02, -1.3107e-02, -4.7254e-03]],

         [[ 2.3973e-02,  2.7010e-02,  2.8934e-02],
          [-9.8858e-03,  1.2433e-02, -1.9331e-02],
          [-7.0637e-03,  2.5486e-02,  2.6681e-02]],

         [[-3.7619e-03, -1.0854e-02, -2.7201e-02],
          [-1.3034e-02, -1.6587e-02, -3.0154e-03],
          [ 1.5792e-02, -2.0849e-02, -1.5636e-02]],

         ...,

         [[-1.8111e-02,  2.2302e-02, -1.9152e-02],
          [ 4.9366e-03,  2.5540e-03, -2.2800e-02],
          [ 3.8942e-04,  2.0090e-03,  9.0730e-03]],

         [[ 2.0263e-02,  1.9014e-03, -2.0461e-02],
          [-1.2447e-02,  2.9251e-03, -8.5657e-03],
          [-2.6866e-02,  1.5457e-02,  9.9455e-03]],

         [[ 3.9073e-03,  2.5559e-02,  7.6809e-03],
          [-1.0762e-02,  1.4787e-02, -1.7910e-02],
          [ 9.3415e-03, -4.8817e-03,  1.6054e-02]]],


        [[[ 9.2694e-04,  2.0810e-02,  3.3247e-02],
          [-2.4587e-02,  1.9433e-02,  8.0302e-03],
          [ 3.0796e-03, -4.3590e-03,  7.7799e-03]],

         [[ 2.0112e-02,  1.5970e-02, -2.0682e-02],
          [ 1.1484e-02,  2.8888e-03, -4.1059e-03],
          [-2.8754e-02, -1.1085e-02, -1.7184e-02]],

         [[ 3.0124e-03, -2.6465e-03,  1.0445e-02],
          [ 4.3964e-03, -2.6266e-02, -1.5902e-02],
          [ 1.5556e-02, -8.6227e-03,  1.6616e-02]],

         ...,

         [[-1.8535e-02, -3.4880e-03, -1.4157e-02],
          [ 7.7690e-03,  1.2584e-02, -4.0421e-04],
          [-2.0938e-03,  1.3993e-02, -9.3083e-03]],

         [[ 2.7225e-03, -7.1191e-03,  2.1021e-02],
          [-8.4103e-04, -4.2719e-03,  2.0490e-02],
          [ 6.3400e-03,  9.9428e-03,  1.4401e-02]],

         [[-2.2901e-02, -1.5911e-02, -2.2342e-02],
          [-1.8458e-02, -1.5514e-02, -1.7590e-02],
          [-1.9107e-02, -1.6851e-02,  2.8201e-03]]],


        [[[-2.7267e-02, -1.9792e-02, -1.6288e-02],
          [ 1.5851e-03,  1.6799e-02,  7.9541e-03],
          [ 1.0782e-02, -9.1409e-03, -1.6050e-02]],

         [[-1.6423e-02,  1.4619e-02,  3.5313e-03],
          [-9.5751e-03,  3.1831e-03,  2.6177e-03],
          [-3.4452e-03, -1.4301e-02,  1.7853e-02]],

         [[-6.8843e-03,  7.7722e-03,  1.5784e-03],
          [-6.3428e-04, -1.6762e-02, -2.4364e-02],
          [ 9.6118e-03, -5.4022e-03,  2.3453e-03]],

         ...,

         [[-5.2103e-03,  2.7483e-02, -8.8813e-03],
          [-1.7848e-02,  1.7533e-03, -1.5856e-02],
          [-2.0876e-02, -2.3711e-02,  7.6603e-03]],

         [[-3.5911e-05,  2.3585e-03, -3.3933e-04],
          [-3.0824e-03,  1.2912e-02,  9.0332e-03],
          [-1.8461e-02, -1.6898e-02,  8.3566e-04]],

         [[-2.0239e-02, -1.0737e-02, -3.5377e-04],
          [ 1.0845e-02, -8.5840e-03,  1.9236e-02],
          [-6.4089e-03, -1.9620e-03, -1.2804e-02]]],


        ...,


        [[[-1.3730e-02, -1.6352e-02,  2.4061e-03],
          [-2.1646e-03,  1.0803e-02,  1.2397e-02],
          [-1.2888e-02, -1.1742e-02,  5.3515e-03]],

         [[-6.8406e-04,  2.0434e-02, -2.9036e-03],
          [ 1.4331e-02, -3.3479e-03,  2.8010e-02],
          [-2.0265e-03, -1.3105e-02, -1.0466e-02]],

         [[-5.9589e-04, -1.8069e-02,  8.1563e-03],
          [ 1.1039e-05,  1.9786e-02, -2.6458e-03],
          [-3.5586e-02, -6.1832e-03,  2.3717e-02]],

         ...,

         [[ 2.3951e-02,  4.3561e-03,  1.3695e-02],
          [ 9.0437e-03,  1.7384e-02,  1.6206e-02],
          [-1.9969e-03, -1.0702e-02,  6.9300e-03]],

         [[ 3.4570e-03, -1.4219e-02, -2.3616e-02],
          [-4.8209e-03,  1.8861e-02, -5.9389e-03],
          [ 1.1261e-02, -1.2745e-02,  1.6357e-02]],

         [[ 1.1813e-02,  1.3793e-02, -6.6060e-03],
          [-1.1917e-02, -2.2015e-03,  1.8338e-02],
          [ 1.2961e-02, -1.8244e-03,  6.4349e-03]]],


        [[[ 8.2935e-03, -1.7688e-02,  7.1024e-03],
          [ 2.6009e-02,  4.6502e-03, -1.0116e-02],
          [-3.7886e-03,  1.9181e-03, -1.7850e-02]],

         [[ 4.0277e-03, -7.8900e-03,  1.4454e-02],
          [ 1.9408e-02,  2.0815e-02,  3.5402e-02],
          [ 3.2138e-02,  2.2860e-02,  3.4813e-02]],

         [[-1.3278e-02,  1.6528e-02, -7.1321e-03],
          [ 2.7891e-02,  3.0117e-02,  1.1967e-02],
          [-1.7342e-02,  4.3326e-04,  2.4074e-02]],

         ...,

         [[-1.3912e-02,  6.2939e-03,  2.0289e-02],
          [ 1.7212e-02,  9.6462e-03,  1.1676e-02],
          [-1.4044e-02,  9.2819e-03, -1.5102e-02]],

         [[-2.1591e-02, -2.0629e-02,  8.6588e-03],
          [-1.0935e-02,  9.2426e-03, -4.8083e-04],
          [ 1.6693e-02, -1.4944e-02, -8.4176e-03]],

         [[-1.8710e-02,  2.0826e-03,  2.2735e-02],
          [ 1.3046e-02,  2.0952e-02, -1.1414e-02],
          [ 1.2998e-02,  1.2341e-02, -7.0243e-03]]],


        [[[ 1.5078e-02, -1.1911e-02, -2.3156e-03],
          [-6.8879e-03, -1.0764e-02, -2.3138e-03],
          [-2.1584e-02, -1.8455e-02, -9.7665e-04]],

         [[ 4.4917e-03, -6.0911e-03, -1.2960e-02],
          [-6.7041e-03, -2.1753e-03, -1.7199e-02],
          [ 3.0302e-04, -2.0598e-02, -9.3479e-03]],

         [[ 3.1258e-02,  2.1877e-03,  2.3585e-03],
          [ 2.5450e-02,  5.4255e-03,  9.2272e-03],
          [-2.5318e-03, -5.8447e-03, -1.3700e-02]],

         ...,

         [[ 1.4933e-02,  1.2811e-02,  1.9828e-03],
          [ 1.9823e-02, -7.1316e-03,  5.7727e-03],
          [-6.2461e-03, -3.7680e-03,  1.1557e-04]],

         [[-6.8961e-03,  3.8331e-03, -1.9896e-02],
          [ 2.1086e-02,  1.8619e-02,  1.2572e-02],
          [ 7.1750e-04,  3.8276e-02,  3.3237e-02]],

         [[ 2.0822e-02, -7.3777e-04, -1.1676e-02],
          [ 7.8305e-03,  7.8443e-03, -1.2491e-02],
          [-3.0967e-02, -3.9899e-03, -1.7597e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([8.1411e+06, 5.5824e+05, 5.3055e+05,  ..., 7.9733e+00, 7.6561e+00,
        6.6886e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 967]) 

NULL SPACE BASIS :  tensor([[-2.2770e-02,  1.7920e-02,  2.9017e-02,  ..., -4.0859e-03,
         -8.0802e-04,  4.9834e-03],
        [-5.8222e-03,  2.3011e-02,  1.9295e-02,  ...,  5.1155e-03,
          2.8544e-03, -3.2808e-04],
        [ 8.5527e-03,  2.1908e-02, -2.8714e-02,  ...,  1.9684e-03,
          1.1617e-03, -4.3120e-03],
        ...,
        [-2.4535e-02,  6.9678e-03,  1.0697e-02,  ...,  2.0419e-03,
         -5.4276e-03,  2.8619e-05],
        [ 4.0414e-05, -6.9327e-03,  4.8848e-03,  ...,  1.9395e-03,
          3.6075e-03,  6.9106e-04],
        [ 1.8973e-02,  1.1673e-02,  8.9703e-03,  ..., -6.8227e-04,
         -8.0027e-04, -4.3573e-04]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.7051e-02, -5.1933e-03, -6.0156e-04,  ...,  1.9981e-04,
         -5.6234e-05,  4.5812e-04],
        [-5.1933e-03,  1.6502e-02, -5.4869e-03,  ...,  5.6318e-05,
          9.1164e-06, -2.5659e-04],
        [-6.0156e-04, -5.4869e-03,  1.9390e-02,  ...,  2.1396e-04,
         -3.2450e-05,  3.7602e-05],
        ...,
        [ 1.9981e-04,  5.6318e-05,  2.1396e-04,  ...,  3.6625e-03,
         -1.2293e-03, -1.8057e-04],
        [-5.6234e-05,  9.1164e-06, -3.2450e-05,  ..., -1.2293e-03,
          3.4510e-03, -9.9589e-04],
        [ 4.5812e-04, -2.5659e-04,  3.7602e-05,  ..., -1.8057e-04,
         -9.9589e-04,  3.3055e-03]], device='cuda:0') 

reserving basis 1211/2304; cond: 1095824.625, radio:0.00401843199506402
PARAMETER       :  Parameter containing:
tensor([[[[-6.4751e-03, -8.8992e-03, -7.0043e-03],
          [-7.9580e-03,  1.5239e-02,  1.3029e-02],
          [-2.3684e-02, -4.4011e-03, -2.8439e-03]],

         [[ 1.5978e-03,  1.7809e-03, -1.1750e-02],
          [-7.3344e-03,  1.1515e-02, -5.2332e-03],
          [ 1.8826e-02, -2.4341e-02,  5.7453e-03]],

         [[ 1.4775e-02, -4.4586e-03,  8.7977e-03],
          [ 1.2823e-02, -1.0211e-02,  1.7079e-03],
          [-1.8326e-02, -1.2102e-02, -7.8437e-03]],

         ...,

         [[ 1.5465e-02, -1.5337e-02,  1.9128e-03],
          [ 1.0741e-02,  1.0517e-02,  1.3565e-02],
          [-1.2124e-03, -2.5948e-02,  1.3095e-02]],

         [[ 2.0532e-02,  1.1424e-02, -1.3186e-03],
          [-1.0615e-02,  2.2068e-03, -1.0821e-02],
          [-2.5708e-03,  1.8330e-02, -1.6926e-02]],

         [[-6.0866e-03, -4.3074e-03,  7.9001e-03],
          [-3.0714e-02,  3.3693e-03, -1.5455e-04],
          [ 5.7546e-03, -1.0697e-02,  1.9959e-02]]],


        [[[-1.2029e-02,  7.5950e-03,  7.5582e-03],
          [ 1.0450e-02, -7.2502e-03,  1.5131e-02],
          [ 1.5578e-02, -6.5141e-03, -9.1990e-04]],

         [[ 1.8671e-02, -1.5243e-03, -1.9203e-02],
          [ 2.0123e-03, -3.9451e-03,  8.5358e-03],
          [ 1.3803e-02, -2.1493e-02, -1.5480e-02]],

         [[ 1.6216e-02, -2.5027e-02,  3.3486e-03],
          [-2.2613e-02,  2.8504e-04,  8.6456e-03],
          [-1.0913e-02, -1.8121e-02,  4.1761e-03]],

         ...,

         [[-2.1364e-02, -1.8034e-02, -8.2229e-03],
          [ 3.5877e-04, -1.2254e-02, -2.5994e-02],
          [ 4.3731e-03, -5.0108e-03, -1.9582e-04]],

         [[-8.7090e-03,  5.8703e-03, -5.8730e-03],
          [ 8.4921e-03,  2.4423e-02, -1.0233e-02],
          [-1.5212e-02,  1.1583e-02, -9.0625e-03]],

         [[ 1.0915e-02,  2.1094e-02,  4.5375e-03],
          [ 1.0960e-03,  6.8955e-03, -8.9634e-03],
          [-7.3866e-03,  2.6755e-02, -2.3689e-03]]],


        [[[ 3.3132e-03, -2.4713e-02, -4.3424e-03],
          [-3.0030e-03,  1.0864e-02, -5.1832e-03],
          [ 6.7980e-03,  1.0636e-02,  3.2765e-03]],

         [[ 2.1938e-02,  1.7799e-02,  2.0209e-02],
          [ 2.0529e-02,  9.4797e-03,  5.4437e-03],
          [ 9.1207e-03, -4.4382e-03,  8.6972e-03]],

         [[-1.7739e-02, -1.4070e-02,  2.8749e-03],
          [ 6.4180e-03, -1.9239e-02, -5.9113e-03],
          [-1.3483e-02, -9.1018e-03, -3.1414e-02]],

         ...,

         [[ 1.4812e-03, -4.1338e-03, -3.1427e-02],
          [-5.3880e-03,  3.0573e-03, -8.4584e-03],
          [ 1.8542e-02,  9.1369e-04, -3.1773e-03]],

         [[ 1.2602e-02,  5.0881e-03, -4.9180e-03],
          [-5.8284e-03, -1.7126e-03,  6.6154e-03],
          [-2.1896e-02, -2.1799e-02, -1.4813e-02]],

         [[ 1.0555e-03,  5.5159e-03, -6.9679e-03],
          [ 1.2502e-03,  1.6885e-02,  3.7843e-04],
          [ 1.1581e-02,  1.1080e-02, -9.4810e-03]]],


        ...,


        [[[-6.5703e-05,  1.0965e-03, -1.4109e-02],
          [-1.4262e-02,  2.5130e-03,  3.9205e-04],
          [ 7.9249e-03, -1.0773e-02, -8.1095e-03]],

         [[ 1.7435e-02, -3.6913e-03, -1.1629e-02],
          [ 2.1812e-02,  1.5247e-02, -1.5190e-02],
          [-6.0373e-03,  2.5487e-03, -1.5357e-02]],

         [[-2.0730e-02, -1.8917e-03,  8.3327e-03],
          [-6.6215e-03, -1.5150e-02,  6.2787e-03],
          [ 5.2447e-03,  3.8533e-03, -2.4951e-02]],

         ...,

         [[ 8.5706e-03, -1.3184e-02,  7.6387e-03],
          [ 1.4659e-03,  4.3273e-03, -5.4902e-03],
          [-6.0589e-03, -7.6758e-03, -2.9763e-02]],

         [[-1.5727e-02,  6.3456e-03, -2.9628e-03],
          [ 7.6724e-03, -2.3420e-02, -2.1412e-02],
          [-7.5347e-03, -1.3319e-02, -1.1386e-02]],

         [[ 1.0368e-02,  1.8286e-02,  6.9949e-03],
          [-1.0143e-02,  1.1946e-02, -1.0145e-02],
          [-2.2522e-03,  9.2282e-03, -5.7630e-03]]],


        [[[-1.9032e-02, -3.8702e-03, -1.1324e-03],
          [-9.8434e-03,  2.5797e-03,  7.5556e-03],
          [ 1.6607e-03,  1.1603e-02,  8.0514e-03]],

         [[-1.1874e-02,  2.1550e-02,  1.0983e-02],
          [-3.4789e-03,  2.8119e-03,  1.1792e-02],
          [ 7.2323e-03, -1.1217e-02,  1.1619e-02]],

         [[-8.3813e-03,  4.0760e-03,  1.7719e-02],
          [ 1.2959e-02, -5.1708e-03,  1.4616e-02],
          [-5.1714e-03, -1.0794e-02, -1.1928e-02]],

         ...,

         [[ 1.0977e-02, -1.8952e-02, -2.1883e-03],
          [-2.3482e-03, -1.5070e-02,  1.0564e-02],
          [-1.5706e-02, -1.2326e-02,  3.6483e-03]],

         [[-3.2932e-03,  6.5217e-03, -2.6301e-02],
          [-1.5632e-02, -6.0703e-03, -3.3751e-02],
          [-1.4644e-02, -2.0358e-02, -5.8271e-03]],

         [[ 1.1333e-02,  1.7716e-02,  2.7524e-02],
          [ 1.3785e-02,  2.3807e-03,  3.2241e-03],
          [ 7.3629e-03, -1.4534e-03,  1.2372e-02]]],


        [[[ 2.1726e-03,  7.5047e-03,  2.3219e-02],
          [-3.6998e-03, -3.0229e-03, -1.6875e-02],
          [ 6.1652e-03,  4.9475e-03, -8.2523e-03]],

         [[ 2.4060e-05, -4.3199e-03, -1.1150e-02],
          [-9.9689e-03, -1.0136e-02, -1.7156e-02],
          [ 5.5842e-03, -2.6850e-02,  2.2967e-03]],

         [[ 1.6621e-03, -1.7688e-02,  8.3943e-03],
          [-1.3048e-02,  9.5148e-03,  4.6556e-03],
          [-2.0173e-02,  7.4193e-04, -9.2915e-03]],

         ...,

         [[-1.4684e-02, -1.6292e-02,  1.9770e-02],
          [-1.2408e-02, -1.2296e-02, -2.3116e-02],
          [ 8.4695e-03, -1.3552e-02,  1.8414e-02]],

         [[ 2.1420e-02,  8.3999e-03, -8.2602e-03],
          [ 1.3899e-02,  1.1449e-02,  1.6287e-02],
          [-9.7060e-03, -2.1961e-02, -1.0768e-03]],

         [[-9.8242e-03, -9.7346e-03,  2.1566e-03],
          [-1.7585e-02,  2.8173e-02,  1.5557e-02],
          [-1.9903e-02, -3.9689e-03, -1.6857e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.5178e+06, 2.8152e+05, 2.7127e+05,  ..., 2.6682e+00, 2.3770e+00,
        2.2976e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1211]) 

NULL SPACE BASIS :  tensor([[ 0.0089,  0.0126, -0.0217,  ..., -0.0080,  0.0035,  0.0077],
        [ 0.0012, -0.0256,  0.0259,  ...,  0.0002, -0.0062, -0.0034],
        [ 0.0001,  0.0115, -0.0140,  ...,  0.0045, -0.0007,  0.0056],
        ...,
        [ 0.0077,  0.0008,  0.0051,  ...,  0.0238, -0.0039,  0.0084],
        [-0.0153,  0.0108, -0.0118,  ..., -0.0310,  0.0144, -0.0068],
        [-0.0231, -0.0344, -0.0041,  ...,  0.0112, -0.0156, -0.0055]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.4000e-03, -3.0737e-03,  2.2235e-04,  ...,  1.0417e-04,
         -1.0794e-04, -8.4059e-05],
        [-3.0737e-03,  6.9535e-03, -2.8737e-03,  ..., -1.4298e-04,
          1.8819e-04, -4.6596e-05],
        [ 2.2235e-04, -2.8737e-03,  4.6395e-03,  ...,  2.6542e-04,
          1.9354e-04, -3.9048e-05],
        ...,
        [ 1.0417e-04, -1.4298e-04,  2.6542e-04,  ...,  2.1939e-02,
         -2.6107e-03, -9.8515e-04],
        [-1.0794e-04,  1.8819e-04,  1.9354e-04,  ..., -2.6107e-03,
          2.0597e-02, -3.2681e-03],
        [-8.4059e-05, -4.6596e-05, -3.9048e-05,  ..., -9.8515e-04,
         -3.2681e-03,  1.9861e-02]], device='cuda:0') 

reserving basis 1429/4608; cond: 5597568.0, radio:0.0008393471944145858
PARAMETER       :  Parameter containing:
tensor([[[[ 4.5734e-03,  3.9992e-03,  1.3097e-02],
          [-7.4065e-03, -8.3422e-03,  1.2804e-02],
          [-4.2335e-03, -4.8186e-03,  1.0132e-02]],

         [[ 4.5038e-03, -6.2739e-03,  1.6088e-03],
          [ 5.8231e-03,  9.9830e-03,  2.2979e-03],
          [ 1.7532e-02, -7.0222e-03,  8.7948e-03]],

         [[-2.4223e-03, -1.3995e-02, -3.3876e-04],
          [-5.2884e-03, -6.6471e-03, -2.3332e-02],
          [-1.1326e-02,  9.5830e-03, -2.1153e-02]],

         ...,

         [[-5.3912e-03,  1.9593e-03,  3.9299e-03],
          [-1.8430e-03, -1.4296e-03, -1.7104e-02],
          [-1.0518e-02, -1.1374e-03, -2.5558e-02]],

         [[-4.7022e-03,  1.0786e-02,  1.4039e-02],
          [ 2.9806e-03,  1.6385e-02,  3.4351e-03],
          [-4.1727e-04,  5.2386e-03,  4.8119e-03]],

         [[-1.4025e-02, -3.1351e-02, -2.8479e-02],
          [-6.1848e-03,  2.7394e-05, -1.9920e-02],
          [ 7.5552e-03,  1.2602e-03, -9.5182e-04]]],


        [[[ 8.6864e-03, -1.6407e-02,  8.5355e-03],
          [-3.6166e-03,  5.6302e-03,  6.5935e-03],
          [ 5.0932e-03, -1.2894e-02, -1.1404e-02]],

         [[-1.4929e-02,  7.5580e-03, -9.4440e-03],
          [-3.1739e-03, -2.5172e-03, -3.5911e-03],
          [ 7.1660e-03,  4.6377e-03, -1.0759e-02]],

         [[-1.4106e-03, -1.6362e-04,  1.1046e-02],
          [ 7.0869e-04,  1.3270e-02,  1.6233e-03],
          [ 1.0817e-02,  2.3078e-02,  1.8304e-02]],

         ...,

         [[-2.6765e-03,  3.0067e-03, -9.2496e-03],
          [ 6.7937e-04, -6.5204e-03, -1.9073e-03],
          [-1.5071e-03,  7.3748e-03,  1.3866e-02]],

         [[ 4.9792e-03, -3.7528e-03, -1.2178e-02],
          [-4.8705e-03, -3.4867e-03,  3.3596e-03],
          [-5.6929e-03,  9.0971e-05, -1.2268e-02]],

         [[ 4.3233e-04,  1.4931e-03, -4.6543e-03],
          [-1.5252e-02, -2.0087e-03,  1.3886e-05],
          [ 1.0978e-02, -6.5899e-03, -4.6041e-03]]],


        [[[ 3.0159e-03,  1.0449e-02,  3.0454e-03],
          [ 7.7781e-03,  1.4856e-02,  1.9089e-02],
          [-6.7262e-03, -2.3735e-03,  4.2291e-03]],

         [[ 9.1704e-04,  1.3443e-02,  8.8624e-03],
          [-1.1801e-02, -7.0582e-03,  3.8244e-03],
          [-9.7718e-03, -7.5080e-03, -5.8540e-03]],

         [[-4.0066e-03, -6.9677e-03,  6.2314e-03],
          [-5.9038e-03, -1.1505e-02,  9.6710e-03],
          [-1.1737e-03, -5.0382e-03, -1.1195e-03]],

         ...,

         [[ 7.8701e-03,  2.2074e-03, -9.9937e-03],
          [ 6.6469e-03, -1.9002e-03, -1.2007e-02],
          [ 8.6962e-03,  3.7025e-03,  1.4876e-03]],

         [[ 9.2073e-03,  5.3305e-03,  1.9986e-02],
          [-6.9044e-03, -6.2658e-03,  1.8061e-02],
          [-5.1851e-03,  2.7841e-03, -4.6987e-03]],

         [[-4.6346e-03,  4.7180e-03, -1.2632e-02],
          [-1.2350e-02,  2.8470e-03, -6.7748e-03],
          [-7.2679e-03,  1.0588e-02,  8.3209e-03]]],


        ...,


        [[[ 5.8422e-03, -3.9905e-03, -1.6166e-02],
          [-1.0485e-02,  5.4713e-03, -3.4978e-03],
          [ 1.5510e-03,  1.0940e-02, -1.3632e-02]],

         [[-6.5588e-03,  4.0242e-04,  3.5569e-03],
          [-8.0529e-03,  1.5625e-02, -1.4016e-04],
          [-7.0403e-03,  9.3087e-03, -7.8769e-03]],

         [[ 9.4513e-04, -9.6581e-03,  1.0537e-03],
          [-3.3637e-03, -2.6397e-02, -1.1514e-02],
          [ 1.2237e-02, -1.9961e-02, -1.7731e-02]],

         ...,

         [[-1.7894e-02,  6.1213e-03, -7.2980e-03],
          [-7.0426e-03, -5.0007e-03, -1.2097e-02],
          [-6.2977e-03,  1.1118e-02,  6.4269e-03]],

         [[-1.0285e-03, -3.5008e-03, -3.6045e-03],
          [-5.9498e-03,  5.6087e-03,  1.3592e-02],
          [ 9.5228e-03, -5.4765e-03, -9.4644e-03]],

         [[ 1.4431e-02,  1.3777e-02,  5.3941e-03],
          [-6.1185e-04,  4.9717e-03,  1.8869e-03],
          [-4.3775e-04,  6.7085e-04, -1.1654e-03]]],


        [[[-1.3723e-02, -3.7710e-03,  3.5861e-03],
          [-1.2985e-02,  8.4930e-03, -1.8315e-02],
          [-6.3091e-03, -2.6283e-03,  2.7079e-03]],

         [[ 1.2806e-03, -1.3943e-02, -2.2649e-02],
          [ 9.8442e-03,  4.1991e-03, -1.6235e-02],
          [ 8.7812e-03,  4.1577e-03,  4.7891e-03]],

         [[-6.7564e-03,  8.3104e-04,  1.0884e-02],
          [ 1.5719e-03, -1.1955e-03, -6.0771e-03],
          [-7.0162e-03, -4.8673e-03, -7.3841e-03]],

         ...,

         [[ 2.5696e-03,  1.3812e-03, -6.3811e-03],
          [-2.4143e-02, -1.7559e-02, -1.6845e-02],
          [-1.5419e-04, -2.1641e-03,  1.3771e-02]],

         [[ 4.5213e-03, -3.6050e-03,  3.7293e-03],
          [-2.9288e-03,  9.1253e-04, -5.3023e-05],
          [ 1.0923e-02,  9.8892e-03,  5.2103e-03]],

         [[ 8.2124e-03, -1.3929e-02, -1.2999e-02],
          [ 2.1829e-03, -3.0568e-03,  1.4370e-02],
          [-1.3741e-02,  3.1633e-03, -4.7651e-03]]],


        [[[ 8.7019e-03, -6.6403e-03, -3.4743e-03],
          [-1.4683e-02, -9.0344e-04,  4.7632e-03],
          [ 9.2129e-04, -9.6061e-03, -1.1852e-02]],

         [[-4.4092e-03,  1.4086e-02,  2.5473e-03],
          [ 5.6089e-03, -1.0170e-02,  4.1708e-04],
          [ 4.2555e-03,  7.0460e-03, -5.5874e-03]],

         [[-1.2254e-02, -3.5228e-03,  1.6682e-02],
          [-2.7558e-03,  4.4644e-03,  1.8326e-02],
          [-8.6394e-03, -1.6556e-03,  2.4538e-03]],

         ...,

         [[ 1.3860e-02,  5.3612e-03, -6.1467e-04],
          [ 1.0997e-02, -1.4415e-02,  5.5296e-03],
          [ 1.8202e-03, -1.5444e-02,  1.0151e-02]],

         [[ 1.9897e-02,  2.3652e-02,  2.4840e-03],
          [ 1.5894e-02,  1.5011e-03, -5.4637e-03],
          [-1.0367e-02,  1.1923e-02, -1.3130e-02]],

         [[-4.0354e-03, -1.3587e-02,  8.0780e-03],
          [-1.1529e-02, -6.1500e-03, -5.1566e-03],
          [ 1.1035e-02, -2.5870e-03,  1.9786e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.9649e+06, 4.7316e+05, 4.6022e+05,  ..., 6.0151e-01, 5.9643e-01,
        5.2968e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1429]) 

NULL SPACE BASIS :  tensor([[-0.0040, -0.0188,  0.0270,  ...,  0.0045, -0.0060,  0.0005],
        [-0.0085,  0.0129,  0.0140,  ...,  0.0002,  0.0026,  0.0005],
        [ 0.0049, -0.0112, -0.0072,  ...,  0.0047,  0.0004, -0.0007],
        ...,
        [-0.0061,  0.0049, -0.0107,  ...,  0.0014,  0.0048, -0.0013],
        [-0.0148, -0.0355,  0.0142,  ...,  0.0037,  0.0055, -0.0011],
        [ 0.0255, -0.0009, -0.0175,  ..., -0.0004, -0.0010, -0.0079]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.6915e-03, -3.8359e-04, -2.4306e-04,  ..., -6.5305e-05,
         -2.2987e-04, -5.2327e-05],
        [-3.8359e-04,  4.7335e-03, -1.2858e-04,  ..., -1.6832e-04,
          2.3748e-04, -1.6493e-04],
        [-2.4306e-04, -1.2858e-04,  6.2263e-03,  ...,  1.5700e-04,
         -1.3912e-04,  4.4099e-05],
        ...,
        [-6.5305e-05, -1.6832e-04,  1.5700e-04,  ...,  1.0430e-02,
         -7.7761e-04, -1.7651e-04],
        [-2.2987e-04,  2.3748e-04, -1.3912e-04,  ..., -7.7761e-04,
          7.1342e-03, -6.5057e-04],
        [-5.2327e-05, -1.6493e-04,  4.4099e-05,  ..., -1.7651e-04,
         -6.5057e-04,  8.1783e-03]], device='cuda:0') 

reserving basis 190/256; cond: 51495.6171875, radio:0.0118058817461133
PARAMETER       :  Parameter containing:
tensor([[[[-0.0169]],

         [[-0.0364]],

         [[-0.0369]],

         ...,

         [[ 0.0108]],

         [[-0.0477]],

         [[-0.0582]]],


        [[[ 0.0618]],

         [[-0.0590]],

         [[ 0.0165]],

         ...,

         [[ 0.0068]],

         [[ 0.0089]],

         [[ 0.0161]]],


        [[[ 0.0439]],

         [[ 0.0246]],

         [[-0.0064]],

         ...,

         [[ 0.0195]],

         [[ 0.0387]],

         [[-0.0272]]],


        ...,


        [[[-0.0285]],

         [[ 0.0077]],

         [[ 0.0456]],

         ...,

         [[ 0.0192]],

         [[-0.0167]],

         [[-0.0166]]],


        [[[ 0.0160]],

         [[ 0.0200]],

         [[-0.0268]],

         ...,

         [[ 0.0412]],

         [[ 0.0073]],

         [[ 0.0393]]],


        [[[-0.0382]],

         [[ 0.0260]],

         [[ 0.0402]],

         ...,

         [[ 0.0515]],

         [[-0.0608]],

         [[ 0.0440]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([4.9961e+05, 2.9538e+04, 2.4795e+04, 9.1226e+03, 7.0006e+03, 6.0690e+03,
        4.5930e+03, 2.4053e+03, 1.7496e+03, 1.5143e+03, 1.2021e+03, 1.1005e+03,
        9.7719e+02, 8.8547e+02, 8.8197e+02, 8.1042e+02, 7.5839e+02, 7.1721e+02,
        6.7140e+02, 6.1946e+02, 5.7510e+02, 5.5555e+02, 4.9503e+02, 4.7743e+02,
        4.3466e+02, 4.0727e+02, 3.8468e+02, 3.4040e+02, 3.2548e+02, 3.1686e+02,
        3.0065e+02, 2.8516e+02, 2.7944e+02, 2.7467e+02, 2.5735e+02, 2.4036e+02,
        2.3572e+02, 2.1215e+02, 2.0727e+02, 2.0068e+02, 1.9074e+02, 1.8847e+02,
        1.7869e+02, 1.7413e+02, 1.6823e+02, 1.6215e+02, 1.5613e+02, 1.5343e+02,
        1.5021e+02, 1.4156e+02, 1.4031e+02, 1.3350e+02, 1.2874e+02, 1.2745e+02,
        1.2518e+02, 1.2143e+02, 1.2130e+02, 1.1746e+02, 1.1444e+02, 1.1285e+02,
        1.0748e+02, 1.0550e+02, 1.0406e+02, 1.0058e+02, 9.9078e+01, 9.7216e+01,
        9.3987e+01, 9.1921e+01, 9.1306e+01, 8.9752e+01, 8.8992e+01, 8.7845e+01,
        8.4651e+01, 8.3532e+01, 8.2434e+01, 8.1334e+01, 7.9678e+01, 7.7935e+01,
        7.6892e+01, 7.5930e+01, 7.4641e+01, 7.3036e+01, 7.1723e+01, 7.0520e+01,
        6.9211e+01, 6.8932e+01, 6.7892e+01, 6.7543e+01, 6.6213e+01, 6.5558e+01,
        6.5110e+01, 6.4288e+01, 6.3644e+01, 6.3004e+01, 6.1759e+01, 6.1103e+01,
        6.0695e+01, 6.0264e+01, 5.9181e+01, 5.8180e+01, 5.7394e+01, 5.6280e+01,
        5.6015e+01, 5.5505e+01, 5.5272e+01, 5.4572e+01, 5.4154e+01, 5.3777e+01,
        5.2556e+01, 5.1875e+01, 5.1774e+01, 5.1061e+01, 5.0262e+01, 4.9653e+01,
        4.8919e+01, 4.8576e+01, 4.7951e+01, 4.7691e+01, 4.6863e+01, 4.6211e+01,
        4.6076e+01, 4.5731e+01, 4.5214e+01, 4.5024e+01, 4.4737e+01, 4.3893e+01,
        4.3727e+01, 4.3514e+01, 4.3127e+01, 4.2869e+01, 4.2579e+01, 4.1729e+01,
        4.1193e+01, 4.1115e+01, 4.0748e+01, 4.0309e+01, 4.0249e+01, 3.9588e+01,
        3.9118e+01, 3.8893e+01, 3.8581e+01, 3.8247e+01, 3.8140e+01, 3.7618e+01,
        3.7247e+01, 3.7151e+01, 3.6881e+01, 3.6257e+01, 3.6160e+01, 3.5847e+01,
        3.5530e+01, 3.5178e+01, 3.5087e+01, 3.4544e+01, 3.4305e+01, 3.4097e+01,
        3.3904e+01, 3.3685e+01, 3.3452e+01, 3.3133e+01, 3.3033e+01, 3.2848e+01,
        3.2442e+01, 3.2263e+01, 3.1774e+01, 3.1572e+01, 3.1468e+01, 3.1095e+01,
        3.0719e+01, 3.0556e+01, 3.0492e+01, 3.0320e+01, 2.9958e+01, 2.9785e+01,
        2.9522e+01, 2.9209e+01, 2.9006e+01, 2.8597e+01, 2.8568e+01, 2.8397e+01,
        2.8293e+01, 2.7650e+01, 2.7309e+01, 2.7232e+01, 2.7036e+01, 2.6739e+01,
        2.6681e+01, 2.6362e+01, 2.6106e+01, 2.5951e+01, 2.5770e+01, 2.5546e+01,
        2.5218e+01, 2.4958e+01, 2.4686e+01, 2.4564e+01, 2.4328e+01, 2.4151e+01,
        2.3949e+01, 2.3822e+01, 2.3761e+01, 2.3496e+01, 2.3258e+01, 2.3204e+01,
        2.2945e+01, 2.2704e+01, 2.2436e+01, 2.2196e+01, 2.2088e+01, 2.1888e+01,
        2.1788e+01, 2.1628e+01, 2.1457e+01, 2.1178e+01, 2.0949e+01, 2.0748e+01,
        2.0627e+01, 2.0345e+01, 2.0308e+01, 2.0134e+01, 1.9719e+01, 1.9607e+01,
        1.9461e+01, 1.9190e+01, 1.8897e+01, 1.8770e+01, 1.8197e+01, 1.8064e+01,
        1.7825e+01, 1.7749e+01, 1.7665e+01, 1.7518e+01, 1.7419e+01, 1.7333e+01,
        1.7063e+01, 1.6914e+01, 1.6507e+01, 1.6302e+01, 1.6043e+01, 1.5677e+01,
        1.5456e+01, 1.5197e+01, 1.5093e+01, 1.4820e+01, 1.4568e+01, 1.4042e+01,
        1.3938e+01, 1.3152e+01, 1.3073e+01, 1.2811e+01, 1.2620e+01, 1.2512e+01,
        1.1832e+01, 1.1574e+01, 1.1103e+01, 9.7020e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 190]) 

NULL SPACE BASIS :  tensor([[ 1.1756e-01, -3.5324e-03, -1.3539e-01,  ..., -3.0757e-03,
          1.3973e-02,  2.7706e-03],
        [-4.3235e-02, -2.4608e-02, -1.0460e-01,  ...,  6.3184e-03,
         -9.8410e-03,  3.7357e-02],
        [ 3.8593e-02, -1.9259e-02,  5.4732e-02,  ..., -4.8063e-02,
          4.8006e-02,  1.4730e-02],
        ...,
        [-4.4584e-02,  1.1085e-02,  6.1325e-02,  ...,  3.2527e-02,
         -1.9872e-02,  9.4609e-03],
        [ 6.4597e-05,  7.5192e-02,  8.4581e-03,  ..., -2.6285e-02,
         -4.7517e-03, -2.9779e-03],
        [-2.2765e-02, -2.7206e-02, -1.0910e-02,  ...,  3.5182e-02,
         -3.0397e-01,  3.1170e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0231,  0.0006, -0.0021,  ...,  0.0025, -0.0028,  0.0013],
        [ 0.0006,  0.0575, -0.0032,  ..., -0.0047,  0.0031,  0.0009],
        [-0.0021, -0.0032,  0.0619,  ..., -0.0026,  0.0031, -0.0006],
        ...,
        [ 0.0025, -0.0047, -0.0026,  ...,  0.0555, -0.0025,  0.0016],
        [-0.0028,  0.0031,  0.0031,  ..., -0.0025,  0.0443, -0.0011],
        [ 0.0013,  0.0009, -0.0006,  ...,  0.0016, -0.0011,  0.0663]],
       device='cuda:0') 

reserving basis 1636/4608; cond: 6354008.5, radio:0.0008240485331043601
PARAMETER       :  Parameter containing:
tensor([[[[-4.1276e-03,  4.6340e-03, -1.2777e-02],
          [-8.3396e-03,  5.3813e-03, -1.6214e-02],
          [ 9.7894e-03, -2.9931e-03,  6.6930e-03]],

         [[-7.4001e-03, -2.6140e-03,  3.7279e-03],
          [ 9.2564e-03,  4.6372e-03, -4.0670e-03],
          [ 5.2978e-03, -5.2609e-03,  6.1061e-03]],

         [[ 1.0660e-03, -1.3657e-02, -1.4259e-02],
          [-4.4753e-03, -1.9334e-03, -1.2240e-02],
          [ 1.6204e-03,  1.4142e-02, -4.4088e-03]],

         ...,

         [[-3.6876e-03,  9.4467e-03, -1.2517e-03],
          [ 1.0162e-02,  4.7080e-04,  6.4681e-03],
          [ 5.2137e-03,  1.0698e-02, -5.0760e-03]],

         [[-1.2166e-02,  3.6960e-04, -1.0898e-02],
          [ 1.3829e-03, -6.9418e-03, -2.1587e-03],
          [-6.0286e-04, -1.0482e-02,  1.0498e-02]],

         [[-2.7485e-04,  1.0325e-02,  8.2372e-03],
          [ 6.6900e-03,  9.1969e-03,  1.2895e-02],
          [-1.2756e-02,  3.0619e-03, -9.5318e-03]]],


        [[[ 5.9203e-03, -3.9291e-03,  7.7805e-03],
          [-1.3593e-02, -9.1082e-03,  5.7480e-03],
          [-2.3071e-02,  9.6216e-04,  6.8566e-03]],

         [[-7.6363e-03,  1.6924e-03,  2.5684e-03],
          [-6.6730e-03,  1.6496e-02, -1.9097e-03],
          [ 1.1472e-02,  7.9904e-03,  1.5701e-02]],

         [[ 5.4046e-03, -7.0431e-03,  5.9540e-03],
          [ 1.3207e-03, -1.1192e-02,  5.0927e-03],
          [ 1.1646e-02, -6.1253e-05,  7.3379e-03]],

         ...,

         [[ 9.0395e-03,  3.6626e-03,  8.1812e-03],
          [ 1.1787e-02, -3.5007e-03, -4.4476e-03],
          [-6.2139e-03, -1.0245e-03,  2.5789e-03]],

         [[ 2.0415e-03, -1.9601e-03, -9.7126e-03],
          [ 9.0361e-04, -4.8689e-03, -1.9870e-03],
          [-2.8892e-03,  1.2569e-02,  2.8867e-03]],

         [[ 1.2204e-02, -7.2852e-03, -9.5397e-03],
          [ 1.2855e-02, -5.1440e-03,  1.9225e-03],
          [-1.8843e-03,  5.9687e-03, -1.0409e-02]]],


        [[[-5.8913e-03, -1.8364e-02, -8.3865e-03],
          [-2.4438e-03,  7.9369e-03, -1.4763e-02],
          [ 1.5513e-02,  1.8032e-02,  5.9102e-03]],

         [[ 6.8953e-03, -1.0546e-02,  9.6545e-04],
          [-5.8066e-03, -3.1328e-03, -2.1479e-02],
          [-4.6291e-03, -4.2618e-03, -2.1008e-02]],

         [[-1.1555e-03, -1.8471e-03,  5.5876e-03],
          [ 1.4084e-03,  2.0698e-03, -3.3113e-04],
          [-5.8675e-03,  1.0068e-02, -1.8770e-03]],

         ...,

         [[ 7.5225e-03,  1.4012e-02,  8.2155e-03],
          [-5.5346e-03,  9.5446e-03, -8.4618e-03],
          [ 5.2599e-04, -1.2646e-02, -2.1686e-03]],

         [[ 5.1078e-03, -1.1579e-02, -6.1181e-03],
          [ 3.9488e-03, -6.6101e-03, -1.9769e-02],
          [-3.7104e-03,  1.5699e-03, -1.3613e-02]],

         [[-1.2865e-02, -1.9107e-02, -3.6883e-03],
          [ 7.4566e-03,  5.9854e-03, -1.1600e-02],
          [-4.9544e-03, -1.1069e-02,  6.1375e-03]]],


        ...,


        [[[-1.1416e-02,  8.6368e-03, -8.0512e-03],
          [ 8.8003e-03,  7.1117e-03,  1.0397e-02],
          [-1.2601e-02, -1.7513e-02,  9.9613e-04]],

         [[-1.6188e-02, -1.7087e-03, -1.4087e-03],
          [ 4.2700e-03,  2.5744e-03, -1.7919e-03],
          [-9.6834e-03,  4.7695e-03, -3.2567e-03]],

         [[-1.5797e-02,  1.1101e-03,  1.4686e-03],
          [-1.3097e-02, -5.5061e-03, -1.5858e-02],
          [-8.2273e-03, -1.4225e-02, -3.1960e-03]],

         ...,

         [[ 1.0116e-02, -1.4861e-02,  5.7386e-03],
          [-3.0816e-03,  4.0683e-03, -1.0651e-02],
          [-8.1505e-03, -1.4328e-03,  5.3756e-03]],

         [[-1.6936e-03, -2.5763e-03, -1.4177e-03],
          [-1.2425e-02, -8.1108e-03, -1.9154e-02],
          [-1.4219e-02, -4.1456e-03, -3.8450e-03]],

         [[-3.3248e-03, -3.9084e-03, -1.0599e-02],
          [ 8.8016e-03,  9.6727e-03, -1.3936e-03],
          [ 1.0651e-02,  5.8912e-03,  1.0490e-02]]],


        [[[-1.4314e-02, -2.6016e-03,  9.7857e-03],
          [ 6.7658e-03, -6.9692e-04,  2.2753e-02],
          [-6.1021e-03, -7.5116e-03,  2.0904e-02]],

         [[ 1.0792e-02, -7.5572e-03, -1.5823e-03],
          [-3.6607e-03, -4.2757e-03,  8.9229e-03],
          [-1.9224e-02, -7.0488e-03, -1.8922e-02]],

         [[-1.5736e-02, -1.4389e-02, -1.3360e-02],
          [ 1.3480e-02,  1.4157e-03, -1.5450e-02],
          [ 8.5217e-03, -1.1975e-02, -3.0417e-03]],

         ...,

         [[ 1.2762e-02,  7.3528e-03, -2.5349e-03],
          [ 1.2963e-02,  9.9960e-03,  1.5559e-02],
          [-5.9994e-03,  9.6196e-03, -1.5074e-02]],

         [[-8.2414e-03,  2.2008e-03, -4.4168e-03],
          [ 5.0833e-03, -4.8068e-03,  1.2232e-03],
          [ 1.6097e-02,  5.5495e-03,  6.4957e-03]],

         [[ 3.6512e-03,  1.1814e-02,  1.0354e-02],
          [-8.6141e-04,  4.4346e-03,  3.1389e-03],
          [-1.6395e-03,  1.6823e-02,  2.2492e-03]]],


        [[[ 1.9855e-02,  4.7583e-03, -1.6280e-03],
          [ 2.6471e-03, -5.7067e-03, -7.3094e-03],
          [-1.0408e-02,  2.3399e-03,  3.6482e-03]],

         [[ 1.5528e-02,  5.6262e-04, -3.2419e-03],
          [-1.1152e-03,  1.7155e-02, -2.8625e-03],
          [ 6.7035e-03,  2.5133e-03,  1.4002e-02]],

         [[ 1.8138e-02,  2.3714e-03,  5.9711e-03],
          [-5.5774e-03, -9.7573e-03, -9.6291e-03],
          [-1.0718e-02, -6.7784e-04, -9.3945e-03]],

         ...,

         [[ 1.0249e-02, -1.4423e-02,  2.0684e-03],
          [ 3.6902e-03,  9.2428e-03,  4.4634e-03],
          [ 1.9413e-02,  2.3682e-02,  2.6562e-02]],

         [[-1.1565e-03,  1.1192e-02, -8.3385e-03],
          [-1.3328e-02,  1.1949e-03, -1.7456e-02],
          [ 7.4880e-03,  5.8725e-04, -1.5988e-03]],

         [[-3.8866e-06,  3.8590e-03, -1.3530e-02],
          [-1.0273e-02,  6.3616e-03, -1.0785e-02],
          [-5.0999e-03, -1.1678e-02,  1.2344e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.8883e+06, 7.7007e+05, 7.4872e+05,  ..., 6.5939e-01, 6.4502e-01,
        6.1194e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1636]) 

NULL SPACE BASIS :  tensor([[ 0.0107, -0.0080,  0.0069,  ..., -0.0033, -0.0049, -0.0012],
        [ 0.0009, -0.0112, -0.0104,  ..., -0.0035, -0.0027, -0.0045],
        [-0.0141, -0.0061,  0.0073,  ...,  0.0025,  0.0048,  0.0048],
        ...,
        [-0.0114,  0.0124,  0.0190,  ..., -0.0003, -0.0053,  0.0047],
        [-0.0023, -0.0096, -0.0028,  ...,  0.0038,  0.0055, -0.0022],
        [-0.0194,  0.0047, -0.0042,  ...,  0.0031, -0.0042,  0.0021]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 9.0640e-03, -2.5037e-03, -1.2895e-04,  ...,  2.6910e-04,
         -1.1411e-04, -3.8132e-05],
        [-2.5037e-03,  7.0510e-03, -2.1768e-03,  ...,  1.4235e-04,
          6.0493e-05,  4.6773e-06],
        [-1.2895e-04, -2.1768e-03,  7.9155e-03,  ...,  3.4515e-05,
         -5.4941e-07,  1.0433e-04],
        ...,
        [ 2.6910e-04,  1.4235e-04,  3.4515e-05,  ...,  4.2094e-03,
         -2.4839e-04,  9.3603e-06],
        [-1.1411e-04,  6.0493e-05, -5.4941e-07,  ..., -2.4839e-04,
          3.2489e-03, -1.7298e-04],
        [-3.8132e-05,  4.6773e-06,  1.0433e-04,  ...,  9.3603e-06,
         -1.7298e-04,  3.6652e-03]], device='cuda:0') 

reserving basis 615/4608; cond: 44574396.0, radio:3.741982436622493e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 7.5698e-03,  5.3986e-03, -4.7251e-03],
          [-2.7788e-03, -8.8302e-03, -2.4695e-03],
          [ 5.8626e-04, -8.0941e-03,  5.9183e-03]],

         [[-4.3181e-03, -1.1766e-02,  1.0327e-03],
          [-4.0409e-03, -1.3039e-03, -2.4506e-03],
          [ 3.0967e-03,  9.2017e-03, -1.7304e-03]],

         [[-1.1060e-02,  6.3460e-03,  1.1975e-02],
          [ 1.5336e-02,  1.3333e-02,  3.1239e-03],
          [-8.4670e-04,  1.7543e-02,  5.1891e-03]],

         ...,

         [[ 1.1960e-02,  3.4129e-03,  6.8839e-04],
          [ 2.5328e-03, -8.4506e-03,  1.4465e-03],
          [ 6.0909e-03, -5.7337e-03,  7.5653e-03]],

         [[ 7.5743e-03, -1.4396e-03,  3.0609e-03],
          [ 3.7352e-03,  1.3069e-03,  7.2720e-03],
          [ 7.9270e-03,  8.4888e-03, -1.3093e-04]],

         [[-7.8868e-03,  1.2897e-02, -4.0276e-04],
          [-1.0071e-02, -1.2995e-02,  1.6409e-03],
          [-1.2206e-02,  5.2277e-03, -1.4383e-02]]],


        [[[-2.2086e-03,  7.0115e-03, -2.5777e-03],
          [ 1.6072e-03, -2.7657e-03, -9.1016e-03],
          [-4.6803e-03, -1.5639e-02,  5.3532e-03]],

         [[ 3.8627e-03,  6.8622e-03,  3.3512e-03],
          [ 9.8573e-03,  1.4163e-02, -1.2670e-02],
          [ 1.6514e-02,  7.7899e-03, -1.0105e-02]],

         [[ 7.5336e-03,  5.8654e-03,  2.6532e-03],
          [ 3.9204e-03, -4.6192e-03,  6.7651e-03],
          [-2.8180e-03, -7.4514e-03,  5.5211e-04]],

         ...,

         [[-1.8857e-03,  9.0590e-03,  4.7550e-03],
          [-7.8198e-03, -8.2548e-03,  7.0040e-03],
          [-7.5878e-03,  6.3457e-03, -3.6292e-03]],

         [[ 4.7951e-03, -2.8359e-03,  9.4292e-03],
          [-2.8719e-03, -1.3985e-02, -4.5287e-03],
          [ 4.7236e-03, -1.8364e-03, -1.4466e-02]],

         [[ 1.6025e-02,  1.9233e-02, -2.0297e-03],
          [ 4.4660e-04, -5.8228e-05,  6.0623e-03],
          [ 4.4936e-03, -2.8914e-03, -1.0335e-03]]],


        [[[-4.2734e-03, -6.1323e-03, -7.8568e-03],
          [-4.7241e-03, -1.3197e-02, -9.1353e-03],
          [-8.3204e-03, -4.2024e-03, -2.9751e-03]],

         [[-3.4790e-03,  5.3706e-03, -1.2226e-02],
          [ 1.8180e-03, -1.0097e-02, -1.3631e-02],
          [-5.4861e-03, -4.0639e-03, -8.6746e-03]],

         [[-2.1577e-03, -1.2501e-02,  4.1512e-03],
          [-5.4974e-03, -7.5882e-03, -1.3474e-02],
          [-4.9159e-03,  1.3626e-02, -6.2390e-03]],

         ...,

         [[ 3.1567e-03, -1.8960e-02, -3.7038e-04],
          [ 1.5978e-02,  1.1270e-03, -5.2415e-03],
          [-4.8796e-03,  8.1503e-03, -2.3548e-03]],

         [[ 3.8884e-03,  1.8552e-02,  4.6121e-03],
          [-2.7833e-03, -4.9899e-03,  8.9043e-03],
          [ 1.7564e-04, -4.3045e-03,  5.2865e-04]],

         [[-1.3892e-02, -1.5080e-02, -1.6452e-02],
          [-1.2354e-02, -3.2493e-04,  1.3428e-03],
          [-1.2653e-02, -1.1017e-02, -1.7035e-02]]],


        ...,


        [[[-9.6394e-04,  1.7712e-03,  9.1182e-03],
          [ 6.8535e-04,  8.5946e-03, -7.0891e-03],
          [ 7.2186e-03, -5.5608e-04,  4.9033e-03]],

         [[-8.4799e-03, -3.5962e-03,  8.9211e-03],
          [ 9.1563e-03,  6.5290e-03, -7.5574e-03],
          [ 1.2373e-02, -4.0005e-03, -3.0431e-03]],

         [[-1.4570e-02, -1.6168e-03, -1.1408e-02],
          [-1.2500e-02, -1.1383e-02, -3.6928e-03],
          [ 1.1681e-02,  1.6677e-02, -7.2609e-03]],

         ...,

         [[-3.8065e-03,  6.5888e-03,  5.8515e-03],
          [ 9.6737e-03,  2.0780e-03,  5.5188e-03],
          [-4.9523e-03, -3.4574e-03, -2.7406e-03]],

         [[-8.7059e-03,  1.1780e-02, -9.1372e-03],
          [ 9.8911e-03, -1.2195e-02, -8.3069e-03],
          [ 4.9775e-04,  1.4428e-03, -1.6893e-02]],

         [[ 1.6897e-02,  2.0571e-02,  2.3986e-02],
          [ 1.0919e-02, -2.3538e-03,  3.9782e-03],
          [ 1.0277e-02,  1.6122e-02,  6.5677e-03]]],


        [[[ 3.4328e-03, -1.6641e-03,  6.8510e-04],
          [-1.6530e-03, -1.1312e-02, -8.8691e-03],
          [-9.5223e-03,  1.5175e-03, -2.4318e-03]],

         [[-4.2315e-03,  9.7906e-03,  2.6919e-03],
          [-1.3357e-03,  5.9905e-03,  1.3065e-02],
          [-6.4085e-03, -1.0395e-02,  1.1548e-03]],

         [[ 4.7963e-03,  7.6433e-04,  4.5044e-03],
          [-1.2305e-02, -8.4960e-03,  9.7146e-03],
          [ 3.1662e-03,  9.8390e-03, -5.9583e-03]],

         ...,

         [[ 4.4472e-03,  3.9210e-03, -4.2099e-03],
          [ 8.5107e-03,  3.5825e-03,  3.0742e-03],
          [ 2.5818e-03,  1.9753e-03, -1.4407e-02]],

         [[-8.9472e-03, -3.3126e-03, -5.1461e-03],
          [ 2.4881e-03,  1.5478e-03, -1.4617e-02],
          [-2.6019e-04, -8.0881e-03,  2.5696e-03]],

         [[-1.3065e-02,  7.1190e-04,  1.6366e-04],
          [ 2.9637e-03, -4.7370e-03,  4.7688e-03],
          [-1.1480e-02, -1.1784e-02,  1.5212e-03]]],


        [[[-6.6597e-03, -1.0911e-02, -9.0745e-03],
          [ 2.6439e-03, -7.4030e-03, -2.0559e-06],
          [ 2.0681e-03,  1.4448e-03, -5.4181e-03]],

         [[-5.8743e-03,  1.0037e-02,  9.1064e-04],
          [ 1.8031e-03,  8.6099e-03,  2.0558e-03],
          [ 2.9606e-03,  1.1637e-02,  2.2450e-03]],

         [[-1.8694e-03,  5.8849e-03,  6.0156e-03],
          [-7.1255e-03,  1.0280e-02, -1.2656e-02],
          [ 5.2499e-04,  2.0437e-03,  2.8065e-03]],

         ...,

         [[ 5.0186e-03, -7.1414e-04,  1.3275e-02],
          [ 1.0446e-02,  3.9040e-03,  1.4600e-02],
          [ 7.9073e-03,  1.0970e-02,  4.1809e-03]],

         [[-7.9408e-03, -1.9397e-02, -6.4833e-04],
          [-1.1527e-02, -1.2060e-02,  4.2531e-03],
          [ 5.5830e-03, -2.6160e-04,  4.7878e-03]],

         [[-6.8678e-03,  5.0208e-03, -1.5187e-02],
          [-4.7324e-03,  1.0265e-02, -1.2445e-02],
          [ 6.8977e-03, -3.6765e-03, -5.7537e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([8.9780e+05, 1.9815e+05, 1.7817e+05,  ..., 3.0258e-02, 2.6354e-02,
        2.0142e-02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 615]) 

NULL SPACE BASIS :  tensor([[-0.0230, -0.0057, -0.0378,  ..., -0.0048,  0.0084, -0.0149],
        [ 0.0192, -0.0263, -0.0127,  ...,  0.0098,  0.0109,  0.0124],
        [-0.0094,  0.0247,  0.0203,  ..., -0.0030, -0.0139,  0.0194],
        ...,
        [-0.0060,  0.0139, -0.0098,  ..., -0.0023,  0.0007, -0.0016],
        [ 0.0109, -0.0182,  0.0138,  ...,  0.0073,  0.0021,  0.0004],
        [-0.0009,  0.0100, -0.0102,  ..., -0.0042,  0.0016,  0.0014]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.0978e-02, -3.7655e-03,  5.0339e-05,  ..., -1.3361e-04,
          2.9505e-04, -7.6362e-06],
        [-3.7655e-03,  1.7672e-02, -3.6906e-03,  ...,  3.7444e-04,
         -1.2279e-04,  1.2474e-04],
        [ 5.0339e-05, -3.6906e-03,  2.0349e-02,  ...,  3.4290e-06,
          2.5862e-04, -1.5036e-04],
        ...,
        [-1.3361e-04,  3.7444e-04,  3.4290e-06,  ...,  8.9707e-04,
         -5.5262e-04,  2.2220e-04],
        [ 2.9505e-04, -1.2279e-04,  2.5862e-04,  ..., -5.5262e-04,
          1.0761e-03, -5.5706e-04],
        [-7.6362e-06,  1.2474e-04, -1.5036e-04,  ...,  2.2220e-04,
         -5.5706e-04,  8.7746e-04]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 85.100, Total time 0.56
 * Val loss 0.766, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 98.160, Total time 3.13
 * Val loss 0.053, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 71.900, Total time 0.57
 * Val loss 0.841, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 73.140, Total time 3.15
 * Val loss 0.778, Total time 0.00
**************************************************
validation split name: 3
 * Val Acc 66.000, Total time 0.56
 * Val loss 1.012, Total time 0.00
**************************************************
training split name: 3
 * Val Acc 69.740, Total time 3.19
 * Val loss 0.901, Total time 0.00
**************************************************
validation split name: 4
 * Val Acc 69.300, Total time 0.57
 * Val loss 0.912, Total time 0.00
**************************************************
training split name: 4
 * Val Acc 71.700, Total time 3.21
 * Val loss 0.838, Total time 0.00
**************************************************
validation split name: 5
 * Val Acc 71.500, Total time 0.58
 * Val loss 0.837, Total time 0.00
**************************************************
training split name: 5
 * Val Acc 75.400, Total time 3.21
 * Val loss 0.716, Total time 0.00
**************************************************
validation split name: 6
 * Val Acc 73.400, Total time 0.58
 * Val loss 0.727, Total time 0.00
**************************************************
training split name: 6
 * Val Acc 75.760, Total time 3.22
 * Val loss 0.673, Total time 0.00
**************************************************
====================== 7 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0422 (0.0422)	0.0084 (0.0084)	2.449 (2.449)	12.50 (12.50)
[10/157]	0.0955 (0.0919)	0.0578 (0.0540)	2.257 (2.272)	15.62 (19.03)
[20/157]	0.0965 (0.0935)	0.0582 (0.0560)	1.742 (2.108)	56.25 (30.06)
[30/157]	0.0944 (0.0940)	0.0571 (0.0566)	1.818 (1.995)	37.50 (35.08)
[40/157]	0.0954 (0.0944)	0.0580 (0.0570)	1.537 (1.913)	59.38 (38.72)
[50/157]	0.0968 (0.0946)	0.0581 (0.0572)	1.347 (1.845)	71.88 (41.67)
[60/157]	0.0949 (0.0947)	0.0566 (0.0574)	1.449 (1.796)	56.25 (43.70)
[70/157]	0.1200 (0.0951)	0.0799 (0.0578)	1.276 (1.753)	62.50 (45.11)
[80/157]	0.0935 (0.0962)	0.0563 (0.0588)	1.538 (1.719)	46.88 (45.72)
[90/157]	0.0947 (0.0962)	0.0563 (0.0587)	1.442 (1.686)	62.50 (46.70)
[100/157]	0.0972 (0.0961)	0.0588 (0.0586)	1.465 (1.663)	59.38 (47.03)
[110/157]	0.0978 (0.0961)	0.0587 (0.0586)	1.511 (1.645)	43.75 (47.52)
[120/157]	0.0964 (0.0960)	0.0584 (0.0585)	1.337 (1.626)	53.12 (47.70)
[130/157]	0.0964 (0.0959)	0.0583 (0.0585)	1.489 (1.607)	37.50 (48.02)
[140/157]	0.0963 (0.0959)	0.0581 (0.0585)	1.326 (1.590)	50.00 (48.60)
[150/157]	0.0951 (0.0959)	0.0574 (0.0584)	1.467 (1.577)	46.88 (48.88)
[156/157]	0.0775 (0.0957)	0.0512 (0.0584)	1.923 (1.572)	50.00 (49.00)
 * Train Acc 49.000
 * Val Acc 57.500, Total time 0.60
 * Val loss 1.185, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0088 (0.0088)	1.325 (1.325)	46.88 (46.88)
[10/157]	0.1061 (0.1009)	0.0665 (0.0614)	1.232 (1.293)	65.62 (55.11)
[20/157]	0.0953 (0.0997)	0.0580 (0.0613)	1.383 (1.312)	50.00 (54.91)
[30/157]	0.0944 (0.0981)	0.0575 (0.0603)	1.291 (1.323)	59.38 (54.03)
[40/157]	0.0966 (0.0975)	0.0584 (0.0598)	1.357 (1.320)	50.00 (54.80)
[50/157]	0.0965 (0.0970)	0.0590 (0.0595)	1.404 (1.327)	59.38 (54.66)
[60/157]	0.0976 (0.0967)	0.0589 (0.0593)	1.584 (1.337)	46.88 (54.66)
[70/157]	0.0955 (0.0964)	0.0580 (0.0591)	1.386 (1.344)	68.75 (54.45)
[80/157]	0.0959 (0.0963)	0.0579 (0.0590)	1.329 (1.340)	50.00 (54.90)
[90/157]	0.0997 (0.0964)	0.0607 (0.0590)	1.142 (1.332)	65.62 (55.46)
[100/157]	0.0999 (0.0967)	0.0607 (0.0592)	1.156 (1.320)	68.75 (55.97)
[110/157]	0.1009 (0.0969)	0.0592 (0.0593)	1.613 (1.325)	50.00 (55.80)
[120/157]	0.0983 (0.0971)	0.0593 (0.0593)	1.168 (1.326)	56.25 (55.58)
[130/157]	0.1000 (0.0973)	0.0608 (0.0595)	1.127 (1.317)	65.62 (55.87)
[140/157]	0.0993 (0.0974)	0.0599 (0.0595)	1.055 (1.311)	68.75 (56.03)
[150/157]	0.0992 (0.0975)	0.0606 (0.0596)	1.274 (1.308)	50.00 (56.27)
[156/157]	0.0820 (0.0974)	0.0556 (0.0596)	1.456 (1.308)	50.00 (56.22)
 * Train Acc 56.220
 * Val Acc 60.600, Total time 0.61
 * Val loss 1.119, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0091 (0.0091)	1.199 (1.199)	53.12 (53.12)
[10/157]	0.0990 (0.0935)	0.0595 (0.0550)	1.368 (1.223)	40.62 (56.82)
[20/157]	0.0989 (0.0961)	0.0597 (0.0579)	1.208 (1.228)	59.38 (56.70)
[30/157]	0.1004 (0.0972)	0.0602 (0.0588)	1.201 (1.226)	46.88 (56.35)
[40/157]	0.0986 (0.0977)	0.0591 (0.0590)	1.345 (1.255)	56.25 (56.40)
[50/157]	0.1007 (0.0981)	0.0603 (0.0593)	1.310 (1.251)	56.25 (56.92)
[60/157]	0.1017 (0.0983)	0.0614 (0.0596)	0.914 (1.236)	68.75 (57.84)
[70/157]	0.0998 (0.0985)	0.0595 (0.0597)	1.438 (1.229)	59.38 (58.23)
[80/157]	0.1002 (0.0985)	0.0608 (0.0597)	1.288 (1.213)	56.25 (58.91)
[90/157]	0.1001 (0.0986)	0.0608 (0.0597)	1.158 (1.219)	56.25 (59.00)
[100/157]	0.0977 (0.0987)	0.0585 (0.0597)	1.128 (1.225)	65.62 (58.88)
[110/157]	0.0985 (0.0987)	0.0599 (0.0598)	1.303 (1.228)	53.12 (58.95)
[120/157]	0.1014 (0.0987)	0.0613 (0.0599)	1.144 (1.233)	62.50 (58.52)
[130/157]	0.1031 (0.0987)	0.0606 (0.0600)	1.050 (1.237)	62.50 (58.30)
[140/157]	0.0985 (0.0987)	0.0594 (0.0600)	1.238 (1.235)	53.12 (58.55)
[150/157]	0.0989 (0.0987)	0.0597 (0.0600)	1.215 (1.234)	65.62 (58.53)
[156/157]	0.0844 (0.0986)	0.0563 (0.0600)	0.891 (1.235)	75.00 (58.58)
 * Train Acc 58.580
 * Val Acc 59.700, Total time 0.60
 * Val loss 1.108, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0085 (0.0085)	1.900 (1.900)	40.62 (40.62)
[10/157]	0.0994 (0.0933)	0.0605 (0.0551)	1.129 (1.378)	68.75 (53.98)
[20/157]	0.0998 (0.0962)	0.0603 (0.0578)	1.532 (1.303)	40.62 (54.32)
[30/157]	0.0985 (0.0971)	0.0589 (0.0586)	1.321 (1.254)	56.25 (56.15)
[40/157]	0.1000 (0.0975)	0.0607 (0.0591)	1.469 (1.231)	56.25 (57.47)
[50/157]	0.1002 (0.0979)	0.0610 (0.0593)	1.162 (1.217)	65.62 (58.27)
[60/157]	0.0988 (0.0980)	0.0597 (0.0595)	1.338 (1.230)	56.25 (58.15)
[70/157]	0.1002 (0.0981)	0.0616 (0.0596)	1.019 (1.225)	68.75 (58.93)
[80/157]	0.0968 (0.0982)	0.0581 (0.0597)	1.116 (1.222)	56.25 (59.10)
[90/157]	0.0966 (0.0983)	0.0578 (0.0598)	1.000 (1.212)	75.00 (59.62)
[100/157]	0.0999 (0.0984)	0.0600 (0.0598)	1.420 (1.213)	59.38 (59.41)
[110/157]	0.1000 (0.0984)	0.0610 (0.0598)	1.192 (1.207)	59.38 (59.74)
[120/157]	0.0983 (0.0985)	0.0587 (0.0598)	1.176 (1.210)	56.25 (59.50)
[130/157]	0.0993 (0.0985)	0.0612 (0.0599)	0.997 (1.203)	71.88 (59.71)
[140/157]	0.0987 (0.0985)	0.0601 (0.0599)	1.405 (1.200)	56.25 (60.00)
[150/157]	0.0993 (0.0985)	0.0611 (0.0600)	1.011 (1.200)	68.75 (59.95)
[156/157]	0.0806 (0.0984)	0.0550 (0.0599)	1.086 (1.202)	62.50 (59.90)
 * Train Acc 59.900
 * Val Acc 63.200, Total time 0.60
 * Val loss 1.041, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0084 (0.0084)	1.334 (1.334)	56.25 (56.25)
[10/157]	0.0989 (0.0933)	0.0597 (0.0551)	1.271 (1.168)	62.50 (59.66)
[20/157]	0.0987 (0.0960)	0.0599 (0.0578)	1.124 (1.148)	59.38 (60.86)
[30/157]	0.1004 (0.0970)	0.0601 (0.0587)	1.167 (1.181)	65.62 (59.58)
[40/157]	0.0993 (0.0973)	0.0606 (0.0591)	1.182 (1.160)	59.38 (60.82)
[50/157]	0.0999 (0.0977)	0.0616 (0.0595)	1.129 (1.177)	71.88 (60.91)
[60/157]	0.0990 (0.0978)	0.0596 (0.0596)	1.359 (1.187)	53.12 (61.48)
[70/157]	0.0998 (0.0979)	0.0610 (0.0597)	1.456 (1.188)	50.00 (61.00)
[80/157]	0.0996 (0.0980)	0.0605 (0.0598)	1.084 (1.177)	50.00 (61.15)
[90/157]	0.0992 (0.0981)	0.0600 (0.0599)	1.143 (1.175)	65.62 (61.02)
[100/157]	0.0987 (0.0982)	0.0604 (0.0600)	1.228 (1.176)	53.12 (60.89)
[110/157]	0.0997 (0.0983)	0.0607 (0.0600)	0.992 (1.184)	68.75 (60.42)
[120/157]	0.0990 (0.0983)	0.0609 (0.0601)	1.265 (1.176)	46.88 (60.80)
[130/157]	0.0998 (0.0984)	0.0597 (0.0601)	1.287 (1.172)	46.88 (60.69)
[140/157]	0.0993 (0.0984)	0.0613 (0.0602)	1.188 (1.171)	62.50 (60.82)
[150/157]	0.1004 (0.0984)	0.0603 (0.0602)	0.788 (1.169)	81.25 (61.26)
[156/157]	0.0824 (0.0983)	0.0562 (0.0602)	1.452 (1.166)	50.00 (61.30)
 * Train Acc 61.300
 * Val Acc 62.400, Total time 0.60
 * Val loss 1.025, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0084 (0.0084)	0.959 (0.959)	68.75 (68.75)
[10/157]	0.0999 (0.0935)	0.0612 (0.0545)	1.021 (1.215)	71.88 (57.10)
[20/157]	0.1086 (0.1007)	0.0688 (0.0619)	1.068 (1.170)	62.50 (60.71)
[30/157]	0.1103 (0.1034)	0.0699 (0.0642)	1.447 (1.177)	59.38 (60.99)
[40/157]	0.1205 (0.1034)	0.0801 (0.0645)	1.191 (1.167)	65.62 (61.74)
[50/157]	0.1048 (0.1039)	0.0652 (0.0651)	1.128 (1.156)	68.75 (62.25)
[60/157]	0.1060 (0.1042)	0.0667 (0.0653)	1.326 (1.160)	62.50 (62.09)
[70/157]	0.1062 (0.1043)	0.0657 (0.0654)	0.953 (1.142)	71.88 (62.81)
[80/157]	0.0954 (0.1038)	0.0576 (0.0650)	1.121 (1.135)	56.25 (62.89)
[90/157]	0.0956 (0.1029)	0.0575 (0.0643)	1.299 (1.137)	62.50 (62.74)
[100/157]	0.1065 (0.1023)	0.0659 (0.0637)	0.961 (1.135)	68.75 (62.75)
[110/157]	0.0969 (0.1027)	0.0578 (0.0640)	0.846 (1.130)	71.88 (62.92)
[120/157]	0.0935 (0.1022)	0.0561 (0.0636)	1.217 (1.138)	56.25 (62.45)
[130/157]	0.0973 (0.1016)	0.0584 (0.0632)	1.483 (1.147)	56.25 (62.00)
[140/157]	0.0973 (0.1012)	0.0583 (0.0628)	0.931 (1.146)	71.88 (61.81)
[150/157]	0.1123 (0.1016)	0.0720 (0.0631)	1.282 (1.149)	56.25 (61.57)
[156/157]	0.0934 (0.1018)	0.0676 (0.0634)	1.589 (1.148)	37.50 (61.56)
 * Train Acc 61.560
 * Val Acc 63.700, Total time 0.60
 * Val loss 1.008, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0085 (0.0085)	0.952 (0.952)	71.88 (71.88)
[10/157]	0.0979 (0.0923)	0.0590 (0.0534)	1.023 (1.076)	62.50 (63.64)
[20/157]	0.0980 (0.0942)	0.0591 (0.0560)	0.808 (1.107)	81.25 (62.80)
[30/157]	0.1009 (0.0948)	0.0623 (0.0570)	0.866 (1.101)	75.00 (63.61)
[40/157]	0.1014 (0.0962)	0.0626 (0.0583)	1.214 (1.105)	53.12 (62.96)
[50/157]	0.1008 (0.0972)	0.0611 (0.0591)	1.044 (1.092)	62.50 (63.17)
[60/157]	0.1028 (0.0979)	0.0631 (0.0597)	1.299 (1.118)	59.38 (62.70)
[70/157]	0.1002 (0.0982)	0.0608 (0.0600)	1.383 (1.126)	56.25 (62.24)
[80/157]	0.1003 (0.0985)	0.0610 (0.0602)	0.929 (1.140)	68.75 (61.77)
[90/157]	0.1015 (0.0987)	0.0629 (0.0605)	1.242 (1.147)	65.62 (61.37)
[100/157]	0.1008 (0.0989)	0.0615 (0.0606)	1.177 (1.140)	53.12 (61.42)
[110/157]	0.1011 (0.0991)	0.0616 (0.0608)	1.122 (1.136)	56.25 (61.68)
[120/157]	0.1015 (0.0992)	0.0623 (0.0609)	1.233 (1.124)	53.12 (62.14)
[130/157]	0.1003 (0.0993)	0.0613 (0.0610)	1.356 (1.125)	53.12 (62.17)
[140/157]	0.1001 (0.0994)	0.0615 (0.0611)	1.197 (1.123)	62.50 (62.10)
[150/157]	0.1021 (0.0995)	0.0626 (0.0612)	1.047 (1.123)	56.25 (61.88)
[156/157]	0.0846 (0.0994)	0.0578 (0.0612)	1.432 (1.125)	50.00 (61.68)
 * Train Acc 61.680
 * Val Acc 64.300, Total time 0.60
 * Val loss 0.988, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0088 (0.0088)	1.096 (1.096)	65.62 (65.62)
[10/157]	0.1004 (0.0949)	0.0613 (0.0564)	1.105 (1.099)	56.25 (63.07)
[20/157]	0.1004 (0.0974)	0.0617 (0.0591)	0.946 (1.100)	68.75 (63.39)
[30/157]	0.1020 (0.0988)	0.0626 (0.0604)	1.239 (1.117)	53.12 (62.20)
[40/157]	0.0992 (0.0992)	0.0597 (0.0605)	0.974 (1.100)	71.88 (62.96)
[50/157]	0.0973 (0.0997)	0.0584 (0.0607)	1.069 (1.102)	59.38 (62.99)
[60/157]	0.1014 (0.0999)	0.0621 (0.0610)	1.141 (1.118)	56.25 (62.55)
[70/157]	0.1014 (0.1001)	0.0618 (0.0612)	0.979 (1.115)	75.00 (62.72)
[80/157]	0.1023 (0.1003)	0.0622 (0.0613)	1.059 (1.110)	62.50 (62.89)
[90/157]	0.0998 (0.1003)	0.0611 (0.0614)	0.827 (1.101)	71.88 (63.12)
[100/157]	0.1011 (0.1004)	0.0617 (0.0615)	1.133 (1.109)	65.62 (62.69)
[110/157]	0.1008 (0.1004)	0.0619 (0.0615)	1.093 (1.111)	53.12 (62.50)
[120/157]	0.1018 (0.1004)	0.0627 (0.0615)	0.991 (1.112)	65.62 (62.32)
[130/157]	0.1004 (0.1005)	0.0607 (0.0616)	1.505 (1.112)	37.50 (62.50)
[140/157]	0.1023 (0.1005)	0.0629 (0.0617)	1.226 (1.113)	56.25 (62.50)
[150/157]	0.1005 (0.1005)	0.0614 (0.0617)	1.024 (1.113)	71.88 (62.75)
[156/157]	0.0840 (0.1004)	0.0571 (0.0617)	1.643 (1.111)	25.00 (62.92)
 * Train Acc 62.920
 * Val Acc 66.000, Total time 0.62
 * Val loss 0.968, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0086 (0.0086)	1.114 (1.114)	59.38 (59.38)
[10/157]	0.0996 (0.0954)	0.0609 (0.0571)	0.992 (1.060)	59.38 (61.36)
[20/157]	0.0997 (0.0977)	0.0593 (0.0594)	1.451 (1.136)	43.75 (59.08)
[30/157]	0.1015 (0.0988)	0.0622 (0.0603)	0.925 (1.096)	75.00 (61.69)
[40/157]	0.1013 (0.0993)	0.0616 (0.0607)	1.033 (1.084)	68.75 (62.58)
[50/157]	0.1006 (0.0995)	0.0613 (0.0609)	1.098 (1.075)	81.25 (63.05)
[60/157]	0.1025 (0.0997)	0.0624 (0.0612)	1.209 (1.084)	56.25 (63.22)
[70/157]	0.0974 (0.0999)	0.0577 (0.0612)	1.232 (1.082)	46.88 (63.78)
[80/157]	0.1009 (0.0999)	0.0616 (0.0613)	1.216 (1.091)	62.50 (63.50)
[90/157]	0.1007 (0.1001)	0.0608 (0.0614)	0.948 (1.087)	68.75 (63.56)
[100/157]	0.1013 (0.1001)	0.0621 (0.0613)	1.344 (1.094)	59.38 (63.34)
[110/157]	0.1006 (0.1002)	0.0609 (0.0615)	1.026 (1.085)	62.50 (63.96)
[120/157]	0.1003 (0.1002)	0.0618 (0.0615)	1.243 (1.085)	59.38 (63.89)
[130/157]	0.1023 (0.1003)	0.0630 (0.0616)	1.399 (1.093)	56.25 (63.62)
[140/157]	0.1004 (0.1004)	0.0612 (0.0616)	1.167 (1.094)	53.12 (63.63)
[150/157]	0.1012 (0.1004)	0.0614 (0.0616)	1.078 (1.099)	65.62 (63.62)
[156/157]	0.0853 (0.1003)	0.0576 (0.0616)	1.020 (1.100)	75.00 (63.70)
 * Train Acc 63.700
 * Val Acc 67.000, Total time 0.59
 * Val loss 0.949, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0081 (0.0081)	0.638 (0.638)	81.25 (81.25)
[10/157]	0.1021 (0.0959)	0.0616 (0.0566)	0.861 (1.081)	71.88 (63.64)
[20/157]	0.1051 (0.0984)	0.0641 (0.0594)	0.998 (1.098)	62.50 (63.10)
[30/157]	0.1026 (0.0991)	0.0626 (0.0600)	0.894 (1.082)	75.00 (63.91)
[40/157]	0.1005 (0.0994)	0.0618 (0.0604)	1.194 (1.075)	62.50 (63.87)
[50/157]	0.1030 (0.0998)	0.0622 (0.0608)	1.459 (1.087)	46.88 (63.17)
[60/157]	0.0993 (0.0998)	0.0606 (0.0608)	1.149 (1.088)	68.75 (63.58)
[70/157]	0.1018 (0.1000)	0.0632 (0.0611)	1.123 (1.097)	65.62 (63.34)
[80/157]	0.1002 (0.1001)	0.0607 (0.0612)	1.062 (1.098)	62.50 (63.46)
[90/157]	0.1008 (0.1001)	0.0610 (0.0612)	1.051 (1.085)	65.62 (64.15)
[100/157]	0.1044 (0.1002)	0.0629 (0.0614)	1.138 (1.090)	62.50 (64.20)
[110/157]	0.1010 (0.1002)	0.0613 (0.0613)	1.077 (1.081)	65.62 (64.61)
[120/157]	0.1001 (0.1002)	0.0609 (0.0614)	0.998 (1.084)	68.75 (64.41)
[130/157]	0.1010 (0.1002)	0.0617 (0.0614)	1.333 (1.080)	53.12 (64.41)
[140/157]	0.0992 (0.1003)	0.0599 (0.0615)	1.055 (1.074)	68.75 (64.58)
[150/157]	0.1007 (0.1003)	0.0607 (0.0615)	0.940 (1.076)	65.62 (64.38)
[156/157]	0.0840 (0.1002)	0.0572 (0.0615)	0.507 (1.073)	87.50 (64.44)
 * Train Acc 64.440
 * Val Acc 65.900, Total time 0.60
 * Val loss 0.975, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0490 (0.0490)	0.0107 (0.0107)	1.275 (1.275)	53.12 (53.12)
[10/157]	0.1027 (0.0964)	0.0611 (0.0571)	1.012 (1.111)	62.50 (60.80)
[20/157]	0.1035 (0.0986)	0.0626 (0.0595)	0.972 (1.042)	68.75 (65.18)
[30/157]	0.1001 (0.0993)	0.0605 (0.0601)	1.160 (1.032)	71.88 (66.23)
[40/157]	0.1013 (0.0995)	0.0616 (0.0605)	1.200 (1.078)	53.12 (64.18)
[50/157]	0.1064 (0.1000)	0.0628 (0.0610)	0.896 (1.083)	75.00 (63.66)
[60/157]	0.1020 (0.1001)	0.0622 (0.0611)	0.918 (1.077)	68.75 (63.99)
[70/157]	0.1005 (0.1002)	0.0610 (0.0612)	1.413 (1.092)	46.88 (63.78)
[80/157]	0.1023 (0.1004)	0.0619 (0.0614)	1.265 (1.090)	56.25 (63.66)
[90/157]	0.1009 (0.1004)	0.0606 (0.0614)	0.837 (1.086)	78.12 (63.67)
[100/157]	0.1008 (0.1006)	0.0595 (0.0615)	0.987 (1.081)	71.88 (63.92)
[110/157]	0.1021 (0.1005)	0.0634 (0.0615)	1.150 (1.073)	59.38 (63.96)
[120/157]	0.1012 (0.1006)	0.0614 (0.0616)	0.774 (1.065)	75.00 (64.26)
[130/157]	0.0997 (0.1006)	0.0606 (0.0616)	1.082 (1.061)	59.38 (64.48)
[140/157]	0.1023 (0.1006)	0.0625 (0.0616)	0.716 (1.055)	78.12 (64.72)
[150/157]	0.1008 (0.1006)	0.0608 (0.0616)	1.294 (1.061)	59.38 (64.76)
[156/157]	0.0844 (0.1005)	0.0568 (0.0616)	0.739 (1.064)	75.00 (64.58)
 * Train Acc 64.580
 * Val Acc 66.800, Total time 0.60
 * Val loss 0.947, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0467 (0.0467)	0.0096 (0.0096)	1.127 (1.127)	59.38 (59.38)
[10/157]	0.1011 (0.0956)	0.0610 (0.0567)	1.073 (1.115)	68.75 (64.77)
[20/157]	0.1006 (0.0978)	0.0606 (0.0590)	1.143 (1.123)	56.25 (63.99)
[30/157]	0.1012 (0.0986)	0.0617 (0.0600)	1.202 (1.103)	59.38 (63.81)
[40/157]	0.0987 (0.0993)	0.0593 (0.0606)	0.754 (1.089)	75.00 (64.10)
[50/157]	0.1010 (0.0994)	0.0616 (0.0607)	1.418 (1.091)	46.88 (63.42)
[60/157]	0.1017 (0.0996)	0.0625 (0.0609)	0.977 (1.072)	62.50 (64.40)
[70/157]	0.0998 (0.0998)	0.0607 (0.0611)	0.994 (1.068)	62.50 (64.79)
[80/157]	0.1015 (0.0999)	0.0628 (0.0612)	1.167 (1.070)	62.50 (64.70)
[90/157]	0.1031 (0.1001)	0.0619 (0.0614)	1.319 (1.073)	62.50 (64.59)
[100/157]	0.1007 (0.1001)	0.0609 (0.0614)	1.183 (1.057)	65.62 (65.19)
[110/157]	0.1011 (0.1001)	0.0619 (0.0614)	1.454 (1.063)	43.75 (64.72)
[120/157]	0.1022 (0.1002)	0.0612 (0.0616)	0.900 (1.058)	68.75 (64.80)
[130/157]	0.1002 (0.1002)	0.0611 (0.0615)	0.991 (1.062)	75.00 (64.81)
[140/157]	0.1004 (0.1002)	0.0619 (0.0615)	0.883 (1.052)	84.38 (65.18)
[150/157]	0.1008 (0.1002)	0.0618 (0.0616)	0.852 (1.056)	68.75 (64.84)
[156/157]	0.0838 (0.1001)	0.0568 (0.0616)	0.525 (1.054)	100.00 (64.92)
 * Train Acc 64.920
 * Val Acc 66.600, Total time 0.62
 * Val loss 0.931, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0092 (0.0092)	1.042 (1.042)	62.50 (62.50)
[10/157]	0.1029 (0.0955)	0.0625 (0.0572)	0.809 (0.983)	78.12 (69.03)
[20/157]	0.0998 (0.0978)	0.0611 (0.0594)	1.092 (1.004)	59.38 (67.11)
[30/157]	0.1003 (0.0986)	0.0613 (0.0601)	0.967 (0.998)	68.75 (67.34)
[40/157]	0.1036 (0.0992)	0.0632 (0.0606)	0.898 (0.980)	75.00 (68.67)
[50/157]	0.1003 (0.0995)	0.0609 (0.0608)	1.332 (0.985)	59.38 (68.32)
[60/157]	0.1013 (0.0997)	0.0618 (0.0610)	1.774 (0.994)	28.12 (67.52)
[70/157]	0.1014 (0.0999)	0.0609 (0.0612)	1.290 (0.997)	56.25 (67.43)
[80/157]	0.0999 (0.0999)	0.0612 (0.0611)	0.965 (1.007)	65.62 (67.09)
[90/157]	0.1006 (0.0999)	0.0615 (0.0612)	1.040 (1.018)	65.62 (66.96)
[100/157]	0.1002 (0.0999)	0.0614 (0.0612)	1.098 (1.021)	59.38 (66.71)
[110/157]	0.1002 (0.0999)	0.0608 (0.0612)	1.291 (1.028)	56.25 (66.55)
[120/157]	0.1007 (0.0999)	0.0612 (0.0613)	0.892 (1.025)	71.88 (66.45)
[130/157]	0.1012 (0.1000)	0.0624 (0.0613)	0.682 (1.023)	75.00 (66.77)
[140/157]	0.1007 (0.1001)	0.0617 (0.0613)	0.872 (1.020)	59.38 (66.62)
[150/157]	0.0982 (0.1001)	0.0588 (0.0613)	1.309 (1.026)	59.38 (66.51)
[156/157]	0.0861 (0.1000)	0.0582 (0.0614)	0.997 (1.027)	75.00 (66.42)
 * Train Acc 66.420
 * Val Acc 66.300, Total time 0.61
 * Val loss 0.924, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0085 (0.0085)	0.947 (0.947)	65.62 (65.62)
[10/157]	0.1024 (0.0956)	0.0628 (0.0566)	1.291 (1.021)	50.00 (63.92)
[20/157]	0.0992 (0.0980)	0.0605 (0.0588)	0.954 (1.003)	78.12 (67.11)
[30/157]	0.0986 (0.0986)	0.0598 (0.0596)	1.279 (1.027)	68.75 (66.03)
[40/157]	0.1008 (0.0989)	0.0618 (0.0601)	1.099 (1.044)	65.62 (65.40)
[50/157]	0.1019 (0.0992)	0.0604 (0.0604)	1.167 (1.032)	65.62 (66.42)
[60/157]	0.1021 (0.0994)	0.0623 (0.0605)	1.393 (1.035)	50.00 (66.14)
[70/157]	0.1004 (0.0997)	0.0604 (0.0606)	1.214 (1.024)	56.25 (66.51)
[80/157]	0.1021 (0.0998)	0.0628 (0.0608)	1.330 (1.023)	46.88 (66.59)
[90/157]	0.1004 (0.1000)	0.0601 (0.0609)	0.874 (1.015)	75.00 (66.90)
[100/157]	0.1009 (0.1000)	0.0609 (0.0609)	0.894 (1.022)	75.00 (67.08)
[110/157]	0.1013 (0.1001)	0.0619 (0.0611)	0.999 (1.022)	65.62 (66.95)
[120/157]	0.1002 (0.1001)	0.0611 (0.0611)	1.180 (1.027)	62.50 (66.74)
[130/157]	0.1003 (0.1001)	0.0614 (0.0611)	1.045 (1.027)	59.38 (66.34)
[140/157]	0.1013 (0.1002)	0.0621 (0.0612)	0.985 (1.031)	71.88 (66.27)
[150/157]	0.1004 (0.1002)	0.0615 (0.0612)	1.038 (1.029)	71.88 (66.20)
[156/157]	0.0827 (0.1001)	0.0547 (0.0612)	0.858 (1.033)	87.50 (66.18)
 * Train Acc 66.180
 * Val Acc 68.400, Total time 0.60
 * Val loss 0.905, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0085 (0.0085)	1.000 (1.000)	53.12 (53.12)
[10/157]	0.1008 (0.0948)	0.0616 (0.0564)	0.777 (1.058)	78.12 (67.33)
[20/157]	0.1008 (0.0973)	0.0621 (0.0591)	0.978 (1.017)	68.75 (66.82)
[30/157]	0.1015 (0.0985)	0.0625 (0.0603)	1.057 (1.020)	53.12 (66.53)
[40/157]	0.0998 (0.0990)	0.0609 (0.0607)	1.071 (1.037)	65.62 (66.46)
[50/157]	0.1005 (0.0992)	0.0611 (0.0609)	0.923 (1.040)	71.88 (66.36)
[60/157]	0.0994 (0.0994)	0.0608 (0.0610)	1.082 (1.040)	75.00 (66.85)
[70/157]	0.1033 (0.0996)	0.0640 (0.0612)	1.333 (1.042)	56.25 (66.55)
[80/157]	0.0989 (0.0998)	0.0595 (0.0614)	0.783 (1.032)	75.00 (66.74)
[90/157]	0.1004 (0.0998)	0.0611 (0.0614)	1.042 (1.020)	71.88 (67.27)
[100/157]	0.1009 (0.0999)	0.0620 (0.0614)	0.894 (1.025)	65.62 (66.96)
[110/157]	0.1004 (0.1000)	0.0610 (0.0615)	1.012 (1.025)	68.75 (66.75)
[120/157]	0.0985 (0.1000)	0.0595 (0.0615)	0.961 (1.022)	65.62 (66.81)
[130/157]	0.1035 (0.1000)	0.0633 (0.0615)	0.963 (1.022)	65.62 (66.87)
[140/157]	0.0998 (0.1001)	0.0605 (0.0615)	1.140 (1.020)	71.88 (67.24)
[150/157]	0.1014 (0.1002)	0.0621 (0.0615)	0.727 (1.022)	78.12 (67.12)
[156/157]	0.0847 (0.1001)	0.0567 (0.0615)	1.908 (1.026)	25.00 (66.94)
 * Train Acc 66.940
 * Val Acc 67.300, Total time 0.61
 * Val loss 0.913, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0090 (0.0090)	1.018 (1.018)	68.75 (68.75)
[10/157]	0.1014 (0.0957)	0.0618 (0.0570)	1.258 (1.064)	53.12 (63.35)
[20/157]	0.1024 (0.0982)	0.0605 (0.0592)	1.033 (1.027)	62.50 (65.77)
[30/157]	0.1022 (0.0992)	0.0626 (0.0603)	0.998 (1.039)	65.62 (65.52)
[40/157]	0.1031 (0.0996)	0.0610 (0.0607)	1.128 (1.015)	62.50 (66.54)
[50/157]	0.1007 (0.1000)	0.0605 (0.0609)	0.869 (1.016)	68.75 (66.05)
[60/157]	0.1030 (0.1003)	0.0625 (0.0612)	0.878 (1.020)	75.00 (66.03)
[70/157]	0.1015 (0.1003)	0.0617 (0.0612)	1.027 (1.024)	62.50 (65.85)
[80/157]	0.1002 (0.1005)	0.0611 (0.0613)	1.005 (1.020)	78.12 (66.17)
[90/157]	0.1015 (0.1005)	0.0622 (0.0613)	0.876 (1.011)	78.12 (66.90)
[100/157]	0.1006 (0.1005)	0.0620 (0.0614)	1.159 (1.015)	68.75 (66.77)
[110/157]	0.1001 (0.1006)	0.0602 (0.0614)	1.040 (1.015)	65.62 (66.86)
[120/157]	0.1000 (0.1006)	0.0607 (0.0614)	0.961 (1.017)	71.88 (66.81)
[130/157]	0.1010 (0.1006)	0.0617 (0.0614)	0.807 (1.017)	65.62 (66.67)
[140/157]	0.0998 (0.1006)	0.0608 (0.0615)	1.139 (1.013)	53.12 (66.67)
[150/157]	0.1015 (0.1006)	0.0614 (0.0616)	0.798 (1.009)	78.12 (66.83)
[156/157]	0.0841 (0.1005)	0.0564 (0.0615)	1.320 (1.010)	50.00 (66.74)
 * Train Acc 66.740
 * Val Acc 68.200, Total time 0.60
 * Val loss 0.898, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0084 (0.0084)	1.262 (1.262)	62.50 (62.50)
[10/157]	0.1000 (0.0947)	0.0610 (0.0561)	1.262 (1.045)	53.12 (63.64)
[20/157]	0.1029 (0.0975)	0.0630 (0.0589)	0.815 (1.050)	75.00 (64.14)
[30/157]	0.1018 (0.0987)	0.0623 (0.0600)	0.823 (1.019)	68.75 (64.92)
[40/157]	0.1007 (0.0991)	0.0617 (0.0603)	1.078 (1.017)	65.62 (64.86)
[50/157]	0.1015 (0.0995)	0.0624 (0.0608)	0.926 (0.999)	71.88 (65.81)
[60/157]	0.1008 (0.0997)	0.0610 (0.0609)	0.863 (1.006)	71.88 (65.78)
[70/157]	0.1028 (0.0999)	0.0628 (0.0612)	0.774 (1.010)	71.88 (65.71)
[80/157]	0.1024 (0.1001)	0.0607 (0.0612)	0.804 (1.017)	81.25 (65.43)
[90/157]	0.1020 (0.1002)	0.0608 (0.0613)	0.944 (1.012)	65.62 (65.66)
[100/157]	0.1042 (0.1002)	0.0607 (0.0612)	0.917 (1.007)	71.88 (66.18)
[110/157]	0.1010 (0.1002)	0.0618 (0.0612)	0.960 (1.008)	71.88 (66.24)
[120/157]	0.1012 (0.1002)	0.0622 (0.0613)	0.993 (1.001)	65.62 (66.81)
[130/157]	0.1005 (0.1003)	0.0617 (0.0614)	0.960 (1.002)	65.62 (66.53)
[140/157]	0.1004 (0.1003)	0.0612 (0.0614)	0.862 (0.998)	75.00 (66.98)
[150/157]	0.1006 (0.1003)	0.0618 (0.0614)	0.909 (1.004)	65.62 (66.74)
[156/157]	0.0843 (0.1001)	0.0575 (0.0614)	1.343 (1.007)	50.00 (66.62)
 * Train Acc 66.620
 * Val Acc 68.100, Total time 0.60
 * Val loss 0.891, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0085 (0.0085)	0.939 (0.939)	59.38 (59.38)
[10/157]	0.1018 (0.0952)	0.0623 (0.0570)	1.379 (1.001)	40.62 (63.35)
[20/157]	0.0999 (0.0975)	0.0611 (0.0592)	0.963 (0.997)	65.62 (63.99)
[30/157]	0.0977 (0.0983)	0.0582 (0.0599)	1.249 (0.989)	50.00 (65.12)
[40/157]	0.0991 (0.0987)	0.0604 (0.0603)	0.965 (0.994)	68.75 (66.31)
[50/157]	0.1000 (0.0990)	0.0610 (0.0606)	1.017 (0.997)	62.50 (66.18)
[60/157]	0.1004 (0.0991)	0.0610 (0.0608)	1.212 (1.004)	50.00 (66.24)
[70/157]	0.1004 (0.0993)	0.0612 (0.0609)	0.978 (0.999)	75.00 (66.90)
[80/157]	0.1013 (0.0995)	0.0623 (0.0611)	1.052 (1.011)	68.75 (66.24)
[90/157]	0.0985 (0.0996)	0.0580 (0.0612)	0.712 (0.999)	68.75 (66.69)
[100/157]	0.1029 (0.0998)	0.0621 (0.0613)	1.040 (1.004)	62.50 (66.46)
[110/157]	0.1005 (0.0999)	0.0611 (0.0613)	0.869 (0.999)	78.12 (66.78)
[120/157]	0.1011 (0.0999)	0.0615 (0.0613)	0.953 (0.999)	71.88 (67.05)
[130/157]	0.1010 (0.1000)	0.0610 (0.0614)	1.038 (0.998)	68.75 (67.06)
[140/157]	0.0997 (0.1000)	0.0608 (0.0614)	1.008 (1.002)	71.88 (67.02)
[150/157]	0.1013 (0.1001)	0.0624 (0.0615)	1.146 (1.001)	62.50 (67.03)
[156/157]	0.0845 (0.1000)	0.0566 (0.0614)	1.227 (1.003)	37.50 (66.94)
 * Train Acc 66.940
 * Val Acc 68.700, Total time 0.60
 * Val loss 0.883, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0100 (0.0100)	1.034 (1.034)	68.75 (68.75)
[10/157]	0.1022 (0.0948)	0.0636 (0.0565)	1.215 (1.021)	65.62 (67.33)
[20/157]	0.1013 (0.0975)	0.0619 (0.0591)	1.098 (1.006)	50.00 (67.41)
[30/157]	0.1016 (0.0988)	0.0612 (0.0602)	0.810 (1.008)	68.75 (67.44)
[40/157]	0.1010 (0.0992)	0.0615 (0.0605)	1.211 (0.997)	56.25 (67.99)
[50/157]	0.1020 (0.0994)	0.0630 (0.0609)	0.896 (1.010)	75.00 (67.40)
[60/157]	0.0994 (0.0997)	0.0603 (0.0611)	1.233 (1.013)	62.50 (66.96)
[70/157]	0.1005 (0.0998)	0.0616 (0.0612)	0.884 (1.019)	68.75 (66.51)
[80/157]	0.1028 (0.0999)	0.0637 (0.0614)	0.826 (1.010)	68.75 (66.98)
[90/157]	0.1011 (0.1000)	0.0611 (0.0614)	0.896 (1.008)	71.88 (66.90)
[100/157]	0.1018 (0.1001)	0.0621 (0.0614)	0.996 (1.005)	71.88 (66.86)
[110/157]	0.1010 (0.1002)	0.0614 (0.0615)	1.297 (1.012)	59.38 (66.75)
[120/157]	0.1001 (0.1002)	0.0612 (0.0615)	0.925 (1.008)	71.88 (66.79)
[130/157]	0.1001 (0.1002)	0.0612 (0.0615)	0.809 (1.007)	62.50 (66.72)
[140/157]	0.1013 (0.1002)	0.0623 (0.0616)	0.906 (1.000)	71.88 (67.13)
[150/157]	0.1004 (0.1002)	0.0612 (0.0616)	0.751 (0.996)	78.12 (67.26)
[156/157]	0.0833 (0.1001)	0.0568 (0.0616)	1.214 (0.993)	62.50 (67.46)
 * Train Acc 67.460
 * Val Acc 67.700, Total time 0.61
 * Val loss 0.902, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0078 (0.0078)	1.223 (1.223)	53.12 (53.12)
[10/157]	0.0998 (0.0945)	0.0608 (0.0560)	0.661 (0.885)	84.38 (69.89)
[20/157]	0.1013 (0.0975)	0.0626 (0.0590)	1.106 (0.954)	56.25 (68.30)
[30/157]	0.0999 (0.0986)	0.0612 (0.0601)	1.027 (0.985)	62.50 (67.04)
[40/157]	0.1026 (0.0991)	0.0610 (0.0604)	1.019 (0.982)	62.50 (67.45)
[50/157]	0.1002 (0.0995)	0.0610 (0.0607)	1.008 (0.978)	65.62 (67.59)
[60/157]	0.1004 (0.0996)	0.0618 (0.0609)	0.828 (0.981)	75.00 (67.21)
[70/157]	0.1023 (0.0999)	0.0625 (0.0611)	1.164 (0.989)	59.38 (67.34)
[80/157]	0.0998 (0.0999)	0.0608 (0.0612)	1.223 (0.994)	53.12 (67.21)
[90/157]	0.1001 (0.0999)	0.0611 (0.0613)	0.926 (1.001)	65.62 (66.69)
[100/157]	0.1001 (0.1000)	0.0610 (0.0613)	1.000 (0.998)	71.88 (66.92)
[110/157]	0.1022 (0.1001)	0.0624 (0.0614)	0.912 (1.000)	62.50 (66.92)
[120/157]	0.1006 (0.1001)	0.0612 (0.0614)	0.896 (0.996)	65.62 (67.12)
[130/157]	0.1028 (0.1002)	0.0628 (0.0615)	1.097 (0.994)	71.88 (67.22)
[140/157]	0.1014 (0.1003)	0.0614 (0.0615)	0.925 (0.999)	71.88 (67.02)
[150/157]	0.1009 (0.1003)	0.0605 (0.0616)	1.016 (0.995)	62.50 (67.26)
[156/157]	0.0841 (0.1002)	0.0555 (0.0615)	0.911 (0.991)	75.00 (67.38)
 * Train Acc 67.380
 * Val Acc 69.000, Total time 0.61
 * Val loss 0.889, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0083 (0.0083)	0.691 (0.691)	81.25 (81.25)
[10/157]	0.0990 (0.0946)	0.0607 (0.0559)	1.038 (0.999)	78.12 (68.75)
[20/157]	0.1015 (0.0973)	0.0624 (0.0589)	0.973 (1.012)	59.38 (67.26)
[30/157]	0.1022 (0.0987)	0.0624 (0.0602)	0.830 (0.977)	78.12 (68.85)
[40/157]	0.1007 (0.0990)	0.0617 (0.0606)	1.190 (0.987)	62.50 (68.45)
[50/157]	0.1005 (0.0992)	0.0612 (0.0608)	0.892 (0.977)	68.75 (68.63)
[60/157]	0.1026 (0.0996)	0.0628 (0.0610)	0.985 (0.982)	65.62 (68.55)
[70/157]	0.1004 (0.0997)	0.0614 (0.0611)	0.886 (0.968)	62.50 (68.05)
[80/157]	0.1001 (0.0997)	0.0610 (0.0612)	1.155 (0.975)	56.25 (67.98)
[90/157]	0.1019 (0.0999)	0.0624 (0.0613)	0.964 (0.977)	75.00 (67.96)
[100/157]	0.1010 (0.1000)	0.0615 (0.0614)	1.063 (0.986)	53.12 (67.73)
[110/157]	0.1004 (0.1000)	0.0614 (0.0614)	1.137 (0.990)	53.12 (67.45)
[120/157]	0.1029 (0.1001)	0.0636 (0.0615)	1.454 (0.996)	46.88 (67.43)
[130/157]	0.1001 (0.1001)	0.0608 (0.0616)	1.152 (0.991)	62.50 (67.70)
[140/157]	0.1009 (0.1002)	0.0622 (0.0616)	0.972 (0.990)	68.75 (67.80)
[150/157]	0.1006 (0.1002)	0.0615 (0.0617)	1.005 (0.990)	65.62 (67.96)
[156/157]	0.0874 (0.1001)	0.0569 (0.0616)	0.634 (0.986)	87.50 (68.08)
 * Train Acc 68.080
 * Val Acc 69.800, Total time 0.61
 * Val loss 0.868, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0091 (0.0091)	0.973 (0.973)	68.75 (68.75)
[10/157]	0.1006 (0.0948)	0.0617 (0.0566)	0.662 (0.950)	81.25 (66.76)
[20/157]	0.1011 (0.0980)	0.0611 (0.0597)	0.816 (0.930)	68.75 (67.86)
[30/157]	0.1002 (0.0987)	0.0613 (0.0604)	1.209 (0.958)	65.62 (67.04)
[40/157]	0.1001 (0.0990)	0.0614 (0.0607)	0.602 (0.951)	84.38 (67.30)
[50/157]	0.1012 (0.0993)	0.0623 (0.0608)	0.996 (0.969)	65.62 (67.22)
[60/157]	0.1012 (0.0996)	0.0616 (0.0611)	1.063 (0.961)	62.50 (67.88)
[70/157]	0.0994 (0.0997)	0.0604 (0.0611)	0.928 (0.972)	65.62 (67.52)
[80/157]	0.1010 (0.0999)	0.0621 (0.0613)	0.781 (0.972)	81.25 (67.67)
[90/157]	0.0994 (0.1000)	0.0603 (0.0614)	1.041 (0.973)	65.62 (67.72)
[100/157]	0.1016 (0.1000)	0.0620 (0.0614)	0.945 (0.977)	62.50 (68.10)
[110/157]	0.1010 (0.1001)	0.0621 (0.0615)	0.784 (0.971)	81.25 (68.38)
[120/157]	0.1001 (0.1002)	0.0611 (0.0615)	0.876 (0.966)	75.00 (68.49)
[130/157]	0.1015 (0.1002)	0.0623 (0.0616)	1.209 (0.964)	53.12 (68.42)
[140/157]	0.0997 (0.1003)	0.0606 (0.0616)	1.067 (0.962)	62.50 (68.42)
[150/157]	0.1003 (0.1002)	0.0612 (0.0616)	1.140 (0.967)	65.62 (68.42)
[156/157]	0.0846 (0.1001)	0.0570 (0.0615)	1.178 (0.967)	62.50 (68.44)
 * Train Acc 68.440
 * Val Acc 69.200, Total time 0.61
 * Val loss 0.879, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0087 (0.0087)	0.753 (0.753)	81.25 (81.25)
[10/157]	0.0993 (0.0957)	0.0605 (0.0559)	0.856 (0.892)	81.25 (71.88)
[20/157]	0.0998 (0.0978)	0.0616 (0.0585)	0.766 (0.911)	78.12 (70.09)
[30/157]	0.1007 (0.0987)	0.0622 (0.0599)	1.141 (0.934)	62.50 (68.75)
[40/157]	0.1010 (0.0993)	0.0618 (0.0604)	0.795 (0.928)	75.00 (69.05)
[50/157]	0.1010 (0.0995)	0.0619 (0.0607)	1.074 (0.949)	62.50 (67.65)
[60/157]	0.1010 (0.0997)	0.0623 (0.0609)	0.622 (0.944)	84.38 (68.08)
[70/157]	0.0992 (0.0999)	0.0604 (0.0611)	1.018 (0.951)	68.75 (68.13)
[80/157]	0.0988 (0.0999)	0.0600 (0.0611)	0.939 (0.949)	71.88 (68.52)
[90/157]	0.0999 (0.0999)	0.0606 (0.0612)	1.088 (0.968)	68.75 (67.89)
[100/157]	0.1033 (0.1000)	0.0641 (0.0613)	0.873 (0.964)	68.75 (68.29)
[110/157]	0.1014 (0.1001)	0.0617 (0.0614)	1.030 (0.969)	78.12 (68.30)
[120/157]	0.1007 (0.1001)	0.0611 (0.0614)	0.974 (0.966)	68.75 (68.36)
[130/157]	0.1016 (0.1002)	0.0615 (0.0615)	0.760 (0.965)	75.00 (68.46)
[140/157]	0.1006 (0.1002)	0.0615 (0.0616)	0.754 (0.958)	84.38 (68.84)
[150/157]	0.0995 (0.1003)	0.0586 (0.0616)	0.828 (0.954)	65.62 (68.92)
[156/157]	0.0825 (0.1002)	0.0543 (0.0615)	1.143 (0.955)	62.50 (68.76)
 * Train Acc 68.760
 * Val Acc 69.600, Total time 0.61
 * Val loss 0.851, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0454 (0.0454)	0.0092 (0.0092)	0.801 (0.801)	78.12 (78.12)
[10/157]	0.0995 (0.1028)	0.0592 (0.0635)	1.024 (0.974)	71.88 (68.47)
[20/157]	0.0983 (0.0998)	0.0593 (0.0612)	0.765 (0.947)	75.00 (69.20)
[30/157]	0.0958 (0.0988)	0.0581 (0.0604)	1.065 (0.968)	62.50 (68.35)
[40/157]	0.0938 (0.0981)	0.0568 (0.0600)	0.979 (0.944)	71.88 (69.13)
[50/157]	0.1047 (0.0996)	0.0651 (0.0613)	0.949 (0.936)	68.75 (69.12)
[60/157]	0.0957 (0.1003)	0.0574 (0.0619)	0.718 (0.945)	78.12 (68.90)
[70/157]	0.0967 (0.0996)	0.0582 (0.0614)	1.033 (0.952)	78.12 (68.88)
[80/157]	0.0949 (0.0991)	0.0568 (0.0610)	1.117 (0.972)	65.62 (67.86)
[90/157]	0.1023 (0.1004)	0.0622 (0.0621)	0.834 (0.968)	75.00 (68.30)
[100/157]	0.1025 (0.1005)	0.0626 (0.0622)	0.874 (0.967)	78.12 (68.53)
[110/157]	0.1016 (0.1007)	0.0626 (0.0623)	1.291 (0.958)	59.38 (68.86)
[120/157]	0.1022 (0.1008)	0.0624 (0.0623)	0.883 (0.966)	68.75 (68.52)
[130/157]	0.1021 (0.1009)	0.0617 (0.0624)	0.957 (0.966)	65.62 (68.46)
[140/157]	0.1030 (0.1010)	0.0633 (0.0624)	0.846 (0.965)	71.88 (68.53)
[150/157]	0.1006 (0.1010)	0.0617 (0.0624)	1.060 (0.962)	68.75 (68.67)
[156/157]	0.0860 (0.1010)	0.0578 (0.0624)	1.224 (0.965)	75.00 (68.50)
 * Train Acc 68.500
 * Val Acc 68.900, Total time 0.61
 * Val loss 0.888, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0096 (0.0096)	0.960 (0.960)	65.62 (65.62)
[10/157]	0.1020 (0.0961)	0.0628 (0.0574)	0.824 (0.972)	78.12 (69.03)
[20/157]	0.1017 (0.0990)	0.0625 (0.0601)	1.026 (0.964)	65.62 (68.90)
[30/157]	0.1026 (0.1000)	0.0634 (0.0612)	1.074 (1.024)	56.25 (65.93)
[40/157]	0.1030 (0.1005)	0.0634 (0.0617)	0.925 (1.014)	59.38 (65.62)
[50/157]	0.1029 (0.1010)	0.0634 (0.0621)	0.888 (1.000)	75.00 (66.48)
[60/157]	0.1011 (0.1012)	0.0606 (0.0622)	0.794 (0.980)	81.25 (67.42)
[70/157]	0.1010 (0.1013)	0.0617 (0.0623)	1.170 (0.963)	62.50 (68.49)
[80/157]	0.1023 (0.1014)	0.0625 (0.0625)	1.126 (0.956)	62.50 (68.56)
[90/157]	0.1021 (0.1015)	0.0625 (0.0626)	0.963 (0.951)	75.00 (68.65)
[100/157]	0.1039 (0.1016)	0.0607 (0.0626)	1.068 (0.953)	65.62 (68.84)
[110/157]	0.1031 (0.1016)	0.0637 (0.0626)	0.984 (0.960)	62.50 (68.30)
[120/157]	0.1005 (0.1016)	0.0619 (0.0626)	1.025 (0.962)	68.75 (68.29)
[130/157]	0.1005 (0.1016)	0.0625 (0.0627)	1.238 (0.962)	62.50 (68.34)
[140/157]	0.1009 (0.1016)	0.0617 (0.0627)	1.311 (0.965)	53.12 (68.20)
[150/157]	0.1001 (0.1016)	0.0612 (0.0628)	1.178 (0.962)	56.25 (68.46)
[156/157]	0.0848 (0.1015)	0.0577 (0.0627)	0.931 (0.959)	62.50 (68.48)
 * Train Acc 68.480
 * Val Acc 68.600, Total time 0.62
 * Val loss 0.871, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0484 (0.0484)	0.0091 (0.0091)	0.693 (0.693)	78.12 (78.12)
[10/157]	0.1019 (0.0959)	0.0623 (0.0569)	1.039 (1.048)	62.50 (66.19)
[20/157]	0.1003 (0.0985)	0.0621 (0.0597)	0.860 (1.014)	68.75 (66.82)
[30/157]	0.1013 (0.0997)	0.0613 (0.0608)	0.668 (0.961)	75.00 (69.05)
[40/157]	0.1014 (0.1002)	0.0627 (0.0611)	1.083 (0.953)	62.50 (69.59)
[50/157]	0.1025 (0.1006)	0.0627 (0.0616)	0.796 (0.960)	81.25 (69.18)
[60/157]	0.1012 (0.1008)	0.0617 (0.0618)	1.081 (0.956)	65.62 (69.01)
[70/157]	0.0998 (0.1009)	0.0607 (0.0620)	0.754 (0.948)	71.88 (69.10)
[80/157]	0.1017 (0.1010)	0.0618 (0.0621)	0.888 (0.946)	68.75 (69.06)
[90/157]	0.0994 (0.1011)	0.0615 (0.0622)	1.147 (0.950)	62.50 (69.16)
[100/157]	0.1025 (0.1012)	0.0626 (0.0623)	1.032 (0.949)	71.88 (69.40)
[110/157]	0.1024 (0.1013)	0.0629 (0.0624)	0.981 (0.953)	59.38 (69.03)
[120/157]	0.1018 (0.1013)	0.0625 (0.0624)	0.833 (0.959)	81.25 (68.72)
[130/157]	0.1040 (0.1013)	0.0635 (0.0625)	1.166 (0.955)	65.62 (69.04)
[140/157]	0.1028 (0.1013)	0.0640 (0.0625)	0.718 (0.949)	81.25 (69.28)
[150/157]	0.0958 (0.1013)	0.0573 (0.0625)	0.944 (0.948)	71.88 (69.41)
[156/157]	0.0808 (0.1010)	0.0555 (0.0623)	1.631 (0.953)	50.00 (69.24)
 * Train Acc 69.240
 * Val Acc 69.800, Total time 0.61
 * Val loss 0.861, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0084 (0.0084)	1.059 (1.059)	65.62 (65.62)
[10/157]	0.1018 (0.0954)	0.0623 (0.0566)	0.826 (0.899)	65.62 (67.61)
[20/157]	0.1020 (0.0982)	0.0622 (0.0597)	1.180 (0.882)	53.12 (70.83)
[30/157]	0.1008 (0.0990)	0.0623 (0.0606)	0.970 (0.886)	71.88 (71.47)
[40/157]	0.1010 (0.0996)	0.0621 (0.0611)	0.858 (0.904)	75.00 (70.50)
[50/157]	0.1004 (0.0998)	0.0620 (0.0614)	0.702 (0.919)	84.38 (69.79)
[60/157]	0.1007 (0.1001)	0.0615 (0.0615)	0.952 (0.930)	68.75 (70.03)
[70/157]	0.1012 (0.1001)	0.0627 (0.0616)	1.065 (0.931)	71.88 (69.81)
[80/157]	0.1009 (0.1003)	0.0618 (0.0618)	0.740 (0.930)	71.88 (69.64)
[90/157]	0.1050 (0.1005)	0.0610 (0.0618)	0.744 (0.934)	84.38 (69.57)
[100/157]	0.1007 (0.1005)	0.0620 (0.0618)	0.718 (0.934)	75.00 (69.77)
[110/157]	0.1019 (0.1006)	0.0622 (0.0619)	0.902 (0.932)	68.75 (69.74)
[120/157]	0.1040 (0.1006)	0.0635 (0.0620)	0.853 (0.941)	81.25 (69.55)
[130/157]	0.1012 (0.1007)	0.0620 (0.0620)	0.781 (0.942)	75.00 (69.61)
[140/157]	0.1004 (0.1007)	0.0613 (0.0621)	1.043 (0.942)	62.50 (69.46)
[150/157]	0.1014 (0.1008)	0.0613 (0.0621)	0.877 (0.942)	71.88 (69.54)
[156/157]	0.0865 (0.1007)	0.0587 (0.0621)	1.116 (0.943)	62.50 (69.56)
 * Train Acc 69.560
 * Val Acc 68.700, Total time 0.60
 * Val loss 0.879, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0454 (0.0454)	0.0086 (0.0086)	1.258 (1.258)	65.62 (65.62)
[10/157]	0.1024 (0.0962)	0.0640 (0.0566)	1.088 (0.960)	68.75 (68.18)
[20/157]	0.1005 (0.0986)	0.0618 (0.0596)	1.071 (0.983)	65.62 (66.96)
[30/157]	0.1026 (0.0995)	0.0623 (0.0607)	0.889 (0.947)	75.00 (67.74)
[40/157]	0.1015 (0.0999)	0.0614 (0.0612)	0.978 (0.950)	68.75 (67.76)
[50/157]	0.1010 (0.1002)	0.0625 (0.0616)	1.028 (0.962)	56.25 (67.77)
[60/157]	0.1032 (0.1006)	0.0619 (0.0617)	0.766 (0.957)	81.25 (67.98)
[70/157]	0.1015 (0.1007)	0.0615 (0.0618)	1.009 (0.952)	68.75 (68.22)
[80/157]	0.1027 (0.1008)	0.0626 (0.0618)	0.829 (0.952)	68.75 (68.36)
[90/157]	0.1020 (0.1008)	0.0634 (0.0619)	0.881 (0.949)	65.62 (68.48)
[100/157]	0.1012 (0.1009)	0.0617 (0.0620)	0.944 (0.959)	75.00 (68.10)
[110/157]	0.1056 (0.1009)	0.0640 (0.0621)	0.836 (0.957)	71.88 (67.96)
[120/157]	0.1003 (0.1009)	0.0616 (0.0621)	1.004 (0.949)	59.38 (68.31)
[130/157]	0.1012 (0.1009)	0.0627 (0.0621)	1.029 (0.943)	62.50 (68.63)
[140/157]	0.1004 (0.1009)	0.0611 (0.0621)	0.763 (0.939)	71.88 (68.73)
[150/157]	0.1018 (0.1009)	0.0624 (0.0622)	0.943 (0.941)	71.88 (68.69)
[156/157]	0.0856 (0.1008)	0.0571 (0.0621)	2.492 (0.944)	50.00 (68.72)
 * Train Acc 68.720
 * Val Acc 70.500, Total time 0.61
 * Val loss 0.863, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0094 (0.0094)	1.287 (1.287)	56.25 (56.25)
[10/157]	0.0996 (0.0955)	0.0609 (0.0570)	0.904 (0.934)	75.00 (69.32)
[20/157]	0.1014 (0.0978)	0.0619 (0.0594)	0.855 (0.934)	75.00 (70.24)
[30/157]	0.1004 (0.0989)	0.0612 (0.0605)	0.775 (0.933)	65.62 (70.36)
[40/157]	0.1045 (0.0994)	0.0611 (0.0610)	0.911 (0.929)	71.88 (70.20)
[50/157]	0.1018 (0.0998)	0.0626 (0.0613)	0.752 (0.933)	81.25 (70.04)
[60/157]	0.1037 (0.1000)	0.0635 (0.0615)	0.837 (0.932)	68.75 (69.77)
[70/157]	0.1006 (0.1002)	0.0615 (0.0616)	1.122 (0.939)	68.75 (69.81)
[80/157]	0.1023 (0.1003)	0.0628 (0.0618)	1.155 (0.943)	62.50 (69.44)
[90/157]	0.1013 (0.1004)	0.0625 (0.0618)	0.626 (0.932)	81.25 (69.88)
[100/157]	0.1003 (0.1005)	0.0612 (0.0619)	0.823 (0.926)	65.62 (70.24)
[110/157]	0.1020 (0.1005)	0.0631 (0.0620)	0.902 (0.923)	71.88 (70.19)
[120/157]	0.0998 (0.1006)	0.0610 (0.0620)	0.815 (0.925)	71.88 (69.94)
[130/157]	0.1004 (0.1007)	0.0612 (0.0621)	0.976 (0.921)	65.62 (70.11)
[140/157]	0.1021 (0.1007)	0.0634 (0.0622)	0.887 (0.927)	71.88 (69.75)
[150/157]	0.1009 (0.1007)	0.0623 (0.0622)	0.937 (0.927)	71.88 (69.68)
[156/157]	0.0882 (0.1007)	0.0592 (0.0622)	1.593 (0.927)	50.00 (69.66)
 * Train Acc 69.660
 * Val Acc 69.600, Total time 0.60
 * Val loss 0.850, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0093 (0.0093)	1.053 (1.053)	65.62 (65.62)
[10/157]	0.1035 (0.0965)	0.0634 (0.0579)	0.639 (0.927)	81.25 (71.88)
[20/157]	0.1043 (0.0988)	0.0634 (0.0601)	1.393 (0.915)	59.38 (72.02)
[30/157]	0.1016 (0.0996)	0.0624 (0.0608)	1.015 (0.923)	68.75 (70.77)
[40/157]	0.1014 (0.1001)	0.0615 (0.0613)	0.841 (0.934)	65.62 (70.12)
[50/157]	0.0999 (0.1005)	0.0608 (0.0616)	1.107 (0.935)	62.50 (70.47)
[60/157]	0.1026 (0.1006)	0.0618 (0.0618)	1.015 (0.924)	65.62 (70.44)
[70/157]	0.1019 (0.1007)	0.0624 (0.0618)	0.658 (0.916)	84.38 (70.64)
[80/157]	0.1002 (0.1008)	0.0610 (0.0619)	0.933 (0.921)	71.88 (70.49)
[90/157]	0.1004 (0.1009)	0.0615 (0.0619)	1.129 (0.928)	65.62 (70.23)
[100/157]	0.1021 (0.1009)	0.0625 (0.0620)	1.222 (0.928)	65.62 (70.27)
[110/157]	0.1015 (0.1009)	0.0623 (0.0620)	1.150 (0.927)	65.62 (70.21)
[120/157]	0.1003 (0.1010)	0.0602 (0.0621)	0.970 (0.928)	65.62 (69.81)
[130/157]	0.1018 (0.1010)	0.0615 (0.0620)	0.561 (0.923)	84.38 (69.82)
[140/157]	0.1027 (0.1010)	0.0635 (0.0621)	1.034 (0.932)	59.38 (69.70)
[150/157]	0.1006 (0.1011)	0.0609 (0.0621)	0.980 (0.935)	68.75 (69.52)
[156/157]	0.0862 (0.1010)	0.0586 (0.0621)	0.898 (0.934)	87.50 (69.54)
 * Train Acc 69.540
 * Val Acc 69.200, Total time 0.57
 * Val loss 0.850, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0408 (0.0408)	0.0086 (0.0086)	1.089 (1.089)	62.50 (62.50)
[10/157]	0.0943 (0.0924)	0.0560 (0.0543)	1.053 (0.955)	68.75 (71.59)
[20/157]	0.0949 (0.0936)	0.0572 (0.0561)	0.745 (0.953)	81.25 (70.24)
[30/157]	0.1078 (0.0982)	0.0682 (0.0602)	0.827 (0.943)	75.00 (70.06)
[40/157]	0.0992 (0.0985)	0.0599 (0.0603)	0.915 (0.931)	71.88 (70.27)
[50/157]	0.1001 (0.0988)	0.0604 (0.0604)	0.932 (0.949)	62.50 (69.18)
[60/157]	0.0990 (0.0990)	0.0602 (0.0606)	0.731 (0.928)	81.25 (70.13)
[70/157]	0.0997 (0.0991)	0.0599 (0.0606)	0.630 (0.917)	87.50 (70.60)
[80/157]	0.1003 (0.0992)	0.0605 (0.0606)	0.978 (0.905)	75.00 (71.18)
[90/157]	0.0999 (0.0993)	0.0606 (0.0607)	0.798 (0.909)	71.88 (71.02)
[100/157]	0.1008 (0.0994)	0.0608 (0.0607)	0.940 (0.915)	75.00 (70.92)
[110/157]	0.1004 (0.0994)	0.0608 (0.0607)	0.941 (0.916)	68.75 (70.86)
[120/157]	0.1004 (0.0995)	0.0609 (0.0608)	1.046 (0.920)	68.75 (70.66)
[130/157]	0.0990 (0.0995)	0.0596 (0.0607)	0.721 (0.917)	75.00 (70.80)
[140/157]	0.1005 (0.0996)	0.0604 (0.0607)	1.002 (0.917)	65.62 (70.55)
[150/157]	0.0999 (0.0996)	0.0604 (0.0607)	0.948 (0.914)	68.75 (70.61)
[156/157]	0.0836 (0.0995)	0.0557 (0.0607)	1.249 (0.913)	50.00 (70.72)
 * Train Acc 70.720
 * Val Acc 70.700, Total time 0.60
 * Val loss 0.829, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0090 (0.0090)	0.998 (0.998)	68.75 (68.75)
[10/157]	0.0997 (0.0938)	0.0614 (0.0560)	1.092 (0.936)	62.50 (68.18)
[20/157]	0.0990 (0.0965)	0.0599 (0.0586)	0.639 (0.935)	87.50 (68.75)
[30/157]	0.1001 (0.0976)	0.0604 (0.0593)	1.019 (0.911)	68.75 (69.96)
[40/157]	0.1001 (0.0981)	0.0605 (0.0597)	0.871 (0.919)	68.75 (69.05)
[50/157]	0.0994 (0.0985)	0.0599 (0.0599)	1.072 (0.900)	68.75 (70.16)
[60/157]	0.1008 (0.0987)	0.0606 (0.0600)	0.768 (0.897)	75.00 (70.18)
[70/157]	0.1001 (0.0989)	0.0595 (0.0601)	0.871 (0.905)	75.00 (69.94)
[80/157]	0.1005 (0.0991)	0.0599 (0.0601)	0.994 (0.905)	65.62 (70.22)
[90/157]	0.1002 (0.0991)	0.0607 (0.0602)	0.908 (0.905)	65.62 (70.30)
[100/157]	0.1000 (0.0992)	0.0605 (0.0603)	0.892 (0.904)	68.75 (70.39)
[110/157]	0.1001 (0.0993)	0.0613 (0.0604)	0.896 (0.903)	62.50 (70.58)
[120/157]	0.1006 (0.0993)	0.0598 (0.0604)	0.895 (0.914)	71.88 (70.30)
[130/157]	0.1001 (0.0994)	0.0603 (0.0604)	0.741 (0.920)	81.25 (70.28)
[140/157]	0.1001 (0.0994)	0.0605 (0.0604)	0.681 (0.915)	78.12 (70.39)
[150/157]	0.0995 (0.0994)	0.0600 (0.0604)	0.858 (0.915)	71.88 (70.38)
[156/157]	0.0825 (0.0993)	0.0558 (0.0604)	0.932 (0.916)	50.00 (70.30)
 * Train Acc 70.300
 * Val Acc 69.700, Total time 0.61
 * Val loss 0.839, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0090 (0.0090)	0.912 (0.912)	68.75 (68.75)
[10/157]	0.0993 (0.0950)	0.0597 (0.0551)	1.082 (0.917)	62.50 (67.61)
[20/157]	0.1008 (0.0973)	0.0608 (0.0578)	0.647 (0.915)	81.25 (69.05)
[30/157]	0.1002 (0.0981)	0.0603 (0.0589)	1.034 (0.895)	62.50 (70.36)
[40/157]	0.1017 (0.0987)	0.0619 (0.0594)	1.002 (0.900)	78.12 (70.27)
[50/157]	0.1023 (0.0992)	0.0623 (0.0600)	1.146 (0.902)	71.88 (70.53)
[60/157]	0.1007 (0.0994)	0.0615 (0.0604)	1.106 (0.907)	71.88 (70.54)
[70/157]	0.1004 (0.0998)	0.0611 (0.0607)	0.950 (0.903)	68.75 (70.16)
[80/157]	0.1034 (0.1000)	0.0626 (0.0610)	0.938 (0.901)	68.75 (70.45)
[90/157]	0.1006 (0.1000)	0.0604 (0.0610)	1.010 (0.909)	71.88 (70.23)
[100/157]	0.1012 (0.1001)	0.0618 (0.0611)	0.814 (0.907)	71.88 (70.36)
[110/157]	0.1015 (0.1002)	0.0617 (0.0612)	1.099 (0.898)	65.62 (70.69)
[120/157]	0.1004 (0.1002)	0.0605 (0.0612)	0.900 (0.901)	68.75 (70.58)
[130/157]	0.1021 (0.1002)	0.0618 (0.0613)	0.902 (0.898)	68.75 (70.61)
[140/157]	0.1009 (0.1003)	0.0615 (0.0613)	0.900 (0.905)	78.12 (70.55)
[150/157]	0.1008 (0.1003)	0.0615 (0.0614)	1.116 (0.906)	59.38 (70.38)
[156/157]	0.0848 (0.1003)	0.0573 (0.0614)	1.288 (0.906)	87.50 (70.62)
 * Train Acc 70.620
 * Val Acc 70.700, Total time 0.61
 * Val loss 0.836, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0092 (0.0092)	0.762 (0.762)	75.00 (75.00)
[10/157]	0.1019 (0.0951)	0.0628 (0.0571)	0.811 (0.899)	75.00 (68.47)
[20/157]	0.1000 (0.0979)	0.0611 (0.0595)	0.691 (0.928)	78.12 (69.64)
[30/157]	0.1007 (0.0986)	0.0626 (0.0602)	1.000 (0.907)	71.88 (71.98)
[40/157]	0.1032 (0.0993)	0.0628 (0.0607)	0.918 (0.912)	68.75 (71.27)
[50/157]	0.1008 (0.0996)	0.0608 (0.0609)	0.686 (0.899)	75.00 (71.45)
[60/157]	0.1022 (0.0999)	0.0624 (0.0611)	0.991 (0.897)	71.88 (71.77)
[70/157]	0.1006 (0.1000)	0.0607 (0.0612)	1.279 (0.903)	56.25 (71.48)
[80/157]	0.1017 (0.1000)	0.0633 (0.0614)	0.553 (0.912)	84.38 (70.83)
[90/157]	0.1000 (0.1002)	0.0612 (0.0615)	0.939 (0.912)	68.75 (70.60)
[100/157]	0.1024 (0.1003)	0.0628 (0.0616)	1.018 (0.914)	62.50 (70.54)
[110/157]	0.1015 (0.1003)	0.0612 (0.0616)	1.013 (0.913)	68.75 (70.50)
[120/157]	0.0959 (0.1001)	0.0575 (0.0613)	1.265 (0.910)	56.25 (70.66)
[130/157]	0.0964 (0.0997)	0.0574 (0.0610)	0.923 (0.906)	78.12 (70.78)
[140/157]	0.0994 (0.0997)	0.0598 (0.0610)	0.913 (0.910)	75.00 (70.68)
[150/157]	0.0996 (0.0996)	0.0612 (0.0609)	0.933 (0.906)	71.88 (70.65)
[156/157]	0.0841 (0.0994)	0.0544 (0.0609)	0.708 (0.909)	87.50 (70.56)
 * Train Acc 70.560
 * Val Acc 70.200, Total time 0.60
 * Val loss 0.832, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0452 (0.0452)	0.0096 (0.0096)	1.079 (1.079)	53.12 (53.12)
[10/157]	0.0983 (0.0930)	0.0590 (0.0549)	0.707 (0.950)	81.25 (67.90)
[20/157]	0.0990 (0.0956)	0.0593 (0.0573)	1.006 (0.916)	65.62 (70.09)
[30/157]	0.0970 (0.0965)	0.0587 (0.0582)	0.950 (0.914)	71.88 (70.26)
[40/157]	0.0990 (0.0971)	0.0605 (0.0588)	0.627 (0.905)	84.38 (71.04)
[50/157]	0.0995 (0.0974)	0.0602 (0.0591)	0.966 (0.914)	65.62 (70.34)
[60/157]	0.1011 (0.0977)	0.0586 (0.0592)	0.871 (0.931)	62.50 (69.72)
[70/157]	0.0999 (0.0978)	0.0606 (0.0594)	0.863 (0.911)	71.88 (70.33)
[80/157]	0.0947 (0.0978)	0.0569 (0.0594)	0.752 (0.912)	75.00 (70.25)
[90/157]	0.0995 (0.0994)	0.0604 (0.0610)	0.954 (0.907)	71.88 (70.30)
[100/157]	0.0995 (0.0995)	0.0596 (0.0609)	1.188 (0.911)	62.50 (70.39)
[110/157]	0.1000 (0.0995)	0.0606 (0.0609)	1.067 (0.909)	65.62 (70.55)
[120/157]	0.1007 (0.0996)	0.0606 (0.0609)	0.995 (0.909)	71.88 (70.58)
[130/157]	0.1001 (0.0996)	0.0604 (0.0609)	1.029 (0.911)	78.12 (70.73)
[140/157]	0.1009 (0.0996)	0.0616 (0.0609)	1.167 (0.909)	68.75 (70.99)
[150/157]	0.0995 (0.0996)	0.0600 (0.0609)	1.078 (0.912)	68.75 (70.90)
[156/157]	0.0829 (0.0995)	0.0563 (0.0608)	0.945 (0.909)	75.00 (71.08)
 * Train Acc 71.080
 * Val Acc 70.500, Total time 0.61
 * Val loss 0.828, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0091 (0.0091)	1.155 (1.155)	65.62 (65.62)
[10/157]	0.0995 (0.0947)	0.0600 (0.0556)	0.870 (0.908)	71.88 (70.17)
[20/157]	0.1013 (0.0972)	0.0612 (0.0580)	1.192 (0.919)	56.25 (69.64)
[30/157]	0.1002 (0.0981)	0.0604 (0.0587)	0.700 (0.889)	87.50 (71.07)
[40/157]	0.0997 (0.0985)	0.0598 (0.0592)	0.684 (0.888)	90.62 (71.80)
[50/157]	0.1000 (0.0988)	0.0600 (0.0595)	1.029 (0.890)	68.75 (72.18)
[60/157]	0.1017 (0.0990)	0.0609 (0.0596)	0.658 (0.897)	71.88 (71.88)
[70/157]	0.0998 (0.0991)	0.0602 (0.0597)	0.950 (0.902)	68.75 (71.74)
[80/157]	0.1016 (0.0992)	0.0608 (0.0599)	0.772 (0.906)	71.88 (71.33)
[90/157]	0.1002 (0.0993)	0.0609 (0.0600)	1.182 (0.902)	65.62 (71.43)
[100/157]	0.1003 (0.0994)	0.0604 (0.0600)	0.691 (0.897)	81.25 (71.72)
[110/157]	0.1008 (0.0994)	0.0610 (0.0601)	0.707 (0.906)	65.62 (71.09)
[120/157]	0.0996 (0.0995)	0.0602 (0.0602)	0.642 (0.898)	81.25 (71.44)
[130/157]	0.0999 (0.0995)	0.0602 (0.0602)	0.817 (0.898)	71.88 (71.42)
[140/157]	0.1002 (0.0995)	0.0604 (0.0602)	0.853 (0.904)	78.12 (71.05)
[150/157]	0.0993 (0.0996)	0.0599 (0.0602)	0.804 (0.902)	65.62 (71.09)
[156/157]	0.0823 (0.0995)	0.0551 (0.0602)	0.465 (0.904)	87.50 (71.00)
 * Train Acc 71.000
 * Val Acc 70.000, Total time 0.60
 * Val loss 0.861, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0088 (0.0088)	1.130 (1.130)	65.62 (65.62)
[10/157]	0.1008 (0.0944)	0.0620 (0.0557)	0.651 (0.834)	81.25 (74.15)
[20/157]	0.1010 (0.0969)	0.0617 (0.0584)	0.912 (0.850)	62.50 (72.17)
[30/157]	0.0974 (0.0978)	0.0581 (0.0592)	0.918 (0.867)	78.12 (71.67)
[40/157]	0.0995 (0.0982)	0.0600 (0.0596)	1.002 (0.884)	65.62 (70.81)
[50/157]	0.0983 (0.0986)	0.0596 (0.0597)	0.856 (0.879)	75.00 (71.57)
[60/157]	0.0999 (0.0988)	0.0602 (0.0599)	0.754 (0.863)	71.88 (72.34)
[70/157]	0.1005 (0.0990)	0.0606 (0.0600)	0.823 (0.875)	75.00 (72.10)
[80/157]	0.0995 (0.0991)	0.0593 (0.0601)	0.945 (0.869)	59.38 (72.42)
[90/157]	0.1005 (0.0992)	0.0608 (0.0601)	1.380 (0.876)	59.38 (72.05)
[100/157]	0.1013 (0.0993)	0.0621 (0.0602)	1.320 (0.869)	59.38 (72.28)
[110/157]	0.1002 (0.0993)	0.0610 (0.0603)	0.983 (0.878)	56.25 (71.79)
[120/157]	0.1003 (0.0994)	0.0598 (0.0603)	0.697 (0.883)	81.25 (71.82)
[130/157]	0.0999 (0.0994)	0.0606 (0.0603)	0.885 (0.884)	71.88 (71.80)
[140/157]	0.1002 (0.0995)	0.0605 (0.0603)	1.117 (0.893)	46.88 (71.32)
[150/157]	0.0999 (0.0995)	0.0599 (0.0604)	1.270 (0.898)	56.25 (71.23)
[156/157]	0.0850 (0.0994)	0.0572 (0.0603)	0.810 (0.898)	62.50 (71.22)
 * Train Acc 71.220
 * Val Acc 70.000, Total time 0.61
 * Val loss 0.830, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0092 (0.0092)	0.805 (0.805)	78.12 (78.12)
[10/157]	0.0984 (0.0946)	0.0592 (0.0553)	0.926 (1.059)	65.62 (65.34)
[20/157]	0.0995 (0.0972)	0.0598 (0.0580)	0.835 (0.980)	71.88 (68.30)
[30/157]	0.1002 (0.0982)	0.0595 (0.0589)	1.012 (0.953)	68.75 (68.85)
[40/157]	0.0996 (0.0986)	0.0599 (0.0593)	0.772 (0.944)	78.12 (69.28)
[50/157]	0.1000 (0.0988)	0.0603 (0.0595)	0.902 (0.927)	71.88 (69.73)
[60/157]	0.1008 (0.0990)	0.0611 (0.0596)	0.672 (0.926)	81.25 (69.47)
[70/157]	0.1018 (0.0992)	0.0602 (0.0598)	0.849 (0.926)	68.75 (69.19)
[80/157]	0.0996 (0.0993)	0.0595 (0.0599)	1.092 (0.922)	68.75 (69.21)
[90/157]	0.1002 (0.0993)	0.0599 (0.0599)	1.130 (0.915)	68.75 (69.64)
[100/157]	0.0998 (0.0994)	0.0603 (0.0599)	1.034 (0.910)	59.38 (70.17)
[110/157]	0.1004 (0.0994)	0.0605 (0.0600)	1.207 (0.917)	56.25 (69.99)
[120/157]	0.1010 (0.0995)	0.0608 (0.0601)	0.937 (0.921)	68.75 (69.96)
[130/157]	0.1050 (0.0996)	0.0613 (0.0601)	0.820 (0.918)	75.00 (70.13)
[140/157]	0.1005 (0.0996)	0.0610 (0.0601)	0.952 (0.921)	62.50 (69.84)
[150/157]	0.1003 (0.0996)	0.0593 (0.0601)	1.119 (0.919)	62.50 (69.87)
[156/157]	0.0830 (0.0995)	0.0561 (0.0601)	0.711 (0.918)	62.50 (70.00)
 * Train Acc 70.000
 * Val Acc 71.200, Total time 0.59
 * Val loss 0.842, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0085 (0.0085)	0.908 (0.908)	62.50 (62.50)
[10/157]	0.1006 (0.0953)	0.0602 (0.0555)	1.019 (0.894)	59.38 (68.75)
[20/157]	0.1008 (0.0974)	0.0610 (0.0581)	0.661 (0.919)	71.88 (69.20)
[30/157]	0.1002 (0.0983)	0.0598 (0.0590)	1.189 (0.943)	53.12 (68.55)
[40/157]	0.1002 (0.0987)	0.0605 (0.0594)	0.926 (0.916)	65.62 (69.59)
[50/157]	0.0999 (0.0989)	0.0601 (0.0596)	1.038 (0.908)	65.62 (70.34)
[60/157]	0.0999 (0.0991)	0.0598 (0.0597)	0.972 (0.904)	65.62 (70.59)
[70/157]	0.1007 (0.0992)	0.0602 (0.0598)	0.784 (0.884)	71.88 (71.61)
[80/157]	0.1003 (0.0993)	0.0607 (0.0599)	0.806 (0.892)	71.88 (71.10)
[90/157]	0.1008 (0.0994)	0.0607 (0.0600)	0.894 (0.907)	75.00 (70.47)
[100/157]	0.0998 (0.0994)	0.0597 (0.0601)	0.995 (0.912)	75.00 (70.42)
[110/157]	0.0982 (0.0995)	0.0593 (0.0601)	1.022 (0.912)	65.62 (70.44)
[120/157]	0.1005 (0.0995)	0.0614 (0.0602)	0.899 (0.910)	68.75 (70.38)
[130/157]	0.1002 (0.0996)	0.0594 (0.0602)	0.980 (0.908)	68.75 (70.40)
[140/157]	0.1007 (0.0996)	0.0608 (0.0602)	0.729 (0.906)	78.12 (70.43)
[150/157]	0.1008 (0.0996)	0.0606 (0.0602)	0.959 (0.903)	75.00 (70.53)
[156/157]	0.0842 (0.0995)	0.0562 (0.0602)	0.742 (0.903)	62.50 (70.52)
 * Train Acc 70.520
 * Val Acc 71.400, Total time 0.59
 * Val loss 0.817, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0084 (0.0084)	0.919 (0.919)	65.62 (65.62)
[10/157]	0.1011 (0.0944)	0.0619 (0.0561)	0.808 (0.980)	81.25 (67.05)
[20/157]	0.1027 (0.0972)	0.0606 (0.0589)	0.907 (0.949)	81.25 (70.83)
[30/157]	0.1034 (0.0982)	0.0630 (0.0596)	0.929 (0.966)	65.62 (68.85)
[40/157]	0.1018 (0.0986)	0.0617 (0.0600)	0.502 (0.937)	96.88 (69.82)
[50/157]	0.1001 (0.0988)	0.0615 (0.0603)	0.930 (0.930)	71.88 (69.98)
[60/157]	0.1210 (0.0995)	0.0802 (0.0611)	0.829 (0.925)	68.75 (70.24)
[70/157]	0.0993 (0.1015)	0.0594 (0.0628)	0.920 (0.910)	65.62 (70.64)
[80/157]	0.0981 (0.1011)	0.0584 (0.0625)	1.055 (0.910)	65.62 (71.06)
[90/157]	0.1096 (0.1031)	0.0679 (0.0642)	0.902 (0.912)	78.12 (70.74)
[100/157]	0.0969 (0.1025)	0.0580 (0.0637)	1.033 (0.911)	56.25 (70.64)
[110/157]	0.1040 (0.1023)	0.0632 (0.0635)	0.981 (0.909)	68.75 (70.61)
[120/157]	0.1020 (0.1023)	0.0621 (0.0634)	1.121 (0.903)	65.62 (70.74)
[130/157]	0.1029 (0.1023)	0.0633 (0.0634)	1.174 (0.903)	68.75 (70.71)
[140/157]	0.1015 (0.1022)	0.0612 (0.0633)	1.125 (0.901)	71.88 (70.74)
[150/157]	0.1005 (0.1022)	0.0606 (0.0632)	0.879 (0.900)	65.62 (70.72)
[156/157]	0.0855 (0.1020)	0.0567 (0.0632)	0.794 (0.899)	62.50 (70.68)
 * Train Acc 70.680
 * Val Acc 71.200, Total time 0.61
 * Val loss 0.836, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0458 (0.0458)	0.0088 (0.0088)	0.794 (0.794)	71.88 (71.88)
[10/157]	0.1023 (0.0965)	0.0622 (0.0568)	0.990 (0.883)	62.50 (72.73)
[20/157]	0.1032 (0.0991)	0.0628 (0.0596)	0.828 (0.890)	78.12 (71.88)
[30/157]	0.1019 (0.0999)	0.0629 (0.0606)	0.793 (0.891)	84.38 (72.08)
[40/157]	0.1025 (0.1004)	0.0638 (0.0612)	0.736 (0.889)	78.12 (72.10)
[50/157]	0.1011 (0.1006)	0.0620 (0.0615)	0.946 (0.900)	62.50 (71.38)
[60/157]	0.1022 (0.1008)	0.0631 (0.0618)	0.799 (0.893)	71.88 (71.57)
[70/157]	0.1039 (0.1009)	0.0640 (0.0619)	1.047 (0.896)	68.75 (71.43)
[80/157]	0.1036 (0.1010)	0.0638 (0.0620)	0.718 (0.900)	75.00 (70.95)
[90/157]	0.1010 (0.1010)	0.0612 (0.0620)	0.785 (0.890)	65.62 (71.19)
[100/157]	0.1010 (0.1011)	0.0614 (0.0621)	0.602 (0.891)	87.50 (71.50)
[110/157]	0.1036 (0.1012)	0.0623 (0.0623)	0.851 (0.895)	78.12 (71.34)
[120/157]	0.1018 (0.1012)	0.0612 (0.0623)	0.798 (0.893)	84.38 (71.57)
[130/157]	0.1048 (0.1012)	0.0636 (0.0623)	0.915 (0.893)	71.88 (71.33)
[140/157]	0.1037 (0.1013)	0.0633 (0.0624)	0.707 (0.896)	75.00 (71.25)
[150/157]	0.1029 (0.1014)	0.0632 (0.0624)	0.620 (0.900)	84.38 (71.13)
[156/157]	0.0851 (0.1012)	0.0571 (0.0624)	1.769 (0.903)	25.00 (70.96)
 * Train Acc 70.960
 * Val Acc 71.000, Total time 0.61
 * Val loss 0.836, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0087 (0.0087)	1.085 (1.085)	71.88 (71.88)
[10/157]	0.1023 (0.0959)	0.0630 (0.0572)	0.893 (0.883)	71.88 (71.59)
[20/157]	0.1025 (0.0989)	0.0625 (0.0601)	0.876 (0.889)	62.50 (71.28)
[30/157]	0.1015 (0.0998)	0.0629 (0.0608)	1.087 (0.899)	59.38 (70.46)
[40/157]	0.1019 (0.1002)	0.0624 (0.0614)	0.758 (0.900)	75.00 (70.27)
[50/157]	0.1009 (0.1006)	0.0617 (0.0618)	0.776 (0.901)	71.88 (70.34)
[60/157]	0.1008 (0.1008)	0.0617 (0.0620)	0.884 (0.901)	68.75 (70.24)
[70/157]	0.1019 (0.1009)	0.0622 (0.0622)	0.706 (0.893)	78.12 (70.64)
[80/157]	0.0992 (0.1010)	0.0602 (0.0622)	0.724 (0.892)	75.00 (70.72)
[90/157]	0.1006 (0.1011)	0.0617 (0.0623)	0.843 (0.890)	75.00 (70.81)
[100/157]	0.1025 (0.1012)	0.0631 (0.0624)	0.843 (0.888)	81.25 (70.88)
[110/157]	0.1024 (0.1012)	0.0626 (0.0625)	0.894 (0.887)	71.88 (71.26)
[120/157]	0.1015 (0.1012)	0.0624 (0.0625)	0.944 (0.887)	68.75 (71.33)
[130/157]	0.1038 (0.1013)	0.0622 (0.0625)	0.895 (0.891)	81.25 (71.23)
[140/157]	0.1010 (0.1013)	0.0614 (0.0625)	0.905 (0.890)	71.88 (71.34)
[150/157]	0.1018 (0.1013)	0.0625 (0.0626)	0.892 (0.892)	56.25 (71.19)
[156/157]	0.0854 (0.1012)	0.0576 (0.0625)	0.844 (0.890)	75.00 (71.34)
 * Train Acc 71.340
 * Val Acc 71.100, Total time 0.61
 * Val loss 0.834, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0089 (0.0089)	0.641 (0.641)	84.38 (84.38)
[10/157]	0.1006 (0.0961)	0.0619 (0.0578)	0.980 (0.817)	68.75 (74.43)
[20/157]	0.1044 (0.0988)	0.0645 (0.0605)	0.960 (0.848)	59.38 (73.21)
[30/157]	0.1028 (0.0996)	0.0635 (0.0611)	0.790 (0.856)	71.88 (73.08)
[40/157]	0.1034 (0.1001)	0.0634 (0.0616)	0.696 (0.879)	75.00 (71.49)
[50/157]	0.1016 (0.1004)	0.0620 (0.0618)	0.684 (0.879)	81.25 (71.32)
[60/157]	0.1006 (0.1006)	0.0620 (0.0620)	0.722 (0.886)	84.38 (71.47)
[70/157]	0.1028 (0.1008)	0.0634 (0.0622)	1.128 (0.886)	62.50 (71.35)
[80/157]	0.1022 (0.1009)	0.0629 (0.0623)	1.088 (0.895)	56.25 (70.91)
[90/157]	0.1023 (0.1010)	0.0632 (0.0625)	0.920 (0.893)	65.62 (70.60)
[100/157]	0.1005 (0.1011)	0.0614 (0.0625)	0.658 (0.890)	87.50 (71.07)
[110/157]	0.1031 (0.1012)	0.0629 (0.0626)	1.067 (0.894)	68.75 (71.11)
[120/157]	0.1023 (0.1012)	0.0631 (0.0626)	0.809 (0.892)	71.88 (71.28)
[130/157]	0.1013 (0.1012)	0.0627 (0.0627)	0.931 (0.895)	71.88 (71.14)
[140/157]	0.1026 (0.1013)	0.0628 (0.0627)	0.811 (0.895)	75.00 (71.12)
[150/157]	0.1012 (0.1013)	0.0620 (0.0627)	0.841 (0.892)	75.00 (71.17)
[156/157]	0.0869 (0.1012)	0.0592 (0.0627)	0.563 (0.891)	87.50 (71.24)
 * Train Acc 71.240
 * Val Acc 70.600, Total time 0.60
 * Val loss 0.831, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0086 (0.0086)	0.825 (0.825)	75.00 (75.00)
[10/157]	0.1018 (0.0958)	0.0619 (0.0573)	0.776 (0.861)	75.00 (73.01)
[20/157]	0.1007 (0.0986)	0.0619 (0.0602)	0.787 (0.844)	78.12 (72.17)
[30/157]	0.1019 (0.0992)	0.0630 (0.0609)	1.091 (0.850)	68.75 (72.48)
[40/157]	0.1015 (0.0998)	0.0629 (0.0615)	0.881 (0.870)	68.75 (72.03)
[50/157]	0.1011 (0.1002)	0.0617 (0.0618)	0.788 (0.874)	78.12 (71.81)
[60/157]	0.1013 (0.1005)	0.0623 (0.0620)	0.782 (0.874)	71.88 (71.57)
[70/157]	0.1040 (0.1006)	0.0642 (0.0622)	0.916 (0.888)	71.88 (71.30)
[80/157]	0.1016 (0.1006)	0.0619 (0.0622)	0.997 (0.891)	65.62 (71.18)
[90/157]	0.1014 (0.1008)	0.0630 (0.0623)	0.850 (0.887)	68.75 (71.22)
[100/157]	0.1012 (0.1009)	0.0621 (0.0624)	0.731 (0.886)	81.25 (71.23)
[110/157]	0.1014 (0.1010)	0.0620 (0.0624)	1.023 (0.891)	68.75 (70.97)
[120/157]	0.1020 (0.1010)	0.0630 (0.0625)	0.837 (0.891)	71.88 (71.02)
[130/157]	0.1014 (0.1011)	0.0615 (0.0625)	0.804 (0.887)	68.75 (71.04)
[140/157]	0.1016 (0.1012)	0.0620 (0.0626)	0.867 (0.887)	65.62 (71.17)
[150/157]	0.1007 (0.1012)	0.0611 (0.0626)	0.821 (0.885)	75.00 (71.34)
[156/157]	0.0863 (0.1011)	0.0582 (0.0625)	1.161 (0.887)	62.50 (71.10)
 * Train Acc 71.100
 * Val Acc 70.800, Total time 0.61
 * Val loss 0.831, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0087 (0.0087)	1.043 (1.043)	65.62 (65.62)
[10/157]	0.1023 (0.0959)	0.0634 (0.0568)	0.971 (0.912)	68.75 (69.60)
[20/157]	0.1009 (0.0988)	0.0620 (0.0600)	0.719 (0.867)	75.00 (71.88)
[30/157]	0.1020 (0.0998)	0.0632 (0.0611)	0.841 (0.896)	71.88 (70.97)
[40/157]	0.1022 (0.1003)	0.0617 (0.0615)	0.958 (0.898)	65.62 (70.73)
[50/157]	0.1004 (0.1006)	0.0613 (0.0617)	0.722 (0.892)	71.88 (71.20)
[60/157]	0.1018 (0.1008)	0.0624 (0.0619)	0.638 (0.878)	90.62 (71.98)
[70/157]	0.1026 (0.1009)	0.0626 (0.0620)	0.755 (0.882)	81.25 (71.92)
[80/157]	0.1047 (0.1010)	0.0641 (0.0621)	0.942 (0.892)	78.12 (71.14)
[90/157]	0.1036 (0.1011)	0.0637 (0.0622)	1.008 (0.900)	65.62 (71.02)
[100/157]	0.1028 (0.1012)	0.0626 (0.0622)	0.690 (0.908)	78.12 (70.54)
[110/157]	0.1012 (0.1012)	0.0617 (0.0622)	0.623 (0.906)	78.12 (70.52)
[120/157]	0.1019 (0.1012)	0.0625 (0.0623)	0.702 (0.906)	75.00 (70.56)
[130/157]	0.0997 (0.1013)	0.0608 (0.0624)	1.077 (0.908)	62.50 (70.59)
[140/157]	0.0992 (0.1013)	0.0608 (0.0624)	0.643 (0.902)	81.25 (70.90)
[150/157]	0.1044 (0.1009)	0.0641 (0.0621)	1.129 (0.897)	65.62 (71.07)
[156/157]	0.0906 (0.1009)	0.0619 (0.0621)	0.685 (0.896)	100.00 (71.12)
 * Train Acc 71.120
 * Val Acc 70.900, Total time 0.61
 * Val loss 0.815, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0084 (0.0084)	0.805 (0.805)	78.12 (78.12)
[10/157]	0.1025 (0.0968)	0.0632 (0.0583)	0.931 (0.835)	78.12 (73.86)
[20/157]	0.1038 (0.0995)	0.0645 (0.0610)	1.014 (0.863)	62.50 (72.77)
[30/157]	0.0982 (0.1005)	0.0602 (0.0619)	0.884 (0.845)	75.00 (73.19)
[40/157]	0.0950 (0.0993)	0.0564 (0.0609)	1.062 (0.834)	62.50 (73.25)
[50/157]	0.0977 (0.1000)	0.0588 (0.0615)	0.911 (0.848)	71.88 (72.73)
[60/157]	0.0947 (0.0993)	0.0561 (0.0609)	0.795 (0.858)	75.00 (72.90)
[70/157]	0.0986 (0.0991)	0.0593 (0.0607)	0.884 (0.860)	71.88 (73.15)
[80/157]	0.0980 (0.0989)	0.0591 (0.0605)	0.833 (0.876)	81.25 (72.92)
[90/157]	0.0996 (0.0989)	0.0597 (0.0605)	0.991 (0.886)	59.38 (72.12)
[100/157]	0.0985 (0.0989)	0.0600 (0.0604)	0.859 (0.884)	75.00 (72.28)
[110/157]	0.0983 (0.0988)	0.0596 (0.0604)	0.952 (0.883)	78.12 (72.13)
[120/157]	0.0976 (0.0987)	0.0584 (0.0603)	0.963 (0.881)	62.50 (72.34)
[130/157]	0.0991 (0.0987)	0.0595 (0.0603)	0.703 (0.886)	78.12 (72.04)
[140/157]	0.1001 (0.0987)	0.0608 (0.0603)	0.748 (0.883)	81.25 (72.12)
[150/157]	0.0991 (0.0988)	0.0600 (0.0603)	0.591 (0.879)	81.25 (72.06)
[156/157]	0.0843 (0.0987)	0.0570 (0.0604)	0.908 (0.882)	87.50 (71.96)
 * Train Acc 71.960
 * Val Acc 72.000, Total time 0.61
 * Val loss 0.816, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0460 (0.0460)	0.0096 (0.0096)	0.661 (0.661)	84.38 (84.38)
[10/157]	0.1000 (0.0954)	0.0612 (0.0567)	0.889 (0.888)	65.62 (69.89)
[20/157]	0.1022 (0.0980)	0.0630 (0.0595)	1.022 (0.868)	71.88 (69.94)
[30/157]	0.1004 (0.0990)	0.0614 (0.0605)	0.811 (0.865)	75.00 (71.27)
[40/157]	0.1027 (0.0994)	0.0636 (0.0609)	0.723 (0.877)	78.12 (71.42)
[50/157]	0.1005 (0.0998)	0.0619 (0.0611)	1.132 (0.876)	62.50 (72.12)
[60/157]	0.1028 (0.1000)	0.0619 (0.0614)	1.043 (0.892)	62.50 (71.77)
[70/157]	0.0985 (0.1002)	0.0591 (0.0614)	1.154 (0.892)	62.50 (71.48)
[80/157]	0.1015 (0.1003)	0.0616 (0.0616)	0.870 (0.897)	71.88 (71.26)
[90/157]	0.1012 (0.1003)	0.0619 (0.0616)	0.893 (0.893)	75.00 (71.53)
[100/157]	0.1018 (0.1003)	0.0621 (0.0617)	0.629 (0.885)	81.25 (71.84)
[110/157]	0.0986 (0.1004)	0.0593 (0.0617)	0.841 (0.886)	62.50 (71.51)
[120/157]	0.1011 (0.1006)	0.0618 (0.0618)	1.325 (0.890)	56.25 (71.54)
[130/157]	0.1013 (0.1006)	0.0622 (0.0619)	0.784 (0.888)	75.00 (71.61)
[140/157]	0.1015 (0.1007)	0.0620 (0.0620)	0.702 (0.887)	78.12 (71.41)
[150/157]	0.1025 (0.1008)	0.0629 (0.0620)	0.876 (0.886)	71.88 (71.52)
[156/157]	0.0868 (0.1007)	0.0589 (0.0620)	1.678 (0.890)	25.00 (71.34)
 * Train Acc 71.340
 * Val Acc 72.100, Total time 0.61
 * Val loss 0.826, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0089 (0.0089)	0.982 (0.982)	68.75 (68.75)
[10/157]	0.1006 (0.0961)	0.0612 (0.0565)	0.887 (0.865)	71.88 (72.73)
[20/157]	0.1025 (0.0983)	0.0633 (0.0593)	0.807 (0.895)	78.12 (72.47)
[30/157]	0.1002 (0.0993)	0.0607 (0.0603)	0.757 (0.887)	78.12 (72.38)
[40/157]	0.1037 (0.0998)	0.0637 (0.0608)	0.774 (0.867)	84.38 (73.32)
[50/157]	0.1036 (0.1001)	0.0627 (0.0610)	0.771 (0.842)	71.88 (74.33)
[60/157]	0.0994 (0.1003)	0.0605 (0.0613)	0.965 (0.851)	68.75 (73.92)
[70/157]	0.1010 (0.1005)	0.0615 (0.0614)	0.725 (0.846)	78.12 (74.03)
[80/157]	0.1024 (0.1006)	0.0631 (0.0616)	0.904 (0.858)	71.88 (73.46)
[90/157]	0.0999 (0.1006)	0.0611 (0.0617)	0.702 (0.854)	75.00 (73.73)
[100/157]	0.1021 (0.1007)	0.0629 (0.0618)	0.893 (0.849)	75.00 (73.89)
[110/157]	0.1000 (0.1007)	0.0610 (0.0618)	0.983 (0.860)	68.75 (73.40)
[120/157]	0.1011 (0.1008)	0.0611 (0.0618)	1.105 (0.865)	68.75 (72.91)
[130/157]	0.1019 (0.1008)	0.0630 (0.0618)	0.903 (0.871)	75.00 (72.71)
[140/157]	0.1004 (0.1008)	0.0611 (0.0619)	1.075 (0.872)	62.50 (72.65)
[150/157]	0.1013 (0.1008)	0.0622 (0.0619)	0.766 (0.875)	78.12 (72.56)
[156/157]	0.0855 (0.1007)	0.0577 (0.0619)	0.972 (0.873)	50.00 (72.64)
 * Train Acc 72.640
 * Val Acc 71.500, Total time 0.62
 * Val loss 0.828, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0087 (0.0087)	0.991 (0.991)	71.88 (71.88)
[10/157]	0.1003 (0.0958)	0.0609 (0.0566)	1.252 (0.911)	53.12 (71.88)
[20/157]	0.1024 (0.0986)	0.0617 (0.0596)	1.049 (0.911)	71.88 (72.02)
[30/157]	0.1014 (0.0994)	0.0614 (0.0602)	0.856 (0.904)	65.62 (71.47)
[40/157]	0.1006 (0.0999)	0.0606 (0.0605)	0.904 (0.912)	65.62 (70.43)
[50/157]	0.1017 (0.1002)	0.0623 (0.0609)	1.310 (0.898)	53.12 (70.77)
[60/157]	0.1024 (0.1004)	0.0621 (0.0611)	0.714 (0.899)	81.25 (71.26)
[70/157]	0.1004 (0.1005)	0.0608 (0.0612)	0.613 (0.907)	90.62 (70.91)
[80/157]	0.1018 (0.1006)	0.0621 (0.0614)	0.797 (0.917)	75.00 (70.45)
[90/157]	0.1023 (0.1006)	0.0624 (0.0614)	1.048 (0.908)	68.75 (70.91)
[100/157]	0.1002 (0.1007)	0.0612 (0.0615)	0.746 (0.907)	75.00 (70.98)
[110/157]	0.1010 (0.1008)	0.0620 (0.0615)	0.854 (0.900)	68.75 (71.14)
[120/157]	0.1035 (0.1009)	0.0636 (0.0616)	0.695 (0.889)	84.38 (71.67)
[130/157]	0.0997 (0.1008)	0.0606 (0.0616)	0.608 (0.892)	84.38 (71.59)
[140/157]	0.1023 (0.1009)	0.0618 (0.0617)	1.010 (0.891)	71.88 (71.76)
[150/157]	0.1035 (0.1009)	0.0639 (0.0617)	0.908 (0.887)	81.25 (72.12)
[156/157]	0.0862 (0.1008)	0.0574 (0.0617)	1.032 (0.890)	62.50 (72.04)
 * Train Acc 72.040
 * Val Acc 72.500, Total time 0.61
 * Val loss 0.817, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0087 (0.0087)	1.058 (1.058)	62.50 (62.50)
[10/157]	0.1029 (0.0958)	0.0618 (0.0569)	0.892 (0.866)	62.50 (71.59)
[20/157]	0.1010 (0.0981)	0.0614 (0.0595)	0.773 (0.844)	75.00 (73.36)
[30/157]	0.1062 (0.0992)	0.0629 (0.0606)	0.946 (0.871)	65.62 (73.19)
[40/157]	0.1023 (0.0996)	0.0619 (0.0609)	1.094 (0.879)	59.38 (72.79)
[50/157]	0.1009 (0.0999)	0.0618 (0.0613)	1.287 (0.910)	59.38 (71.20)
[60/157]	0.1030 (0.1002)	0.0628 (0.0616)	0.846 (0.906)	68.75 (70.75)
[70/157]	0.1026 (0.1004)	0.0617 (0.0617)	1.083 (0.916)	56.25 (70.38)
[80/157]	0.1009 (0.1005)	0.0614 (0.0616)	1.026 (0.901)	62.50 (70.64)
[90/157]	0.1014 (0.1006)	0.0622 (0.0617)	1.243 (0.903)	65.62 (71.02)
[100/157]	0.1013 (0.1006)	0.0623 (0.0618)	0.945 (0.900)	71.88 (71.23)
[110/157]	0.1007 (0.1007)	0.0607 (0.0618)	1.066 (0.900)	75.00 (71.20)
[120/157]	0.1012 (0.1006)	0.0619 (0.0618)	0.992 (0.901)	65.62 (71.00)
[130/157]	0.1001 (0.1007)	0.0607 (0.0618)	0.920 (0.897)	65.62 (71.09)
[140/157]	0.1009 (0.1007)	0.0615 (0.0618)	0.771 (0.898)	68.75 (70.79)
[150/157]	0.0995 (0.1007)	0.0603 (0.0618)	1.068 (0.897)	59.38 (70.72)
[156/157]	0.0847 (0.1006)	0.0573 (0.0618)	0.702 (0.899)	87.50 (70.64)
 * Train Acc 70.640
 * Val Acc 72.700, Total time 0.59
 * Val loss 0.811, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0088 (0.0088)	0.920 (0.920)	68.75 (68.75)
[10/157]	0.1033 (0.0957)	0.0640 (0.0576)	0.695 (0.782)	78.12 (75.28)
[20/157]	0.1022 (0.0983)	0.0630 (0.0599)	1.070 (0.839)	65.62 (72.17)
[30/157]	0.0999 (0.0993)	0.0606 (0.0608)	0.972 (0.868)	68.75 (71.77)
[40/157]	0.1010 (0.0996)	0.0614 (0.0610)	0.914 (0.887)	59.38 (71.11)
[50/157]	0.1000 (0.1000)	0.0609 (0.0613)	0.697 (0.892)	81.25 (70.53)
[60/157]	0.1011 (0.1001)	0.0614 (0.0614)	0.923 (0.877)	68.75 (71.31)
[70/157]	0.1012 (0.1003)	0.0622 (0.0615)	0.923 (0.877)	65.62 (71.70)
[80/157]	0.1020 (0.1004)	0.0626 (0.0617)	0.920 (0.872)	71.88 (71.95)
[90/157]	0.1006 (0.1005)	0.0619 (0.0617)	0.854 (0.875)	68.75 (72.01)
[100/157]	0.0999 (0.1006)	0.0607 (0.0618)	0.797 (0.869)	78.12 (72.25)
[110/157]	0.1027 (0.1006)	0.0624 (0.0619)	0.939 (0.877)	68.75 (72.02)
[120/157]	0.1007 (0.1006)	0.0613 (0.0619)	1.062 (0.884)	71.88 (71.67)
[130/157]	0.1018 (0.1007)	0.0626 (0.0619)	0.747 (0.889)	78.12 (71.45)
[140/157]	0.1013 (0.1007)	0.0614 (0.0619)	0.735 (0.887)	84.38 (71.68)
[150/157]	0.1014 (0.1008)	0.0616 (0.0620)	0.703 (0.885)	78.12 (71.79)
[156/157]	0.0849 (0.1006)	0.0581 (0.0619)	1.232 (0.883)	75.00 (71.90)
 * Train Acc 71.900
 * Val Acc 72.500, Total time 0.61
 * Val loss 0.798, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0087 (0.0087)	0.993 (0.993)	75.00 (75.00)
[10/157]	0.1031 (0.0956)	0.0624 (0.0565)	0.941 (0.848)	71.88 (74.15)
[20/157]	0.0994 (0.0978)	0.0610 (0.0589)	1.181 (0.865)	68.75 (74.26)
[30/157]	0.1030 (0.0989)	0.0634 (0.0602)	1.238 (0.867)	59.38 (73.79)
[40/157]	0.1003 (0.0993)	0.0616 (0.0607)	0.770 (0.876)	71.88 (72.94)
[50/157]	0.1019 (0.0996)	0.0633 (0.0610)	0.791 (0.870)	75.00 (72.98)
[60/157]	0.1025 (0.0999)	0.0610 (0.0611)	0.867 (0.864)	68.75 (73.00)
[70/157]	0.1021 (0.1001)	0.0622 (0.0613)	0.971 (0.858)	62.50 (73.02)
[80/157]	0.0996 (0.1001)	0.0606 (0.0614)	0.978 (0.865)	71.88 (72.53)
[90/157]	0.0952 (0.1002)	0.0571 (0.0614)	1.113 (0.870)	62.50 (72.46)
[100/157]	0.0976 (0.0998)	0.0577 (0.0611)	0.888 (0.871)	68.75 (72.28)
[110/157]	0.0992 (0.0994)	0.0592 (0.0609)	0.745 (0.867)	78.12 (72.64)
[120/157]	0.0963 (0.0992)	0.0583 (0.0607)	0.934 (0.869)	75.00 (72.68)
[130/157]	0.0953 (0.0990)	0.0567 (0.0606)	0.537 (0.876)	84.38 (72.30)
[140/157]	0.0962 (0.0988)	0.0586 (0.0604)	0.887 (0.880)	71.88 (72.19)
[150/157]	0.1021 (0.0987)	0.0629 (0.0604)	1.087 (0.877)	65.62 (72.33)
[156/157]	0.0857 (0.0988)	0.0587 (0.0605)	0.845 (0.877)	87.50 (72.30)
 * Train Acc 72.300
 * Val Acc 72.500, Total time 0.61
 * Val loss 0.812, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0084 (0.0084)	0.788 (0.788)	81.25 (81.25)
[10/157]	0.1031 (0.0972)	0.0637 (0.0584)	0.742 (0.843)	81.25 (73.86)
[20/157]	0.1024 (0.0996)	0.0634 (0.0611)	0.665 (0.905)	84.38 (71.28)
[30/157]	0.1044 (0.1006)	0.0644 (0.0620)	0.726 (0.899)	75.00 (71.88)
[40/157]	0.1012 (0.1012)	0.0623 (0.0627)	1.097 (0.893)	62.50 (71.42)
[50/157]	0.1040 (0.1015)	0.0631 (0.0629)	0.781 (0.873)	75.00 (72.00)
[60/157]	0.1037 (0.1018)	0.0641 (0.0632)	0.846 (0.876)	71.88 (71.98)
[70/157]	0.1040 (0.1019)	0.0643 (0.0633)	0.987 (0.891)	68.75 (71.39)
[80/157]	0.1034 (0.1021)	0.0639 (0.0634)	1.040 (0.899)	75.00 (70.83)
[90/157]	0.1033 (0.1022)	0.0638 (0.0634)	0.786 (0.886)	71.88 (71.26)
[100/157]	0.1016 (0.1023)	0.0622 (0.0635)	0.952 (0.882)	68.75 (71.53)
[110/157]	0.1042 (0.1023)	0.0645 (0.0635)	0.839 (0.882)	71.88 (71.48)
[120/157]	0.1048 (0.1024)	0.0651 (0.0636)	1.204 (0.890)	59.38 (71.13)
[130/157]	0.1036 (0.1024)	0.0646 (0.0636)	1.157 (0.889)	59.38 (71.33)
[140/157]	0.1034 (0.1024)	0.0635 (0.0636)	0.696 (0.882)	84.38 (71.54)
[150/157]	0.1025 (0.1024)	0.0634 (0.0636)	1.175 (0.885)	50.00 (71.23)
[156/157]	0.0865 (0.1023)	0.0589 (0.0636)	0.767 (0.886)	62.50 (71.34)
 * Train Acc 71.340
 * Val Acc 71.500, Total time 0.61
 * Val loss 0.845, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0476 (0.0476)	0.0093 (0.0093)	0.847 (0.847)	78.12 (78.12)
[10/157]	0.1028 (0.0973)	0.0631 (0.0580)	0.990 (0.795)	59.38 (76.14)
[20/157]	0.1014 (0.0996)	0.0625 (0.0606)	0.895 (0.827)	75.00 (74.26)
[30/157]	0.1056 (0.1008)	0.0645 (0.0617)	0.749 (0.827)	81.25 (74.70)
[40/157]	0.1008 (0.1012)	0.0618 (0.0622)	0.979 (0.833)	68.75 (73.93)
[50/157]	0.0936 (0.1003)	0.0559 (0.0615)	0.942 (0.830)	68.75 (74.14)
[60/157]	0.0973 (0.0995)	0.0591 (0.0611)	1.001 (0.847)	65.62 (73.26)
[70/157]	0.0999 (0.1007)	0.0604 (0.0623)	0.913 (0.841)	75.00 (73.64)
[80/157]	0.1005 (0.1007)	0.0617 (0.0621)	0.705 (0.858)	78.12 (72.99)
[90/157]	0.1002 (0.1006)	0.0617 (0.0621)	1.006 (0.865)	68.75 (72.66)
[100/157]	0.1016 (0.1006)	0.0619 (0.0620)	1.026 (0.871)	68.75 (72.37)
[110/157]	0.1015 (0.1005)	0.0623 (0.0620)	0.941 (0.870)	71.88 (72.21)
[120/157]	0.1007 (0.1005)	0.0609 (0.0620)	0.814 (0.866)	71.88 (72.49)
[130/157]	0.1005 (0.1005)	0.0607 (0.0619)	0.734 (0.865)	78.12 (72.73)
[140/157]	0.1000 (0.1005)	0.0604 (0.0618)	0.931 (0.866)	68.75 (72.70)
[150/157]	0.1011 (0.1004)	0.0609 (0.0617)	0.760 (0.867)	71.88 (72.66)
[156/157]	0.0832 (0.1003)	0.0547 (0.0616)	1.718 (0.872)	50.00 (72.42)
 * Train Acc 72.420
 * Val Acc 71.200, Total time 0.61
 * Val loss 0.818, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0093 (0.0093)	0.757 (0.757)	81.25 (81.25)
[10/157]	0.1009 (0.0946)	0.0612 (0.0555)	0.655 (0.762)	78.12 (77.27)
[20/157]	0.0993 (0.0971)	0.0598 (0.0582)	0.702 (0.832)	87.50 (75.89)
[30/157]	0.1003 (0.0980)	0.0608 (0.0591)	0.768 (0.829)	78.12 (75.40)
[40/157]	0.0996 (0.0985)	0.0600 (0.0595)	0.573 (0.824)	90.62 (75.15)
[50/157]	0.1011 (0.0988)	0.0613 (0.0598)	0.934 (0.840)	78.12 (74.20)
[60/157]	0.1008 (0.0990)	0.0609 (0.0600)	0.721 (0.858)	78.12 (73.41)
[70/157]	0.1005 (0.0991)	0.0605 (0.0602)	0.853 (0.869)	65.62 (72.98)
[80/157]	0.0992 (0.0992)	0.0604 (0.0602)	0.850 (0.864)	71.88 (73.03)
[90/157]	0.0992 (0.0993)	0.0596 (0.0603)	0.814 (0.870)	75.00 (72.84)
[100/157]	0.0995 (0.0993)	0.0601 (0.0604)	0.890 (0.864)	78.12 (72.96)
[110/157]	0.1013 (0.0994)	0.0614 (0.0605)	0.934 (0.869)	75.00 (72.83)
[120/157]	0.1014 (0.0995)	0.0616 (0.0605)	0.888 (0.872)	68.75 (72.68)
[130/157]	0.1003 (0.0995)	0.0606 (0.0606)	1.030 (0.875)	56.25 (72.38)
[140/157]	0.1002 (0.0995)	0.0606 (0.0606)	1.081 (0.881)	65.62 (71.96)
[150/157]	0.1000 (0.0996)	0.0607 (0.0606)	1.080 (0.881)	62.50 (72.00)
[156/157]	0.0848 (0.0995)	0.0562 (0.0606)	1.233 (0.882)	50.00 (71.88)
 * Train Acc 71.880
 * Val Acc 72.400, Total time 0.62
 * Val loss 0.808, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0092 (0.0092)	1.075 (1.075)	65.62 (65.62)
[10/157]	0.1015 (0.0954)	0.0610 (0.0564)	0.992 (0.897)	59.38 (71.59)
[20/157]	0.0998 (0.0978)	0.0593 (0.0587)	0.780 (0.911)	68.75 (70.54)
[30/157]	0.1010 (0.0985)	0.0616 (0.0594)	1.018 (0.916)	68.75 (70.77)
[40/157]	0.0994 (0.0988)	0.0598 (0.0597)	0.890 (0.917)	75.00 (70.27)
[50/157]	0.1004 (0.0991)	0.0607 (0.0599)	0.858 (0.898)	62.50 (70.47)
[60/157]	0.1001 (0.0992)	0.0606 (0.0600)	1.093 (0.896)	65.62 (70.80)
[70/157]	0.1010 (0.0993)	0.0612 (0.0601)	0.716 (0.888)	81.25 (71.17)
[80/157]	0.1001 (0.0994)	0.0609 (0.0602)	0.939 (0.885)	81.25 (71.72)
[90/157]	0.0997 (0.0995)	0.0605 (0.0603)	0.928 (0.880)	71.88 (71.74)
[100/157]	0.1003 (0.0995)	0.0606 (0.0604)	0.906 (0.877)	65.62 (72.12)
[110/157]	0.1007 (0.0996)	0.0609 (0.0604)	0.921 (0.875)	65.62 (72.04)
[120/157]	0.1001 (0.0996)	0.0604 (0.0604)	0.676 (0.869)	71.88 (72.13)
[130/157]	0.0999 (0.0996)	0.0605 (0.0605)	0.848 (0.871)	68.75 (71.95)
[140/157]	0.1000 (0.0996)	0.0604 (0.0605)	1.027 (0.870)	71.88 (72.10)
[150/157]	0.0999 (0.0997)	0.0607 (0.0606)	1.094 (0.875)	68.75 (72.08)
[156/157]	0.0851 (0.0996)	0.0563 (0.0605)	0.716 (0.875)	75.00 (71.90)
 * Train Acc 71.900
 * Val Acc 72.200, Total time 0.61
 * Val loss 0.813, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0086 (0.0086)	0.873 (0.873)	71.88 (71.88)
[10/157]	0.1004 (0.0948)	0.0606 (0.0553)	0.683 (0.829)	81.25 (73.30)
[20/157]	0.1008 (0.0974)	0.0603 (0.0580)	0.845 (0.829)	71.88 (73.96)
[30/157]	0.0999 (0.0982)	0.0603 (0.0590)	0.918 (0.830)	78.12 (74.40)
[40/157]	0.1001 (0.0987)	0.0608 (0.0595)	0.795 (0.837)	78.12 (74.62)
[50/157]	0.1038 (0.0992)	0.0626 (0.0600)	0.848 (0.841)	75.00 (73.90)
[60/157]	0.0996 (0.0994)	0.0590 (0.0601)	0.889 (0.846)	65.62 (73.26)
[70/157]	0.1002 (0.0995)	0.0605 (0.0603)	0.740 (0.834)	65.62 (73.33)
[80/157]	0.1041 (0.0996)	0.0613 (0.0604)	0.593 (0.836)	81.25 (72.99)
[90/157]	0.1005 (0.0996)	0.0613 (0.0604)	0.987 (0.841)	71.88 (72.80)
[100/157]	0.1000 (0.0996)	0.0610 (0.0605)	0.825 (0.839)	75.00 (72.99)
[110/157]	0.1008 (0.0996)	0.0612 (0.0605)	1.060 (0.846)	59.38 (72.86)
[120/157]	0.1004 (0.0997)	0.0611 (0.0606)	0.905 (0.853)	71.88 (72.57)
[130/157]	0.0996 (0.0997)	0.0606 (0.0606)	0.917 (0.858)	68.75 (72.35)
[140/157]	0.1002 (0.0997)	0.0609 (0.0606)	0.842 (0.860)	65.62 (72.23)
[150/157]	0.0997 (0.0997)	0.0607 (0.0607)	0.708 (0.868)	78.12 (71.81)
[156/157]	0.0830 (0.0996)	0.0558 (0.0607)	1.050 (0.871)	62.50 (71.80)
 * Train Acc 71.800
 * Val Acc 72.000, Total time 0.60
 * Val loss 0.804, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0080 (0.0080)	0.871 (0.871)	81.25 (81.25)
[10/157]	0.0982 (0.0952)	0.0590 (0.0558)	0.704 (0.833)	75.00 (72.44)
[20/157]	0.1009 (0.0976)	0.0612 (0.0584)	1.142 (0.861)	59.38 (70.68)
[30/157]	0.1006 (0.0984)	0.0608 (0.0592)	0.643 (0.862)	81.25 (70.67)
[40/157]	0.1009 (0.0988)	0.0603 (0.0596)	0.856 (0.863)	75.00 (71.34)
[50/157]	0.0993 (0.0990)	0.0602 (0.0599)	0.711 (0.865)	75.00 (71.32)
[60/157]	0.1010 (0.0991)	0.0614 (0.0601)	1.144 (0.868)	62.50 (71.31)
[70/157]	0.1002 (0.0993)	0.0605 (0.0602)	0.752 (0.870)	75.00 (71.21)
[80/157]	0.0993 (0.0993)	0.0597 (0.0603)	1.050 (0.880)	53.12 (70.76)
[90/157]	0.1000 (0.0994)	0.0598 (0.0603)	0.850 (0.877)	68.75 (71.12)
[100/157]	0.0997 (0.0995)	0.0600 (0.0604)	0.654 (0.867)	84.38 (71.84)
[110/157]	0.0999 (0.0995)	0.0606 (0.0604)	0.874 (0.865)	68.75 (71.76)
[120/157]	0.1005 (0.0995)	0.0599 (0.0604)	0.753 (0.865)	75.00 (71.77)
[130/157]	0.1009 (0.0996)	0.0615 (0.0604)	0.871 (0.870)	71.88 (71.61)
[140/157]	0.0994 (0.0996)	0.0592 (0.0604)	0.867 (0.867)	59.38 (71.63)
[150/157]	0.1206 (0.1001)	0.0800 (0.0609)	0.797 (0.869)	78.12 (71.59)
[156/157]	0.0781 (0.1000)	0.0528 (0.0609)	0.696 (0.872)	62.50 (71.48)
 * Train Acc 71.480
 * Val Acc 72.400, Total time 0.58
 * Val loss 0.803, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0089 (0.0089)	0.928 (0.928)	71.88 (71.88)
[10/157]	0.1181 (0.1104)	0.0782 (0.0705)	0.764 (0.858)	78.12 (70.74)
[20/157]	0.1060 (0.1087)	0.0660 (0.0691)	0.744 (0.856)	71.88 (71.88)
[30/157]	0.1082 (0.1083)	0.0688 (0.0687)	0.922 (0.860)	75.00 (72.68)
[40/157]	0.0950 (0.1075)	0.0570 (0.0680)	0.920 (0.846)	75.00 (73.86)
[50/157]	0.1026 (0.1053)	0.0625 (0.0662)	0.782 (0.844)	75.00 (73.53)
[60/157]	0.1037 (0.1052)	0.0642 (0.0660)	0.951 (0.847)	65.62 (73.10)
[70/157]	0.1038 (0.1050)	0.0634 (0.0658)	0.997 (0.850)	65.62 (73.02)
[80/157]	0.1042 (0.1049)	0.0646 (0.0657)	0.701 (0.862)	75.00 (72.53)
[90/157]	0.0955 (0.1040)	0.0579 (0.0650)	0.778 (0.871)	71.88 (71.84)
[100/157]	0.0943 (0.1032)	0.0567 (0.0643)	0.978 (0.874)	71.88 (71.84)
[110/157]	0.1018 (0.1033)	0.0621 (0.0645)	0.785 (0.867)	75.00 (72.30)
[120/157]	0.1000 (0.1031)	0.0601 (0.0643)	0.867 (0.868)	59.38 (72.18)
[130/157]	0.1006 (0.1028)	0.0617 (0.0640)	0.947 (0.871)	75.00 (71.97)
[140/157]	0.1003 (0.1026)	0.0608 (0.0638)	0.727 (0.875)	84.38 (71.83)
[150/157]	0.1007 (0.1025)	0.0612 (0.0636)	0.941 (0.872)	71.88 (71.88)
[156/157]	0.0824 (0.1023)	0.0555 (0.0635)	0.778 (0.870)	75.00 (71.90)
 * Train Acc 71.900
 * Val Acc 72.700, Total time 0.61
 * Val loss 0.801, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0090 (0.0090)	0.893 (0.893)	68.75 (68.75)
[10/157]	0.1031 (0.0957)	0.0622 (0.0569)	1.083 (0.909)	65.62 (72.44)
[20/157]	0.0990 (0.0978)	0.0588 (0.0589)	0.942 (0.877)	71.88 (72.77)
[30/157]	0.1001 (0.0985)	0.0614 (0.0597)	0.809 (0.862)	71.88 (72.88)
[40/157]	0.0995 (0.0988)	0.0604 (0.0601)	0.957 (0.870)	65.62 (72.48)
[50/157]	0.0986 (0.0991)	0.0599 (0.0604)	0.594 (0.852)	84.38 (73.10)
[60/157]	0.1007 (0.0993)	0.0611 (0.0605)	1.036 (0.848)	71.88 (73.05)
[70/157]	0.1009 (0.0995)	0.0619 (0.0608)	0.849 (0.863)	71.88 (72.36)
[80/157]	0.1002 (0.0996)	0.0611 (0.0609)	1.007 (0.861)	56.25 (72.38)
[90/157]	0.0998 (0.0997)	0.0607 (0.0609)	0.779 (0.861)	78.12 (72.36)
[100/157]	0.0997 (0.0997)	0.0608 (0.0610)	0.884 (0.864)	75.00 (72.06)
[110/157]	0.1002 (0.0998)	0.0610 (0.0610)	0.668 (0.868)	84.38 (71.79)
[120/157]	0.1017 (0.0998)	0.0626 (0.0611)	0.855 (0.866)	71.88 (71.90)
[130/157]	0.1029 (0.0999)	0.0622 (0.0611)	0.722 (0.867)	81.25 (71.95)
[140/157]	0.1000 (0.0999)	0.0608 (0.0612)	1.235 (0.870)	59.38 (71.85)
[150/157]	0.0998 (0.0999)	0.0605 (0.0611)	0.800 (0.870)	65.62 (71.85)
[156/157]	0.0854 (0.0998)	0.0575 (0.0611)	0.971 (0.870)	50.00 (71.82)
 * Train Acc 71.820
 * Val Acc 73.100, Total time 0.61
 * Val loss 0.796, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0084 (0.0084)	0.767 (0.767)	75.00 (75.00)
[10/157]	0.0999 (0.0945)	0.0603 (0.0557)	0.849 (0.783)	75.00 (75.57)
[20/157]	0.1002 (0.0971)	0.0611 (0.0585)	0.906 (0.828)	75.00 (73.36)
[30/157]	0.0997 (0.0980)	0.0606 (0.0593)	0.763 (0.831)	71.88 (72.98)
[40/157]	0.0990 (0.0985)	0.0610 (0.0598)	0.818 (0.860)	84.38 (71.95)
[50/157]	0.1020 (0.0988)	0.0614 (0.0602)	1.192 (0.865)	62.50 (72.00)
[60/157]	0.1008 (0.0990)	0.0622 (0.0604)	0.827 (0.863)	75.00 (71.93)
[70/157]	0.1032 (0.0993)	0.0632 (0.0607)	1.087 (0.859)	59.38 (72.10)
[80/157]	0.1009 (0.0995)	0.0614 (0.0608)	0.972 (0.853)	71.88 (72.65)
[90/157]	0.1001 (0.0995)	0.0607 (0.0609)	0.957 (0.853)	71.88 (72.63)
[100/157]	0.1004 (0.0996)	0.0618 (0.0609)	0.781 (0.846)	81.25 (73.08)
[110/157]	0.1009 (0.0996)	0.0616 (0.0610)	0.951 (0.848)	68.75 (73.00)
[120/157]	0.1026 (0.0998)	0.0615 (0.0610)	1.028 (0.857)	62.50 (72.52)
[130/157]	0.1013 (0.0998)	0.0615 (0.0611)	0.608 (0.861)	87.50 (72.35)
[140/157]	0.1009 (0.0998)	0.0603 (0.0611)	0.672 (0.864)	81.25 (72.14)
[150/157]	0.1005 (0.0999)	0.0579 (0.0611)	0.805 (0.864)	65.62 (72.10)
[156/157]	0.0841 (0.0997)	0.0571 (0.0610)	1.117 (0.867)	50.00 (71.90)
 * Train Acc 71.900
 * Val Acc 72.800, Total time 0.61
 * Val loss 0.791, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0091 (0.0091)	0.979 (0.979)	68.75 (68.75)
[10/157]	0.1017 (0.0949)	0.0629 (0.0569)	0.849 (0.865)	68.75 (73.58)
[20/157]	0.1015 (0.0974)	0.0621 (0.0594)	0.707 (0.842)	81.25 (73.36)
[30/157]	0.1002 (0.0985)	0.0611 (0.0602)	0.794 (0.828)	78.12 (73.99)
[40/157]	0.1007 (0.0989)	0.0612 (0.0604)	0.935 (0.839)	65.62 (73.17)
[50/157]	0.0999 (0.0991)	0.0608 (0.0606)	0.917 (0.832)	71.88 (73.47)
[60/157]	0.1003 (0.0992)	0.0609 (0.0608)	1.198 (0.853)	56.25 (72.85)
[70/157]	0.0996 (0.0993)	0.0608 (0.0608)	0.856 (0.854)	71.88 (72.84)
[80/157]	0.0969 (0.0994)	0.0592 (0.0609)	0.521 (0.844)	84.38 (73.34)
[90/157]	0.1004 (0.0995)	0.0614 (0.0610)	0.781 (0.841)	75.00 (73.28)
[100/157]	0.1001 (0.0995)	0.0613 (0.0610)	0.784 (0.842)	68.75 (73.42)
[110/157]	0.1000 (0.0996)	0.0611 (0.0611)	0.910 (0.845)	59.38 (72.97)
[120/157]	0.0991 (0.0996)	0.0603 (0.0611)	0.697 (0.848)	81.25 (73.11)
[130/157]	0.1002 (0.0996)	0.0609 (0.0611)	0.794 (0.845)	78.12 (73.33)
[140/157]	0.1001 (0.0996)	0.0610 (0.0612)	0.797 (0.844)	75.00 (73.32)
[150/157]	0.1002 (0.0997)	0.0604 (0.0611)	0.702 (0.848)	81.25 (73.10)
[156/157]	0.0840 (0.0996)	0.0570 (0.0611)	0.968 (0.849)	75.00 (73.04)
 * Train Acc 73.040
 * Val Acc 72.500, Total time 0.61
 * Val loss 0.799, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0094 (0.0094)	0.896 (0.896)	71.88 (71.88)
[10/157]	0.1002 (0.0947)	0.0605 (0.0558)	0.846 (0.846)	71.88 (73.86)
[20/157]	0.1012 (0.0973)	0.0619 (0.0585)	0.932 (0.808)	75.00 (75.45)
[30/157]	0.1007 (0.0981)	0.0616 (0.0595)	0.752 (0.819)	75.00 (73.59)
[40/157]	0.0995 (0.0986)	0.0603 (0.0600)	0.772 (0.834)	78.12 (73.78)
[50/157]	0.1001 (0.0989)	0.0609 (0.0602)	0.657 (0.841)	78.12 (73.47)
[60/157]	0.1020 (0.0991)	0.0622 (0.0604)	1.004 (0.845)	65.62 (73.10)
[70/157]	0.1025 (0.0994)	0.0624 (0.0607)	0.922 (0.853)	71.88 (72.93)
[80/157]	0.1002 (0.0995)	0.0610 (0.0607)	0.734 (0.862)	78.12 (72.99)
[90/157]	0.1003 (0.0996)	0.0593 (0.0608)	0.784 (0.857)	68.75 (73.04)
[100/157]	0.0989 (0.0996)	0.0593 (0.0608)	0.867 (0.858)	71.88 (73.14)
[110/157]	0.1007 (0.0996)	0.0614 (0.0608)	0.988 (0.860)	71.88 (73.11)
[120/157]	0.0999 (0.0997)	0.0603 (0.0608)	1.085 (0.863)	65.62 (73.01)
[130/157]	0.1001 (0.0997)	0.0609 (0.0609)	0.523 (0.872)	90.62 (72.66)
[140/157]	0.1181 (0.0998)	0.0778 (0.0610)	0.768 (0.869)	78.12 (72.67)
[150/157]	0.1217 (0.1012)	0.0802 (0.0623)	0.843 (0.865)	68.75 (72.64)
[156/157]	0.0828 (0.1015)	0.0536 (0.0627)	0.910 (0.863)	75.00 (72.84)
 * Train Acc 72.840
 * Val Acc 72.400, Total time 0.60
 * Val loss 0.810, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0087 (0.0087)	0.777 (0.777)	81.25 (81.25)
[10/157]	0.0970 (0.0914)	0.0587 (0.0535)	0.841 (0.824)	75.00 (76.99)
[20/157]	0.1197 (0.0969)	0.0798 (0.0589)	0.906 (0.863)	68.75 (74.11)
[30/157]	0.1037 (0.1032)	0.0639 (0.0646)	0.657 (0.881)	87.50 (72.88)
[40/157]	0.1055 (0.1032)	0.0647 (0.0646)	1.103 (0.889)	65.62 (72.79)
[50/157]	0.1063 (0.1035)	0.0647 (0.0647)	1.195 (0.895)	68.75 (72.86)
[60/157]	0.1037 (0.1036)	0.0643 (0.0647)	0.831 (0.883)	68.75 (72.69)
[70/157]	0.1039 (0.1037)	0.0649 (0.0648)	0.765 (0.878)	81.25 (72.93)
[80/157]	0.1058 (0.1037)	0.0658 (0.0648)	0.881 (0.877)	78.12 (72.99)
[90/157]	0.1018 (0.1037)	0.0625 (0.0648)	0.864 (0.872)	68.75 (72.94)
[100/157]	0.1049 (0.1037)	0.0650 (0.0648)	0.626 (0.866)	90.62 (72.93)
[110/157]	0.1054 (0.1038)	0.0653 (0.0648)	0.958 (0.865)	62.50 (72.89)
[120/157]	0.1043 (0.1038)	0.0646 (0.0648)	1.014 (0.868)	65.62 (72.55)
[130/157]	0.1045 (0.1038)	0.0647 (0.0647)	1.246 (0.873)	62.50 (72.26)
[140/157]	0.1039 (0.1038)	0.0644 (0.0647)	0.668 (0.863)	81.25 (72.76)
[150/157]	0.0962 (0.1037)	0.0578 (0.0646)	0.816 (0.867)	65.62 (72.62)
[156/157]	0.0762 (0.1032)	0.0516 (0.0643)	1.321 (0.864)	62.50 (72.66)
 * Train Acc 72.660
 * Val Acc 72.300, Total time 0.62
 * Val loss 0.805, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0091 (0.0091)	0.785 (0.785)	75.00 (75.00)
[10/157]	0.0995 (0.0946)	0.0607 (0.0561)	0.683 (0.858)	75.00 (73.86)
[20/157]	0.1016 (0.0975)	0.0627 (0.0590)	0.980 (0.856)	62.50 (73.21)
[30/157]	0.0997 (0.0985)	0.0607 (0.0598)	0.873 (0.856)	71.88 (72.98)
[40/157]	0.0988 (0.0989)	0.0601 (0.0601)	0.892 (0.864)	71.88 (73.09)
[50/157]	0.1007 (0.0992)	0.0606 (0.0604)	1.159 (0.853)	68.75 (73.90)
[60/157]	0.1001 (0.0993)	0.0615 (0.0606)	0.908 (0.843)	71.88 (74.23)
[70/157]	0.1014 (0.0995)	0.0623 (0.0608)	0.958 (0.844)	62.50 (73.90)
[80/157]	0.1006 (0.0997)	0.0613 (0.0610)	0.909 (0.855)	75.00 (73.38)
[90/157]	0.0999 (0.0997)	0.0609 (0.0611)	0.681 (0.858)	81.25 (73.01)
[100/157]	0.1002 (0.0997)	0.0612 (0.0611)	0.820 (0.857)	78.12 (72.87)
[110/157]	0.1004 (0.0998)	0.0614 (0.0611)	0.762 (0.856)	71.88 (72.83)
[120/157]	0.1001 (0.0998)	0.0612 (0.0611)	0.784 (0.857)	87.50 (72.73)
[130/157]	0.1003 (0.0998)	0.0610 (0.0612)	0.851 (0.858)	68.75 (72.73)
[140/157]	0.1007 (0.0998)	0.0613 (0.0612)	0.684 (0.855)	81.25 (73.01)
[150/157]	0.1004 (0.0998)	0.0614 (0.0612)	0.750 (0.854)	81.25 (72.97)
[156/157]	0.0926 (0.0998)	0.0572 (0.0612)	1.024 (0.851)	62.50 (73.06)
 * Train Acc 73.060
 * Val Acc 72.000, Total time 0.60
 * Val loss 0.798, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0088 (0.0088)	0.900 (0.900)	68.75 (68.75)
[10/157]	0.0996 (0.0956)	0.0603 (0.0562)	0.940 (0.818)	65.62 (74.15)
[20/157]	0.1003 (0.0978)	0.0613 (0.0589)	1.236 (0.785)	59.38 (74.85)
[30/157]	0.1012 (0.0990)	0.0605 (0.0599)	0.500 (0.821)	87.50 (74.80)
[40/157]	0.1005 (0.0993)	0.0601 (0.0602)	1.122 (0.843)	62.50 (73.78)
[50/157]	0.1011 (0.0995)	0.0616 (0.0603)	0.681 (0.867)	65.62 (72.61)
[60/157]	0.1032 (0.0998)	0.0625 (0.0607)	0.821 (0.870)	75.00 (71.98)
[70/157]	0.0986 (0.0998)	0.0595 (0.0608)	1.077 (0.872)	78.12 (72.14)
[80/157]	0.1000 (0.0999)	0.0609 (0.0608)	0.712 (0.871)	84.38 (72.22)
[90/157]	0.1012 (0.0999)	0.0619 (0.0609)	1.023 (0.868)	71.88 (72.46)
[100/157]	0.1010 (0.1000)	0.0617 (0.0610)	0.596 (0.860)	81.25 (72.71)
[110/157]	0.0999 (0.1001)	0.0609 (0.0610)	0.808 (0.859)	71.88 (72.89)
[120/157]	0.1009 (0.1001)	0.0618 (0.0611)	0.621 (0.855)	90.62 (73.11)
[130/157]	0.1011 (0.1001)	0.0618 (0.0611)	0.932 (0.850)	65.62 (73.40)
[140/157]	0.0992 (0.1001)	0.0597 (0.0611)	0.979 (0.852)	62.50 (73.12)
[150/157]	0.0999 (0.1000)	0.0606 (0.0611)	0.860 (0.850)	62.50 (73.03)
[156/157]	0.0841 (0.0999)	0.0570 (0.0611)	0.913 (0.854)	87.50 (72.86)
 * Train Acc 72.860
 * Val Acc 72.200, Total time 0.59
 * Val loss 0.799, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0085 (0.0085)	0.655 (0.655)	78.12 (78.12)
[10/157]	0.0997 (0.0952)	0.0604 (0.0558)	0.980 (0.871)	75.00 (74.43)
[20/157]	0.1002 (0.0976)	0.0604 (0.0584)	0.716 (0.874)	81.25 (73.36)
[30/157]	0.1016 (0.0984)	0.0613 (0.0592)	0.954 (0.872)	68.75 (73.49)
[40/157]	0.1002 (0.0987)	0.0601 (0.0598)	0.690 (0.863)	75.00 (73.32)
[50/157]	0.0996 (0.0989)	0.0607 (0.0601)	0.940 (0.866)	71.88 (73.16)
[60/157]	0.0996 (0.0991)	0.0606 (0.0602)	0.755 (0.850)	78.12 (73.92)
[70/157]	0.1009 (0.0993)	0.0608 (0.0604)	0.787 (0.852)	81.25 (73.90)
[80/157]	0.1024 (0.0995)	0.0622 (0.0606)	0.888 (0.847)	68.75 (73.73)
[90/157]	0.1015 (0.0997)	0.0617 (0.0606)	0.769 (0.854)	71.88 (73.63)
[100/157]	0.1037 (0.0998)	0.0620 (0.0608)	0.931 (0.858)	65.62 (73.39)
[110/157]	0.1007 (0.0999)	0.0610 (0.0608)	0.761 (0.858)	75.00 (73.06)
[120/157]	0.0999 (0.0999)	0.0609 (0.0608)	0.713 (0.854)	78.12 (73.19)
[130/157]	0.0984 (0.1000)	0.0596 (0.0609)	0.695 (0.853)	78.12 (73.16)
[140/157]	0.0997 (0.1000)	0.0608 (0.0609)	0.737 (0.852)	81.25 (73.14)
[150/157]	0.0997 (0.1000)	0.0609 (0.0610)	1.255 (0.851)	59.38 (73.26)
[156/157]	0.0828 (0.0999)	0.0556 (0.0610)	0.805 (0.850)	75.00 (73.32)
 * Train Acc 73.320
 * Val Acc 71.800, Total time 0.59
 * Val loss 0.798, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0087 (0.0087)	0.977 (0.977)	59.38 (59.38)
[10/157]	0.0976 (0.0911)	0.0586 (0.0533)	1.003 (0.900)	62.50 (70.45)
[20/157]	0.0968 (0.0931)	0.0592 (0.0557)	1.259 (0.946)	56.25 (69.49)
[30/157]	0.1161 (0.0998)	0.0757 (0.0618)	1.096 (0.907)	56.25 (70.26)
[40/157]	0.0976 (0.1012)	0.0591 (0.0630)	1.052 (0.889)	68.75 (71.11)
[50/157]	0.0985 (0.1007)	0.0595 (0.0625)	0.733 (0.884)	75.00 (70.77)
[60/157]	0.0984 (0.1003)	0.0585 (0.0621)	1.112 (0.877)	71.88 (71.26)
[70/157]	0.1004 (0.1001)	0.0610 (0.0618)	0.753 (0.860)	75.00 (71.96)
[80/157]	0.1023 (0.1002)	0.0630 (0.0619)	1.120 (0.856)	56.25 (72.38)
[90/157]	0.1001 (0.1003)	0.0606 (0.0619)	1.013 (0.865)	65.62 (71.74)
[100/157]	0.1008 (0.1002)	0.0613 (0.0619)	0.718 (0.862)	78.12 (71.91)
[110/157]	0.1020 (0.1003)	0.0623 (0.0619)	0.892 (0.867)	68.75 (71.76)
[120/157]	0.0993 (0.1004)	0.0599 (0.0619)	0.796 (0.869)	75.00 (71.80)
[130/157]	0.1009 (0.1004)	0.0620 (0.0619)	0.640 (0.863)	84.38 (72.09)
[140/157]	0.0998 (0.1004)	0.0605 (0.0619)	1.203 (0.859)	65.62 (72.21)
[150/157]	0.1020 (0.1005)	0.0619 (0.0620)	0.721 (0.856)	78.12 (72.39)
[156/157]	0.0916 (0.1004)	0.0580 (0.0619)	1.348 (0.858)	75.00 (72.28)
 * Train Acc 72.280
 * Val Acc 72.600, Total time 0.61
 * Val loss 0.788, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0101 (0.0101)	0.738 (0.738)	84.38 (84.38)
[10/157]	0.1016 (0.0948)	0.0626 (0.0567)	0.957 (0.885)	68.75 (72.73)
[20/157]	0.1003 (0.0979)	0.0614 (0.0594)	0.899 (0.837)	68.75 (73.21)
[30/157]	0.1003 (0.0986)	0.0610 (0.0600)	0.701 (0.847)	84.38 (72.48)
[40/157]	0.1017 (0.0991)	0.0626 (0.0605)	0.933 (0.871)	65.62 (72.33)
[50/157]	0.1007 (0.0995)	0.0609 (0.0609)	0.842 (0.854)	78.12 (72.92)
[60/157]	0.1000 (0.0996)	0.0601 (0.0609)	0.662 (0.838)	75.00 (73.10)
[70/157]	0.0972 (0.0997)	0.0582 (0.0610)	1.034 (0.844)	65.62 (72.84)
[80/157]	0.1004 (0.0997)	0.0613 (0.0610)	0.871 (0.851)	71.88 (72.42)
[90/157]	0.1012 (0.0998)	0.0618 (0.0611)	0.784 (0.852)	78.12 (72.46)
[100/157]	0.1010 (0.0999)	0.0616 (0.0613)	1.166 (0.857)	68.75 (72.34)
[110/157]	0.0996 (0.0999)	0.0602 (0.0613)	1.012 (0.857)	68.75 (72.55)
[120/157]	0.1005 (0.0999)	0.0615 (0.0613)	0.593 (0.857)	84.38 (72.60)
[130/157]	0.0993 (0.0999)	0.0606 (0.0613)	0.908 (0.859)	71.88 (72.57)
[140/157]	0.0994 (0.1000)	0.0604 (0.0613)	0.686 (0.858)	78.12 (72.54)
[150/157]	0.1016 (0.1000)	0.0622 (0.0614)	0.935 (0.852)	71.88 (72.83)
[156/157]	0.0876 (0.0999)	0.0586 (0.0614)	1.714 (0.852)	50.00 (72.82)
 * Train Acc 72.820
 * Val Acc 72.500, Total time 0.61
 * Val loss 0.794, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0082 (0.0082)	0.758 (0.758)	78.12 (78.12)
[10/157]	0.1188 (0.1012)	0.0782 (0.0627)	0.873 (0.879)	71.88 (73.01)
[20/157]	0.1018 (0.1017)	0.0629 (0.0631)	0.873 (0.858)	62.50 (73.07)
[30/157]	0.1018 (0.1017)	0.0618 (0.0629)	0.845 (0.861)	78.12 (73.19)
[40/157]	0.0993 (0.1017)	0.0600 (0.0628)	0.866 (0.865)	81.25 (73.09)
[50/157]	0.1024 (0.1017)	0.0620 (0.0627)	0.838 (0.869)	75.00 (72.49)
[60/157]	0.1015 (0.1015)	0.0622 (0.0626)	1.176 (0.879)	53.12 (71.98)
[70/157]	0.1006 (0.1015)	0.0611 (0.0626)	0.742 (0.875)	75.00 (72.05)
[80/157]	0.1021 (0.1016)	0.0612 (0.0626)	0.994 (0.876)	62.50 (71.91)
[90/157]	0.1017 (0.1016)	0.0616 (0.0626)	1.084 (0.872)	65.62 (72.01)
[100/157]	0.1029 (0.1016)	0.0626 (0.0625)	0.940 (0.870)	78.12 (72.25)
[110/157]	0.1014 (0.1015)	0.0614 (0.0625)	0.800 (0.860)	68.75 (72.35)
[120/157]	0.1016 (0.1016)	0.0614 (0.0625)	0.670 (0.853)	84.38 (72.75)
[130/157]	0.1013 (0.1015)	0.0628 (0.0625)	0.840 (0.852)	78.12 (72.69)
[140/157]	0.1028 (0.1015)	0.0638 (0.0625)	0.778 (0.852)	81.25 (72.78)
[150/157]	0.1001 (0.1015)	0.0611 (0.0625)	0.935 (0.854)	68.75 (72.72)
[156/157]	0.0863 (0.1014)	0.0580 (0.0625)	2.043 (0.856)	25.00 (72.68)
 * Train Acc 72.680
 * Val Acc 72.200, Total time 0.60
 * Val loss 0.793, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0092 (0.0092)	0.621 (0.621)	87.50 (87.50)
[10/157]	0.1006 (0.0961)	0.0605 (0.0558)	1.006 (0.734)	68.75 (78.98)
[20/157]	0.1009 (0.0988)	0.0624 (0.0591)	0.706 (0.844)	71.88 (72.77)
[30/157]	0.1025 (0.0995)	0.0627 (0.0603)	1.059 (0.834)	68.75 (73.69)
[40/157]	0.1010 (0.0999)	0.0622 (0.0609)	1.517 (0.849)	56.25 (73.09)
[50/157]	0.1008 (0.1002)	0.0619 (0.0613)	0.747 (0.841)	81.25 (73.84)
[60/157]	0.1030 (0.1005)	0.0626 (0.0616)	1.100 (0.851)	71.88 (73.10)
[70/157]	0.1012 (0.1005)	0.0625 (0.0617)	0.850 (0.853)	71.88 (73.06)
[80/157]	0.1011 (0.1006)	0.0618 (0.0618)	0.852 (0.866)	75.00 (72.34)
[90/157]	0.1007 (0.1008)	0.0615 (0.0620)	0.709 (0.853)	78.12 (72.97)
[100/157]	0.1029 (0.1008)	0.0631 (0.0621)	0.763 (0.855)	81.25 (72.77)
[110/157]	0.1030 (0.1009)	0.0635 (0.0621)	0.794 (0.850)	78.12 (73.20)
[120/157]	0.1010 (0.1009)	0.0617 (0.0622)	0.895 (0.857)	75.00 (73.09)
[130/157]	0.1011 (0.1010)	0.0622 (0.0622)	0.870 (0.850)	71.88 (73.19)
[140/157]	0.1012 (0.1009)	0.0616 (0.0622)	0.867 (0.846)	71.88 (73.40)
[150/157]	0.1011 (0.1010)	0.0619 (0.0623)	0.871 (0.846)	75.00 (73.47)
[156/157]	0.0867 (0.1009)	0.0594 (0.0622)	0.692 (0.844)	75.00 (73.48)
 * Train Acc 73.480
 * Val Acc 72.600, Total time 0.61
 * Val loss 0.783, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0469 (0.0469)	0.0097 (0.0097)	1.026 (1.026)	65.62 (65.62)
[10/157]	0.1000 (0.0956)	0.0608 (0.0569)	0.726 (0.828)	75.00 (75.28)
[20/157]	0.1013 (0.0984)	0.0616 (0.0598)	1.034 (0.834)	65.62 (74.26)
[30/157]	0.1007 (0.0992)	0.0626 (0.0608)	0.755 (0.826)	65.62 (74.19)
[40/157]	0.1010 (0.0998)	0.0621 (0.0614)	0.902 (0.828)	75.00 (73.86)
[50/157]	0.0983 (0.1003)	0.0593 (0.0616)	0.881 (0.834)	65.62 (73.71)
[60/157]	0.1002 (0.1005)	0.0612 (0.0617)	0.626 (0.833)	78.12 (73.26)
[70/157]	0.1010 (0.1005)	0.0623 (0.0617)	0.935 (0.833)	56.25 (73.20)
[80/157]	0.1013 (0.1006)	0.0617 (0.0618)	0.773 (0.823)	81.25 (73.80)
[90/157]	0.1010 (0.1007)	0.0615 (0.0619)	0.696 (0.814)	75.00 (74.04)
[100/157]	0.1026 (0.1008)	0.0623 (0.0620)	0.778 (0.815)	65.62 (73.86)
[110/157]	0.1014 (0.1008)	0.0622 (0.0620)	1.161 (0.826)	65.62 (73.51)
[120/157]	0.1012 (0.1009)	0.0612 (0.0620)	0.913 (0.837)	68.75 (73.09)
[130/157]	0.1003 (0.1010)	0.0603 (0.0620)	0.965 (0.839)	68.75 (73.02)
[140/157]	0.1040 (0.1010)	0.0625 (0.0620)	0.859 (0.840)	62.50 (72.98)
[150/157]	0.1018 (0.1010)	0.0627 (0.0620)	1.371 (0.850)	59.38 (72.58)
[156/157]	0.0838 (0.1009)	0.0571 (0.0620)	1.869 (0.855)	50.00 (72.50)
 * Train Acc 72.500
 * Val Acc 72.800, Total time 0.61
 * Val loss 0.798, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0082 (0.0082)	0.700 (0.700)	81.25 (81.25)
[10/157]	0.1007 (0.0954)	0.0623 (0.0563)	0.639 (0.862)	78.12 (72.73)
[20/157]	0.1014 (0.0983)	0.0621 (0.0595)	1.136 (0.853)	65.62 (73.36)
[30/157]	0.1029 (0.0992)	0.0637 (0.0606)	0.783 (0.841)	78.12 (72.98)
[40/157]	0.1006 (0.0997)	0.0614 (0.0610)	0.788 (0.865)	71.88 (72.18)
[50/157]	0.1013 (0.1001)	0.0619 (0.0614)	0.542 (0.848)	90.62 (72.92)
[60/157]	0.1006 (0.1001)	0.0617 (0.0615)	0.869 (0.857)	78.12 (72.59)
[70/157]	0.1032 (0.1003)	0.0632 (0.0616)	0.921 (0.841)	71.88 (73.28)
[80/157]	0.1016 (0.1003)	0.0619 (0.0616)	1.012 (0.841)	71.88 (73.15)
[90/157]	0.1010 (0.1005)	0.0615 (0.0618)	0.820 (0.828)	75.00 (73.83)
[100/157]	0.1049 (0.1006)	0.0646 (0.0619)	1.081 (0.827)	62.50 (73.86)
[110/157]	0.0985 (0.1006)	0.0593 (0.0618)	0.805 (0.827)	75.00 (73.99)
[120/157]	0.1014 (0.1007)	0.0617 (0.0619)	0.929 (0.837)	68.75 (73.53)
[130/157]	0.1011 (0.1008)	0.0607 (0.0619)	0.852 (0.839)	81.25 (73.54)
[140/157]	0.1034 (0.1008)	0.0635 (0.0620)	0.827 (0.843)	78.12 (73.32)
[150/157]	0.1016 (0.1008)	0.0616 (0.0620)	0.950 (0.845)	62.50 (73.22)
[156/157]	0.0849 (0.1007)	0.0574 (0.0620)	1.230 (0.849)	37.50 (72.98)
 * Train Acc 72.980
 * Val Acc 72.300, Total time 0.60
 * Val loss 0.797, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0104 (0.0104)	0.677 (0.677)	78.12 (78.12)
[10/157]	0.1016 (0.0955)	0.0624 (0.0572)	0.902 (0.837)	62.50 (73.30)
[20/157]	0.1005 (0.0983)	0.0614 (0.0597)	0.880 (0.810)	78.12 (73.21)
[30/157]	0.1009 (0.0994)	0.0617 (0.0606)	0.919 (0.855)	75.00 (72.08)
[40/157]	0.1012 (0.0998)	0.0617 (0.0608)	0.804 (0.853)	71.88 (72.26)
[50/157]	0.1012 (0.1000)	0.0615 (0.0611)	0.772 (0.853)	71.88 (71.94)
[60/157]	0.1027 (0.1002)	0.0629 (0.0614)	1.093 (0.860)	65.62 (72.13)
[70/157]	0.1020 (0.1004)	0.0624 (0.0615)	1.012 (0.851)	59.38 (72.32)
[80/157]	0.1015 (0.1005)	0.0609 (0.0616)	0.705 (0.841)	75.00 (72.69)
[90/157]	0.1014 (0.1006)	0.0610 (0.0617)	0.773 (0.841)	68.75 (72.77)
[100/157]	0.1003 (0.1006)	0.0615 (0.0617)	0.641 (0.840)	84.38 (73.05)
[110/157]	0.1023 (0.1006)	0.0628 (0.0618)	1.050 (0.839)	59.38 (73.03)
[120/157]	0.0973 (0.1007)	0.0575 (0.0618)	0.858 (0.840)	78.12 (73.04)
[130/157]	0.1046 (0.1007)	0.0648 (0.0619)	0.814 (0.845)	68.75 (72.83)
[140/157]	0.0991 (0.1007)	0.0609 (0.0619)	0.853 (0.846)	78.12 (72.78)
[150/157]	0.1008 (0.1008)	0.0612 (0.0620)	0.477 (0.848)	87.50 (72.70)
[156/157]	0.0838 (0.1007)	0.0567 (0.0620)	0.564 (0.848)	75.00 (72.66)
 * Train Acc 72.660
 * Val Acc 73.000, Total time 0.60
 * Val loss 0.787, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0090 (0.0090)	0.970 (0.970)	71.88 (71.88)
[10/157]	0.1028 (0.0957)	0.0637 (0.0571)	0.918 (0.899)	75.00 (72.73)
[20/157]	0.1008 (0.0983)	0.0620 (0.0595)	0.888 (0.871)	68.75 (72.92)
[30/157]	0.1023 (0.0993)	0.0622 (0.0606)	0.780 (0.846)	81.25 (73.79)
[40/157]	0.1005 (0.0996)	0.0610 (0.0608)	1.055 (0.851)	62.50 (73.48)
[50/157]	0.1017 (0.1000)	0.0620 (0.0611)	0.817 (0.854)	75.00 (73.41)
[60/157]	0.1011 (0.1002)	0.0619 (0.0613)	0.724 (0.859)	81.25 (73.46)
[70/157]	0.1014 (0.1003)	0.0616 (0.0614)	0.827 (0.853)	75.00 (73.37)
[80/157]	0.1040 (0.1005)	0.0621 (0.0616)	0.927 (0.858)	81.25 (73.38)
[90/157]	0.1012 (0.1005)	0.0617 (0.0616)	0.806 (0.861)	68.75 (72.91)
[100/157]	0.1011 (0.1006)	0.0608 (0.0616)	0.842 (0.867)	75.00 (72.83)
[110/157]	0.1020 (0.1007)	0.0621 (0.0617)	0.717 (0.862)	75.00 (72.97)
[120/157]	0.1014 (0.1007)	0.0621 (0.0618)	0.842 (0.862)	65.62 (72.91)
[130/157]	0.1008 (0.1007)	0.0617 (0.0618)	0.813 (0.859)	71.88 (72.90)
[140/157]	0.1038 (0.1008)	0.0633 (0.0618)	0.775 (0.858)	71.88 (72.96)
[150/157]	0.0984 (0.1008)	0.0590 (0.0618)	0.782 (0.857)	81.25 (72.99)
[156/157]	0.0846 (0.1007)	0.0573 (0.0617)	0.830 (0.857)	75.00 (73.10)
 * Train Acc 73.100
 * Val Acc 73.300, Total time 0.60
 * Val loss 0.781, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0090 (0.0090)	1.195 (1.195)	53.12 (53.12)
[10/157]	0.1000 (0.0959)	0.0611 (0.0567)	0.769 (0.864)	62.50 (68.47)
[20/157]	0.0998 (0.0980)	0.0610 (0.0593)	0.872 (0.862)	65.62 (70.09)
[30/157]	0.0994 (0.0992)	0.0601 (0.0605)	0.777 (0.873)	75.00 (69.76)
[40/157]	0.1012 (0.0998)	0.0619 (0.0609)	1.062 (0.884)	56.25 (69.13)
[50/157]	0.1017 (0.1000)	0.0623 (0.0612)	1.030 (0.878)	65.62 (70.10)
[60/157]	0.1027 (0.1002)	0.0624 (0.0615)	0.861 (0.880)	78.12 (69.83)
[70/157]	0.1007 (0.1002)	0.0618 (0.0615)	0.958 (0.877)	71.88 (70.11)
[80/157]	0.1023 (0.1003)	0.0633 (0.0617)	0.816 (0.873)	78.12 (70.45)
[90/157]	0.1001 (0.1004)	0.0610 (0.0617)	0.881 (0.856)	78.12 (71.36)
[100/157]	0.1017 (0.1004)	0.0624 (0.0618)	0.722 (0.858)	78.12 (71.44)
[110/157]	0.1008 (0.1005)	0.0616 (0.0619)	0.599 (0.856)	78.12 (71.51)
[120/157]	0.1016 (0.1006)	0.0614 (0.0619)	0.937 (0.863)	68.75 (71.46)
[130/157]	0.1017 (0.1006)	0.0626 (0.0620)	0.655 (0.861)	81.25 (71.49)
[140/157]	0.0998 (0.1007)	0.0608 (0.0620)	0.776 (0.857)	75.00 (71.92)
[150/157]	0.1033 (0.1007)	0.0634 (0.0620)	0.732 (0.849)	81.25 (72.31)
[156/157]	0.0836 (0.1006)	0.0558 (0.0620)	0.473 (0.846)	87.50 (72.44)
 * Train Acc 72.440
 * Val Acc 73.000, Total time 0.61
 * Val loss 0.798, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0452 (0.0452)	0.0088 (0.0088)	1.026 (1.026)	71.88 (71.88)
[10/157]	0.1015 (0.0950)	0.0623 (0.0565)	0.990 (0.880)	62.50 (71.59)
[20/157]	0.1019 (0.0982)	0.0613 (0.0595)	0.942 (0.862)	68.75 (72.02)
[30/157]	0.1008 (0.0992)	0.0610 (0.0602)	0.756 (0.860)	75.00 (71.57)
[40/157]	0.1019 (0.0997)	0.0610 (0.0607)	0.852 (0.841)	71.88 (72.71)
[50/157]	0.1007 (0.1000)	0.0619 (0.0610)	0.940 (0.859)	71.88 (72.18)
[60/157]	0.1017 (0.1003)	0.0617 (0.0613)	1.052 (0.856)	68.75 (72.85)
[70/157]	0.1005 (0.1003)	0.0613 (0.0614)	1.106 (0.849)	71.88 (73.33)
[80/157]	0.1020 (0.1003)	0.0627 (0.0615)	0.621 (0.843)	78.12 (73.26)
[90/157]	0.1003 (0.1004)	0.0614 (0.0616)	0.972 (0.849)	68.75 (73.18)
[100/157]	0.1009 (0.1004)	0.0615 (0.0616)	0.834 (0.839)	75.00 (73.58)
[110/157]	0.0996 (0.1004)	0.0604 (0.0616)	1.123 (0.838)	68.75 (73.62)
[120/157]	0.1002 (0.1005)	0.0609 (0.0616)	0.746 (0.844)	78.12 (73.14)
[130/157]	0.1031 (0.1005)	0.0634 (0.0617)	1.154 (0.850)	59.38 (72.92)
[140/157]	0.1013 (0.1006)	0.0621 (0.0617)	0.883 (0.851)	71.88 (72.83)
[150/157]	0.1009 (0.1006)	0.0616 (0.0618)	1.005 (0.851)	59.38 (72.83)
[156/157]	0.0838 (0.1005)	0.0567 (0.0618)	0.798 (0.851)	75.00 (72.90)
 * Train Acc 72.900
 * Val Acc 72.900, Total time 0.61
 * Val loss 0.794, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0092 (0.0092)	0.916 (0.916)	65.62 (65.62)
[10/157]	0.1011 (0.0957)	0.0621 (0.0565)	0.861 (0.857)	78.12 (75.28)
[20/157]	0.1017 (0.0985)	0.0613 (0.0593)	1.049 (0.854)	59.38 (73.51)
[30/157]	0.1019 (0.0992)	0.0608 (0.0602)	0.810 (0.842)	75.00 (73.89)
[40/157]	0.1002 (0.0998)	0.0601 (0.0606)	0.809 (0.845)	78.12 (74.09)
[50/157]	0.1019 (0.1000)	0.0624 (0.0609)	0.608 (0.834)	81.25 (74.39)
[60/157]	0.1007 (0.1002)	0.0612 (0.0611)	0.895 (0.822)	71.88 (75.00)
[70/157]	0.1007 (0.1004)	0.0616 (0.0612)	1.165 (0.823)	65.62 (74.78)
[80/157]	0.1002 (0.1004)	0.0612 (0.0613)	0.826 (0.829)	81.25 (74.42)
[90/157]	0.1013 (0.1004)	0.0620 (0.0614)	1.004 (0.840)	71.88 (73.90)
[100/157]	0.1009 (0.1005)	0.0619 (0.0615)	0.942 (0.847)	71.88 (73.30)
[110/157]	0.0997 (0.1005)	0.0612 (0.0616)	0.982 (0.852)	75.00 (73.06)
[120/157]	0.1020 (0.1006)	0.0616 (0.0616)	0.931 (0.855)	71.88 (72.86)
[130/157]	0.1007 (0.1007)	0.0610 (0.0617)	0.712 (0.856)	81.25 (72.88)
[140/157]	0.1015 (0.1007)	0.0619 (0.0617)	0.669 (0.858)	81.25 (72.83)
[150/157]	0.1006 (0.1007)	0.0608 (0.0617)	0.857 (0.854)	75.00 (73.12)
[156/157]	0.0858 (0.1006)	0.0584 (0.0617)	0.792 (0.855)	75.00 (73.08)
 * Train Acc 73.080
 * Val Acc 72.800, Total time 0.59
 * Val loss 0.788, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0083 (0.0083)	1.192 (1.192)	68.75 (68.75)
[10/157]	0.0952 (0.0919)	0.0575 (0.0538)	0.961 (0.990)	75.00 (71.31)
[20/157]	0.0973 (0.0936)	0.0589 (0.0560)	0.728 (0.904)	75.00 (72.02)
[30/157]	0.1031 (0.0953)	0.0628 (0.0576)	1.012 (0.884)	68.75 (73.19)
[40/157]	0.1065 (0.0977)	0.0666 (0.0595)	1.025 (0.858)	62.50 (72.87)
[50/157]	0.0956 (0.0986)	0.0577 (0.0603)	0.888 (0.859)	75.00 (72.98)
[60/157]	0.1215 (0.0997)	0.0802 (0.0613)	0.864 (0.881)	75.00 (71.72)
[70/157]	0.0967 (0.1001)	0.0579 (0.0617)	1.142 (0.880)	59.38 (71.74)
[80/157]	0.0964 (0.0997)	0.0586 (0.0613)	0.905 (0.875)	65.62 (71.91)
[90/157]	0.0963 (0.1005)	0.0581 (0.0620)	0.592 (0.859)	78.12 (72.70)
[100/157]	0.1185 (0.1009)	0.0791 (0.0625)	0.754 (0.857)	68.75 (72.74)
[110/157]	0.1075 (0.1015)	0.0680 (0.0630)	0.852 (0.862)	71.88 (72.52)
[120/157]	0.0952 (0.1013)	0.0578 (0.0628)	0.712 (0.857)	81.25 (72.68)
[130/157]	0.0941 (0.1008)	0.0562 (0.0625)	0.805 (0.852)	78.12 (73.02)
[140/157]	0.0960 (0.1005)	0.0586 (0.0622)	0.624 (0.848)	78.12 (73.23)
[150/157]	0.1071 (0.1005)	0.0655 (0.0622)	0.744 (0.849)	78.12 (73.14)
[156/157]	0.0790 (0.1004)	0.0524 (0.0621)	0.835 (0.851)	75.00 (73.08)
 * Train Acc 73.080
 * Val Acc 72.900, Total time 0.59
 * Val loss 0.794, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0086 (0.0086)	0.577 (0.577)	87.50 (87.50)
[10/157]	0.1016 (0.0960)	0.0620 (0.0576)	1.118 (0.824)	65.62 (73.01)
[20/157]	0.1016 (0.0983)	0.0629 (0.0600)	0.626 (0.856)	81.25 (72.47)
[30/157]	0.1004 (0.0993)	0.0614 (0.0609)	0.806 (0.870)	78.12 (72.18)
[40/157]	0.1029 (0.0998)	0.0633 (0.0614)	0.717 (0.860)	75.00 (72.41)
[50/157]	0.1025 (0.1001)	0.0636 (0.0617)	0.815 (0.844)	71.88 (72.79)
[60/157]	0.1022 (0.1004)	0.0635 (0.0620)	0.715 (0.841)	75.00 (73.10)
[70/157]	0.1019 (0.1005)	0.0621 (0.0621)	0.977 (0.838)	68.75 (73.24)
[80/157]	0.1019 (0.1008)	0.0631 (0.0622)	0.746 (0.841)	75.00 (72.84)
[90/157]	0.1027 (0.1009)	0.0608 (0.0623)	0.880 (0.846)	65.62 (72.39)
[100/157]	0.1027 (0.1009)	0.0630 (0.0623)	0.840 (0.850)	81.25 (72.49)
[110/157]	0.1020 (0.1009)	0.0628 (0.0624)	0.826 (0.844)	75.00 (72.55)
[120/157]	0.1003 (0.1010)	0.0616 (0.0624)	0.781 (0.844)	71.88 (72.68)
[130/157]	0.1008 (0.1010)	0.0619 (0.0625)	0.837 (0.847)	68.75 (72.45)
[140/157]	0.1011 (0.1010)	0.0618 (0.0625)	0.957 (0.848)	65.62 (72.61)
[150/157]	0.1008 (0.1010)	0.0616 (0.0625)	0.956 (0.846)	62.50 (72.45)
[156/157]	0.0849 (0.1009)	0.0578 (0.0624)	1.025 (0.845)	75.00 (72.40)
 * Train Acc 72.400
 * Val Acc 72.100, Total time 0.60
 * Val loss 0.793, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 5/27; cond: 429597.09375, radio:3.508462032186799e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0040,  0.1073, -0.1579],
          [-0.1379, -0.0745,  0.0499],
          [ 0.0051,  0.1572, -0.0148]],

         [[ 0.0549, -0.0551, -0.0376],
          [-0.1812, -0.1279, -0.0788],
          [ 0.0137,  0.0788,  0.1179]],

         [[-0.1299, -0.0832,  0.0676],
          [ 0.1595, -0.0409,  0.1447],
          [-0.0280,  0.0216,  0.1757]]],


        [[[-0.1681, -0.1049, -0.0260],
          [-0.0663,  0.1734, -0.1109],
          [-0.0793, -0.1350, -0.1714]],

         [[-0.1060,  0.1790,  0.1031],
          [ 0.0968,  0.0175, -0.0887],
          [ 0.0345, -0.1829, -0.1369]],

         [[-0.0979,  0.1279,  0.1212],
          [-0.0830, -0.0012,  0.1283],
          [ 0.1914,  0.0731,  0.0267]]],


        [[[ 0.1332, -0.1135,  0.0363],
          [-0.1476, -0.1349, -0.0988],
          [ 0.0870,  0.0754, -0.1128]],

         [[ 0.0592,  0.1010, -0.0279],
          [ 0.0059,  0.0401,  0.1156],
          [ 0.1801, -0.1533, -0.0733]],

         [[ 0.0775,  0.1566,  0.1640],
          [ 0.1684,  0.0345, -0.1718],
          [ 0.0118, -0.1273, -0.1839]]],


        ...,


        [[[ 0.0685, -0.0766, -0.1207],
          [ 0.0759,  0.0337, -0.1409],
          [-0.1321,  0.0945,  0.1444]],

         [[ 0.0182, -0.0974,  0.1002],
          [ 0.0359, -0.1679, -0.1243],
          [ 0.0344,  0.1228, -0.0628]],

         [[ 0.0805, -0.1306,  0.1823],
          [ 0.1279,  0.0265,  0.0757],
          [ 0.0450, -0.1179, -0.0129]]],


        [[[ 0.0626, -0.0405,  0.1924],
          [-0.0630, -0.1656, -0.0866],
          [-0.1705,  0.1684, -0.0841]],

         [[-0.1291, -0.0212, -0.1254],
          [-0.0925,  0.1033, -0.1426],
          [ 0.0977,  0.1512,  0.0007]],

         [[ 0.0849, -0.1124,  0.1497],
          [ 0.1869,  0.0453,  0.1723],
          [-0.1953, -0.0193,  0.0317]]],


        [[[-0.1668,  0.0944, -0.0725],
          [-0.0709, -0.1546,  0.0822],
          [-0.1406, -0.0456,  0.0851]],

         [[ 0.1450,  0.0467, -0.0711],
          [-0.0833,  0.1101,  0.0388],
          [ 0.1595,  0.0672,  0.1518]],

         [[-0.1558,  0.1529, -0.0940],
          [-0.0555, -0.0039, -0.0431],
          [ 0.0337,  0.1604, -0.1260]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.8727e+06, 3.5018e+05, 3.4646e+05, 1.4885e+05, 1.2870e+05, 1.0791e+05,
        2.0056e+04, 1.3209e+04, 4.3337e+03, 4.1295e+03, 4.0238e+03, 3.8350e+03,
        1.4491e+03, 8.7242e+02, 8.5969e+02, 7.2589e+02, 6.6680e+02, 4.1758e+02,
        1.7681e+02, 1.6024e+02, 1.3556e+02, 9.0281e+01, 8.1701e+01, 3.2530e+01,
        2.9702e+01, 2.2830e+01, 9.0147e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 5]) 

NULL SPACE BASIS :  tensor([[-2.0363e-01,  1.3129e-01,  1.1817e-01, -1.2631e-01,  8.3432e-02],
        [-4.2046e-03, -2.2754e-01,  1.4948e-02,  2.3150e-01, -1.5034e-01],
        [ 2.0849e-01,  1.1959e-01, -1.3437e-01, -1.2125e-01,  8.3190e-02],
        [ 3.8983e-01, -5.8196e-03, -2.2341e-01,  2.3399e-01, -1.4977e-01],
        [-4.9801e-04, -9.6141e-03, -9.7081e-03, -4.2520e-01,  2.6851e-01],
        [-3.8984e-01,  1.5448e-02,  2.3343e-01,  2.2431e-01, -1.4969e-01],
        [-2.1451e-01, -1.2428e-01,  1.2847e-01, -1.2771e-01,  8.3633e-02],
        [ 5.3471e-03,  2.3705e-01, -4.7098e-03,  2.3171e-01, -1.5125e-01],
        [ 2.0886e-01, -1.3620e-01, -1.2287e-01, -1.2217e-01,  8.3590e-02],
        [ 1.8104e-02, -2.5534e-01, -2.3247e-01,  1.4415e-02, -1.5280e-01],
        [-3.8640e-04,  4.4079e-01, -2.1281e-02, -1.6848e-02,  2.7441e-01],
        [-1.8473e-02, -2.3178e-01,  2.5574e-01,  5.2031e-03, -1.5093e-01],
        [-2.9895e-02,  2.0900e-02,  4.3988e-01, -1.8024e-02,  2.7326e-01],
        [ 3.4677e-04,  1.1287e-03,  1.8176e-03,  1.2421e-02, -4.8900e-01],
        [ 3.0431e-02, -2.0365e-02, -4.4148e-01,  1.1089e-03,  2.7136e-01],
        [ 1.2479e-02,  2.3213e-01, -2.5480e-01,  5.7457e-03, -1.5208e-01],
        [-8.5085e-04, -4.4196e-01,  2.0248e-02,  1.6255e-03,  2.7484e-01],
        [-1.1499e-02,  2.5445e-01,  2.3224e-01, -5.9636e-03, -1.5155e-01],
        [ 2.0980e-01,  1.4677e-01,  1.3509e-01,  1.2719e-01,  8.1961e-02],
        [ 5.3312e-03, -2.5254e-01,  7.6950e-03, -2.4361e-01, -1.4682e-01],
        [-2.1505e-01,  1.3312e-01, -1.4369e-01,  1.3216e-01,  8.0293e-02],
        [-4.0459e-01, -1.7757e-02, -2.5553e-01, -2.4387e-01, -1.4595e-01],
        [-1.9577e-04,  1.0108e-02,  8.7938e-03,  4.6571e-01,  2.6098e-01],
        [ 4.0445e-01,  5.3059e-03,  2.4617e-01, -2.5524e-01, -1.4413e-01],
        [ 2.2720e-01, -1.2766e-01,  1.4873e-01,  1.3759e-01,  8.0800e-02],
        [-4.7008e-03,  2.4255e-01, -1.7870e-02, -2.6351e-01, -1.4610e-01],
        [-2.2239e-01, -1.3964e-01, -1.2927e-01,  1.4505e-01,  8.0373e-02]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 4.2745e-02, -3.0874e-02, -9.1119e-03, -6.6453e-02,  3.3005e-02,
          3.0488e-02,  2.9361e-02, -5.5493e-03, -2.3490e-02, -3.5441e-02,
          3.5983e-02, -4.3381e-03,  3.8409e-02, -1.8816e-02, -1.7235e-02,
         -6.9704e-03, -1.4640e-02,  2.2943e-02, -7.4749e-03, -6.6247e-03,
          1.5337e-02,  3.0629e-02, -1.5493e-02, -1.4472e-02, -2.5083e-02,
          2.3159e-02,  2.7125e-05],
        [-3.0874e-02,  5.7337e-02, -3.1606e-02,  3.2660e-02, -6.1160e-02,
          3.4009e-02, -4.9365e-03,  9.9951e-03, -5.6231e-03,  3.6160e-02,
         -6.5190e-02,  3.6017e-02, -1.9369e-02,  3.4061e-02, -1.9066e-02,
         -1.4528e-02,  2.6800e-02, -1.4746e-02, -6.7703e-03,  1.0391e-02,
         -5.8184e-03, -1.4576e-02,  2.9698e-02, -1.6389e-02,  2.2369e-02,
         -4.2250e-02,  2.3377e-02],
        [-9.1119e-03, -3.1606e-02,  4.3580e-02,  3.1201e-02,  3.3069e-02,
         -6.7282e-02, -2.4331e-02, -4.7324e-03,  2.9308e-02, -4.4650e-03,
          3.5940e-02, -3.5384e-02, -1.6959e-02, -1.8883e-02,  3.8313e-02,
          2.2920e-02, -1.4797e-02, -6.7338e-03,  1.5446e-02, -5.7241e-03,
         -8.4763e-03, -1.5524e-02, -1.5550e-02,  3.1681e-02,  9.6429e-04,
          2.2461e-02, -2.5312e-02],
        [-6.6453e-02,  3.2660e-02,  3.1201e-02,  1.2481e-01, -6.1570e-02,
         -5.7828e-02, -6.8875e-02,  3.5163e-02,  3.0660e-02,  3.8789e-02,
         -1.9231e-02, -1.7515e-02, -6.9404e-02,  3.3928e-02,  3.1408e-02,
          3.7816e-02, -1.9259e-02, -1.6344e-02,  3.0516e-02, -1.4840e-02,
         -1.5029e-02, -6.0703e-02,  3.0314e-02,  2.8848e-02,  3.4066e-02,
         -1.7454e-02, -1.5697e-02],
        [ 3.3005e-02, -6.1160e-02,  3.3069e-02, -6.1570e-02,  1.1318e-01,
         -6.1622e-02,  3.4352e-02, -6.3224e-02,  3.4341e-02, -1.8986e-02,
          3.4353e-02, -1.9223e-02,  3.4248e-02, -6.1095e-02,  3.4372e-02,
         -1.9249e-02,  3.4506e-02, -1.9164e-02, -1.5607e-02,  2.9744e-02,
         -1.5390e-02,  3.0123e-02, -5.7300e-02,  3.0046e-02, -1.6607e-02,
          3.1599e-02, -1.6719e-02],
        [ 3.0488e-02,  3.4009e-02, -6.7282e-02, -5.7828e-02, -6.1622e-02,
          1.2496e-01,  3.1541e-02,  3.3583e-02, -6.8032e-02, -1.7514e-02,
         -1.9169e-02,  3.8943e-02,  3.1177e-02,  3.4118e-02, -6.9588e-02,
         -1.6415e-02, -1.9026e-02,  3.7554e-02, -1.4188e-02, -1.6480e-02,
          3.1293e-02,  2.9045e-02,  3.0269e-02, -6.0734e-02, -1.6572e-02,
         -1.6024e-02,  3.3483e-02],
        [ 2.9361e-02, -4.9365e-03, -2.4331e-02, -6.8875e-02,  3.4352e-02,
          3.1541e-02,  4.5289e-02, -3.2850e-02, -9.4220e-03, -7.4399e-03,
         -1.4458e-02,  2.3406e-02,  3.8229e-02, -1.8990e-02, -1.7067e-02,
         -3.4754e-02,  3.5995e-02, -5.0230e-03, -2.4721e-02,  2.2389e-02,
          4.3039e-04,  3.3589e-02, -1.6876e-02, -1.5764e-02, -1.0991e-02,
         -4.4705e-03,  1.6390e-02],
        [-5.5493e-03,  9.9951e-03, -4.7324e-03,  3.5163e-02, -6.3224e-02,
          3.3583e-02, -3.2850e-02,  5.9394e-02, -3.1994e-02, -1.4707e-02,
          2.6465e-02, -1.4406e-02, -1.9134e-02,  3.4480e-02, -1.9396e-02,
          3.6057e-02, -6.5319e-02,  3.6090e-02,  2.3413e-02, -4.2088e-02,
          2.2164e-02, -1.7711e-02,  3.1659e-02, -1.5689e-02, -4.5108e-03,
          8.3156e-03, -5.4689e-03],
        [-2.3490e-02, -5.6231e-03,  2.9308e-02,  3.0660e-02,  3.4341e-02,
         -6.8032e-02, -9.4220e-03, -3.1994e-02,  4.4356e-02,  2.3517e-02,
         -1.4536e-02, -7.5862e-03, -1.7036e-02, -1.9095e-02,  3.8424e-02,
         -4.9716e-03,  3.5913e-02, -3.4672e-02, -6.5101e-04,  2.3278e-02,
         -2.4519e-02, -1.4801e-02, -1.6805e-02,  3.2487e-02,  1.6328e-02,
         -5.2954e-03, -1.0085e-02],
        [-3.5441e-02,  3.6160e-02, -4.4650e-03,  3.8789e-02, -1.8986e-02,
         -1.7514e-02, -7.4399e-03, -1.4707e-02,  2.3517e-02,  6.4005e-02,
         -6.6984e-02,  1.0077e-02, -6.7149e-02,  3.3180e-02,  2.9934e-02,
          1.0513e-02,  2.9586e-02, -4.2975e-02, -3.3886e-02,  3.6543e-02,
         -6.6385e-03,  3.3718e-02, -1.6901e-02, -1.4720e-02, -3.6802e-03,
         -1.7592e-02,  2.3027e-02],
        [ 3.5983e-02, -6.5190e-02,  3.5940e-02, -1.9231e-02,  3.4353e-02,
         -1.9169e-02, -1.4458e-02,  2.6465e-02, -1.4536e-02, -6.6984e-02,
          1.2090e-01, -6.6682e-02,  3.3609e-02, -5.9899e-02,  3.3474e-02,
          2.9475e-02, -5.3599e-02,  2.9397e-02,  3.6711e-02, -6.6038e-02,
          3.6504e-02, -1.7072e-02,  3.0427e-02, -1.7131e-02, -1.7740e-02,
          3.2040e-02, -1.7488e-02],
        [-4.3381e-03,  3.6017e-02, -3.5384e-02, -1.7515e-02, -1.9223e-02,
          3.8943e-02,  2.3406e-02, -1.4406e-02, -7.5862e-03,  1.0077e-02,
         -6.6682e-02,  6.3626e-02,  2.9903e-02,  3.3123e-02, -6.6945e-02,
         -4.3027e-02,  2.9588e-02,  1.0496e-02, -6.7335e-03,  3.6356e-02,
         -3.3568e-02, -1.4758e-02, -1.6572e-02,  3.3397e-02,  2.3232e-02,
         -1.7899e-02, -3.5597e-03],
        [ 3.8409e-02, -1.9369e-02, -1.6959e-02, -6.9404e-02,  3.4248e-02,
          3.1177e-02,  3.8229e-02, -1.9134e-02, -1.7036e-02, -6.7149e-02,
          3.3609e-02,  2.9903e-02,  1.2067e-01, -5.9496e-02, -5.4293e-02,
         -6.6753e-02,  3.3438e-02,  2.9746e-02,  3.4133e-02, -1.6896e-02,
         -1.5400e-02, -6.0894e-02,  2.9966e-02,  2.7512e-02,  3.3793e-02,
         -1.6916e-02, -1.5109e-02],
        [-1.8816e-02,  3.4061e-02, -1.8883e-02,  3.3928e-02, -6.1095e-02,
          3.4118e-02, -1.8990e-02,  3.4480e-02, -1.9095e-02,  3.3180e-02,
         -5.9899e-02,  3.3123e-02, -5.9496e-02,  1.0701e-01, -5.9701e-02,
          3.3202e-02, -6.0302e-02,  3.3424e-02, -1.7001e-02,  3.0633e-02,
         -1.6908e-02,  3.0283e-02, -5.4473e-02,  3.0367e-02, -1.6814e-02,
          3.0594e-02, -1.6981e-02],
        [-1.7235e-02, -1.9066e-02,  3.8313e-02,  3.1408e-02,  3.4372e-02,
         -6.9588e-02, -1.7067e-02, -1.9396e-02,  3.8424e-02,  2.9934e-02,
          3.3474e-02, -6.6945e-02, -5.4293e-02, -5.9701e-02,  1.2069e-01,
          2.9909e-02,  3.3369e-02, -6.6720e-02, -1.5144e-02, -1.7084e-02,
          3.4040e-02,  2.7273e-02,  3.0070e-02, -6.0763e-02, -1.5236e-02,
         -1.6605e-02,  3.3593e-02],
        [-6.9704e-03, -1.4528e-02,  2.2920e-02,  3.7816e-02, -1.9249e-02,
         -1.6415e-02, -3.4754e-02,  3.6057e-02, -4.9716e-03,  1.0513e-02,
          2.9475e-02, -4.3027e-02, -6.6753e-02,  3.3202e-02,  2.9909e-02,
          6.3559e-02, -6.6880e-02,  1.0178e-02, -4.2333e-03, -1.7704e-02,
          2.3871e-02,  3.4315e-02, -1.6507e-02, -1.6096e-02, -3.4073e-02,
          3.6449e-02, -6.1009e-03],
        [-1.4640e-02,  2.6800e-02, -1.4797e-02, -1.9259e-02,  3.4506e-02,
         -1.9026e-02,  3.5995e-02, -6.5319e-02,  3.5913e-02,  2.9586e-02,
         -5.3599e-02,  2.9588e-02,  3.3438e-02, -6.0302e-02,  3.3369e-02,
         -6.6880e-02,  1.2132e-01, -6.6815e-02, -1.7700e-02,  3.1760e-02,
         -1.7565e-02, -1.6766e-02,  3.0497e-02, -1.6874e-02,  3.6523e-02,
         -6.6249e-02,  3.6499e-02],
        [ 2.2943e-02, -1.4746e-02, -6.7338e-03, -1.6344e-02, -1.9164e-02,
          3.7554e-02, -5.0230e-03,  3.6090e-02, -3.4672e-02, -4.2975e-02,
          2.9397e-02,  1.0496e-02,  2.9746e-02,  3.3424e-02, -6.6720e-02,
          1.0178e-02, -6.6815e-02,  6.3420e-02,  2.3759e-02, -1.7365e-02,
         -4.4638e-03, -1.5937e-02, -1.6865e-02,  3.4539e-02, -6.0906e-03,
          3.6373e-02, -3.4006e-02],
        [-7.4749e-03, -6.7703e-03,  1.5446e-02,  3.0516e-02, -1.5607e-02,
         -1.4188e-02, -2.4721e-02,  2.3413e-02, -6.5101e-04, -3.3886e-02,
          3.6711e-02, -6.7335e-03,  3.4133e-02, -1.7001e-02, -1.5144e-02,
         -4.2333e-03, -1.7700e-02,  2.3759e-02,  4.7718e-02, -3.4849e-02,
         -9.6594e-03, -7.3784e-02,  3.7231e-02,  3.3367e-02,  3.2710e-02,
         -5.9430e-03, -2.6645e-02],
        [-6.6247e-03,  1.0391e-02, -5.7241e-03, -1.4840e-02,  2.9744e-02,
         -1.6480e-02,  2.2389e-02, -4.2088e-02,  2.3278e-02,  3.6543e-02,
         -6.6038e-02,  3.6356e-02, -1.6896e-02,  3.0633e-02, -1.7084e-02,
         -1.7704e-02,  3.1760e-02, -1.7365e-02, -3.4849e-02,  6.4740e-02,
         -3.5711e-02,  3.6312e-02, -6.8983e-02,  3.8482e-02, -4.8235e-03,
          1.0834e-02, -6.2832e-03],
        [ 1.5337e-02, -5.8184e-03, -8.4763e-03, -1.5029e-02, -1.5390e-02,
          3.1293e-02,  4.3039e-04,  2.2164e-02, -2.4519e-02, -6.6385e-03,
          3.6504e-02, -3.3568e-02, -1.5400e-02, -1.6908e-02,  3.4040e-02,
          2.3871e-02, -1.7565e-02, -4.4638e-03, -9.6594e-03, -3.5711e-02,
          4.8534e-02,  3.4619e-02,  3.6951e-02, -7.4661e-02, -2.7974e-02,
         -4.7804e-03,  3.2840e-02],
        [ 3.0629e-02, -1.4576e-02, -1.5524e-02, -6.0703e-02,  3.0123e-02,
          2.9045e-02,  3.3589e-02, -1.7711e-02, -1.4801e-02,  3.3718e-02,
         -1.7072e-02, -1.4758e-02, -6.0894e-02,  3.0283e-02,  2.7273e-02,
          3.4315e-02, -1.6766e-02, -1.5937e-02, -7.3784e-02,  3.6312e-02,
          3.4619e-02,  1.3867e-01, -6.8875e-02, -6.4109e-02, -7.7370e-02,
          3.9241e-02,  3.5055e-02],
        [-1.5493e-02,  2.9698e-02, -1.5550e-02,  3.0314e-02, -5.7300e-02,
          3.0269e-02, -1.6876e-02,  3.1659e-02, -1.6805e-02, -1.6901e-02,
          3.0427e-02, -1.6572e-02,  2.9966e-02, -5.4473e-02,  3.0070e-02,
         -1.6507e-02,  3.0497e-02, -1.6865e-02,  3.7231e-02, -6.8983e-02,
          3.6951e-02, -6.8875e-02,  1.2753e-01, -6.9024e-02,  3.8073e-02,
         -7.0906e-02,  3.8470e-02],
        [-1.4472e-02, -1.6389e-02,  3.1681e-02,  2.8848e-02,  3.0046e-02,
         -6.0734e-02, -1.5764e-02, -1.5689e-02,  3.2487e-02, -1.4720e-02,
         -1.7131e-02,  3.3397e-02,  2.7512e-02,  3.0367e-02, -6.0763e-02,
         -1.6096e-02, -1.6874e-02,  3.4539e-02,  3.3367e-02,  3.8482e-02,
         -7.4661e-02, -6.4109e-02, -6.9024e-02,  1.3869e-01,  3.6252e-02,
          3.7254e-02, -7.6525e-02],
        [-2.5083e-02,  2.2369e-02,  9.6429e-04,  3.4066e-02, -1.6607e-02,
         -1.6572e-02, -1.0991e-02, -4.5108e-03,  1.6328e-02, -3.6802e-03,
         -1.7740e-02,  2.3232e-02,  3.3793e-02, -1.6814e-02, -1.5236e-02,
         -3.4073e-02,  3.6523e-02, -6.0906e-03,  3.2710e-02, -4.8235e-03,
         -2.7974e-02, -7.7370e-02,  3.8073e-02,  3.6252e-02,  5.1650e-02,
         -3.7006e-02, -1.1393e-02],
        [ 2.3159e-02, -4.2250e-02,  2.2461e-02, -1.7454e-02,  3.1599e-02,
         -1.6024e-02, -4.4705e-03,  8.3156e-03, -5.2954e-03, -1.7592e-02,
          3.2040e-02, -1.7899e-02, -1.6916e-02,  3.0594e-02, -1.6605e-02,
          3.6449e-02, -6.6249e-02,  3.6373e-02, -5.9430e-03,  1.0834e-02,
         -4.7804e-03,  3.9241e-02, -7.0906e-02,  3.7254e-02, -3.7006e-02,
          6.7062e-02, -3.5991e-02],
        [ 2.7125e-05,  2.3377e-02, -2.5312e-02, -1.5697e-02, -1.6719e-02,
          3.3483e-02,  1.6390e-02, -5.4689e-03, -1.0085e-02,  2.3027e-02,
         -1.7488e-02, -3.5597e-03, -1.5109e-02, -1.6981e-02,  3.3593e-02,
         -6.1009e-03,  3.6499e-02, -3.4006e-02, -2.6645e-02, -6.2832e-03,
          3.2840e-02,  3.5055e-02,  3.8470e-02, -7.6525e-02, -1.1393e-02,
         -3.5991e-02,  5.0610e-02]], device='cuda:0') 

reserving basis 78/576; cond: 9660006.0, radio:3.704571645357646e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1467e-02, -3.9773e-02, -7.3123e-03],
          [ 3.8361e-02,  2.6560e-02,  3.1695e-02],
          [ 2.2130e-02, -1.7545e-03,  8.7356e-03]],

         [[-1.4076e-02,  3.7962e-02,  1.8363e-02],
          [-2.9873e-02,  3.6674e-02, -3.0700e-03],
          [-7.0653e-03, -1.6960e-02, -2.0482e-02]],

         [[-3.1211e-03, -2.0731e-02, -3.6682e-02],
          [-2.9196e-02, -1.0434e-02,  3.3312e-02],
          [ 2.0573e-02, -3.1935e-02, -9.4760e-03]],

         ...,

         [[-2.5501e-02,  1.1643e-02,  2.2738e-05],
          [ 1.4930e-02, -6.5047e-03, -9.4639e-03],
          [-2.0391e-03,  8.6772e-03, -3.1093e-02]],

         [[-4.1509e-02,  1.3738e-02, -3.4959e-02],
          [-6.7216e-03,  1.2054e-02,  1.1405e-03],
          [ 2.0796e-02,  1.4362e-04,  4.0804e-03]],

         [[-1.5929e-02,  1.9255e-03,  1.1366e-02],
          [ 2.3099e-02, -3.9663e-02, -1.6094e-02],
          [-4.1583e-02, -1.1374e-02,  2.9621e-02]]],


        [[[-4.2332e-02, -4.7078e-02,  1.4024e-02],
          [-2.3510e-02,  1.9817e-02,  2.8308e-03],
          [ 8.0651e-03,  2.3393e-02, -4.2315e-02]],

         [[ 3.2950e-02,  7.8756e-03, -3.0225e-02],
          [ 1.1936e-02, -1.1382e-02, -1.7392e-02],
          [ 3.3194e-03, -1.1615e-02,  4.0690e-02]],

         [[ 2.6490e-02, -1.6379e-02, -4.4825e-02],
          [-1.7243e-02, -1.1006e-02,  1.0021e-02],
          [-1.2336e-02, -2.3793e-03, -6.2982e-03]],

         ...,

         [[-5.5008e-03, -3.8312e-03,  8.7091e-03],
          [ 1.5780e-02, -3.4073e-02,  1.7226e-02],
          [ 3.3104e-02,  7.5031e-03,  3.0862e-02]],

         [[-2.1727e-02,  6.7538e-03, -4.5416e-04],
          [-1.1090e-02,  2.3849e-02, -4.6516e-02],
          [-1.0890e-03, -1.4444e-02, -2.0026e-02]],

         [[-1.4821e-02, -3.2050e-02,  4.3015e-03],
          [-6.7731e-03, -3.1436e-02, -8.2065e-03],
          [-8.7677e-03,  1.5892e-02, -4.1594e-02]]],


        [[[-3.0190e-03,  1.8550e-02,  2.0502e-02],
          [ 7.9307e-03,  1.2931e-02,  4.1884e-02],
          [-1.3437e-02,  2.5605e-02,  3.2367e-02]],

         [[-3.1212e-02,  2.0065e-02,  2.8974e-02],
          [-2.5536e-03,  3.2287e-02,  6.3297e-03],
          [ 3.5462e-02,  6.7127e-03, -2.1026e-02]],

         [[ 3.8714e-02,  1.5423e-02,  3.4270e-02],
          [ 1.8334e-02,  2.5608e-02,  2.0322e-02],
          [ 1.6777e-02, -5.9748e-03,  9.4119e-03]],

         ...,

         [[ 2.1059e-02, -1.6868e-02,  4.4106e-02],
          [-3.1675e-03,  3.8395e-02, -3.4314e-03],
          [ 2.4399e-02,  3.3257e-02, -3.6231e-02]],

         [[-2.0637e-02, -3.6415e-02,  3.6818e-02],
          [ 4.5180e-02, -1.3427e-02, -1.8916e-02],
          [ 1.2015e-02,  3.0953e-02,  1.3890e-02]],

         [[-1.4880e-02, -2.3240e-02,  3.0084e-02],
          [-2.9752e-02, -4.1762e-02, -2.1424e-02],
          [ 3.6625e-03,  5.9485e-03,  6.1645e-03]]],


        ...,


        [[[-4.4278e-02, -1.8178e-02,  9.4099e-03],
          [ 2.4280e-02, -1.7199e-02, -8.6249e-03],
          [-4.2264e-03,  2.0978e-02, -3.4640e-02]],

         [[-4.1719e-02,  2.7645e-02,  5.8548e-03],
          [-1.3589e-02, -1.1809e-02,  8.7179e-03],
          [-1.9292e-02,  2.8349e-02,  1.2285e-02]],

         [[ 2.3907e-02,  3.2021e-02,  1.0356e-03],
          [ 4.7904e-02, -7.7051e-03, -3.4086e-02],
          [ 3.8551e-02, -1.4977e-02, -2.8924e-03]],

         ...,

         [[-2.5748e-02, -5.3325e-02,  2.3095e-02],
          [-2.0493e-02,  2.2727e-02, -4.7234e-02],
          [ 1.6400e-02,  4.0662e-02,  2.7408e-02]],

         [[-4.6429e-02, -5.5607e-02,  1.6259e-02],
          [-1.3052e-02, -1.4856e-02, -6.4562e-03],
          [-2.0933e-02,  9.7958e-03, -9.3252e-03]],

         [[ 3.6295e-02, -2.2432e-02,  4.1223e-02],
          [ 9.5551e-03, -5.7764e-03,  4.7862e-02],
          [ 3.1212e-02,  2.7995e-02,  4.8050e-02]]],


        [[[ 2.5333e-02,  1.4085e-02, -1.3523e-02],
          [-1.3432e-02, -3.2009e-02, -2.0715e-02],
          [ 1.5840e-02, -5.8681e-02, -4.1922e-02]],

         [[-8.2332e-03,  2.0881e-02,  8.9300e-03],
          [ 9.8057e-03,  9.7153e-04, -4.0424e-02],
          [-9.3858e-03,  3.7847e-02,  1.2207e-02]],

         [[-9.3584e-03,  8.4285e-03,  4.0407e-02],
          [ 2.8006e-04,  5.5748e-03,  3.0476e-02],
          [-3.1410e-02,  2.5761e-02,  3.2468e-02]],

         ...,

         [[ 4.0211e-02,  1.9570e-02,  3.8791e-02],
          [-3.5801e-02,  1.0003e-02,  8.0264e-04],
          [-1.1620e-03, -4.1773e-02, -3.9175e-02]],

         [[ 1.6730e-02,  2.3223e-02, -9.0822e-03],
          [ 2.6110e-02,  8.0840e-03,  1.8128e-02],
          [-4.8389e-02, -2.4921e-02, -5.2696e-03]],

         [[ 2.7885e-02,  2.5222e-02, -3.1969e-02],
          [-3.3096e-02, -2.8956e-02, -1.1941e-02],
          [-8.0717e-03, -3.9799e-02,  2.2228e-02]]],


        [[[ 2.7712e-02,  1.0083e-02, -1.6835e-02],
          [-6.1478e-03,  6.5113e-03, -1.2512e-02],
          [-1.7758e-03, -3.5083e-02, -7.7743e-03]],

         [[-4.3892e-02, -2.8665e-02, -2.9430e-02],
          [ 1.0905e-02, -5.6340e-02, -1.3391e-03],
          [ 2.6640e-02, -3.5973e-02, -6.8032e-03]],

         [[-1.7076e-02,  1.3561e-02,  2.2341e-02],
          [ 1.5409e-02, -1.3371e-02,  1.0140e-02],
          [-2.2101e-02,  3.5934e-02,  3.8801e-03]],

         ...,

         [[-2.1317e-02, -9.0622e-03,  9.9939e-03],
          [ 3.9666e-03,  3.7248e-02, -4.2789e-02],
          [-2.2905e-02,  1.5718e-02,  2.4710e-02]],

         [[-3.1368e-02, -3.8703e-02,  5.5314e-03],
          [-1.9653e-02, -1.0643e-02, -4.8752e-02],
          [-2.8326e-02, -5.1249e-02, -3.9133e-02]],

         [[-4.0216e-03, -1.0394e-02,  3.1111e-02],
          [ 3.3880e-02,  4.6072e-03,  3.5764e-02],
          [-2.8786e-02,  4.5802e-02, -3.2612e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([7.9001e+07, 3.5588e+06, 2.8280e+06, 2.6595e+06, 2.0169e+06, 1.4469e+06,
        1.3686e+06, 1.2733e+06, 1.0852e+06, 8.9905e+05, 7.5059e+05, 6.6105e+05,
        5.5646e+05, 3.9341e+05, 2.6378e+05, 2.1273e+05, 2.0558e+05, 1.8146e+05,
        1.5058e+05, 1.4183e+05, 1.2543e+05, 1.1240e+05, 9.3221e+04, 8.2731e+04,
        7.7081e+04, 7.4245e+04, 6.9964e+04, 6.7008e+04, 6.3745e+04, 6.0273e+04,
        5.9660e+04, 5.0423e+04, 4.7709e+04, 4.4569e+04, 4.2246e+04, 4.0139e+04,
        3.6754e+04, 3.5525e+04, 3.2665e+04, 2.9250e+04, 2.8668e+04, 2.6814e+04,
        2.6432e+04, 2.5318e+04, 2.4028e+04, 2.3206e+04, 2.0669e+04, 2.0422e+04,
        1.9067e+04, 1.8783e+04, 1.7903e+04, 1.7482e+04, 1.6569e+04, 1.6479e+04,
        1.5757e+04, 1.4530e+04, 1.4007e+04, 1.3621e+04, 1.3145e+04, 1.3054e+04,
        1.2485e+04, 1.2026e+04, 1.1889e+04, 1.1492e+04, 1.1318e+04, 1.1156e+04,
        1.0764e+04, 1.0614e+04, 1.0484e+04, 9.9466e+03, 9.6314e+03, 9.4467e+03,
        9.1306e+03, 9.0434e+03, 8.8720e+03, 8.4540e+03, 8.2507e+03, 7.9513e+03,
        7.9047e+03, 7.5976e+03, 7.3830e+03, 7.1890e+03, 7.0793e+03, 7.0504e+03,
        6.7897e+03, 6.6440e+03, 6.3831e+03, 6.2330e+03, 6.0783e+03, 6.0282e+03,
        5.9425e+03, 5.7251e+03, 5.5527e+03, 5.4054e+03, 5.2626e+03, 5.0615e+03,
        4.9630e+03, 4.8015e+03, 4.7193e+03, 4.6098e+03, 4.4969e+03, 4.3384e+03,
        4.3077e+03, 4.1815e+03, 4.1033e+03, 3.9948e+03, 3.9163e+03, 3.8929e+03,
        3.8145e+03, 3.7744e+03, 3.6365e+03, 3.6006e+03, 3.4871e+03, 3.4451e+03,
        3.4294e+03, 3.3456e+03, 3.3013e+03, 3.2213e+03, 3.1759e+03, 3.1284e+03,
        3.0559e+03, 3.0288e+03, 2.9759e+03, 2.8725e+03, 2.8387e+03, 2.7891e+03,
        2.7460e+03, 2.6893e+03, 2.6545e+03, 2.6020e+03, 2.5950e+03, 2.5341e+03,
        2.4938e+03, 2.4888e+03, 2.4612e+03, 2.3905e+03, 2.3522e+03, 2.3212e+03,
        2.2811e+03, 2.2359e+03, 2.2012e+03, 2.1728e+03, 2.1610e+03, 2.1229e+03,
        2.1067e+03, 2.0770e+03, 2.0603e+03, 2.0008e+03, 1.9762e+03, 1.9376e+03,
        1.9209e+03, 1.9046e+03, 1.8852e+03, 1.8588e+03, 1.8332e+03, 1.8059e+03,
        1.7782e+03, 1.7382e+03, 1.7160e+03, 1.6920e+03, 1.6865e+03, 1.6489e+03,
        1.6342e+03, 1.6214e+03, 1.5998e+03, 1.5583e+03, 1.5222e+03, 1.5134e+03,
        1.5002e+03, 1.4874e+03, 1.4785e+03, 1.4553e+03, 1.4312e+03, 1.3929e+03,
        1.3763e+03, 1.3644e+03, 1.3350e+03, 1.3290e+03, 1.3230e+03, 1.3069e+03,
        1.2954e+03, 1.2822e+03, 1.2394e+03, 1.2320e+03, 1.2000e+03, 1.1905e+03,
        1.1775e+03, 1.1660e+03, 1.1590e+03, 1.1524e+03, 1.1270e+03, 1.1144e+03,
        1.1015e+03, 1.0906e+03, 1.0848e+03, 1.0725e+03, 1.0619e+03, 1.0484e+03,
        1.0360e+03, 1.0193e+03, 1.0159e+03, 1.0009e+03, 9.9613e+02, 9.9351e+02,
        9.6831e+02, 9.6465e+02, 9.4749e+02, 9.3807e+02, 9.3691e+02, 9.1737e+02,
        9.1214e+02, 9.0292e+02, 8.9980e+02, 8.8608e+02, 8.8126e+02, 8.6344e+02,
        8.4583e+02, 8.4401e+02, 8.3143e+02, 8.2518e+02, 8.1281e+02, 8.0313e+02,
        7.8919e+02, 7.8492e+02, 7.8443e+02, 7.7677e+02, 7.7016e+02, 7.6172e+02,
        7.4679e+02, 7.3782e+02, 7.3392e+02, 7.2391e+02, 7.2185e+02, 7.0833e+02,
        7.0298e+02, 6.9549e+02, 6.8706e+02, 6.8529e+02, 6.7543e+02, 6.7213e+02,
        6.5901e+02, 6.5609e+02, 6.5027e+02, 6.4048e+02, 6.3602e+02, 6.3336e+02,
        6.2227e+02, 6.1853e+02, 6.1324e+02, 6.0745e+02, 6.0524e+02, 5.8939e+02,
        5.8780e+02, 5.8432e+02, 5.8124e+02, 5.7840e+02, 5.6859e+02, 5.6409e+02,
        5.6052e+02, 5.5149e+02, 5.4815e+02, 5.4601e+02, 5.3789e+02, 5.3379e+02,
        5.2977e+02, 5.2209e+02, 5.1879e+02, 5.1443e+02, 5.0780e+02, 5.0275e+02,
        4.9778e+02, 4.9492e+02, 4.9286e+02, 4.8800e+02, 4.8517e+02, 4.7966e+02,
        4.7665e+02, 4.7102e+02, 4.6885e+02, 4.6289e+02, 4.5720e+02, 4.5496e+02,
        4.4905e+02, 4.4492e+02, 4.4120e+02, 4.3773e+02, 4.3474e+02, 4.3326e+02,
        4.2980e+02, 4.2226e+02, 4.1717e+02, 4.1654e+02, 4.1305e+02, 4.1204e+02,
        4.0766e+02, 4.0355e+02, 4.0283e+02, 3.9670e+02, 3.9154e+02, 3.9051e+02,
        3.8469e+02, 3.7929e+02, 3.7812e+02, 3.7312e+02, 3.7190e+02, 3.6991e+02,
        3.6587e+02, 3.6374e+02, 3.6162e+02, 3.5890e+02, 3.5389e+02, 3.5297e+02,
        3.5003e+02, 3.4765e+02, 3.4443e+02, 3.4166e+02, 3.3977e+02, 3.3691e+02,
        3.3404e+02, 3.3000e+02, 3.2867e+02, 3.2413e+02, 3.1883e+02, 3.1775e+02,
        3.1431e+02, 3.1312e+02, 3.1059e+02, 3.0980e+02, 3.0807e+02, 3.0313e+02,
        3.0120e+02, 3.0003e+02, 2.9718e+02, 2.9421e+02, 2.9280e+02, 2.9023e+02,
        2.8761e+02, 2.8600e+02, 2.8184e+02, 2.8018e+02, 2.7722e+02, 2.7570e+02,
        2.7312e+02, 2.7133e+02, 2.6841e+02, 2.6683e+02, 2.6645e+02, 2.6479e+02,
        2.6107e+02, 2.5830e+02, 2.5689e+02, 2.5584e+02, 2.5423e+02, 2.5295e+02,
        2.5188e+02, 2.5040e+02, 2.4751e+02, 2.4458e+02, 2.4364e+02, 2.4259e+02,
        2.4163e+02, 2.3975e+02, 2.3683e+02, 2.3454e+02, 2.3242e+02, 2.2910e+02,
        2.2897e+02, 2.2721e+02, 2.2645e+02, 2.2550e+02, 2.2264e+02, 2.2213e+02,
        2.2012e+02, 2.1914e+02, 2.1473e+02, 2.1330e+02, 2.1262e+02, 2.1052e+02,
        2.1029e+02, 2.0725e+02, 2.0660e+02, 2.0616e+02, 2.0231e+02, 2.0184e+02,
        2.0124e+02, 1.9950e+02, 1.9903e+02, 1.9803e+02, 1.9779e+02, 1.9525e+02,
        1.9301e+02, 1.9164e+02, 1.9010e+02, 1.8919e+02, 1.8795e+02, 1.8564e+02,
        1.8444e+02, 1.8412e+02, 1.8295e+02, 1.8051e+02, 1.7957e+02, 1.7778e+02,
        1.7673e+02, 1.7501e+02, 1.7411e+02, 1.7315e+02, 1.7118e+02, 1.6979e+02,
        1.6907e+02, 1.6873e+02, 1.6743e+02, 1.6571e+02, 1.6561e+02, 1.6387e+02,
        1.6329e+02, 1.6160e+02, 1.6110e+02, 1.5961e+02, 1.5796e+02, 1.5673e+02,
        1.5660e+02, 1.5534e+02, 1.5452e+02, 1.5297e+02, 1.5227e+02, 1.5108e+02,
        1.4961e+02, 1.4703e+02, 1.4653e+02, 1.4528e+02, 1.4487e+02, 1.4313e+02,
        1.4236e+02, 1.4109e+02, 1.3996e+02, 1.3894e+02, 1.3740e+02, 1.3616e+02,
        1.3552e+02, 1.3408e+02, 1.3379e+02, 1.3327e+02, 1.3268e+02, 1.3165e+02,
        1.3043e+02, 1.2900e+02, 1.2853e+02, 1.2699e+02, 1.2584e+02, 1.2462e+02,
        1.2302e+02, 1.2280e+02, 1.2176e+02, 1.2078e+02, 1.1965e+02, 1.1865e+02,
        1.1744e+02, 1.1594e+02, 1.1522e+02, 1.1475e+02, 1.1398e+02, 1.1318e+02,
        1.1217e+02, 1.1089e+02, 1.1014e+02, 1.0980e+02, 1.0914e+02, 1.0705e+02,
        1.0630e+02, 1.0544e+02, 1.0514e+02, 1.0350e+02, 1.0275e+02, 1.0236e+02,
        1.0124e+02, 9.9916e+01, 9.9670e+01, 9.7804e+01, 9.7431e+01, 9.5885e+01,
        9.5535e+01, 9.3990e+01, 9.3480e+01, 9.2931e+01, 9.1688e+01, 9.1247e+01,
        9.0409e+01, 9.0380e+01, 8.9079e+01, 8.7504e+01, 8.7124e+01, 8.6708e+01,
        8.5313e+01, 8.4409e+01, 8.3708e+01, 8.3067e+01, 8.2641e+01, 8.2232e+01,
        8.0959e+01, 8.0268e+01, 7.9841e+01, 7.8254e+01, 7.7469e+01, 7.7096e+01,
        7.6689e+01, 7.6099e+01, 7.4932e+01, 7.4164e+01, 7.4005e+01, 7.2604e+01,
        7.2362e+01, 7.1586e+01, 7.1075e+01, 7.0124e+01, 6.8731e+01, 6.7595e+01,
        6.7470e+01, 6.6497e+01, 6.5618e+01, 6.4652e+01, 6.3894e+01, 6.2962e+01,
        6.2651e+01, 6.1406e+01, 6.0886e+01, 6.0225e+01, 5.9594e+01, 5.8772e+01,
        5.7101e+01, 5.6379e+01, 5.5971e+01, 5.3810e+01, 5.3023e+01, 5.2721e+01,
        5.1394e+01, 5.1278e+01, 5.0286e+01, 4.9589e+01, 4.8659e+01, 4.7785e+01,
        4.7008e+01, 4.6221e+01, 4.5570e+01, 4.3972e+01, 4.3628e+01, 4.3196e+01,
        4.2125e+01, 4.1581e+01, 4.0458e+01, 3.8899e+01, 3.7790e+01, 3.6361e+01,
        3.6152e+01, 3.4205e+01, 3.3187e+01, 3.1523e+01, 3.0286e+01, 2.8592e+01,
        2.8070e+01, 2.6889e+01, 2.6363e+01, 2.5247e+01, 2.4616e+01, 2.4351e+01,
        2.3283e+01, 2.3057e+01, 2.1158e+01, 2.0374e+01, 1.8407e+01, 1.6982e+01,
        1.6358e+01, 1.2992e+01, 1.1710e+01, 1.0949e+01, 9.7985e+00, 8.1782e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 78]) 

NULL SPACE BASIS :  tensor([[ 1.4011e-02, -7.0685e-02,  3.1088e-02,  ..., -1.3488e-02,
         -4.9098e-03,  5.8815e-03],
        [ 2.5697e-02, -9.2932e-03, -1.4105e-02,  ...,  4.3084e-03,
          3.4273e-03, -5.3627e-03],
        [-1.6500e-02,  8.0149e-02,  1.3942e-02,  ...,  3.6434e-03,
         -2.1342e-03,  1.7663e-03],
        ...,
        [-1.0153e-02, -3.9389e-03, -3.5403e-02,  ..., -3.5508e-03,
          3.8754e-05, -2.2984e-03],
        [ 2.1539e-03,  4.0742e-02, -3.3699e-03,  ...,  8.4136e-04,
         -1.1090e-04,  1.3984e-03],
        [ 1.1761e-02, -4.2026e-02, -3.0047e-03,  ...,  4.3378e-03,
         -2.4838e-04,  3.1322e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.6661e-03, -6.2538e-03,  4.5991e-04,  ..., -6.0763e-04,
          5.4075e-04, -1.4995e-04],
        [-6.2538e-03,  1.3401e-02, -7.6414e-03,  ...,  1.3924e-03,
         -2.1444e-04, -3.8168e-04],
        [ 4.5991e-04, -7.6414e-03,  8.4896e-03,  ..., -1.0590e-03,
          2.0950e-04,  3.9492e-04],
        ...,
        [-6.0763e-04,  1.3924e-03, -1.0590e-03,  ...,  2.0948e-03,
         -1.4045e-03, -6.0064e-05],
        [ 5.4075e-04, -2.1444e-04,  2.0950e-04,  ..., -1.4045e-03,
          2.8833e-03, -1.3154e-03],
        [-1.4995e-04, -3.8168e-04,  3.9492e-04,  ..., -6.0064e-05,
         -1.3154e-03,  1.6279e-03]], device='cuda:0') 

reserving basis 144/576; cond: 1263061.0, radio:0.0005082685383968055
PARAMETER       :  Parameter containing:
tensor([[[[-1.3896e-02,  3.9811e-03,  1.4159e-02],
          [ 2.3818e-02,  7.5222e-03,  7.6081e-03],
          [-5.2427e-02, -4.9164e-02, -1.1285e-02]],

         [[ 3.1460e-03, -1.1667e-02, -6.4596e-03],
          [ 2.6836e-02, -2.9006e-02,  2.6238e-02],
          [-6.2344e-02, -7.7863e-03, -5.6857e-02]],

         [[-2.0218e-04,  1.9347e-02,  2.5472e-02],
          [-1.4285e-02, -3.4131e-02,  4.6754e-02],
          [-1.6611e-02, -8.1536e-03,  5.1462e-02]],

         ...,

         [[-3.9794e-02,  2.7593e-02,  2.1476e-02],
          [-1.1141e-02,  7.6577e-03, -6.7721e-03],
          [-3.4973e-02,  1.2353e-02,  2.3733e-02]],

         [[-1.7348e-02,  1.2868e-02,  1.5689e-03],
          [-3.3751e-02,  1.9598e-02, -4.5847e-03],
          [ 1.7566e-02, -3.9230e-02,  3.4556e-02]],

         [[ 2.1124e-02, -9.2490e-04,  2.4009e-02],
          [ 1.0246e-02,  1.1158e-02,  4.0292e-02],
          [-2.9702e-02,  2.0516e-02, -5.8930e-03]]],


        [[[ 3.2373e-02,  1.1105e-02, -2.2504e-02],
          [-8.6227e-03,  7.6024e-06,  1.0507e-02],
          [ 3.8487e-03, -4.6822e-02,  8.9990e-03]],

         [[ 3.4632e-02,  2.4504e-02,  2.2291e-02],
          [-2.0344e-02, -8.9045e-03, -2.3773e-02],
          [ 8.2610e-03, -2.9334e-02,  1.5307e-02]],

         [[-3.0624e-02, -1.4690e-02, -2.7844e-02],
          [-3.9317e-03,  1.3337e-02,  9.9758e-03],
          [ 3.5062e-02,  1.1109e-03, -4.2749e-02]],

         ...,

         [[ 1.7021e-02,  3.2658e-02,  6.0255e-03],
          [ 3.6077e-02,  3.9182e-02, -8.0756e-03],
          [-8.2245e-03, -2.9654e-02,  3.6093e-02]],

         [[-4.5765e-03,  1.2661e-02, -1.8442e-02],
          [-5.5201e-03, -2.0772e-02,  1.5205e-02],
          [-4.0731e-02, -2.3372e-02, -3.4018e-02]],

         [[ 3.4936e-02, -1.8424e-02,  9.9430e-03],
          [ 1.0746e-02,  1.5468e-02,  3.8320e-03],
          [ 2.3077e-02, -1.3439e-02,  2.3163e-02]]],


        [[[ 3.0752e-02, -5.1515e-03, -4.5743e-02],
          [ 3.1382e-02,  3.7484e-02,  3.0061e-02],
          [-1.4810e-02, -7.0039e-03,  3.8927e-02]],

         [[ 1.4044e-02,  3.5181e-03,  9.2664e-03],
          [ 1.9196e-02, -2.9016e-02, -2.9039e-03],
          [ 2.9849e-02,  2.8106e-02, -1.5845e-02]],

         [[ 4.1761e-02,  2.1396e-03,  9.3900e-03],
          [ 1.4201e-02, -1.8369e-02,  3.9066e-02],
          [-3.3733e-02, -3.8400e-02,  8.1577e-03]],

         ...,

         [[-3.4154e-04,  3.0812e-02, -1.2531e-02],
          [-1.6055e-02, -4.4988e-02,  6.1608e-03],
          [-1.0218e-02, -2.5105e-02, -2.5369e-02]],

         [[-8.4946e-03,  2.4644e-02, -4.5761e-02],
          [ 3.2681e-02,  2.3947e-02, -2.2678e-02],
          [-2.7273e-02, -2.3910e-02,  1.1019e-02]],

         [[ 3.8152e-02, -3.5804e-03, -8.7648e-03],
          [-2.5180e-02, -2.6142e-02,  9.0602e-03],
          [ 3.0814e-03,  4.1723e-02,  3.4204e-02]]],


        ...,


        [[[ 3.7291e-02, -3.1397e-02, -3.4394e-02],
          [ 3.6507e-02, -3.5176e-02, -4.7159e-02],
          [-1.4947e-02, -2.2074e-02,  1.4411e-02]],

         [[ 3.8231e-02,  1.6604e-02,  2.8149e-02],
          [-4.1523e-02, -4.2916e-02,  3.3902e-03],
          [-4.7587e-02,  2.4332e-02, -3.8764e-02]],

         [[ 1.2472e-02,  4.5110e-02,  5.3020e-03],
          [-4.4547e-02, -4.2004e-02,  2.0602e-02],
          [-2.1532e-02,  1.7427e-02,  2.7971e-02]],

         ...,

         [[-2.5222e-02, -2.4189e-02,  1.8379e-02],
          [ 2.3941e-03,  2.9488e-02,  3.6609e-02],
          [ 3.4419e-02, -4.8502e-02,  1.1672e-02]],

         [[ 7.8684e-04, -2.3311e-02,  3.3788e-02],
          [-3.8303e-02, -6.1298e-03, -2.6261e-03],
          [-8.1403e-03, -4.4520e-02,  6.5194e-03]],

         [[-3.8827e-03, -2.0367e-02, -7.7575e-03],
          [ 3.1463e-02,  4.8410e-02, -2.5167e-03],
          [-1.9973e-03,  3.0993e-02,  7.3949e-03]]],


        [[[-4.8484e-04, -4.1743e-02,  1.4456e-02],
          [-4.0765e-02,  2.1196e-02,  3.3694e-02],
          [ 1.3241e-02,  2.8020e-02, -1.2135e-02]],

         [[ 1.2986e-02, -2.0662e-02, -1.9064e-02],
          [ 7.3363e-03, -4.4914e-02,  1.7567e-03],
          [-2.1949e-02, -2.0079e-02, -5.1638e-03]],

         [[ 2.6553e-02,  2.5010e-02,  1.5249e-02],
          [-4.7806e-03,  5.9405e-03, -2.0145e-02],
          [ 3.3162e-02, -3.5008e-02, -1.2581e-03]],

         ...,

         [[-2.1695e-02,  4.2867e-02,  4.9123e-03],
          [ 2.5130e-02,  2.3781e-02, -3.0782e-02],
          [-4.3205e-03, -3.8872e-02,  1.4547e-02]],

         [[-2.3427e-02, -1.4299e-02, -3.5607e-02],
          [-3.9424e-03,  2.0865e-02,  2.9787e-03],
          [ 1.0262e-02, -3.1325e-02,  3.2970e-02]],

         [[ 1.0546e-02,  4.2867e-02,  1.2239e-02],
          [-2.7742e-03,  2.2160e-02,  1.8197e-02],
          [-1.2339e-02,  4.8153e-03,  2.2716e-02]]],


        [[[ 8.8078e-03, -9.2893e-03, -7.5481e-03],
          [-1.6302e-02,  1.6318e-02, -7.5889e-03],
          [-3.9408e-02, -4.3212e-02,  4.2550e-02]],

         [[-2.5099e-02,  4.6909e-03, -1.8546e-02],
          [-3.1849e-02, -2.8475e-02,  8.2302e-04],
          [-3.3233e-02,  1.2066e-02,  1.0636e-02]],

         [[ 4.5418e-04,  1.3926e-02, -1.4902e-02],
          [ 6.6238e-03, -3.7320e-02, -8.1621e-03],
          [-2.3220e-02,  2.7803e-02, -4.7443e-02]],

         ...,

         [[ 4.0264e-02, -3.6229e-02, -3.3745e-02],
          [-1.0949e-02,  1.1713e-02,  4.0344e-02],
          [-1.4280e-02,  3.0665e-02, -3.6973e-02]],

         [[-1.2253e-02, -3.1131e-02, -1.1356e-02],
          [ 1.6440e-02,  2.1659e-02, -2.9633e-02],
          [ 3.0705e-02, -3.1332e-02,  2.9685e-02]],

         [[-3.1287e-03, -3.1538e-02, -2.7787e-02],
          [-2.1197e-02,  3.2386e-02,  4.1158e-04],
          [-1.5264e-02,  3.9653e-02,  5.8134e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([6.9905e+07, 2.7254e+06, 2.2491e+06, 2.1292e+06, 2.0456e+06, 1.2608e+06,
        1.0503e+06, 9.5796e+05, 8.7914e+05, 6.1880e+05, 5.9682e+05, 5.7441e+05,
        5.0778e+05, 4.7161e+05, 3.7984e+05, 2.7856e+05, 2.5092e+05, 2.3389e+05,
        2.1348e+05, 1.8857e+05, 1.7759e+05, 1.4041e+05, 1.3272e+05, 1.2580e+05,
        1.2207e+05, 1.0613e+05, 1.0068e+05, 9.5666e+04, 9.1278e+04, 8.1146e+04,
        7.6828e+04, 7.2560e+04, 6.8164e+04, 6.7835e+04, 5.9142e+04, 5.6059e+04,
        5.5326e+04, 5.2322e+04, 4.5486e+04, 4.3947e+04, 4.2014e+04, 3.8089e+04,
        3.6980e+04, 3.6286e+04, 3.4125e+04, 3.2831e+04, 3.1562e+04, 2.9911e+04,
        2.8671e+04, 2.7537e+04, 2.6714e+04, 2.6473e+04, 2.5896e+04, 2.5320e+04,
        2.4472e+04, 2.3304e+04, 2.2956e+04, 2.2367e+04, 2.1862e+04, 2.1164e+04,
        2.0612e+04, 1.9681e+04, 1.9175e+04, 1.8753e+04, 1.8527e+04, 1.7578e+04,
        1.7074e+04, 1.6606e+04, 1.6034e+04, 1.5859e+04, 1.5562e+04, 1.5068e+04,
        1.4997e+04, 1.4866e+04, 1.4686e+04, 1.3968e+04, 1.3833e+04, 1.3362e+04,
        1.3162e+04, 1.3023e+04, 1.2662e+04, 1.2506e+04, 1.2321e+04, 1.2001e+04,
        1.1685e+04, 1.1537e+04, 1.1456e+04, 1.1091e+04, 1.0919e+04, 1.0826e+04,
        1.0643e+04, 1.0429e+04, 1.0344e+04, 9.9827e+03, 9.9105e+03, 9.7257e+03,
        9.5951e+03, 9.3551e+03, 9.2721e+03, 9.1965e+03, 8.9622e+03, 8.8764e+03,
        8.8411e+03, 8.6232e+03, 8.5083e+03, 8.4226e+03, 8.1423e+03, 8.0975e+03,
        7.9664e+03, 7.8384e+03, 7.8100e+03, 7.7184e+03, 7.4970e+03, 7.4340e+03,
        7.3625e+03, 7.1832e+03, 7.1431e+03, 6.9627e+03, 6.8932e+03, 6.7266e+03,
        6.6806e+03, 6.6422e+03, 6.5920e+03, 6.4641e+03, 6.4147e+03, 6.3289e+03,
        6.2892e+03, 6.1712e+03, 6.0250e+03, 5.9472e+03, 5.9273e+03, 5.8735e+03,
        5.7707e+03, 5.7602e+03, 5.6904e+03, 5.6360e+03, 5.4666e+03, 5.4095e+03,
        5.3527e+03, 5.2490e+03, 5.1828e+03, 5.1551e+03, 5.0408e+03, 4.9699e+03,
        4.9527e+03, 4.9049e+03, 4.8290e+03, 4.7864e+03, 4.7025e+03, 4.6817e+03,
        4.6200e+03, 4.5597e+03, 4.5329e+03, 4.4280e+03, 4.3585e+03, 4.3392e+03,
        4.2979e+03, 4.2813e+03, 4.1986e+03, 4.1261e+03, 4.1051e+03, 4.0858e+03,
        4.0607e+03, 3.9853e+03, 3.9269e+03, 3.8975e+03, 3.8753e+03, 3.8390e+03,
        3.7751e+03, 3.7278e+03, 3.6981e+03, 3.6922e+03, 3.6457e+03, 3.6301e+03,
        3.6087e+03, 3.5776e+03, 3.5392e+03, 3.5010e+03, 3.4211e+03, 3.4099e+03,
        3.3724e+03, 3.3407e+03, 3.3030e+03, 3.2606e+03, 3.2350e+03, 3.2054e+03,
        3.1716e+03, 3.1348e+03, 3.1234e+03, 3.1108e+03, 3.0929e+03, 3.0572e+03,
        3.0135e+03, 2.9872e+03, 2.9534e+03, 2.9320e+03, 2.9126e+03, 2.8928e+03,
        2.8609e+03, 2.8331e+03, 2.8137e+03, 2.7936e+03, 2.7659e+03, 2.7155e+03,
        2.6911e+03, 2.6750e+03, 2.6427e+03, 2.6360e+03, 2.6195e+03, 2.5919e+03,
        2.5847e+03, 2.5548e+03, 2.5296e+03, 2.5154e+03, 2.4994e+03, 2.4402e+03,
        2.4318e+03, 2.4119e+03, 2.4017e+03, 2.3917e+03, 2.3386e+03, 2.3237e+03,
        2.3069e+03, 2.2948e+03, 2.2771e+03, 2.2721e+03, 2.2439e+03, 2.2317e+03,
        2.1904e+03, 2.1719e+03, 2.1600e+03, 2.1309e+03, 2.1231e+03, 2.1105e+03,
        2.0883e+03, 2.0731e+03, 2.0544e+03, 2.0456e+03, 2.0342e+03, 2.0232e+03,
        2.0014e+03, 1.9829e+03, 1.9770e+03, 1.9556e+03, 1.9494e+03, 1.9245e+03,
        1.9183e+03, 1.8973e+03, 1.8768e+03, 1.8630e+03, 1.8426e+03, 1.8343e+03,
        1.8276e+03, 1.8047e+03, 1.7949e+03, 1.7793e+03, 1.7559e+03, 1.7550e+03,
        1.7434e+03, 1.7358e+03, 1.7229e+03, 1.7076e+03, 1.6974e+03, 1.6803e+03,
        1.6723e+03, 1.6614e+03, 1.6420e+03, 1.6299e+03, 1.6198e+03, 1.5958e+03,
        1.5898e+03, 1.5813e+03, 1.5712e+03, 1.5561e+03, 1.5491e+03, 1.5349e+03,
        1.5268e+03, 1.5155e+03, 1.5057e+03, 1.5007e+03, 1.4840e+03, 1.4722e+03,
        1.4656e+03, 1.4522e+03, 1.4396e+03, 1.4281e+03, 1.4245e+03, 1.4180e+03,
        1.4070e+03, 1.3905e+03, 1.3896e+03, 1.3601e+03, 1.3558e+03, 1.3452e+03,
        1.3351e+03, 1.3325e+03, 1.3267e+03, 1.3169e+03, 1.2985e+03, 1.2961e+03,
        1.2934e+03, 1.2816e+03, 1.2723e+03, 1.2695e+03, 1.2559e+03, 1.2423e+03,
        1.2383e+03, 1.2312e+03, 1.2246e+03, 1.2141e+03, 1.2102e+03, 1.2010e+03,
        1.1918e+03, 1.1852e+03, 1.1776e+03, 1.1750e+03, 1.1581e+03, 1.1448e+03,
        1.1370e+03, 1.1303e+03, 1.1245e+03, 1.1193e+03, 1.1156e+03, 1.1111e+03,
        1.1000e+03, 1.0931e+03, 1.0793e+03, 1.0782e+03, 1.0743e+03, 1.0609e+03,
        1.0606e+03, 1.0548e+03, 1.0455e+03, 1.0412e+03, 1.0370e+03, 1.0217e+03,
        1.0194e+03, 1.0165e+03, 1.0024e+03, 9.9447e+02, 9.9412e+02, 9.8547e+02,
        9.7803e+02, 9.7347e+02, 9.7035e+02, 9.5603e+02, 9.5115e+02, 9.4634e+02,
        9.4198e+02, 9.3672e+02, 9.3504e+02, 9.2330e+02, 9.1856e+02, 9.1504e+02,
        9.0945e+02, 9.0193e+02, 8.9179e+02, 8.8722e+02, 8.7761e+02, 8.7246e+02,
        8.6995e+02, 8.6768e+02, 8.5721e+02, 8.5532e+02, 8.5262e+02, 8.4588e+02,
        8.4361e+02, 8.3519e+02, 8.3397e+02, 8.2488e+02, 8.2355e+02, 8.2158e+02,
        8.1295e+02, 8.0748e+02, 8.0149e+02, 7.9660e+02, 7.9482e+02, 7.8701e+02,
        7.8565e+02, 7.8212e+02, 7.7545e+02, 7.6701e+02, 7.6145e+02, 7.5631e+02,
        7.5066e+02, 7.4627e+02, 7.4407e+02, 7.3996e+02, 7.3338e+02, 7.2740e+02,
        7.2206e+02, 7.1987e+02, 7.1261e+02, 7.0789e+02, 7.0445e+02, 6.9669e+02,
        6.9321e+02, 6.8851e+02, 6.8607e+02, 6.7814e+02, 6.7723e+02, 6.7062e+02,
        6.6686e+02, 6.6209e+02, 6.5816e+02, 6.5366e+02, 6.4987e+02, 6.4553e+02,
        6.4367e+02, 6.3809e+02, 6.3510e+02, 6.3181e+02, 6.2844e+02, 6.1923e+02,
        6.1607e+02, 6.1321e+02, 6.1230e+02, 6.0695e+02, 6.0613e+02, 6.0061e+02,
        5.9710e+02, 5.9470e+02, 5.9015e+02, 5.8535e+02, 5.8328e+02, 5.7688e+02,
        5.7528e+02, 5.7045e+02, 5.6885e+02, 5.6361e+02, 5.6270e+02, 5.5940e+02,
        5.5311e+02, 5.5047e+02, 5.4322e+02, 5.3999e+02, 5.3386e+02, 5.3255e+02,
        5.3025e+02, 5.2335e+02, 5.2013e+02, 5.1737e+02, 5.1411e+02, 5.1252e+02,
        5.0925e+02, 5.0721e+02, 4.9689e+02, 4.9532e+02, 4.9398e+02, 4.9354e+02,
        4.8859e+02, 4.8570e+02, 4.8254e+02, 4.7931e+02, 4.7533e+02, 4.7240e+02,
        4.7142e+02, 4.6967e+02, 4.6758e+02, 4.6195e+02, 4.5377e+02, 4.5162e+02,
        4.4668e+02, 4.4527e+02, 4.4132e+02, 4.3880e+02, 4.3407e+02, 4.3204e+02,
        4.3046e+02, 4.2920e+02, 4.2645e+02, 4.2206e+02, 4.2099e+02, 4.1359e+02,
        4.1339e+02, 4.0818e+02, 4.0437e+02, 4.0400e+02, 4.0164e+02, 3.9992e+02,
        3.9792e+02, 3.9356e+02, 3.9056e+02, 3.8792e+02, 3.8393e+02, 3.8067e+02,
        3.7489e+02, 3.7325e+02, 3.7139e+02, 3.7000e+02, 3.6573e+02, 3.6284e+02,
        3.6045e+02, 3.5681e+02, 3.5297e+02, 3.4995e+02, 3.4687e+02, 3.4223e+02,
        3.4182e+02, 3.3683e+02, 3.3297e+02, 3.2734e+02, 3.2646e+02, 3.2099e+02,
        3.1923e+02, 3.1642e+02, 3.1101e+02, 3.0960e+02, 3.0740e+02, 3.0382e+02,
        3.0333e+02, 3.0126e+02, 2.9800e+02, 2.9687e+02, 2.9381e+02, 2.9198e+02,
        2.8697e+02, 2.8127e+02, 2.7906e+02, 2.7787e+02, 2.7221e+02, 2.6898e+02,
        2.6706e+02, 2.6600e+02, 2.6163e+02, 2.6115e+02, 2.5696e+02, 2.5222e+02,
        2.4961e+02, 2.4652e+02, 2.4588e+02, 2.3758e+02, 2.3447e+02, 2.3143e+02,
        2.2961e+02, 2.2575e+02, 2.2481e+02, 2.2147e+02, 2.2073e+02, 2.1841e+02,
        2.1335e+02, 2.0827e+02, 2.0740e+02, 2.0484e+02, 2.0256e+02, 2.0025e+02,
        1.9605e+02, 1.9215e+02, 1.8644e+02, 1.8519e+02, 1.8005e+02, 1.7769e+02,
        1.7381e+02, 1.7278e+02, 1.6912e+02, 1.6576e+02, 1.6299e+02, 1.5938e+02,
        1.5652e+02, 1.5254e+02, 1.4628e+02, 1.4394e+02, 1.4118e+02, 1.3334e+02,
        1.2626e+02, 1.2002e+02, 1.1634e+02, 1.1319e+02, 1.0729e+02, 1.0420e+02,
        1.0324e+02, 9.8284e+01, 9.0215e+01, 8.9750e+01, 7.2700e+01, 5.5346e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 144]) 

NULL SPACE BASIS :  tensor([[-0.0320,  0.0098, -0.0601,  ...,  0.0038,  0.0043, -0.0137],
        [-0.0073, -0.0314,  0.0048,  ..., -0.0170,  0.0035,  0.0265],
        [ 0.0903, -0.0632, -0.0475,  ...,  0.0168,  0.0027, -0.0160],
        ...,
        [ 0.0267, -0.0212, -0.0771,  ..., -0.0345, -0.2041, -0.0438],
        [-0.0090, -0.0171, -0.0463,  ...,  0.0133,  0.3340,  0.0473],
        [-0.0261, -0.0237,  0.0089,  ...,  0.0118, -0.1567, -0.0172]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.6430e-02, -2.1558e-02, -6.7813e-03,  ...,  6.4783e-04,
          4.1209e-04, -6.4199e-05],
        [-2.1558e-02,  4.2431e-02, -2.1228e-02,  ..., -1.3980e-03,
         -3.0390e-04, -2.5132e-05],
        [-6.7813e-03, -2.1228e-02,  3.4805e-02,  ..., -3.6802e-04,
          5.0292e-04,  3.7088e-04],
        ...,
        [ 6.4783e-04, -1.3980e-03, -3.6802e-04,  ...,  4.9153e-02,
         -2.2949e-02, -1.0620e-02],
        [ 4.1209e-04, -3.0390e-04,  5.0292e-04,  ..., -2.2949e-02,
          5.3661e-02, -2.1415e-02],
        [-6.4199e-05, -2.5132e-05,  3.7088e-04,  ..., -1.0620e-02,
         -2.1415e-02,  4.7203e-02]], device='cuda:0') 

reserving basis 223/576; cond: 1085530.0, radio:0.0008216994465328753
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0187,  0.0400,  0.0321],
          [ 0.0037, -0.0030, -0.0085],
          [-0.0351,  0.0285,  0.0166]],

         [[-0.0424, -0.0196, -0.0225],
          [-0.0471, -0.0462, -0.0275],
          [-0.0238,  0.0054, -0.0061]],

         [[ 0.0177,  0.0239, -0.0501],
          [-0.0145, -0.0301, -0.0194],
          [ 0.0120,  0.0107,  0.0192]],

         ...,

         [[-0.0554,  0.0319,  0.0447],
          [-0.0297, -0.0067, -0.0152],
          [ 0.0237, -0.0233, -0.0336]],

         [[ 0.0111, -0.0287,  0.0349],
          [ 0.0403, -0.0259, -0.0034],
          [-0.0189, -0.0330,  0.0198]],

         [[-0.0265, -0.0210,  0.0059],
          [ 0.0169,  0.0149, -0.0234],
          [-0.0261,  0.0037,  0.0037]]],


        [[[-0.0302, -0.0202, -0.0117],
          [ 0.0361,  0.0210,  0.0291],
          [-0.0137,  0.0462,  0.0012]],

         [[-0.0389, -0.0331, -0.0311],
          [ 0.0002, -0.0404, -0.0240],
          [-0.0148,  0.0200,  0.0222]],

         [[-0.0092,  0.0160, -0.0231],
          [-0.0354, -0.0301,  0.0143],
          [ 0.0296,  0.0115, -0.0192]],

         ...,

         [[ 0.0245, -0.0459,  0.0175],
          [-0.0125,  0.0120, -0.0202],
          [ 0.0272,  0.0253, -0.0071]],

         [[ 0.0119, -0.0402, -0.0077],
          [-0.0134, -0.0188, -0.0413],
          [-0.0042,  0.0056, -0.0004]],

         [[-0.0013, -0.0269, -0.0108],
          [-0.0311, -0.0277, -0.0163],
          [ 0.0345,  0.0213, -0.0176]]],


        [[[-0.0407, -0.0086, -0.0021],
          [-0.0574, -0.0422, -0.0352],
          [-0.0015,  0.0349,  0.0384]],

         [[-0.0185, -0.0097, -0.0039],
          [-0.0234, -0.0405, -0.0389],
          [-0.0174, -0.0177, -0.0098]],

         [[ 0.0076, -0.0145,  0.0051],
          [-0.0356,  0.0500, -0.0087],
          [-0.0053,  0.0267,  0.0380]],

         ...,

         [[-0.0452,  0.0227,  0.0364],
          [-0.0184, -0.0405,  0.0337],
          [-0.0116, -0.0294,  0.0130]],

         [[ 0.0269,  0.0184,  0.0403],
          [ 0.0232, -0.0300, -0.0222],
          [-0.0343, -0.0500,  0.0196]],

         [[-0.0288,  0.0499,  0.0324],
          [ 0.0202,  0.0084, -0.0120],
          [-0.0152, -0.0007, -0.0375]]],


        ...,


        [[[ 0.0343,  0.0054, -0.0064],
          [ 0.0394,  0.0451,  0.0015],
          [-0.0158,  0.0128, -0.0382]],

         [[-0.0309,  0.0014, -0.0284],
          [-0.0104,  0.0343,  0.0141],
          [-0.0048, -0.0174,  0.0210]],

         [[-0.0416, -0.0407, -0.0359],
          [-0.0276,  0.0353,  0.0429],
          [-0.0119, -0.0347, -0.0272]],

         ...,

         [[ 0.0338, -0.0063,  0.0339],
          [-0.0117,  0.0431, -0.0124],
          [-0.0394,  0.0259, -0.0237]],

         [[-0.0184, -0.0423, -0.0142],
          [-0.0115,  0.0188, -0.0351],
          [ 0.0244,  0.0226,  0.0133]],

         [[ 0.0216, -0.0154,  0.0107],
          [ 0.0329, -0.0040, -0.0269],
          [-0.0405,  0.0146, -0.0117]]],


        [[[-0.0379,  0.0239, -0.0474],
          [ 0.0032, -0.0421,  0.0207],
          [-0.0163, -0.0072,  0.0367]],

         [[ 0.0186, -0.0306,  0.0073],
          [-0.0081,  0.0272,  0.0054],
          [ 0.0244, -0.0116,  0.0163]],

         [[-0.0347, -0.0421, -0.0252],
          [-0.0235, -0.0176,  0.0184],
          [ 0.0086, -0.0330, -0.0361]],

         ...,

         [[-0.0091, -0.0163, -0.0307],
          [-0.0205,  0.0285, -0.0234],
          [-0.0409,  0.0343,  0.0318]],

         [[-0.0300, -0.0107, -0.0294],
          [ 0.0367,  0.0024, -0.0162],
          [ 0.0300,  0.0324, -0.0207]],

         [[ 0.0150, -0.0266, -0.0073],
          [-0.0400, -0.0343,  0.0387],
          [ 0.0382,  0.0223,  0.0389]]],


        [[[-0.0387,  0.0327, -0.0044],
          [-0.0341,  0.0353, -0.0079],
          [-0.0383, -0.0226,  0.0298]],

         [[-0.0036, -0.0257,  0.0090],
          [ 0.0184,  0.0020, -0.0097],
          [-0.0147, -0.0025,  0.0313]],

         [[-0.0222, -0.0370, -0.0066],
          [-0.0013, -0.0009,  0.0025],
          [ 0.0349, -0.0265,  0.0024]],

         ...,

         [[ 0.0123, -0.0165, -0.0195],
          [-0.0200,  0.0308, -0.0445],
          [ 0.0001,  0.0154, -0.0299]],

         [[ 0.0359, -0.0065,  0.0316],
          [ 0.0050, -0.0212, -0.0384],
          [-0.0207,  0.0086, -0.0361]],

         [[ 0.0102,  0.0416, -0.0179],
          [-0.0316,  0.0422,  0.0360],
          [ 0.0268,  0.0157,  0.0076]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([7.1388e+07, 2.7755e+06, 2.4071e+06, 1.9927e+06, 1.6597e+06, 1.1730e+06,
        1.0806e+06, 8.5802e+05, 8.1904e+05, 6.3688e+05, 5.7258e+05, 5.4269e+05,
        4.2760e+05, 3.2816e+05, 1.7533e+05, 1.6019e+05, 1.4754e+05, 1.3849e+05,
        1.3172e+05, 1.1737e+05, 1.1263e+05, 9.8995e+04, 9.2643e+04, 8.6298e+04,
        8.0242e+04, 7.1821e+04, 7.1238e+04, 6.7696e+04, 6.5001e+04, 6.3052e+04,
        5.9673e+04, 5.4373e+04, 5.3782e+04, 5.2599e+04, 4.6868e+04, 4.4413e+04,
        4.2883e+04, 4.1347e+04, 3.9664e+04, 3.8453e+04, 3.7517e+04, 3.5465e+04,
        3.5044e+04, 3.3490e+04, 3.2680e+04, 3.0829e+04, 2.9965e+04, 2.8592e+04,
        2.8084e+04, 2.7244e+04, 2.5535e+04, 2.5281e+04, 2.3373e+04, 2.2367e+04,
        2.1702e+04, 2.0416e+04, 1.9844e+04, 1.9352e+04, 1.8805e+04, 1.8387e+04,
        1.7176e+04, 1.6850e+04, 1.6340e+04, 1.6162e+04, 1.5660e+04, 1.5435e+04,
        1.5069e+04, 1.4932e+04, 1.4338e+04, 1.4204e+04, 1.4142e+04, 1.3188e+04,
        1.2889e+04, 1.2642e+04, 1.2179e+04, 1.1830e+04, 1.1599e+04, 1.1575e+04,
        1.1483e+04, 1.1191e+04, 1.0951e+04, 1.0749e+04, 1.0399e+04, 1.0210e+04,
        9.9616e+03, 9.8146e+03, 9.7075e+03, 9.4526e+03, 9.2892e+03, 9.1423e+03,
        9.0518e+03, 8.9392e+03, 8.7998e+03, 8.6340e+03, 8.2786e+03, 8.1795e+03,
        7.7884e+03, 7.7188e+03, 7.6146e+03, 7.5414e+03, 7.3847e+03, 7.2974e+03,
        7.1642e+03, 7.0539e+03, 6.9674e+03, 6.8697e+03, 6.7597e+03, 6.5987e+03,
        6.4384e+03, 6.3949e+03, 6.2244e+03, 6.1370e+03, 6.0556e+03, 5.9181e+03,
        5.8515e+03, 5.7877e+03, 5.7652e+03, 5.5271e+03, 5.4746e+03, 5.4242e+03,
        5.3186e+03, 5.1760e+03, 5.0932e+03, 5.0593e+03, 4.9623e+03, 4.9441e+03,
        4.8618e+03, 4.8458e+03, 4.8024e+03, 4.6788e+03, 4.6246e+03, 4.5605e+03,
        4.4710e+03, 4.3809e+03, 4.3597e+03, 4.2900e+03, 4.1902e+03, 4.0891e+03,
        4.0815e+03, 3.9715e+03, 3.9526e+03, 3.9190e+03, 3.8855e+03, 3.8429e+03,
        3.8185e+03, 3.7855e+03, 3.7027e+03, 3.6519e+03, 3.6123e+03, 3.5724e+03,
        3.5375e+03, 3.4762e+03, 3.3870e+03, 3.3728e+03, 3.3287e+03, 3.3170e+03,
        3.2635e+03, 3.2292e+03, 3.1869e+03, 3.1467e+03, 3.1156e+03, 3.0845e+03,
        3.0613e+03, 3.0434e+03, 3.0270e+03, 3.0188e+03, 3.0048e+03, 2.9636e+03,
        2.8837e+03, 2.8579e+03, 2.8202e+03, 2.8071e+03, 2.7901e+03, 2.7752e+03,
        2.7474e+03, 2.7208e+03, 2.6894e+03, 2.6446e+03, 2.6108e+03, 2.5869e+03,
        2.5674e+03, 2.5428e+03, 2.5221e+03, 2.4908e+03, 2.4599e+03, 2.4343e+03,
        2.4239e+03, 2.3937e+03, 2.3916e+03, 2.3784e+03, 2.3575e+03, 2.3194e+03,
        2.2937e+03, 2.2863e+03, 2.2196e+03, 2.2041e+03, 2.1715e+03, 2.1526e+03,
        2.1300e+03, 2.1096e+03, 2.0782e+03, 2.0614e+03, 2.0560e+03, 2.0285e+03,
        2.0192e+03, 2.0026e+03, 1.9725e+03, 1.9468e+03, 1.9363e+03, 1.9289e+03,
        1.9070e+03, 1.8961e+03, 1.8660e+03, 1.8607e+03, 1.8530e+03, 1.8422e+03,
        1.8261e+03, 1.8087e+03, 1.7957e+03, 1.7766e+03, 1.7614e+03, 1.7554e+03,
        1.7355e+03, 1.7212e+03, 1.7010e+03, 1.6797e+03, 1.6761e+03, 1.6557e+03,
        1.6329e+03, 1.6207e+03, 1.6085e+03, 1.5988e+03, 1.5788e+03, 1.5733e+03,
        1.5705e+03, 1.5478e+03, 1.5289e+03, 1.5237e+03, 1.5188e+03, 1.5102e+03,
        1.4968e+03, 1.4820e+03, 1.4489e+03, 1.4420e+03, 1.4309e+03, 1.4257e+03,
        1.4199e+03, 1.4026e+03, 1.3962e+03, 1.3833e+03, 1.3772e+03, 1.3415e+03,
        1.3361e+03, 1.3331e+03, 1.3203e+03, 1.3143e+03, 1.3031e+03, 1.3018e+03,
        1.2858e+03, 1.2676e+03, 1.2634e+03, 1.2594e+03, 1.2461e+03, 1.2295e+03,
        1.2260e+03, 1.2110e+03, 1.2069e+03, 1.2019e+03, 1.1930e+03, 1.1832e+03,
        1.1660e+03, 1.1625e+03, 1.1610e+03, 1.1479e+03, 1.1411e+03, 1.1310e+03,
        1.1281e+03, 1.1132e+03, 1.1070e+03, 1.0953e+03, 1.0897e+03, 1.0894e+03,
        1.0781e+03, 1.0712e+03, 1.0548e+03, 1.0513e+03, 1.0499e+03, 1.0361e+03,
        1.0270e+03, 1.0240e+03, 1.0164e+03, 1.0115e+03, 1.0055e+03, 9.9582e+02,
        9.9194e+02, 9.8718e+02, 9.8194e+02, 9.7295e+02, 9.6313e+02, 9.5445e+02,
        9.5228e+02, 9.4544e+02, 9.3560e+02, 9.2759e+02, 9.2078e+02, 9.1716e+02,
        9.1139e+02, 9.0439e+02, 8.9563e+02, 8.8638e+02, 8.7977e+02, 8.7220e+02,
        8.6700e+02, 8.6212e+02, 8.5883e+02, 8.5193e+02, 8.4880e+02, 8.4521e+02,
        8.4086e+02, 8.3022e+02, 8.2068e+02, 8.1583e+02, 8.1017e+02, 7.9964e+02,
        7.9358e+02, 7.8691e+02, 7.8170e+02, 7.7116e+02, 7.6930e+02, 7.6816e+02,
        7.6040e+02, 7.5444e+02, 7.5123e+02, 7.4906e+02, 7.4109e+02, 7.3587e+02,
        7.3073e+02, 7.2737e+02, 7.2205e+02, 7.1807e+02, 7.1209e+02, 7.0657e+02,
        6.9976e+02, 6.9900e+02, 6.9724e+02, 6.8965e+02, 6.8682e+02, 6.8103e+02,
        6.6919e+02, 6.6816e+02, 6.6512e+02, 6.6356e+02, 6.5913e+02, 6.5566e+02,
        6.5003e+02, 6.4822e+02, 6.4334e+02, 6.4189e+02, 6.3590e+02, 6.3241e+02,
        6.2568e+02, 6.2423e+02, 6.2285e+02, 6.1253e+02, 6.0850e+02, 6.0447e+02,
        5.9851e+02, 5.9299e+02, 5.8747e+02, 5.8560e+02, 5.8170e+02, 5.7794e+02,
        5.7139e+02, 5.6996e+02, 5.6618e+02, 5.6372e+02, 5.5794e+02, 5.5544e+02,
        5.5323e+02, 5.4897e+02, 5.4812e+02, 5.4352e+02, 5.3884e+02, 5.3595e+02,
        5.3469e+02, 5.2828e+02, 5.2315e+02, 5.2011e+02, 5.1849e+02, 5.1617e+02,
        5.0913e+02, 5.0617e+02, 5.0220e+02, 4.9964e+02, 4.9706e+02, 4.9493e+02,
        4.9303e+02, 4.8914e+02, 4.8864e+02, 4.8084e+02, 4.8027e+02, 4.7777e+02,
        4.7301e+02, 4.7079e+02, 4.6957e+02, 4.6432e+02, 4.6108e+02, 4.5837e+02,
        4.5653e+02, 4.5582e+02, 4.5254e+02, 4.4883e+02, 4.4502e+02, 4.4285e+02,
        4.4236e+02, 4.3930e+02, 4.3556e+02, 4.3238e+02, 4.3065e+02, 4.2475e+02,
        4.2168e+02, 4.1630e+02, 4.1365e+02, 4.1209e+02, 4.0795e+02, 4.0624e+02,
        4.0331e+02, 4.0303e+02, 3.9930e+02, 3.9853e+02, 3.9522e+02, 3.9472e+02,
        3.9214e+02, 3.8875e+02, 3.8589e+02, 3.8346e+02, 3.8261e+02, 3.7646e+02,
        3.7498e+02, 3.7136e+02, 3.7057e+02, 3.6778e+02, 3.6686e+02, 3.6301e+02,
        3.6108e+02, 3.6058e+02, 3.5755e+02, 3.5269e+02, 3.5155e+02, 3.4889e+02,
        3.4774e+02, 3.4615e+02, 3.4519e+02, 3.4172e+02, 3.4041e+02, 3.3708e+02,
        3.3504e+02, 3.3128e+02, 3.2962e+02, 3.2701e+02, 3.2482e+02, 3.2348e+02,
        3.1861e+02, 3.1721e+02, 3.1535e+02, 3.1174e+02, 3.0915e+02, 3.0530e+02,
        3.0440e+02, 3.0225e+02, 2.9940e+02, 2.9827e+02, 2.9496e+02, 2.9302e+02,
        2.9259e+02, 2.9102e+02, 2.8960e+02, 2.8671e+02, 2.8403e+02, 2.8158e+02,
        2.7769e+02, 2.7627e+02, 2.7492e+02, 2.7335e+02, 2.7072e+02, 2.6881e+02,
        2.6759e+02, 2.6608e+02, 2.6395e+02, 2.6122e+02, 2.5957e+02, 2.5819e+02,
        2.5374e+02, 2.5257e+02, 2.5056e+02, 2.4886e+02, 2.4807e+02, 2.4492e+02,
        2.4427e+02, 2.4188e+02, 2.3946e+02, 2.3707e+02, 2.3460e+02, 2.3287e+02,
        2.3249e+02, 2.2767e+02, 2.2694e+02, 2.2406e+02, 2.2167e+02, 2.1976e+02,
        2.1787e+02, 2.1717e+02, 2.1461e+02, 2.1348e+02, 2.1205e+02, 2.0998e+02,
        2.0674e+02, 2.0471e+02, 2.0328e+02, 2.0184e+02, 1.9903e+02, 1.9720e+02,
        1.9477e+02, 1.9226e+02, 1.9027e+02, 1.8972e+02, 1.8897e+02, 1.8767e+02,
        1.8725e+02, 1.8564e+02, 1.8274e+02, 1.8099e+02, 1.7925e+02, 1.7846e+02,
        1.7453e+02, 1.7237e+02, 1.7053e+02, 1.6818e+02, 1.6624e+02, 1.6570e+02,
        1.6400e+02, 1.6272e+02, 1.5977e+02, 1.5905e+02, 1.5594e+02, 1.5482e+02,
        1.5369e+02, 1.5329e+02, 1.5183e+02, 1.4901e+02, 1.4790e+02, 1.4679e+02,
        1.4294e+02, 1.4175e+02, 1.4043e+02, 1.3974e+02, 1.3598e+02, 1.3391e+02,
        1.3140e+02, 1.2675e+02, 1.2627e+02, 1.2323e+02, 1.1936e+02, 1.1762e+02,
        1.1511e+02, 1.1329e+02, 1.0806e+02, 1.0527e+02, 1.0211e+02, 9.8650e+01,
        9.4848e+01, 9.2083e+01, 9.1650e+01, 8.7645e+01, 6.8920e+01, 6.5763e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 223]) 

NULL SPACE BASIS :  tensor([[-9.8717e-02, -3.2428e-02, -2.2762e-02,  ...,  9.0699e-04,
          2.0338e-02,  2.0081e-03],
        [ 7.7844e-02, -2.9787e-02,  4.1817e-02,  ..., -2.7967e-03,
         -2.8502e-02, -2.4537e-03],
        [ 6.8986e-02,  4.4627e-02,  3.7516e-02,  ...,  1.7630e-03,
          9.4671e-03,  1.7534e-03],
        ...,
        [ 6.7784e-02,  3.3377e-03, -6.0303e-02,  ...,  2.1806e-03,
         -6.5936e-05,  1.4226e-02],
        [-2.0327e-02,  4.1380e-02, -1.9029e-02,  ..., -2.3807e-02,
         -7.5605e-04, -1.7791e-02],
        [-5.4017e-02, -4.6251e-03,  3.6391e-02,  ...,  2.0918e-02,
         -5.3894e-03,  1.2982e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0215, -0.0167, -0.0006,  ..., -0.0001,  0.0003,  0.0010],
        [-0.0167,  0.0322, -0.0132,  ..., -0.0015,  0.0010, -0.0013],
        [-0.0006, -0.0132,  0.0174,  ...,  0.0013, -0.0014, -0.0002],
        ...,
        [-0.0001, -0.0015,  0.0013,  ...,  0.0195, -0.0106, -0.0026],
        [ 0.0003,  0.0010, -0.0014,  ..., -0.0106,  0.0256, -0.0111],
        [ 0.0010, -0.0013, -0.0002,  ..., -0.0026, -0.0111,  0.0213]],
       device='cuda:0') 

reserving basis 302/576; cond: 297139.0625, radio:0.0038648422341793776
PARAMETER       :  Parameter containing:
tensor([[[[-2.7574e-02, -4.7808e-03,  1.8772e-02],
          [-2.3589e-02, -2.7910e-02, -1.2939e-02],
          [ 1.1397e-02,  1.1957e-02,  8.1419e-03]],

         [[ 1.1813e-03,  1.5457e-02,  5.7839e-03],
          [-5.1417e-03,  5.1965e-04,  1.7863e-02],
          [-1.9595e-02, -1.6525e-02, -4.8144e-02]],

         [[ 1.9167e-03,  4.3135e-02,  4.3405e-02],
          [-3.8093e-03, -3.7120e-02, -3.6341e-03],
          [-3.4911e-02, -4.8434e-03, -2.5183e-02]],

         ...,

         [[-1.7679e-02,  4.4193e-02,  3.8894e-03],
          [-2.7316e-02, -9.8435e-03,  4.1892e-02],
          [-1.4228e-02, -3.9702e-02, -1.7468e-02]],

         [[ 2.8988e-02,  1.6117e-02, -4.5191e-02],
          [ 2.8471e-02,  6.6628e-03,  4.5574e-02],
          [-3.3624e-02,  2.1928e-02, -2.0779e-02]],

         [[ 3.4865e-02,  2.1388e-02,  2.1556e-02],
          [-3.6085e-02, -9.5714e-03,  2.5627e-02],
          [ 3.0912e-02, -1.4771e-02, -2.6265e-02]]],


        [[[-2.7777e-02, -2.3891e-02, -4.6670e-02],
          [ 9.3654e-03, -9.1741e-03,  8.3814e-03],
          [-5.2881e-02,  7.5787e-03,  1.6558e-03]],

         [[-3.2856e-02,  1.9292e-02,  5.2565e-02],
          [ 1.2090e-03,  2.2208e-02, -1.2364e-02],
          [-1.3857e-02,  2.9483e-02,  4.5708e-02]],

         [[-2.1391e-03, -1.6852e-02,  3.0910e-02],
          [-1.5072e-03, -3.1957e-02,  3.6357e-02],
          [ 3.4596e-02,  3.3895e-02, -3.0918e-03]],

         ...,

         [[-7.0649e-03,  2.1442e-02, -2.2307e-02],
          [-2.7644e-02,  2.2532e-02,  1.3937e-02],
          [-3.9043e-02,  2.7734e-02, -4.3286e-02]],

         [[-8.8286e-03,  2.1718e-02, -2.1631e-02],
          [-1.9409e-02, -2.9839e-02, -3.6113e-02],
          [-4.4559e-02, -1.0509e-02, -3.9248e-02]],

         [[ 2.3244e-02, -3.7979e-02, -3.5337e-02],
          [-2.0361e-02, -3.4159e-02, -1.6293e-02],
          [-2.2992e-02,  1.4308e-03,  9.7402e-03]]],


        [[[-6.1263e-03, -5.3955e-03, -1.6544e-02],
          [-2.8887e-04, -4.5098e-02, -3.9312e-02],
          [-4.1318e-02, -2.0782e-03, -1.3766e-02]],

         [[-2.5104e-02, -2.2208e-02, -1.9912e-02],
          [-1.6855e-04, -2.7391e-02, -1.2660e-02],
          [ 1.6166e-02, -2.6107e-02,  1.1795e-03]],

         [[ 4.7230e-02,  4.2001e-02,  2.5030e-03],
          [ 2.9919e-02,  4.9861e-03, -3.9573e-02],
          [ 1.1608e-02,  8.0839e-03, -7.3847e-04]],

         ...,

         [[ 1.3668e-02, -1.3626e-02, -2.0229e-03],
          [ 1.5778e-02,  1.6842e-02, -4.0547e-02],
          [-3.4435e-02,  9.1469e-03, -1.3527e-02]],

         [[ 1.1655e-02,  6.3770e-03, -2.5647e-02],
          [-2.8191e-02,  9.3599e-03,  2.5329e-02],
          [ 4.9026e-03, -2.1358e-02,  1.9401e-04]],

         [[ 1.8164e-02, -1.2064e-02, -3.1311e-02],
          [ 1.1251e-02, -3.4182e-02, -3.3044e-02],
          [ 3.1492e-02, -1.5767e-02,  1.2442e-03]]],


        ...,


        [[[ 1.2442e-02,  3.1710e-02,  1.4565e-02],
          [-9.0594e-04, -1.7514e-02, -1.7987e-02],
          [ 4.8634e-02,  2.3514e-02, -5.4521e-02]],

         [[ 4.4809e-03, -2.5717e-02,  2.8739e-03],
          [-3.1649e-02,  2.6583e-02, -2.0706e-02],
          [ 1.6011e-02, -2.5737e-02,  4.3826e-02]],

         [[ 2.4672e-02, -2.0165e-02,  4.3389e-02],
          [ 3.3068e-02, -3.9694e-02, -3.9369e-02],
          [ 1.7258e-02, -1.1285e-02, -2.7880e-03]],

         ...,

         [[-1.4695e-02, -1.3582e-02,  3.9448e-03],
          [ 1.3988e-02,  9.1376e-03,  1.3826e-02],
          [ 2.3809e-03, -1.0856e-02,  2.8345e-03]],

         [[ 1.4329e-02,  2.1315e-02, -4.6422e-02],
          [-3.1048e-02, -1.9213e-02, -3.5841e-03],
          [ 1.0368e-02,  8.1580e-03,  1.3743e-02]],

         [[ 3.1147e-02,  6.4310e-03,  2.9959e-02],
          [ 1.7222e-02,  4.3436e-02,  3.0466e-03],
          [ 4.0041e-02, -1.0019e-02,  2.0802e-02]]],


        [[[-1.2395e-02,  2.0238e-02,  3.4258e-02],
          [ 1.6075e-02,  2.4402e-02,  7.5462e-03],
          [-4.2485e-03,  2.7598e-02, -5.1642e-02]],

         [[ 3.5643e-02, -1.4454e-02, -2.7397e-02],
          [-3.7686e-02,  5.9739e-03, -3.5207e-02],
          [-4.0058e-02, -4.0094e-03,  4.0304e-02]],

         [[ 2.0441e-02, -4.2549e-02, -2.3344e-02],
          [-3.4809e-02,  2.7313e-02, -1.6134e-02],
          [-3.4303e-02,  1.5161e-02, -3.5698e-03]],

         ...,

         [[-1.7186e-02, -2.8203e-02,  1.4725e-02],
          [ 1.3055e-02,  3.8025e-02,  9.3601e-03],
          [-2.0430e-02, -3.4214e-02, -6.2231e-03]],

         [[-2.0528e-02,  3.7400e-02, -2.4080e-02],
          [ 6.3370e-03, -1.5454e-02,  3.8169e-02],
          [ 4.6426e-02, -2.3359e-03, -1.0468e-02]],

         [[-2.3179e-02,  6.4230e-03,  2.9279e-02],
          [-3.3107e-02, -2.2252e-02,  9.8331e-03],
          [ 1.1182e-02,  1.0915e-02, -2.8165e-03]]],


        [[[ 3.8208e-02,  4.3387e-02,  1.5962e-04],
          [ 2.0527e-02, -1.1643e-02,  2.5024e-02],
          [ 7.4088e-03, -3.4731e-02, -4.9686e-02]],

         [[ 4.6189e-02, -5.1869e-03,  3.2295e-02],
          [ 2.0176e-02,  3.4578e-02,  9.0011e-03],
          [ 2.5972e-04, -8.8377e-03,  3.7784e-02]],

         [[-2.3100e-02,  4.3588e-04, -4.2716e-02],
          [-2.4171e-03, -9.0516e-03,  3.9863e-02],
          [ 2.1235e-02, -1.8844e-02,  2.7065e-02]],

         ...,

         [[-8.8730e-03,  3.7203e-02, -3.4767e-02],
          [-9.6871e-03, -3.7904e-02,  2.7834e-02],
          [-3.7365e-02,  7.6273e-03,  1.6080e-03]],

         [[ 3.3968e-03, -3.5194e-02, -1.5848e-02],
          [-3.6506e-02,  2.6324e-02, -2.1745e-02],
          [-2.3796e-02, -4.0039e-02,  6.9299e-03]],

         [[ 4.1160e-02, -4.9378e-03,  3.5281e-02],
          [ 3.4511e-02, -1.4308e-02,  2.4059e-02],
          [ 7.2066e-05,  2.4858e-02,  2.3786e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([8.5432e+07, 2.3390e+06, 2.0326e+06, 1.7727e+06, 1.5572e+06, 1.0819e+06,
        1.0489e+06, 9.0346e+05, 7.8717e+05, 6.7913e+05, 6.4529e+05, 5.1990e+05,
        5.0154e+05, 3.9301e+05, 3.3179e+05, 2.7140e+05, 2.4310e+05, 1.9839e+05,
        1.8545e+05, 1.7311e+05, 1.6450e+05, 1.5696e+05, 1.5404e+05, 1.3709e+05,
        1.2921e+05, 1.2148e+05, 1.1450e+05, 1.1124e+05, 1.0292e+05, 9.3657e+04,
        9.0201e+04, 8.7171e+04, 8.0757e+04, 7.8748e+04, 6.9026e+04, 6.6132e+04,
        6.5181e+04, 6.0977e+04, 5.8514e+04, 5.7366e+04, 5.6893e+04, 5.3609e+04,
        5.1578e+04, 4.7730e+04, 4.5823e+04, 4.4199e+04, 4.2570e+04, 4.2091e+04,
        4.1262e+04, 3.9996e+04, 3.9098e+04, 3.6856e+04, 3.5401e+04, 3.4735e+04,
        3.3275e+04, 3.2936e+04, 3.2279e+04, 3.1831e+04, 3.1225e+04, 3.0584e+04,
        2.9787e+04, 2.8905e+04, 2.8183e+04, 2.6225e+04, 2.5958e+04, 2.5376e+04,
        2.4728e+04, 2.4529e+04, 2.4268e+04, 2.2931e+04, 2.2773e+04, 2.2540e+04,
        2.2205e+04, 2.1996e+04, 2.1168e+04, 2.1014e+04, 2.0643e+04, 2.0189e+04,
        1.9885e+04, 1.9058e+04, 1.8986e+04, 1.8802e+04, 1.7973e+04, 1.7886e+04,
        1.7718e+04, 1.7363e+04, 1.7112e+04, 1.6662e+04, 1.6251e+04, 1.6173e+04,
        1.5896e+04, 1.5544e+04, 1.5339e+04, 1.5003e+04, 1.4544e+04, 1.4345e+04,
        1.4146e+04, 1.4072e+04, 1.3606e+04, 1.3496e+04, 1.3303e+04, 1.3141e+04,
        1.2963e+04, 1.2827e+04, 1.2722e+04, 1.2518e+04, 1.2268e+04, 1.2133e+04,
        1.2012e+04, 1.1964e+04, 1.1522e+04, 1.1497e+04, 1.1277e+04, 1.0983e+04,
        1.0949e+04, 1.0873e+04, 1.0667e+04, 1.0615e+04, 1.0555e+04, 1.0472e+04,
        1.0211e+04, 1.0140e+04, 1.0086e+04, 9.9870e+03, 9.8460e+03, 9.7216e+03,
        9.6743e+03, 9.5482e+03, 9.3102e+03, 9.2305e+03, 9.1093e+03, 9.0008e+03,
        8.9718e+03, 8.7900e+03, 8.7572e+03, 8.6646e+03, 8.5289e+03, 8.4004e+03,
        8.3266e+03, 8.2627e+03, 8.1900e+03, 8.1880e+03, 8.0740e+03, 7.9634e+03,
        7.8687e+03, 7.7128e+03, 7.6322e+03, 7.6105e+03, 7.5648e+03, 7.4991e+03,
        7.4106e+03, 7.2985e+03, 7.2100e+03, 7.1241e+03, 7.0560e+03, 7.0272e+03,
        6.9107e+03, 6.8342e+03, 6.7586e+03, 6.6712e+03, 6.6204e+03, 6.5928e+03,
        6.5077e+03, 6.5046e+03, 6.4612e+03, 6.4124e+03, 6.3354e+03, 6.2801e+03,
        6.1890e+03, 6.1613e+03, 6.1275e+03, 6.0365e+03, 6.0098e+03, 5.9879e+03,
        5.9644e+03, 5.9059e+03, 5.8906e+03, 5.8423e+03, 5.8000e+03, 5.7497e+03,
        5.6861e+03, 5.6476e+03, 5.6282e+03, 5.5741e+03, 5.5478e+03, 5.5175e+03,
        5.4133e+03, 5.3572e+03, 5.3217e+03, 5.2774e+03, 5.2202e+03, 5.1977e+03,
        5.1123e+03, 5.0908e+03, 5.0646e+03, 5.0413e+03, 4.9736e+03, 4.9058e+03,
        4.8959e+03, 4.8682e+03, 4.8109e+03, 4.7861e+03, 4.7483e+03, 4.7181e+03,
        4.6740e+03, 4.6618e+03, 4.6174e+03, 4.5708e+03, 4.5669e+03, 4.5115e+03,
        4.4866e+03, 4.4496e+03, 4.3897e+03, 4.3393e+03, 4.3328e+03, 4.2795e+03,
        4.2601e+03, 4.2486e+03, 4.2112e+03, 4.1876e+03, 4.1296e+03, 4.1065e+03,
        4.0692e+03, 4.0077e+03, 4.0025e+03, 3.9680e+03, 3.9482e+03, 3.9166e+03,
        3.8724e+03, 3.8555e+03, 3.8302e+03, 3.8231e+03, 3.7923e+03, 3.7675e+03,
        3.7330e+03, 3.7138e+03, 3.6508e+03, 3.6468e+03, 3.6409e+03, 3.6007e+03,
        3.5917e+03, 3.5772e+03, 3.5515e+03, 3.5376e+03, 3.5270e+03, 3.4513e+03,
        3.4449e+03, 3.4202e+03, 3.3978e+03, 3.3894e+03, 3.3616e+03, 3.3142e+03,
        3.3039e+03, 3.2812e+03, 3.2634e+03, 3.2420e+03, 3.2369e+03, 3.1866e+03,
        3.1801e+03, 3.1503e+03, 3.1421e+03, 3.1162e+03, 3.1037e+03, 3.0815e+03,
        3.0617e+03, 3.0521e+03, 3.0183e+03, 2.9946e+03, 2.9747e+03, 2.9672e+03,
        2.9388e+03, 2.9227e+03, 2.9132e+03, 2.8966e+03, 2.8666e+03, 2.8583e+03,
        2.8370e+03, 2.8178e+03, 2.8015e+03, 2.7820e+03, 2.7560e+03, 2.7414e+03,
        2.7318e+03, 2.7145e+03, 2.6865e+03, 2.6745e+03, 2.6591e+03, 2.6484e+03,
        2.6294e+03, 2.6247e+03, 2.6043e+03, 2.5916e+03, 2.5758e+03, 2.5686e+03,
        2.5579e+03, 2.5269e+03, 2.5120e+03, 2.5066e+03, 2.4874e+03, 2.4660e+03,
        2.4499e+03, 2.4392e+03, 2.4009e+03, 2.3973e+03, 2.3880e+03, 2.3812e+03,
        2.3650e+03, 2.3597e+03, 2.3448e+03, 2.3260e+03, 2.3196e+03, 2.3004e+03,
        2.2930e+03, 2.2815e+03, 2.2597e+03, 2.2423e+03, 2.2369e+03, 2.2184e+03,
        2.2125e+03, 2.1878e+03, 2.1843e+03, 2.1714e+03, 2.1599e+03, 2.1488e+03,
        2.1319e+03, 2.1213e+03, 2.1145e+03, 2.1055e+03, 2.0893e+03, 2.0886e+03,
        2.0760e+03, 2.0496e+03, 2.0411e+03, 2.0365e+03, 2.0330e+03, 2.0080e+03,
        2.0071e+03, 1.9931e+03, 1.9760e+03, 1.9680e+03, 1.9563e+03, 1.9503e+03,
        1.9328e+03, 1.9210e+03, 1.9182e+03, 1.9090e+03, 1.9037e+03, 1.8837e+03,
        1.8689e+03, 1.8615e+03, 1.8577e+03, 1.8458e+03, 1.8409e+03, 1.8277e+03,
        1.8204e+03, 1.8061e+03, 1.8032e+03, 1.7931e+03, 1.7773e+03, 1.7693e+03,
        1.7639e+03, 1.7425e+03, 1.7377e+03, 1.7311e+03, 1.7261e+03, 1.7109e+03,
        1.7065e+03, 1.6969e+03, 1.6804e+03, 1.6753e+03, 1.6717e+03, 1.6514e+03,
        1.6472e+03, 1.6424e+03, 1.6357e+03, 1.6280e+03, 1.6153e+03, 1.6076e+03,
        1.5989e+03, 1.5923e+03, 1.5841e+03, 1.5708e+03, 1.5599e+03, 1.5504e+03,
        1.5434e+03, 1.5411e+03, 1.5374e+03, 1.5253e+03, 1.5186e+03, 1.5036e+03,
        1.4990e+03, 1.4911e+03, 1.4783e+03, 1.4772e+03, 1.4688e+03, 1.4604e+03,
        1.4485e+03, 1.4460e+03, 1.4335e+03, 1.4315e+03, 1.4286e+03, 1.4198e+03,
        1.4127e+03, 1.4064e+03, 1.3999e+03, 1.3939e+03, 1.3818e+03, 1.3682e+03,
        1.3634e+03, 1.3576e+03, 1.3542e+03, 1.3424e+03, 1.3357e+03, 1.3247e+03,
        1.3184e+03, 1.3155e+03, 1.3047e+03, 1.2966e+03, 1.2940e+03, 1.2834e+03,
        1.2780e+03, 1.2716e+03, 1.2545e+03, 1.2515e+03, 1.2437e+03, 1.2353e+03,
        1.2292e+03, 1.2232e+03, 1.2218e+03, 1.2189e+03, 1.2129e+03, 1.2057e+03,
        1.1887e+03, 1.1865e+03, 1.1835e+03, 1.1759e+03, 1.1654e+03, 1.1612e+03,
        1.1582e+03, 1.1464e+03, 1.1394e+03, 1.1346e+03, 1.1305e+03, 1.1199e+03,
        1.1125e+03, 1.1069e+03, 1.0993e+03, 1.0954e+03, 1.0924e+03, 1.0801e+03,
        1.0745e+03, 1.0680e+03, 1.0638e+03, 1.0606e+03, 1.0545e+03, 1.0512e+03,
        1.0477e+03, 1.0416e+03, 1.0365e+03, 1.0267e+03, 1.0232e+03, 1.0094e+03,
        1.0060e+03, 9.9986e+02, 9.9784e+02, 9.8962e+02, 9.8230e+02, 9.7804e+02,
        9.7070e+02, 9.6342e+02, 9.5806e+02, 9.5016e+02, 9.4560e+02, 9.4447e+02,
        9.3421e+02, 9.2986e+02, 9.2508e+02, 9.2146e+02, 9.1069e+02, 9.0433e+02,
        9.0241e+02, 8.9923e+02, 8.9222e+02, 8.8988e+02, 8.7696e+02, 8.7251e+02,
        8.6927e+02, 8.6164e+02, 8.5653e+02, 8.4951e+02, 8.4458e+02, 8.3481e+02,
        8.2415e+02, 8.1951e+02, 8.1890e+02, 8.1407e+02, 8.0903e+02, 8.0490e+02,
        7.8819e+02, 7.8760e+02, 7.8376e+02, 7.7926e+02, 7.7421e+02, 7.7369e+02,
        7.7016e+02, 7.5765e+02, 7.5711e+02, 7.5086e+02, 7.4366e+02, 7.3609e+02,
        7.3289e+02, 7.2784e+02, 7.1555e+02, 7.1353e+02, 7.0652e+02, 7.0333e+02,
        7.0042e+02, 6.8926e+02, 6.8187e+02, 6.7788e+02, 6.7624e+02, 6.6846e+02,
        6.6288e+02, 6.5492e+02, 6.4954e+02, 6.4090e+02, 6.3858e+02, 6.3202e+02,
        6.2405e+02, 6.1868e+02, 6.1469e+02, 6.1115e+02, 6.0792e+02, 6.0026e+02,
        5.9783e+02, 5.9315e+02, 5.8184e+02, 5.7929e+02, 5.7873e+02, 5.6852e+02,
        5.6270e+02, 5.6187e+02, 5.5349e+02, 5.5172e+02, 5.4137e+02, 5.3974e+02,
        5.3341e+02, 5.3121e+02, 5.2274e+02, 5.2014e+02, 5.0603e+02, 4.9736e+02,
        4.9344e+02, 4.8669e+02, 4.8267e+02, 4.7427e+02, 4.7261e+02, 4.6717e+02,
        4.5623e+02, 4.4690e+02, 4.4242e+02, 4.3301e+02, 4.2692e+02, 4.2158e+02,
        4.1442e+02, 4.0755e+02, 3.9829e+02, 3.8954e+02, 3.7848e+02, 3.7260e+02,
        3.6159e+02, 3.4980e+02, 3.4063e+02, 3.3016e+02, 3.0634e+02, 2.8751e+02],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 302]) 

NULL SPACE BASIS :  tensor([[-0.0777, -0.0078,  0.0840,  ..., -0.0118, -0.0030,  0.0096],
        [ 0.0464, -0.0100,  0.0067,  ...,  0.0125,  0.0194, -0.0163],
        [ 0.0534,  0.0717,  0.0709,  ..., -0.0095, -0.0135,  0.0103],
        ...,
        [-0.0465,  0.0168,  0.0103,  ...,  0.0179,  0.0319, -0.0047],
        [-0.0564,  0.0227, -0.0100,  ..., -0.0157, -0.0527, -0.0095],
        [ 0.0074, -0.0006, -0.0167,  ...,  0.0049,  0.0168,  0.0083]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0267, -0.0101, -0.0016,  ...,  0.0011, -0.0009, -0.0010],
        [-0.0101,  0.0313, -0.0092,  ..., -0.0009, -0.0009, -0.0008],
        [-0.0016, -0.0092,  0.0220,  ...,  0.0005, -0.0006, -0.0004],
        ...,
        [ 0.0011, -0.0009,  0.0005,  ...,  0.0320, -0.0086, -0.0042],
        [-0.0009, -0.0009, -0.0006,  ..., -0.0086,  0.0367, -0.0090],
        [-0.0010, -0.0008, -0.0004,  ..., -0.0042, -0.0090,  0.0292]],
       device='cuda:0') 

reserving basis 273/576; cond: 511057.15625, radio:0.0020982332061976194
PARAMETER       :  Parameter containing:
tensor([[[[-2.8037e-02, -1.2399e-02, -2.4606e-02],
          [ 3.2710e-02, -1.6350e-02, -2.3828e-02],
          [ 1.6239e-02, -6.6669e-03,  4.5394e-02]],

         [[ 3.5564e-02, -7.5803e-03, -2.6314e-02],
          [ 3.1837e-02,  6.9471e-03,  3.5734e-02],
          [ 8.3684e-03, -1.9997e-02,  2.7182e-03]],

         [[-8.4521e-03, -2.2208e-02, -1.9079e-02],
          [ 2.5424e-02, -4.5331e-02,  2.1880e-02],
          [-3.2998e-02,  3.3672e-03, -1.0983e-02]],

         ...,

         [[-1.7203e-02,  1.5465e-02,  2.5030e-02],
          [ 1.4508e-02, -1.3718e-02, -1.0202e-02],
          [ 1.4794e-02, -1.4312e-02, -3.2677e-02]],

         [[ 2.4839e-03,  3.1404e-02,  4.2501e-02],
          [ 3.5860e-03,  2.5126e-02,  2.4164e-02],
          [ 6.3256e-03,  1.0083e-02,  3.4018e-02]],

         [[-4.0179e-02, -2.2552e-02, -1.3718e-02],
          [-2.1375e-02, -2.2414e-02,  2.3783e-02],
          [-3.6013e-02, -2.5387e-03, -3.7810e-02]]],


        [[[ 5.6323e-03, -1.7614e-02, -5.0113e-02],
          [-1.1619e-02, -2.3577e-02, -4.5144e-02],
          [-5.3238e-03, -1.8308e-02,  1.2096e-02]],

         [[-1.3975e-02, -5.5478e-03,  9.2218e-03],
          [ 2.0257e-02,  3.9770e-02,  2.5490e-02],
          [-1.2063e-02, -4.3934e-03,  8.1564e-03]],

         [[-2.0431e-02,  1.0445e-02, -1.8757e-02],
          [ 2.8929e-02, -3.3501e-02,  3.8883e-02],
          [-1.3207e-02,  6.9051e-03, -6.8718e-04]],

         ...,

         [[ 4.1526e-03,  4.2287e-02, -1.3708e-02],
          [-1.4827e-03, -9.0784e-04, -2.0470e-02],
          [-2.6027e-02, -1.0441e-02, -1.2155e-02]],

         [[-5.1292e-02,  3.3006e-03,  1.4810e-02],
          [ 1.3690e-02, -1.4873e-03, -1.0941e-02],
          [ 4.8248e-02, -9.0860e-03,  3.5853e-04]],

         [[-4.2248e-02,  9.0598e-03,  1.4501e-02],
          [-1.4622e-02,  2.7744e-02, -9.8169e-03],
          [ 2.2878e-02,  3.6168e-02,  3.7400e-02]]],


        [[[-3.8648e-02,  2.3095e-02,  1.3228e-02],
          [ 2.9833e-02,  1.7603e-02,  3.0728e-02],
          [ 3.3367e-02,  1.5982e-02,  2.2657e-02]],

         [[-5.8779e-03,  4.4687e-03, -7.5398e-03],
          [ 2.3322e-02, -2.1372e-02,  1.9975e-02],
          [-8.2148e-03, -3.6540e-02, -2.6504e-02]],

         [[ 4.2501e-02, -1.5820e-02, -9.6518e-03],
          [ 1.1241e-02,  3.6147e-02, -5.6416e-03],
          [-2.0788e-02,  1.2577e-02,  3.8432e-02]],

         ...,

         [[ 1.9280e-02, -2.4622e-02,  2.3469e-02],
          [ 4.3336e-02,  1.2523e-02, -1.9816e-02],
          [ 3.4227e-02,  2.4798e-02, -1.3612e-02]],

         [[ 2.0711e-02, -3.9800e-02, -1.7394e-02],
          [-3.0213e-02,  2.7625e-02,  3.9746e-03],
          [ 3.3072e-03,  2.9654e-02,  2.1779e-02]],

         [[-2.3794e-02,  7.6693e-03,  7.9529e-03],
          [-2.4424e-02,  3.7897e-03, -3.3791e-02],
          [ 4.1183e-03,  1.6757e-02,  4.7469e-03]]],


        ...,


        [[[-1.5397e-02, -2.7087e-02, -3.5556e-02],
          [-1.2262e-03,  1.2372e-02, -4.9453e-02],
          [-1.0296e-02, -9.6556e-03, -3.8576e-02]],

         [[-2.1314e-02,  2.6506e-02,  3.8602e-02],
          [-1.5166e-02, -3.0072e-02, -3.2933e-02],
          [-1.1810e-02,  3.8496e-02,  4.2287e-02]],

         [[-2.9262e-03, -1.8660e-02,  3.9342e-02],
          [-5.7131e-03,  3.0555e-02,  3.9351e-02],
          [ 1.5056e-03,  4.2649e-02,  1.7960e-02]],

         ...,

         [[ 3.0703e-02, -4.3494e-02, -3.5877e-02],
          [ 1.5897e-02,  2.1074e-02, -9.2246e-03],
          [ 1.7054e-03, -4.8132e-03,  8.6629e-03]],

         [[ 2.2405e-02,  4.2996e-05,  8.7269e-03],
          [ 2.6442e-02,  9.8995e-03, -4.0721e-02],
          [ 4.5227e-02,  3.6967e-02, -3.8414e-02]],

         [[ 2.5760e-03,  4.2702e-02, -5.2108e-04],
          [ 3.0709e-02, -3.4051e-02, -6.7452e-03],
          [ 2.9533e-03, -3.3252e-02,  1.8450e-02]]],


        [[[ 3.6689e-02, -1.1731e-02,  4.5507e-02],
          [-4.8107e-02,  1.9765e-02, -1.4187e-02],
          [ 3.1090e-02, -1.3544e-02, -2.0698e-02]],

         [[ 2.6459e-02, -2.3486e-02, -1.1972e-02],
          [ 2.3600e-02,  8.7046e-03, -2.2455e-02],
          [ 5.1435e-02, -2.3594e-02,  4.5932e-02]],

         [[-2.9784e-02, -3.5702e-02,  2.8161e-02],
          [ 1.6664e-02, -3.7511e-02, -5.3570e-02],
          [-3.8570e-02, -6.0932e-04, -1.1796e-02]],

         ...,

         [[-2.0050e-02,  1.9025e-02,  3.1945e-02],
          [-3.4938e-02,  9.5039e-03, -8.9667e-03],
          [ 3.1279e-02, -3.3892e-02,  3.6343e-02]],

         [[-4.5431e-03, -1.6726e-02, -3.1971e-02],
          [ 2.3175e-02, -1.0612e-02,  1.2447e-02],
          [-3.2979e-04,  3.4705e-02,  1.2900e-02]],

         [[-3.0648e-02, -1.5091e-02, -4.1601e-02],
          [ 2.7389e-02, -4.2018e-02, -2.9506e-02],
          [-3.6262e-02,  4.3724e-02,  1.6911e-02]]],


        [[[-1.1169e-03, -5.1548e-03,  1.6113e-02],
          [-3.9000e-02, -5.6870e-03, -7.6054e-03],
          [ 2.4539e-02,  4.4649e-03,  3.6196e-02]],

         [[-2.3583e-02,  2.7826e-02,  1.4997e-02],
          [ 3.9085e-02,  2.6362e-02,  1.9038e-02],
          [-4.8646e-03,  2.3716e-02,  1.2354e-02]],

         [[-1.4438e-02, -2.6258e-02,  3.4833e-02],
          [-3.7038e-02,  1.9991e-02,  1.6798e-02],
          [ 2.8619e-03, -2.7261e-02, -3.3328e-02]],

         ...,

         [[ 5.1478e-02, -2.0014e-02,  2.7521e-02],
          [ 3.3030e-02, -3.9063e-02,  2.7320e-02],
          [-8.0872e-03, -4.1712e-02, -1.9839e-02]],

         [[-1.6085e-02, -9.1182e-04, -1.8559e-02],
          [-3.0990e-02, -4.8461e-02, -3.8875e-02],
          [ 2.2331e-02,  1.8640e-02, -1.4502e-02]],

         [[-9.1205e-03, -1.3559e-02,  4.7098e-02],
          [ 2.3664e-02,  1.8495e-02,  2.5882e-03],
          [ 1.1643e-02,  3.0744e-02, -1.3876e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.6541e+07, 8.3019e+05, 7.5444e+05, 4.8390e+05, 3.4737e+05, 1.9880e+05,
        1.8638e+05, 1.2460e+05, 1.1135e+05, 8.6827e+04, 5.5080e+04, 4.6644e+04,
        4.2733e+04, 4.1157e+04, 3.8194e+04, 3.5394e+04, 3.4173e+04, 3.2450e+04,
        2.9685e+04, 2.7204e+04, 2.6066e+04, 2.3771e+04, 2.2945e+04, 2.0542e+04,
        1.9987e+04, 1.8961e+04, 1.7799e+04, 1.7200e+04, 1.5656e+04, 1.4915e+04,
        1.4278e+04, 1.3845e+04, 1.3212e+04, 1.2353e+04, 1.1611e+04, 1.1116e+04,
        1.0265e+04, 1.0067e+04, 9.2299e+03, 8.8095e+03, 8.1328e+03, 7.7320e+03,
        7.6718e+03, 7.4883e+03, 7.2981e+03, 7.0541e+03, 6.7732e+03, 6.3097e+03,
        6.1975e+03, 6.1004e+03, 5.8920e+03, 5.4723e+03, 5.2617e+03, 5.2258e+03,
        5.0406e+03, 4.9330e+03, 4.7479e+03, 4.5789e+03, 4.5373e+03, 4.4533e+03,
        4.3171e+03, 4.2660e+03, 4.2108e+03, 4.1130e+03, 4.0947e+03, 3.8715e+03,
        3.8226e+03, 3.7226e+03, 3.6289e+03, 3.5314e+03, 3.4613e+03, 3.3580e+03,
        3.3148e+03, 3.2769e+03, 3.1796e+03, 3.1027e+03, 3.0888e+03, 3.0011e+03,
        2.9315e+03, 2.8886e+03, 2.8498e+03, 2.7586e+03, 2.6979e+03, 2.5981e+03,
        2.5718e+03, 2.5168e+03, 2.4802e+03, 2.4305e+03, 2.4061e+03, 2.3848e+03,
        2.3289e+03, 2.2947e+03, 2.2645e+03, 2.2355e+03, 2.1785e+03, 2.1633e+03,
        2.1034e+03, 2.0728e+03, 2.0304e+03, 1.9968e+03, 1.9834e+03, 1.9308e+03,
        1.8696e+03, 1.8486e+03, 1.8266e+03, 1.8139e+03, 1.7861e+03, 1.7737e+03,
        1.7386e+03, 1.7247e+03, 1.7001e+03, 1.6755e+03, 1.6435e+03, 1.6294e+03,
        1.6024e+03, 1.5655e+03, 1.5461e+03, 1.5277e+03, 1.5036e+03, 1.4802e+03,
        1.4687e+03, 1.4639e+03, 1.4419e+03, 1.4237e+03, 1.4163e+03, 1.3971e+03,
        1.3883e+03, 1.3661e+03, 1.3534e+03, 1.3369e+03, 1.3138e+03, 1.3012e+03,
        1.2903e+03, 1.2763e+03, 1.2606e+03, 1.2445e+03, 1.2277e+03, 1.2086e+03,
        1.1921e+03, 1.1824e+03, 1.1763e+03, 1.1743e+03, 1.1391e+03, 1.1270e+03,
        1.1230e+03, 1.1142e+03, 1.0892e+03, 1.0792e+03, 1.0764e+03, 1.0695e+03,
        1.0477e+03, 1.0376e+03, 1.0291e+03, 1.0082e+03, 1.0033e+03, 9.9738e+02,
        9.8487e+02, 9.7819e+02, 9.6873e+02, 9.6439e+02, 9.4337e+02, 9.4016e+02,
        9.2795e+02, 9.1987e+02, 9.1257e+02, 8.9442e+02, 8.9072e+02, 8.8634e+02,
        8.7641e+02, 8.7175e+02, 8.6242e+02, 8.5093e+02, 8.4544e+02, 8.4045e+02,
        8.3495e+02, 8.1597e+02, 8.1300e+02, 8.0279e+02, 7.9867e+02, 7.8875e+02,
        7.8384e+02, 7.7673e+02, 7.7280e+02, 7.6860e+02, 7.5954e+02, 7.5074e+02,
        7.4588e+02, 7.3957e+02, 7.3373e+02, 7.2375e+02, 7.1161e+02, 7.0881e+02,
        7.0414e+02, 6.9623e+02, 6.9327e+02, 6.8289e+02, 6.7607e+02, 6.7166e+02,
        6.6442e+02, 6.5906e+02, 6.5389e+02, 6.5183e+02, 6.4762e+02, 6.4438e+02,
        6.3186e+02, 6.3149e+02, 6.2548e+02, 6.2435e+02, 6.1629e+02, 6.0907e+02,
        6.0560e+02, 5.9787e+02, 5.9539e+02, 5.9380e+02, 5.9107e+02, 5.8507e+02,
        5.8126e+02, 5.7572e+02, 5.7505e+02, 5.6857e+02, 5.6702e+02, 5.6430e+02,
        5.6253e+02, 5.6023e+02, 5.5378e+02, 5.4639e+02, 5.4474e+02, 5.4037e+02,
        5.3455e+02, 5.3341e+02, 5.2499e+02, 5.1956e+02, 5.1562e+02, 5.1266e+02,
        5.0795e+02, 5.0391e+02, 5.0002e+02, 4.9895e+02, 4.9475e+02, 4.9434e+02,
        4.8819e+02, 4.8687e+02, 4.8487e+02, 4.8116e+02, 4.7462e+02, 4.7140e+02,
        4.6640e+02, 4.6331e+02, 4.5900e+02, 4.5698e+02, 4.5569e+02, 4.5164e+02,
        4.4713e+02, 4.4152e+02, 4.4013e+02, 4.3667e+02, 4.3495e+02, 4.3186e+02,
        4.2750e+02, 4.2368e+02, 4.2031e+02, 4.1974e+02, 4.1399e+02, 4.1110e+02,
        4.0944e+02, 4.0670e+02, 4.0428e+02, 4.0066e+02, 3.9885e+02, 3.9721e+02,
        3.9557e+02, 3.9301e+02, 3.9245e+02, 3.8845e+02, 3.8641e+02, 3.8594e+02,
        3.7909e+02, 3.7794e+02, 3.7702e+02, 3.7430e+02, 3.7377e+02, 3.7112e+02,
        3.6782e+02, 3.6417e+02, 3.6317e+02, 3.6092e+02, 3.5824e+02, 3.5449e+02,
        3.5321e+02, 3.5054e+02, 3.5033e+02, 3.4656e+02, 3.4592e+02, 3.4390e+02,
        3.3953e+02, 3.3667e+02, 3.3599e+02, 3.3427e+02, 3.3248e+02, 3.3049e+02,
        3.2901e+02, 3.2497e+02, 3.2442e+02, 3.2167e+02, 3.2022e+02, 3.1786e+02,
        3.1575e+02, 3.1458e+02, 3.1381e+02, 3.1144e+02, 3.0889e+02, 3.0667e+02,
        3.0575e+02, 3.0266e+02, 3.0188e+02, 2.9954e+02, 2.9795e+02, 2.9574e+02,
        2.9466e+02, 2.9282e+02, 2.9206e+02, 2.9138e+02, 2.8981e+02, 2.8736e+02,
        2.8580e+02, 2.8452e+02, 2.8345e+02, 2.7832e+02, 2.7715e+02, 2.7504e+02,
        2.7449e+02, 2.7361e+02, 2.7125e+02, 2.7080e+02, 2.6909e+02, 2.6757e+02,
        2.6423e+02, 2.6321e+02, 2.6138e+02, 2.6097e+02, 2.5975e+02, 2.5880e+02,
        2.5619e+02, 2.5404e+02, 2.5266e+02, 2.4997e+02, 2.4834e+02, 2.4806e+02,
        2.4716e+02, 2.4493e+02, 2.4386e+02, 2.4308e+02, 2.3965e+02, 2.3916e+02,
        2.3796e+02, 2.3640e+02, 2.3567e+02, 2.3513e+02, 2.3216e+02, 2.3176e+02,
        2.3025e+02, 2.2999e+02, 2.2871e+02, 2.2688e+02, 2.2639e+02, 2.2543e+02,
        2.2380e+02, 2.2125e+02, 2.2057e+02, 2.1953e+02, 2.1830e+02, 2.1803e+02,
        2.1577e+02, 2.1488e+02, 2.1246e+02, 2.1014e+02, 2.0943e+02, 2.0773e+02,
        2.0703e+02, 2.0647e+02, 2.0591e+02, 2.0499e+02, 2.0364e+02, 2.0224e+02,
        2.0138e+02, 2.0033e+02, 1.9958e+02, 1.9950e+02, 1.9818e+02, 1.9737e+02,
        1.9570e+02, 1.9380e+02, 1.9296e+02, 1.9173e+02, 1.9163e+02, 1.9090e+02,
        1.8902e+02, 1.8778e+02, 1.8728e+02, 1.8601e+02, 1.8563e+02, 1.8341e+02,
        1.8222e+02, 1.8143e+02, 1.8097e+02, 1.7951e+02, 1.7746e+02, 1.7686e+02,
        1.7617e+02, 1.7473e+02, 1.7419e+02, 1.7377e+02, 1.7314e+02, 1.7186e+02,
        1.7073e+02, 1.7060e+02, 1.6990e+02, 1.6854e+02, 1.6813e+02, 1.6703e+02,
        1.6569e+02, 1.6490e+02, 1.6391e+02, 1.6288e+02, 1.6205e+02, 1.6188e+02,
        1.6057e+02, 1.5957e+02, 1.5822e+02, 1.5801e+02, 1.5674e+02, 1.5509e+02,
        1.5470e+02, 1.5382e+02, 1.5363e+02, 1.5322e+02, 1.5265e+02, 1.5128e+02,
        1.5043e+02, 1.4960e+02, 1.4914e+02, 1.4855e+02, 1.4725e+02, 1.4679e+02,
        1.4640e+02, 1.4561e+02, 1.4501e+02, 1.4421e+02, 1.4385e+02, 1.4129e+02,
        1.4089e+02, 1.4001e+02, 1.3898e+02, 1.3881e+02, 1.3714e+02, 1.3656e+02,
        1.3599e+02, 1.3386e+02, 1.3351e+02, 1.3326e+02, 1.3259e+02, 1.3139e+02,
        1.3089e+02, 1.3057e+02, 1.2942e+02, 1.2815e+02, 1.2717e+02, 1.2690e+02,
        1.2638e+02, 1.2551e+02, 1.2510e+02, 1.2373e+02, 1.2333e+02, 1.2218e+02,
        1.2164e+02, 1.2015e+02, 1.1991e+02, 1.1880e+02, 1.1857e+02, 1.1794e+02,
        1.1713e+02, 1.1642e+02, 1.1516e+02, 1.1449e+02, 1.1406e+02, 1.1271e+02,
        1.1201e+02, 1.1071e+02, 1.1013e+02, 1.0907e+02, 1.0879e+02, 1.0788e+02,
        1.0688e+02, 1.0573e+02, 1.0506e+02, 1.0457e+02, 1.0365e+02, 1.0269e+02,
        1.0205e+02, 1.0134e+02, 1.0094e+02, 1.0016e+02, 9.9145e+01, 9.8921e+01,
        9.7386e+01, 9.7292e+01, 9.6358e+01, 9.5367e+01, 9.4434e+01, 9.3818e+01,
        9.3238e+01, 9.2951e+01, 9.2414e+01, 9.1618e+01, 9.0419e+01, 9.0148e+01,
        8.9774e+01, 8.8949e+01, 8.8272e+01, 8.7903e+01, 8.6431e+01, 8.5992e+01,
        8.5589e+01, 8.4917e+01, 8.4176e+01, 8.3437e+01, 8.3065e+01, 8.2261e+01,
        8.1627e+01, 8.0785e+01, 7.9742e+01, 7.9310e+01, 7.8718e+01, 7.8430e+01,
        7.7949e+01, 7.7265e+01, 7.6474e+01, 7.5602e+01, 7.4627e+01, 7.4415e+01,
        7.3645e+01, 7.2352e+01, 7.2076e+01, 7.1555e+01, 7.0333e+01, 6.9750e+01,
        6.8987e+01, 6.7677e+01, 6.7531e+01, 6.7122e+01, 6.6499e+01, 6.4351e+01,
        6.3450e+01, 6.3279e+01, 6.2322e+01, 6.0697e+01, 5.9790e+01, 5.9424e+01,
        5.8544e+01, 5.7280e+01, 5.5608e+01, 5.5357e+01, 5.3095e+01, 5.2088e+01,
        5.2005e+01, 5.0026e+01, 4.8839e+01, 4.8288e+01, 4.8067e+01, 4.5914e+01,
        4.4625e+01, 4.4136e+01, 4.2969e+01, 4.1866e+01, 3.6087e+01, 3.2365e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 273]) 

NULL SPACE BASIS :  tensor([[-3.9294e-05, -6.0921e-03,  2.3884e-02,  ...,  3.8185e-03,
          1.1059e-03,  1.1474e-02],
        [-3.7661e-02,  1.3149e-02, -5.6373e-02,  ..., -5.6326e-03,
          1.8928e-03, -1.2549e-02],
        [ 2.0660e-02,  1.0459e-02,  7.0474e-02,  ...,  4.6499e-03,
         -5.9670e-03,  2.5491e-03],
        ...,
        [-3.6979e-02,  3.1869e-02, -2.0834e-02,  ...,  8.1187e-03,
          4.4186e-03,  2.0655e-03],
        [ 1.7114e-02,  4.4062e-03,  6.8981e-02,  ..., -4.1514e-03,
         -1.2867e-04,  1.4884e-02],
        [-5.9160e-02,  8.8114e-03, -7.5101e-03,  ...,  2.3680e-03,
          8.2768e-04, -1.2581e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.9124e-02, -1.6758e-02,  2.6368e-03,  ..., -1.4170e-03,
          1.3853e-03,  1.0958e-04],
        [-1.6758e-02,  3.0612e-02, -1.4151e-02,  ...,  2.6217e-04,
         -6.2172e-04, -5.0903e-04],
        [ 2.6368e-03, -1.4151e-02,  1.5877e-02,  ...,  1.0035e-03,
         -3.9650e-05,  1.5354e-04],
        ...,
        [-1.4170e-03,  2.6217e-04,  1.0035e-03,  ...,  1.2349e-02,
         -7.5005e-03, -5.2443e-04],
        [ 1.3853e-03, -6.2172e-04, -3.9650e-05,  ..., -7.5005e-03,
          1.7124e-02, -6.3877e-03],
        [ 1.0958e-04, -5.0903e-04,  1.5354e-04,  ..., -5.2443e-04,
         -6.3877e-03,  1.2045e-02]], device='cuda:0') 

reserving basis 725/1152; cond: 369170.6875, radio:0.007577909156680107
PARAMETER       :  Parameter containing:
tensor([[[[ 1.4759e-02,  2.9965e-02, -1.8470e-02],
          [-4.5213e-04, -1.6271e-02,  1.0358e-02],
          [ 1.8654e-02,  1.8904e-02, -3.1805e-03]],

         [[-1.9235e-02, -2.2896e-02, -2.5649e-02],
          [-2.3259e-02, -2.3307e-02,  6.5296e-03],
          [-1.6056e-02,  8.5344e-03, -2.8814e-02]],

         [[ 9.9045e-04, -1.3659e-02,  4.2846e-03],
          [ 7.9646e-03,  1.5141e-02, -1.5423e-02],
          [-1.5246e-02, -2.5740e-02,  2.7100e-02]],

         ...,

         [[-1.2929e-02,  1.7992e-03,  5.1695e-03],
          [-2.9875e-02, -2.0173e-02, -1.1926e-03],
          [ 6.9982e-03,  1.5847e-02,  2.9006e-02]],

         [[ 1.0035e-02, -3.3233e-02, -1.8329e-02],
          [ 2.3405e-02,  1.5741e-02,  8.0945e-04],
          [ 1.5654e-02, -1.4702e-02,  3.4649e-02]],

         [[ 1.9098e-02,  6.4726e-03,  1.5122e-02],
          [-2.2050e-02, -3.3373e-02, -2.6159e-02],
          [-2.8858e-02,  1.9287e-02, -1.5643e-02]]],


        [[[ 2.5366e-02,  1.1611e-03,  7.5974e-03],
          [-1.4667e-02, -1.1545e-02,  2.2523e-02],
          [-2.5841e-02, -2.8884e-02, -1.5008e-02]],

         [[ 8.1826e-03, -2.3314e-02, -2.2659e-02],
          [ 9.2325e-03, -1.2358e-02,  1.5695e-03],
          [-2.7389e-02,  6.0080e-03, -1.3153e-02]],

         [[ 7.7179e-04, -1.7914e-02, -1.9615e-02],
          [-4.2570e-03,  1.4063e-02, -2.9660e-02],
          [ 2.5663e-02,  1.7653e-02,  1.8868e-03]],

         ...,

         [[ 5.3722e-03,  1.3821e-02, -4.1407e-03],
          [-2.0662e-02,  2.5825e-02,  2.6450e-02],
          [ 1.5375e-02,  2.0344e-02,  2.1312e-02]],

         [[ 2.4962e-02,  2.2358e-03,  7.6079e-03],
          [-7.8438e-03, -2.1210e-02, -2.6417e-02],
          [ 4.2373e-02, -1.8850e-02, -2.9314e-02]],

         [[-1.4265e-02, -1.7533e-02, -3.1676e-02],
          [-2.2139e-02, -1.2829e-02, -7.5010e-03],
          [ 8.4994e-03, -1.0369e-02, -1.5318e-03]]],


        [[[-1.6821e-02,  1.8689e-02, -2.8279e-02],
          [-4.6078e-03,  1.7795e-02, -2.1541e-03],
          [-3.2789e-06,  2.7900e-02,  1.6720e-02]],

         [[ 4.6809e-03, -1.2305e-02,  2.4132e-02],
          [-4.6452e-03, -9.7857e-03,  4.8692e-03],
          [-2.1176e-02,  5.2548e-03,  9.4239e-03]],

         [[-1.7291e-02, -1.4963e-02, -1.2607e-02],
          [-2.6386e-02, -2.7207e-02, -3.1154e-02],
          [-1.3297e-02, -1.8034e-02,  1.6314e-02]],

         ...,

         [[ 2.4752e-02,  8.0513e-03, -1.7560e-02],
          [-3.1208e-02,  1.5244e-02,  4.5228e-03],
          [ 1.9609e-02, -1.6327e-02, -3.2611e-02]],

         [[-4.0692e-02,  2.0249e-03, -1.1444e-02],
          [-1.7119e-02,  1.6815e-02,  8.1620e-03],
          [-1.4012e-02,  7.5869e-03, -1.4374e-02]],

         [[-1.3968e-02, -1.9017e-02,  1.1072e-02],
          [-4.1703e-03, -9.7597e-03,  5.8231e-03],
          [ 1.0930e-02, -1.7632e-02,  3.1179e-02]]],


        ...,


        [[[ 2.7081e-02,  2.9456e-04,  3.1526e-02],
          [ 2.3900e-02,  1.6703e-02, -2.9796e-02],
          [ 3.0285e-03, -4.3584e-04,  2.7646e-02]],

         [[ 8.9693e-03, -8.2112e-03,  1.2318e-02],
          [ 4.8271e-03, -2.4766e-02,  4.6859e-03],
          [-1.5419e-02, -7.6952e-03,  1.5715e-02]],

         [[ 1.7142e-02,  1.2929e-02, -9.2043e-03],
          [-1.3642e-02,  2.7820e-02,  6.1295e-03],
          [ 5.5752e-03, -7.4129e-03, -1.3826e-02]],

         ...,

         [[-2.9312e-02,  1.4204e-02, -1.5097e-02],
          [ 7.3428e-03, -2.8358e-02, -1.6115e-02],
          [-1.8995e-02,  3.1027e-03,  7.9066e-03]],

         [[ 1.9174e-02,  1.7983e-02,  2.8395e-02],
          [-2.1794e-02,  3.8533e-04,  1.5911e-02],
          [-5.3453e-03,  1.0270e-02,  4.5616e-03]],

         [[-1.4755e-02, -2.5977e-03,  2.3701e-02],
          [ 1.5838e-02,  9.2661e-03,  3.1015e-02],
          [ 7.6340e-03,  9.8613e-03,  1.9807e-02]]],


        [[[-3.2403e-03, -1.1184e-02,  5.5013e-03],
          [-2.8131e-02, -8.2738e-03,  1.9999e-02],
          [ 1.0625e-02, -2.6676e-02, -2.1885e-02]],

         [[-1.3588e-02, -1.5587e-02,  2.4431e-02],
          [-1.9643e-02, -3.4758e-02, -4.7564e-03],
          [-2.5035e-02, -8.6667e-04, -2.2684e-02]],

         [[-1.6216e-02, -2.4628e-03, -2.1092e-02],
          [-1.1652e-02,  1.4972e-02,  2.3328e-02],
          [ 8.1870e-03, -2.0072e-02,  2.0098e-02]],

         ...,

         [[-1.4562e-02,  1.1560e-02,  5.1546e-03],
          [-2.9627e-02, -2.0901e-02, -1.3654e-02],
          [-2.3408e-02, -5.0082e-02, -1.7363e-02]],

         [[-2.2693e-03, -6.5566e-03, -1.2452e-02],
          [-3.0857e-02,  3.0572e-02,  2.9840e-02],
          [-3.7022e-02,  2.2635e-02, -7.8543e-03]],

         [[ 2.0476e-02, -2.3678e-02, -9.5605e-03],
          [ 2.8693e-02, -1.6765e-02,  3.1411e-02],
          [-5.3073e-03,  1.7823e-02,  9.5132e-04]]],


        [[[ 4.3987e-03, -2.7627e-02,  2.8278e-02],
          [-2.4838e-02,  3.8084e-03, -3.6767e-02],
          [-1.8987e-03, -2.7949e-02, -1.8424e-02]],

         [[ 1.5962e-02,  1.5836e-02, -1.4694e-02],
          [ 4.7645e-03,  1.4575e-02,  9.9803e-03],
          [-1.5972e-02, -2.5872e-02,  2.3988e-02]],

         [[-9.7042e-03, -8.9997e-03,  6.8203e-03],
          [-1.9524e-02,  2.9704e-04,  2.5150e-02],
          [-1.7391e-02,  2.4686e-02, -4.7676e-03]],

         ...,

         [[ 2.7359e-02,  9.9454e-03, -8.4277e-03],
          [-1.6579e-02,  3.0048e-02,  1.4332e-02],
          [ 2.8359e-02, -1.1171e-02, -1.7230e-03]],

         [[-1.4924e-02,  1.0335e-02, -4.2069e-03],
          [-2.2077e-02,  3.2056e-02,  2.8068e-02],
          [ 1.6614e-02,  1.2814e-02,  7.1221e-03]],

         [[ 1.0751e-02,  3.8144e-02, -1.7951e-02],
          [-1.4740e-02,  1.1946e-02,  3.1959e-02],
          [ 4.0883e-03, -2.6153e-02, -1.8729e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.2142e+07, 1.2533e+06, 1.1892e+06,  ..., 1.1755e+02, 1.0312e+02,
        8.7065e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 725]) 

NULL SPACE BASIS :  tensor([[-0.0156,  0.0257,  0.0522,  ..., -0.0182, -0.0137,  0.0081],
        [-0.0081, -0.0016,  0.0046,  ...,  0.0066,  0.0161, -0.0027],
        [-0.0046,  0.0210,  0.0150,  ...,  0.0139, -0.0041, -0.0020],
        ...,
        [-0.0028,  0.0169, -0.0106,  ..., -0.0026,  0.0051, -0.0059],
        [ 0.0360,  0.0025, -0.0288,  ..., -0.0050, -0.0158, -0.0065],
        [ 0.0633,  0.0049,  0.0265,  ...,  0.0203,  0.0135,  0.0098]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.9074e-02, -1.2662e-03, -5.3294e-04,  ..., -1.6295e-04,
          5.7977e-04,  1.6081e-05],
        [-1.2662e-03,  3.0442e-02, -8.8600e-04,  ...,  1.3226e-04,
          5.3022e-05,  6.0850e-04],
        [-5.3294e-04, -8.8600e-04,  3.0018e-02,  ...,  2.8419e-05,
          2.4584e-04,  2.8779e-04],
        ...,
        [-1.6295e-04,  1.3226e-04,  2.8419e-05,  ...,  2.2161e-02,
         -3.2339e-03, -1.4203e-03],
        [ 5.7977e-04,  5.3022e-05,  2.4584e-04,  ..., -3.2339e-03,
          2.3300e-02, -2.9466e-03],
        [ 1.6081e-05,  6.0850e-04,  2.8779e-04,  ..., -1.4203e-03,
         -2.9466e-03,  2.4424e-02]], device='cuda:0') 

reserving basis 44/64; cond: 7225.689453125, radio:0.017804952338337898
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0156]],

         [[ 0.1027]],

         [[ 0.0586]],

         ...,

         [[-0.0886]],

         [[-0.0863]],

         [[-0.0395]]],


        [[[ 0.0268]],

         [[-0.0193]],

         [[-0.0883]],

         ...,

         [[-0.0437]],

         [[ 0.0203]],

         [[ 0.0288]]],


        [[[ 0.0299]],

         [[-0.0658]],

         [[-0.0061]],

         ...,

         [[-0.1412]],

         [[ 0.0928]],

         [[ 0.0225]]],


        ...,


        [[[ 0.0272]],

         [[-0.0852]],

         [[-0.1007]],

         ...,

         [[-0.0357]],

         [[ 0.0598]],

         [[-0.0120]]],


        [[[ 0.0233]],

         [[-0.0536]],

         [[ 0.1306]],

         ...,

         [[ 0.0010]],

         [[ 0.0286]],

         [[ 0.0655]]],


        [[[-0.0006]],

         [[-0.0019]],

         [[ 0.0874]],

         ...,

         [[ 0.0027]],

         [[ 0.0537]],

         [[ 0.0876]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.0755e+06, 1.1637e+05, 9.7897e+04, 4.5414e+04, 3.0256e+04, 2.4104e+04,
        1.7175e+04, 1.4353e+04, 9.0318e+03, 8.0529e+03, 7.2614e+03, 6.8293e+03,
        5.9506e+03, 5.2705e+03, 4.8926e+03, 4.5645e+03, 4.0568e+03, 4.0461e+03,
        3.4768e+03, 3.3106e+03, 2.7779e+03, 2.6093e+03, 2.2600e+03, 2.1341e+03,
        2.0337e+03, 2.0114e+03, 1.8946e+03, 1.6507e+03, 1.5741e+03, 1.4736e+03,
        1.3661e+03, 1.3026e+03, 1.2759e+03, 1.2359e+03, 1.1360e+03, 1.0910e+03,
        1.0400e+03, 1.0127e+03, 9.6998e+02, 9.2657e+02, 8.9604e+02, 8.5939e+02,
        8.1006e+02, 7.7554e+02, 7.0829e+02, 7.0063e+02, 6.5534e+02, 6.1841e+02,
        5.9481e+02, 5.9283e+02, 5.6913e+02, 5.4679e+02, 5.3034e+02, 5.1532e+02,
        4.8551e+02, 4.6244e+02, 4.5586e+02, 4.2485e+02, 4.0345e+02, 3.9306e+02,
        3.6341e+02, 3.4238e+02, 3.3093e+02, 2.8724e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 44]) 

NULL SPACE BASIS :  tensor([[ 0.1825,  0.1192, -0.1939,  ..., -0.0038, -0.1522, -0.0945],
        [ 0.0470,  0.0924,  0.0752,  ..., -0.0451,  0.0338, -0.0421],
        [ 0.0205,  0.1331, -0.0155,  ...,  0.2687,  0.2369,  0.0372],
        ...,
        [ 0.1309, -0.1167,  0.0281,  ...,  0.0205,  0.0111,  0.0165],
        [-0.0392, -0.0906, -0.0671,  ...,  0.0158, -0.0242, -0.0326],
        [-0.2463, -0.0301, -0.0114,  ...,  0.0086,  0.0056,  0.0322]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0959,  0.0040,  0.0192,  ...,  0.0009, -0.0029, -0.0126],
        [ 0.0040,  0.1232, -0.0108,  ...,  0.0105, -0.0092, -0.0041],
        [ 0.0192, -0.0108,  0.1044,  ..., -0.0067,  0.0026,  0.0060],
        ...,
        [ 0.0009,  0.0105, -0.0067,  ...,  0.0350, -0.0317, -0.0033],
        [-0.0029, -0.0092,  0.0026,  ..., -0.0317,  0.1148,  0.0029],
        [-0.0126, -0.0041,  0.0060,  ..., -0.0033,  0.0029,  0.0849]],
       device='cuda:0') 

reserving basis 780/1152; cond: 301802.0625, radio:0.009198890998959541
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1730e-02, -3.6775e-03,  9.7929e-03],
          [-3.2685e-03,  1.6508e-02,  1.1712e-02],
          [-1.0732e-02, -1.2282e-02, -1.2753e-02]],

         [[ 5.0574e-03, -1.6189e-02, -2.1982e-02],
          [ 2.6711e-02, -1.2960e-03,  6.8870e-04],
          [ 6.9480e-03,  1.9884e-02, -1.6604e-02]],

         [[ 6.6433e-03,  3.1878e-04,  6.3761e-03],
          [ 1.5363e-02,  1.7136e-02,  1.6098e-02],
          [-2.4415e-02, -1.4059e-02,  1.2707e-02]],

         ...,

         [[ 1.6353e-02,  6.0798e-03, -1.1813e-03],
          [-1.0930e-02,  2.7240e-02,  5.4236e-03],
          [ 1.4914e-02, -2.1102e-03, -7.6229e-03]],

         [[ 8.4512e-03,  1.1981e-02,  6.9817e-03],
          [ 2.5083e-03,  2.2769e-02,  2.1054e-02],
          [ 2.3805e-02,  2.7519e-02, -6.1054e-03]],

         [[ 7.2808e-04,  2.6794e-02,  4.7037e-02],
          [-1.8181e-02,  1.4262e-02,  2.0112e-02],
          [ 6.0402e-03, -2.0490e-02,  1.7015e-02]]],


        [[[-1.0257e-02,  7.0667e-03, -2.2129e-02],
          [ 1.3662e-02, -8.2188e-03,  4.0788e-03],
          [ 3.0782e-03, -2.4195e-02,  1.9466e-02]],

         [[-3.5291e-03,  1.7510e-02,  2.4875e-02],
          [ 2.0822e-02,  1.7047e-02, -2.1038e-04],
          [-9.7064e-03,  1.6614e-02,  1.8343e-02]],

         [[ 1.8271e-02,  1.6424e-02,  8.2685e-03],
          [ 2.5950e-02, -6.2664e-03,  2.0406e-02],
          [-1.5988e-02,  1.4146e-03,  8.5460e-03]],

         ...,

         [[-2.8261e-03,  1.6896e-02, -1.7457e-02],
          [-2.6911e-02, -2.2343e-02, -3.1392e-02],
          [-2.1607e-02, -2.2331e-02, -2.3857e-02]],

         [[-2.1851e-02,  1.8926e-03,  3.0917e-02],
          [-1.5283e-02, -1.7293e-02, -2.3219e-03],
          [ 1.4261e-02, -1.0703e-02, -1.8090e-02]],

         [[ 2.3319e-02, -2.3662e-02,  1.3815e-02],
          [-7.7709e-03, -1.3728e-02, -2.7987e-02],
          [ 1.8702e-03,  1.5902e-02, -3.1864e-02]]],


        [[[ 1.8249e-02,  1.9863e-02,  1.4275e-02],
          [-6.1303e-03, -2.9626e-02,  2.8548e-02],
          [-1.2097e-02, -1.4169e-03, -2.4739e-02]],

         [[-1.0038e-02,  6.0170e-03, -1.4875e-02],
          [ 3.2345e-02,  6.6823e-03, -4.9134e-03],
          [-6.8994e-03, -1.9079e-02,  1.8324e-02]],

         [[-3.0968e-02, -1.5409e-02, -2.2779e-02],
          [-1.5820e-02, -1.4192e-02,  2.1111e-03],
          [ 2.3957e-02, -1.2580e-02, -7.4189e-03]],

         ...,

         [[-3.5379e-02, -7.2833e-03, -2.1695e-02],
          [-6.9812e-03, -3.4671e-02, -2.6713e-02],
          [-3.2667e-03, -2.7487e-02, -7.0339e-04]],

         [[ 2.4591e-02, -1.3460e-02,  6.3304e-03],
          [ 1.9140e-02, -1.7837e-02, -1.9652e-02],
          [-3.0150e-03, -2.9683e-02,  1.0597e-02]],

         [[ 1.0921e-02,  1.3895e-02, -8.5926e-04],
          [ 6.8321e-03,  1.0563e-02, -1.1324e-02],
          [ 4.3881e-03,  2.3742e-02, -1.2499e-02]]],


        ...,


        [[[-1.5313e-02, -2.3875e-02, -3.4677e-02],
          [-1.0023e-02, -1.5298e-02,  1.9949e-03],
          [ 1.5366e-02,  6.8979e-03,  2.2952e-02]],

         [[-3.1311e-02, -6.8078e-03, -2.4665e-02],
          [-1.2720e-02,  1.5615e-02,  1.1636e-02],
          [-1.2568e-02,  3.1388e-02,  2.8458e-02]],

         [[ 2.9711e-02,  6.3936e-03,  1.1330e-03],
          [-1.6161e-02,  2.6475e-03, -2.7953e-02],
          [ 3.2093e-02,  2.4070e-02,  9.2084e-03]],

         ...,

         [[-2.2279e-02, -2.6942e-02,  7.4123e-03],
          [ 1.6438e-02, -1.4077e-02,  1.6547e-02],
          [-1.1392e-02,  3.4647e-02, -5.3590e-03]],

         [[ 1.1896e-02,  3.5690e-02,  1.1083e-02],
          [-1.0293e-02, -1.8958e-02,  2.4605e-02],
          [ 1.6822e-02, -1.8859e-03, -4.8843e-03]],

         [[ 3.6976e-02,  7.5637e-03,  1.7136e-02],
          [ 2.0492e-02, -1.5210e-02,  1.5936e-02],
          [-1.1352e-02, -3.3511e-02, -3.7199e-02]]],


        [[[-2.7893e-02, -1.4208e-03,  1.4070e-02],
          [ 2.0640e-02,  1.4563e-02, -2.0564e-02],
          [ 2.9572e-02,  2.5589e-02, -1.7755e-03]],

         [[ 7.7136e-03, -4.0029e-03,  1.8217e-03],
          [ 2.0454e-02,  3.8939e-02,  1.9191e-02],
          [ 2.2760e-02,  3.7924e-02, -1.1029e-02]],

         [[-1.9700e-02, -1.6468e-02, -2.8013e-02],
          [-1.7356e-02, -1.5295e-03, -9.1396e-03],
          [-2.2650e-03,  5.4771e-03, -2.7350e-02]],

         ...,

         [[-2.6317e-02,  1.6079e-02, -2.5094e-02],
          [ 1.6727e-02, -1.0937e-02, -5.6844e-03],
          [-1.0502e-02,  2.7625e-02,  1.2618e-02]],

         [[-2.7576e-02,  2.1591e-03,  1.4781e-02],
          [-3.0068e-03, -5.1354e-03,  3.4913e-03],
          [ 2.0731e-02,  2.3475e-02,  2.6542e-03]],

         [[-2.8626e-02, -7.7806e-03,  9.6767e-03],
          [-2.5903e-02,  1.2727e-02,  2.7771e-02],
          [-3.0083e-02,  1.4659e-03,  1.4951e-02]]],


        [[[-1.5214e-02,  1.5404e-02, -2.5689e-03],
          [-3.6031e-03,  1.1443e-02,  4.7213e-03],
          [-4.8849e-03, -1.5443e-03,  1.9555e-02]],

         [[-7.1753e-04, -2.1339e-02, -1.8920e-02],
          [-2.4246e-02, -1.2984e-02, -3.8788e-02],
          [ 3.8348e-02,  1.5488e-02,  3.0882e-03]],

         [[ 2.9169e-02,  2.7983e-04, -9.8889e-03],
          [-1.7491e-02, -1.4067e-02, -1.6179e-02],
          [ 2.6567e-02, -7.4294e-03, -2.1281e-02]],

         ...,

         [[ 1.0096e-02, -5.3730e-03,  9.4219e-03],
          [-2.0822e-02, -1.1831e-02,  5.6273e-03],
          [ 1.4179e-02,  6.0937e-05, -1.2382e-02]],

         [[-1.5552e-02, -1.0231e-02,  2.5963e-03],
          [ 1.7332e-02,  1.0408e-02, -3.1577e-02],
          [-1.6999e-02, -6.7682e-03, -9.5752e-03]],

         [[-2.7850e-03, -3.0429e-02, -1.6285e-02],
          [ 2.9804e-03,  2.2991e-02, -2.7137e-02],
          [ 2.5713e-02,  1.1759e-02,  2.1677e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.1113e+07, 1.2196e+06, 1.1606e+06,  ..., 1.2633e+02, 1.2422e+02,
        1.0309e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 780]) 

NULL SPACE BASIS :  tensor([[ 7.0787e-03, -1.4004e-02,  2.6039e-02,  ..., -9.5105e-03,
         -3.0272e-03,  1.8744e-02],
        [ 5.7277e-03,  3.0617e-02,  1.2203e-03,  ...,  1.9186e-02,
          5.6836e-03, -1.7462e-02],
        [ 6.0468e-03, -6.4979e-03, -7.9259e-03,  ..., -2.2752e-03,
         -9.0984e-03,  7.2327e-03],
        ...,
        [ 7.6202e-02,  2.6430e-03, -3.4591e-02,  ...,  3.4914e-03,
          7.1074e-04, -3.1461e-03],
        [ 2.4713e-02, -7.5298e-03, -1.6991e-02,  ..., -9.7833e-05,
          5.0880e-03,  5.0132e-03],
        [ 3.2826e-02, -4.1369e-02,  1.1943e-02,  ..., -1.7596e-03,
         -5.2735e-03, -2.1749e-04]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2126e-02, -5.4859e-03, -2.5022e-03,  ..., -4.1266e-04,
         -4.9428e-04,  4.5882e-04],
        [-5.4859e-03,  2.2971e-02, -5.1719e-03,  ..., -7.9097e-05,
          1.2935e-04, -1.3275e-04],
        [-2.5022e-03, -5.1719e-03,  2.1789e-02,  ..., -1.1231e-04,
         -3.3733e-05,  4.6647e-05],
        ...,
        [-4.1266e-04, -7.9097e-05, -1.1231e-04,  ...,  1.9766e-02,
         -6.4911e-03, -2.6275e-03],
        [-4.9428e-04,  1.2935e-04, -3.3733e-05,  ..., -6.4911e-03,
          2.0257e-02, -6.5596e-03],
        [ 4.5882e-04, -1.3275e-04,  4.6647e-05,  ..., -2.6275e-03,
         -6.5596e-03,  1.9753e-02]], device='cuda:0') 

reserving basis 344/1152; cond: 790356.0625, radio:0.0021830748300999403
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0021, -0.0130, -0.0072],
          [ 0.0174,  0.0180,  0.0373],
          [-0.0110, -0.0053,  0.0162]],

         [[-0.0165,  0.0195, -0.0093],
          [-0.0115, -0.0234, -0.0253],
          [-0.0009, -0.0172, -0.0036]],

         [[-0.0194, -0.0128,  0.0331],
          [ 0.0373,  0.0242,  0.0009],
          [ 0.0050,  0.0064, -0.0059]],

         ...,

         [[ 0.0377,  0.0123, -0.0062],
          [ 0.0400,  0.0157,  0.0159],
          [-0.0157,  0.0011, -0.0153]],

         [[ 0.0439,  0.0052,  0.0295],
          [ 0.0306,  0.0124,  0.0197],
          [-0.0223, -0.0038,  0.0194]],

         [[ 0.0107, -0.0135, -0.0141],
          [ 0.0202,  0.0181,  0.0376],
          [-0.0141,  0.0151,  0.0009]]],


        [[[ 0.0200,  0.0365, -0.0262],
          [ 0.0242, -0.0037, -0.0271],
          [ 0.0092, -0.0253, -0.0186]],

         [[-0.0311, -0.0342, -0.0426],
          [-0.0198, -0.0233, -0.0069],
          [ 0.0177, -0.0141,  0.0059]],

         [[ 0.0120,  0.0199,  0.0290],
          [-0.0047,  0.0138, -0.0063],
          [ 0.0127, -0.0097, -0.0303]],

         ...,

         [[-0.0110,  0.0307, -0.0168],
          [ 0.0319, -0.0082,  0.0116],
          [ 0.0089,  0.0184, -0.0208]],

         [[-0.0253,  0.0193, -0.0175],
          [-0.0162, -0.0032, -0.0200],
          [ 0.0283, -0.0100,  0.0215]],

         [[ 0.0124, -0.0257,  0.0344],
          [-0.0191,  0.0226,  0.0012],
          [ 0.0174,  0.0002, -0.0073]]],


        [[[-0.0017, -0.0006,  0.0095],
          [-0.0209,  0.0057, -0.0175],
          [-0.0161,  0.0019, -0.0054]],

         [[ 0.0140, -0.0170, -0.0153],
          [ 0.0257, -0.0005,  0.0308],
          [ 0.0341,  0.0199, -0.0142]],

         [[-0.0157,  0.0099, -0.0249],
          [ 0.0064,  0.0176,  0.0028],
          [-0.0001, -0.0066,  0.0312]],

         ...,

         [[ 0.0190,  0.0236,  0.0161],
          [-0.0165, -0.0250,  0.0044],
          [ 0.0031,  0.0171, -0.0103]],

         [[ 0.0155,  0.0167,  0.0088],
          [ 0.0082, -0.0090, -0.0204],
          [ 0.0040,  0.0108,  0.0316]],

         [[-0.0133, -0.0153, -0.0046],
          [-0.0218,  0.0020,  0.0010],
          [-0.0033,  0.0065,  0.0405]]],


        ...,


        [[[-0.0099, -0.0247,  0.0115],
          [ 0.0190, -0.0072,  0.0168],
          [ 0.0032, -0.0156,  0.0243]],

         [[-0.0221, -0.0239, -0.0079],
          [-0.0190, -0.0181,  0.0018],
          [-0.0332,  0.0225, -0.0061]],

         [[-0.0057,  0.0033, -0.0058],
          [-0.0063,  0.0187,  0.0033],
          [ 0.0316,  0.0107,  0.0194]],

         ...,

         [[ 0.0005, -0.0123,  0.0392],
          [-0.0018,  0.0206,  0.0207],
          [ 0.0144,  0.0102,  0.0093]],

         [[-0.0204,  0.0021, -0.0054],
          [-0.0177,  0.0129,  0.0057],
          [ 0.0119, -0.0013,  0.0206]],

         [[-0.0051, -0.0063, -0.0096],
          [ 0.0051,  0.0229, -0.0176],
          [-0.0009, -0.0131, -0.0056]]],


        [[[-0.0083,  0.0388,  0.0110],
          [-0.0026, -0.0010, -0.0008],
          [-0.0236,  0.0237,  0.0228]],

         [[ 0.0161, -0.0088, -0.0003],
          [-0.0031,  0.0324,  0.0118],
          [ 0.0054, -0.0076, -0.0244]],

         [[-0.0081, -0.0144,  0.0055],
          [-0.0141, -0.0321, -0.0052],
          [ 0.0135, -0.0141,  0.0260]],

         ...,

         [[-0.0020,  0.0121, -0.0203],
          [ 0.0159,  0.0005,  0.0175],
          [ 0.0300,  0.0127,  0.0239]],

         [[ 0.0089, -0.0275,  0.0036],
          [-0.0173,  0.0012, -0.0200],
          [ 0.0181,  0.0203,  0.0248]],

         [[-0.0039, -0.0294, -0.0203],
          [ 0.0133,  0.0119,  0.0032],
          [ 0.0164,  0.0061, -0.0285]]],


        [[[ 0.0222, -0.0252,  0.0010],
          [ 0.0112,  0.0071, -0.0287],
          [ 0.0212,  0.0243,  0.0115]],

         [[-0.0414, -0.0170, -0.0310],
          [ 0.0003,  0.0124, -0.0018],
          [ 0.0047, -0.0264,  0.0106]],

         [[-0.0132, -0.0014, -0.0100],
          [-0.0249, -0.0151, -0.0212],
          [ 0.0029, -0.0093,  0.0146]],

         ...,

         [[-0.0146, -0.0280, -0.0283],
          [ 0.0111,  0.0017,  0.0011],
          [ 0.0189, -0.0156, -0.0156]],

         [[-0.0035, -0.0173, -0.0065],
          [ 0.0025,  0.0045,  0.0191],
          [ 0.0013,  0.0019,  0.0026]],

         [[ 0.0211,  0.0134,  0.0018],
          [ 0.0205, -0.0070,  0.0199],
          [-0.0141, -0.0144,  0.0148]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.4385e+07, 1.2991e+06, 1.1548e+06,  ..., 7.2987e+01, 4.8196e+01,
        4.3506e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 344]) 

NULL SPACE BASIS :  tensor([[-2.3672e-02, -2.2044e-02, -1.2715e-02,  ...,  7.9847e-05,
         -6.5048e-04, -5.1756e-04],
        [-3.4141e-02, -1.2885e-03,  3.3544e-02,  ...,  1.2001e-03,
          2.0947e-03, -1.4931e-03],
        [ 7.1861e-03,  2.1668e-02, -2.7772e-02,  ...,  1.0531e-03,
          9.1321e-05,  2.6546e-03],
        ...,
        [-6.8384e-02, -1.3159e-02,  3.0228e-02,  ..., -6.8548e-03,
          2.1456e-03, -1.9814e-04],
        [ 3.2250e-02, -3.7559e-02,  2.4770e-02,  ..., -1.2733e-03,
          1.3623e-03,  1.9472e-03],
        [ 4.6548e-02,  1.6059e-02,  4.3114e-02,  ...,  3.8733e-03,
          6.2799e-04, -3.0213e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.7978e-03, -7.9438e-04, -3.0862e-04,  ...,  5.3659e-04,
         -3.2076e-04,  3.4654e-04],
        [-7.9438e-04,  3.2626e-03, -9.2431e-04,  ..., -2.1497e-04,
          2.9391e-04, -5.4621e-04],
        [-3.0862e-04, -9.2431e-04,  2.5773e-03,  ..., -5.2235e-05,
         -3.0886e-04,  7.3637e-05],
        ...,
        [ 5.3659e-04, -2.1497e-04, -5.2235e-05,  ...,  3.1103e-02,
         -9.4685e-03, -3.0655e-03],
        [-3.2076e-04,  2.9391e-04, -3.0886e-04,  ..., -9.4685e-03,
          3.2909e-02, -8.4708e-03],
        [ 3.4654e-04, -5.4621e-04,  7.3637e-05,  ..., -3.0655e-03,
         -8.4708e-03,  3.1290e-02]], device='cuda:0') 

reserving basis 750/1152; cond: 367261.6875, radio:0.00772477313876152
PARAMETER       :  Parameter containing:
tensor([[[[-2.0049e-02, -1.3906e-02,  4.1742e-02],
          [ 2.1320e-02,  8.9707e-03, -2.3108e-03],
          [ 1.5673e-02,  2.1514e-02, -2.7525e-02]],

         [[ 7.3608e-03,  1.3942e-03,  3.3480e-02],
          [-2.3196e-03,  1.7597e-02,  2.9716e-02],
          [ 2.7331e-02, -2.3898e-02, -6.8752e-03]],

         [[ 1.8069e-02,  5.1667e-03,  7.5489e-03],
          [-6.3703e-03, -8.7492e-03,  2.5548e-02],
          [ 1.0208e-02,  1.9858e-02,  2.6287e-02]],

         ...,

         [[ 7.1506e-03,  1.5738e-02,  2.6514e-02],
          [ 3.2485e-02, -2.0332e-02,  2.3738e-02],
          [-7.4782e-04, -1.4177e-02, -1.3214e-02]],

         [[-7.4688e-03, -6.9230e-03, -1.8934e-02],
          [-1.9970e-03,  3.7586e-03,  4.9468e-03],
          [ 1.7211e-03, -1.0190e-03, -7.8534e-03]],

         [[-3.7897e-02,  8.4133e-03, -1.7622e-02],
          [ 1.0822e-02,  2.4871e-03,  1.7384e-02],
          [ 2.6284e-02, -4.9721e-03,  2.3716e-02]]],


        [[[ 5.2559e-03, -3.6746e-03,  1.5103e-02],
          [ 2.0538e-02,  1.2400e-02, -1.4061e-03],
          [-2.3932e-03, -9.7809e-03, -2.9759e-03]],

         [[ 3.5036e-03,  1.1915e-02,  1.7433e-02],
          [-2.5612e-02, -2.8774e-03, -2.5372e-02],
          [-1.3897e-02,  1.8545e-02, -2.7833e-03]],

         [[-1.7616e-02, -6.1669e-03,  2.7385e-02],
          [ 4.1054e-03, -1.1169e-02, -1.1502e-02],
          [ 1.0602e-03,  1.3513e-02, -1.3414e-02]],

         ...,

         [[-2.0958e-02, -1.6517e-02, -2.1910e-02],
          [-2.6828e-02,  7.6812e-03, -2.0639e-02],
          [ 3.2359e-02,  1.9220e-03, -3.5128e-02]],

         [[ 1.7774e-02,  2.6698e-02,  3.4145e-02],
          [-1.4276e-02,  1.9173e-02, -9.9229e-03],
          [ 2.1499e-02,  1.9241e-02,  4.0434e-02]],

         [[ 4.5650e-04,  3.7400e-03, -5.1028e-05],
          [-3.7033e-03, -8.4883e-03,  2.0963e-02],
          [ 5.4709e-03,  4.6604e-02,  4.0867e-02]]],


        [[[-2.1131e-02, -2.0885e-02,  4.1225e-03],
          [-2.3217e-02,  4.0559e-03,  9.8578e-04],
          [ 5.6825e-03,  3.0894e-03,  4.5338e-03]],

         [[-2.8139e-03,  8.2062e-03,  9.5472e-03],
          [-6.1557e-04,  2.2798e-02,  6.8632e-03],
          [ 1.5919e-03,  3.4216e-02, -1.9044e-02]],

         [[-4.5264e-03, -1.8513e-02, -2.2728e-02],
          [-2.2515e-02, -3.0051e-02, -6.0007e-03],
          [-6.8800e-03, -2.7966e-02, -1.1187e-02]],

         ...,

         [[ 1.2261e-02,  1.0665e-02,  4.1672e-02],
          [-2.2941e-02,  7.8557e-03, -1.6307e-02],
          [-1.9189e-02,  2.8091e-02, -2.3840e-02]],

         [[-3.5801e-02, -1.0665e-02, -2.0828e-03],
          [ 6.2361e-03, -1.0851e-02,  7.8348e-03],
          [-9.7418e-03,  1.7376e-02, -2.6121e-02]],

         [[ 2.6360e-03, -8.4659e-03, -1.5125e-02],
          [-3.0424e-02,  3.4357e-03,  2.4239e-02],
          [-2.1679e-02,  1.0458e-02,  1.9275e-02]]],


        ...,


        [[[-1.1847e-03,  1.0937e-02, -2.0434e-02],
          [-1.9102e-02, -1.3837e-02, -2.3640e-02],
          [-4.7335e-03, -1.6208e-02, -2.3971e-02]],

         [[ 3.1344e-02,  3.0284e-03,  7.2060e-03],
          [-8.1750e-03,  6.0676e-03, -2.3461e-02],
          [-3.4754e-02, -3.8418e-02, -1.5892e-02]],

         [[ 8.9214e-03,  1.1542e-02, -2.1451e-03],
          [-5.3768e-03, -1.6560e-02,  3.7830e-03],
          [-4.0510e-03,  3.5760e-02,  2.3011e-02]],

         ...,

         [[ 2.0520e-02, -6.9606e-03,  9.1813e-03],
          [-1.2555e-02, -2.2727e-02,  2.0616e-02],
          [ 1.2178e-02, -2.6102e-02, -1.7155e-02]],

         [[-8.1085e-03, -1.1948e-02,  2.9205e-02],
          [-3.6008e-02,  2.5678e-02,  1.6063e-02],
          [-1.8183e-02,  5.0314e-03,  1.6165e-02]],

         [[-2.1052e-02, -2.0556e-02,  3.9776e-03],
          [-2.2148e-03, -1.8865e-02, -2.4076e-02],
          [-4.8286e-03,  1.4014e-02, -1.8135e-02]]],


        [[[-3.0304e-02,  8.4162e-03,  1.0293e-02],
          [-8.8657e-03, -1.5606e-02, -2.4924e-02],
          [-2.4407e-02, -9.4720e-03,  9.3688e-04]],

         [[ 9.9894e-03,  1.8666e-02,  1.2361e-02],
          [-1.8853e-02, -2.1420e-02, -2.3505e-02],
          [-9.3687e-03, -8.5568e-03, -1.7907e-02]],

         [[-2.7595e-02, -2.0013e-02, -1.9063e-02],
          [ 1.9209e-02,  2.2330e-02,  3.0569e-02],
          [ 1.6955e-02, -1.6843e-02, -1.7962e-02]],

         ...,

         [[-3.6617e-02,  1.4387e-02,  1.1170e-02],
          [-3.2717e-02,  8.1010e-04, -5.8296e-03],
          [-2.3529e-03,  1.1666e-03,  1.0765e-02]],

         [[ 1.7468e-03, -1.2900e-02, -6.6554e-04],
          [ 3.9984e-03, -2.9934e-02, -1.6832e-03],
          [ 5.0857e-03, -4.7513e-03,  9.8351e-03]],

         [[-4.2390e-03,  6.1844e-03,  3.8984e-03],
          [-1.1128e-02,  1.1829e-03, -2.1830e-02],
          [ 5.5323e-03,  1.0000e-02, -1.7716e-02]]],


        [[[ 6.4806e-03, -7.5672e-03,  1.7654e-02],
          [ 1.2811e-02, -1.9557e-02,  9.6075e-03],
          [ 7.2061e-03,  2.0857e-02,  4.3285e-03]],

         [[ 3.1766e-02,  6.1860e-03,  9.9149e-03],
          [-9.3439e-03,  3.1446e-02,  1.8124e-02],
          [ 8.6488e-03,  1.2228e-03,  1.4232e-02]],

         [[ 6.7930e-03, -1.2909e-02,  4.3973e-03],
          [-7.0028e-03, -3.5660e-02, -1.9142e-02],
          [-3.6758e-02, -1.2581e-02,  4.2448e-03]],

         ...,

         [[ 8.3272e-03, -3.2812e-02,  1.3554e-02],
          [-3.7122e-02,  1.6986e-02,  2.3532e-03],
          [-1.6579e-02,  4.7153e-03, -1.2411e-02]],

         [[ 1.6076e-02, -2.2346e-03,  1.1006e-03],
          [ 3.1894e-03,  2.7733e-02,  2.8193e-02],
          [ 9.2882e-03,  1.4466e-02,  2.4089e-02]],

         [[ 2.0880e-02,  3.0770e-02,  2.2546e-02],
          [-6.4950e-03,  7.0608e-03, -5.5709e-03],
          [-2.1903e-02,  8.4817e-03, -1.5662e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([8.3594e+06, 4.9587e+05, 4.7208e+05,  ..., 2.9466e+01, 2.6753e+01,
        2.2761e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 750]) 

NULL SPACE BASIS :  tensor([[-0.0174, -0.0261, -0.0321,  ..., -0.0018, -0.0126, -0.0055],
        [-0.0236,  0.0310, -0.0130,  ...,  0.0046,  0.0077,  0.0045],
        [ 0.0296, -0.0037,  0.0081,  ..., -0.0038, -0.0007,  0.0008],
        ...,
        [-0.0496,  0.0176,  0.0458,  ..., -0.0125, -0.0099,  0.0046],
        [-0.0218,  0.0073, -0.0277,  ...,  0.0049,  0.0200,  0.0003],
        [ 0.0174, -0.0220, -0.0208,  ..., -0.0020, -0.0023, -0.0056]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.4072e-02, -4.7777e-03, -1.5938e-03,  ..., -2.3969e-04,
          6.5310e-06,  7.7166e-04],
        [-4.7777e-03,  2.3682e-02, -5.3554e-03,  ..., -2.6711e-04,
         -2.2550e-04, -1.4522e-05],
        [-1.5938e-03, -5.3554e-03,  2.3523e-02,  ..., -4.2769e-04,
          1.6020e-04, -1.8432e-04],
        ...,
        [-2.3969e-04, -2.6711e-04, -4.2769e-04,  ...,  2.3505e-02,
         -7.0474e-03, -2.8765e-03],
        [ 6.5310e-06, -2.2550e-04,  1.6020e-04,  ..., -7.0474e-03,
          2.2990e-02, -6.9712e-03],
        [ 7.7166e-04, -1.4522e-05, -1.8432e-04,  ..., -2.8765e-03,
         -6.9712e-03,  2.1096e-02]], device='cuda:0') 

reserving basis 1424/2304; cond: 733260.1875, radio:0.007088326849043369
PARAMETER       :  Parameter containing:
tensor([[[[ 3.6311e-03, -4.7765e-03, -5.3247e-04],
          [-2.4515e-02, -7.6496e-03,  1.3447e-02],
          [-2.6248e-02, -7.2356e-03, -1.7364e-02]],

         [[ 6.8328e-03,  2.1670e-02,  2.6078e-03],
          [-5.9120e-03, -1.8337e-02,  2.0558e-02],
          [-1.0689e-02, -1.2097e-03,  1.3801e-02]],

         [[-1.7562e-02, -1.6528e-02, -1.2266e-02],
          [-1.6597e-03, -1.4543e-02, -2.0737e-02],
          [ 6.1384e-03,  8.5054e-03, -1.1527e-02]],

         ...,

         [[ 1.5827e-02,  1.2822e-02,  1.7069e-02],
          [-1.5782e-02, -1.3118e-02,  1.0988e-02],
          [ 3.5016e-03,  1.4921e-04,  2.0177e-03]],

         [[ 2.1271e-02,  5.9408e-04,  1.4682e-02],
          [-1.3934e-02,  5.0275e-04,  2.3255e-02],
          [ 2.1329e-02, -1.7830e-03,  8.9132e-03]],

         [[ 7.8670e-03,  8.0596e-03, -2.1078e-02],
          [ 5.7262e-03, -4.1170e-03,  2.4831e-03],
          [ 2.3523e-02,  5.0050e-03,  1.1304e-02]]],


        [[[ 8.8515e-03, -5.1770e-03, -2.0960e-03],
          [ 5.0168e-03,  7.6251e-03, -8.8127e-03],
          [ 2.3523e-02, -9.6484e-03, -1.3640e-02]],

         [[-1.1016e-02,  1.1481e-02, -1.3096e-03],
          [-1.9221e-02,  2.2140e-02,  1.2600e-02],
          [ 9.5929e-05, -5.6738e-03, -1.2654e-04]],

         [[-1.5857e-02, -2.0586e-03,  1.1582e-02],
          [ 2.4233e-02,  3.9036e-03,  6.0533e-03],
          [-7.6833e-03,  2.2034e-02, -1.2785e-02]],

         ...,

         [[-1.2598e-02,  2.1913e-02,  9.2643e-04],
          [ 2.0362e-02, -1.4726e-02,  1.4489e-02],
          [ 2.6315e-03, -1.9682e-02, -1.2675e-02]],

         [[-1.4879e-02, -2.6067e-02, -5.8381e-03],
          [-8.5270e-03,  4.1654e-03, -5.3974e-03],
          [-1.5920e-02,  1.6489e-02,  1.8187e-02]],

         [[ 8.2547e-03, -5.5611e-03, -2.0210e-02],
          [-6.8650e-03, -1.3531e-02,  6.8691e-03],
          [-1.8904e-02,  9.8176e-03, -6.3339e-03]]],


        [[[ 1.6025e-02, -2.2243e-02, -1.2583e-02],
          [ 1.6124e-02,  1.8406e-02, -2.7647e-03],
          [ 1.7302e-02,  2.1281e-02, -2.4835e-02]],

         [[-1.3146e-03,  1.2768e-02,  2.6237e-03],
          [-1.0530e-02, -1.3187e-02, -1.9705e-02],
          [-8.0112e-03,  2.3365e-02,  1.7429e-03]],

         [[-5.6043e-03, -1.1868e-02, -1.2167e-03],
          [-6.3275e-03, -5.8798e-03,  9.0367e-03],
          [-1.5910e-04, -1.3452e-02,  9.3830e-03]],

         ...,

         [[-1.5937e-02,  2.1986e-02, -7.7194e-03],
          [ 1.6636e-02, -1.6267e-02, -3.9952e-03],
          [-1.7929e-02, -1.4150e-02, -2.2376e-02]],

         [[-3.1174e-03,  1.8622e-02, -1.1588e-02],
          [-1.7410e-02, -2.1154e-02, -2.0245e-02],
          [ 1.8472e-02, -1.8892e-02, -1.0161e-02]],

         [[ 8.3762e-03,  3.1426e-03, -1.9488e-02],
          [ 2.3822e-03, -4.7270e-03,  1.0731e-02],
          [-9.9018e-03, -3.8907e-04, -9.3205e-03]]],


        ...,


        [[[ 7.3840e-03,  1.8444e-02,  7.4498e-03],
          [ 1.9538e-02, -1.5744e-02, -1.6064e-02],
          [-5.0141e-03, -1.3417e-02, -1.1345e-02]],

         [[-1.5079e-02, -1.0460e-02,  2.9518e-02],
          [ 2.6869e-04,  9.3707e-03, -3.6604e-03],
          [ 2.4128e-02, -1.9705e-02,  1.1274e-02]],

         [[-1.7992e-02, -1.0697e-03, -1.4055e-02],
          [ 4.0295e-03,  2.6674e-02, -2.3787e-02],
          [ 9.5583e-03,  3.1496e-02,  1.3721e-02]],

         ...,

         [[-5.8166e-03,  1.2123e-02,  2.5004e-03],
          [-1.2328e-02, -1.0542e-02,  6.7442e-03],
          [ 1.3100e-03,  1.0238e-03, -2.9006e-02]],

         [[-1.1716e-02,  2.6203e-03, -1.1109e-02],
          [-1.5948e-02, -1.0329e-02, -1.4959e-03],
          [-8.3330e-03,  3.6640e-03,  2.0315e-02]],

         [[-2.6523e-02, -4.2745e-02, -2.8498e-02],
          [ 9.8289e-03, -2.7269e-02, -1.9766e-02],
          [ 1.2223e-02, -1.0898e-02,  1.0027e-02]]],


        [[[-8.9489e-03,  1.6338e-02,  1.1834e-03],
          [-1.3099e-02, -2.0350e-02,  3.3682e-03],
          [ 9.6981e-03, -4.4012e-03, -1.4362e-02]],

         [[-3.3714e-03,  6.0286e-03, -4.1926e-03],
          [ 5.7463e-03, -1.4638e-02,  1.4888e-02],
          [ 9.3127e-03, -1.3223e-02, -1.5515e-02]],

         [[-2.3176e-02, -3.0921e-02, -2.2970e-02],
          [ 2.3536e-02,  1.5842e-02, -2.0700e-03],
          [-4.5899e-03,  7.7564e-03, -6.5803e-03]],

         ...,

         [[ 8.1801e-03, -1.3731e-02, -2.9730e-02],
          [ 1.5748e-02,  1.4355e-02, -2.3908e-02],
          [-1.4596e-02, -2.3456e-02,  1.2583e-02]],

         [[ 5.2737e-03,  1.3276e-03,  1.1130e-02],
          [-1.3787e-02, -7.8327e-03, -1.0015e-02],
          [-7.8778e-03,  8.4164e-03,  4.8241e-03]],

         [[-2.4921e-02,  1.5761e-02,  1.6352e-02],
          [-2.7105e-02, -1.0086e-02,  2.5352e-02],
          [-1.6064e-02,  3.6261e-02,  3.9370e-02]]],


        [[[-1.9821e-03, -3.0794e-02,  9.3948e-03],
          [-1.5663e-02, -8.5884e-03,  2.7573e-02],
          [-2.0036e-02, -8.5875e-03,  9.5582e-03]],

         [[-1.9153e-02, -1.6365e-02,  1.3727e-02],
          [-9.8480e-03,  7.6959e-03, -6.4555e-03],
          [ 5.0927e-03,  1.2106e-02,  1.0066e-03]],

         [[ 9.0289e-03, -6.1880e-03, -5.5354e-03],
          [-1.1912e-02,  2.3147e-02, -2.6693e-03],
          [-2.1097e-02,  1.1584e-03, -5.0589e-03]],

         ...,

         [[-3.7433e-03, -2.9387e-02, -2.7706e-03],
          [-2.3881e-02, -2.5894e-02,  7.5262e-03],
          [ 1.1112e-02, -1.4503e-02,  1.8720e-02]],

         [[ 9.9933e-03,  3.4540e-03,  1.8135e-02],
          [ 7.0840e-03, -1.2065e-02, -5.0833e-04],
          [-2.1875e-02,  1.4711e-02, -2.6317e-02]],

         [[-2.4861e-02,  8.6586e-03, -3.4142e-02],
          [ 1.0437e-02, -1.3568e-02, -5.5989e-03],
          [ 1.3241e-02, -2.5150e-02, -1.9076e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.2158e+07, 7.3959e+05, 6.9305e+05,  ..., 1.8007e+01, 1.6951e+01,
        1.6581e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1424]) 

NULL SPACE BASIS :  tensor([[-0.0033, -0.0403,  0.0203,  ..., -0.0035, -0.0039, -0.0015],
        [-0.0166,  0.0032, -0.0034,  ..., -0.0026, -0.0011, -0.0033],
        [-0.0231, -0.0378,  0.0275,  ...,  0.0052, -0.0006,  0.0022],
        ...,
        [-0.0098,  0.0084,  0.0121,  ...,  0.0014,  0.0011,  0.0094],
        [-0.0064, -0.0035, -0.0066,  ..., -0.0062, -0.0005, -0.0071],
        [ 0.0072, -0.0080, -0.0147,  ...,  0.0002,  0.0002,  0.0078]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.5985e-02, -7.8286e-04, -3.7333e-04,  ..., -3.2279e-06,
         -1.9991e-04,  3.6633e-05],
        [-7.8286e-04,  1.5670e-02, -8.8994e-04,  ...,  5.5730e-05,
         -2.0554e-05, -1.6275e-04],
        [-3.7333e-04, -8.8994e-04,  1.5990e-02,  ...,  2.0154e-05,
         -5.5131e-06, -1.1178e-04],
        ...,
        [-3.2279e-06,  5.5730e-05,  2.0154e-05,  ...,  2.0666e-02,
         -2.0910e-04, -1.3864e-05],
        [-1.9991e-04, -2.0554e-05, -5.5131e-06,  ..., -2.0910e-04,
          2.0280e-02, -2.7938e-04],
        [ 3.6633e-05, -1.6275e-04, -1.1178e-04,  ..., -1.3864e-05,
         -2.7938e-04,  2.0944e-02]], device='cuda:0') 

reserving basis 98/128; cond: 13254.8994140625, radio:0.021107234060764313
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0221]],

         [[-0.0344]],

         [[-0.0254]],

         ...,

         [[-0.0415]],

         [[-0.0746]],

         [[ 0.0851]]],


        [[[-0.0480]],

         [[ 0.0648]],

         [[ 0.0773]],

         ...,

         [[-0.0493]],

         [[-0.0003]],

         [[-0.0193]]],


        [[[ 0.0604]],

         [[-0.0207]],

         [[-0.0272]],

         ...,

         [[ 0.0547]],

         [[ 0.0111]],

         [[-0.0212]]],


        ...,


        [[[-0.0619]],

         [[ 0.0303]],

         [[ 0.0830]],

         ...,

         [[-0.0783]],

         [[-0.0511]],

         [[ 0.0292]]],


        [[[-0.0212]],

         [[-0.0733]],

         [[ 0.0110]],

         ...,

         [[ 0.0331]],

         [[ 0.0502]],

         [[ 0.0263]]],


        [[[ 0.0437]],

         [[-0.0383]],

         [[-0.0626]],

         ...,

         [[-0.0471]],

         [[ 0.0687]],

         [[-0.0344]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.1755e+06, 6.3268e+04, 5.2063e+04, 2.0956e+04, 1.3192e+04, 1.1003e+04,
        8.7164e+03, 6.2017e+03, 5.8059e+03, 5.3120e+03, 5.1847e+03, 3.8948e+03,
        3.5063e+03, 3.0199e+03, 2.5512e+03, 2.4260e+03, 2.0835e+03, 2.0417e+03,
        1.8432e+03, 1.6940e+03, 1.6042e+03, 1.5039e+03, 1.4287e+03, 1.3251e+03,
        1.2258e+03, 1.2086e+03, 1.0872e+03, 1.0491e+03, 9.5288e+02, 9.3628e+02,
        8.5704e+02, 8.3683e+02, 7.9833e+02, 7.6111e+02, 7.3475e+02, 7.1963e+02,
        6.8029e+02, 6.4650e+02, 6.1638e+02, 6.0748e+02, 5.9032e+02, 5.7248e+02,
        5.6418e+02, 5.3921e+02, 5.2198e+02, 4.9812e+02, 4.8285e+02, 4.7973e+02,
        4.6710e+02, 4.4621e+02, 4.3742e+02, 4.3175e+02, 4.1109e+02, 3.9767e+02,
        3.9362e+02, 3.9205e+02, 3.7350e+02, 3.6745e+02, 3.6302e+02, 3.5551e+02,
        3.4199e+02, 3.3605e+02, 3.3364e+02, 3.2521e+02, 3.1775e+02, 3.1564e+02,
        3.1119e+02, 2.9903e+02, 2.9411e+02, 2.8945e+02, 2.8531e+02, 2.7947e+02,
        2.7495e+02, 2.6782e+02, 2.6581e+02, 2.5679e+02, 2.5392e+02, 2.4843e+02,
        2.4375e+02, 2.4157e+02, 2.3638e+02, 2.3087e+02, 2.2791e+02, 2.2579e+02,
        2.2205e+02, 2.2116e+02, 2.1883e+02, 2.1294e+02, 2.1130e+02, 2.0712e+02,
        2.0342e+02, 1.9905e+02, 1.9834e+02, 1.9604e+02, 1.9343e+02, 1.9040e+02,
        1.8890e+02, 1.8557e+02, 1.8266e+02, 1.8131e+02, 1.7936e+02, 1.7503e+02,
        1.7180e+02, 1.7080e+02, 1.6898e+02, 1.6670e+02, 1.6460e+02, 1.6356e+02,
        1.5979e+02, 1.5784e+02, 1.5116e+02, 1.4823e+02, 1.4583e+02, 1.4417e+02,
        1.3946e+02, 1.3592e+02, 1.3415e+02, 1.3192e+02, 1.3106e+02, 1.2952e+02,
        1.2630e+02, 1.2232e+02, 1.2198e+02, 1.1898e+02, 1.1866e+02, 1.1370e+02,
        1.0378e+02, 8.8685e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 98]) 

NULL SPACE BASIS :  tensor([[-0.1698, -0.1353, -0.0144,  ..., -0.0600, -0.0538,  0.0512],
        [ 0.0285,  0.1527, -0.0070,  ...,  0.1505,  0.0266, -0.0242],
        [ 0.2176, -0.0003, -0.0169,  ...,  0.1008, -0.1732, -0.1167],
        ...,
        [ 0.0590, -0.0147,  0.0826,  ...,  0.0063,  0.0917, -0.0129],
        [-0.1084, -0.0034,  0.0205,  ..., -0.0696, -0.0301,  0.0469],
        [ 0.0751,  0.0164,  0.0166,  ..., -0.0668, -0.0362,  0.0762]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0791,  0.0012,  0.0002,  ..., -0.0045,  0.0027, -0.0088],
        [ 0.0012,  0.0658,  0.0011,  ..., -0.0022, -0.0049,  0.0083],
        [ 0.0002,  0.0011,  0.0869,  ...,  0.0006, -0.0011, -0.0037],
        ...,
        [-0.0045, -0.0022,  0.0006,  ...,  0.0768,  0.0008, -0.0069],
        [ 0.0027, -0.0049, -0.0011,  ...,  0.0008,  0.0791, -0.0017],
        [-0.0088,  0.0083, -0.0037,  ..., -0.0069, -0.0017,  0.0708]],
       device='cuda:0') 

reserving basis 1206/2304; cond: 844042.1875, radio:0.005672397557646036
PARAMETER       :  Parameter containing:
tensor([[[[ 5.0184e-03,  1.6039e-03, -1.0745e-02],
          [ 5.2004e-03, -1.9907e-02,  6.3261e-04],
          [-6.3477e-03,  2.1528e-02, -5.3999e-03]],

         [[-2.0246e-03,  1.2525e-02,  1.6137e-03],
          [ 2.6521e-02, -6.3728e-03, -2.1200e-02],
          [-2.6946e-03,  6.2700e-03,  1.2120e-02]],

         [[ 1.5397e-02,  2.7834e-02, -1.3557e-02],
          [ 1.4760e-02,  1.1590e-02,  1.2671e-02],
          [-3.4427e-03,  9.8631e-03,  6.2798e-03]],

         ...,

         [[-6.4283e-04,  2.0381e-02,  2.8756e-03],
          [-2.7455e-02, -5.9033e-03, -3.0705e-02],
          [-6.3780e-04, -1.5635e-02, -2.0964e-03]],

         [[-2.4183e-02,  1.3647e-02,  9.4716e-03],
          [ 4.0133e-03, -2.6193e-02, -1.8790e-02],
          [-2.0102e-02, -1.5354e-02, -1.5319e-02]],

         [[ 2.7076e-03, -7.0932e-04, -2.0802e-02],
          [ 8.4267e-03,  1.1585e-02,  5.2562e-03],
          [ 2.2940e-02,  2.0096e-02, -1.4297e-02]]],


        [[[-4.8154e-03,  2.2951e-02,  3.0654e-02],
          [ 1.9005e-03, -1.8814e-02,  1.5847e-02],
          [ 6.7096e-03,  1.7751e-02, -2.5267e-02]],

         [[-1.5227e-02, -2.3310e-03, -3.9205e-02],
          [-1.0675e-03, -5.8125e-03,  8.5215e-03],
          [ 9.1717e-04,  1.8195e-02,  2.4432e-02]],

         [[-7.3919e-03, -2.3219e-02,  1.4804e-02],
          [-1.0518e-02, -3.7564e-04, -1.5146e-02],
          [ 6.4447e-03,  6.8697e-03,  1.8805e-02]],

         ...,

         [[-1.1486e-02, -2.6423e-02, -1.0517e-02],
          [-8.8248e-03,  5.5112e-03,  1.7063e-02],
          [ 3.2912e-03,  1.3562e-02, -8.7827e-03]],

         [[ 2.1648e-03, -1.7037e-02, -1.2736e-03],
          [ 1.1353e-02,  1.7457e-02,  2.2245e-02],
          [-1.6627e-02, -1.9545e-02,  1.9227e-02]],

         [[ 5.8657e-03, -1.2815e-02, -2.9578e-02],
          [-7.1360e-03,  5.4469e-03,  3.6106e-03],
          [-1.3127e-02, -2.5335e-02, -1.2926e-02]]],


        [[[-1.3333e-02, -5.5186e-05,  3.2799e-03],
          [ 1.2155e-02, -2.1443e-02, -1.7172e-02],
          [-9.4965e-03,  2.1369e-02,  3.6511e-03]],

         [[-1.6757e-02,  7.9210e-03,  4.0675e-03],
          [-1.1009e-02, -4.2496e-03,  1.8443e-02],
          [-1.7965e-02,  1.5547e-02,  1.6482e-03]],

         [[ 2.1021e-02, -9.7988e-03,  1.8710e-02],
          [-1.5156e-02, -3.8638e-03,  2.5580e-02],
          [-1.8586e-02, -1.3071e-02,  2.1747e-02]],

         ...,

         [[ 1.6679e-02, -6.0147e-03, -1.7096e-02],
          [ 1.1747e-02, -2.8650e-02, -3.0702e-03],
          [ 9.4725e-03, -2.8750e-02, -3.3179e-02]],

         [[-9.1425e-03,  8.8359e-04, -1.4148e-02],
          [ 1.3649e-02,  6.6395e-03,  4.1310e-03],
          [ 2.5161e-02, -3.1627e-02, -8.6367e-03]],

         [[-3.3174e-03, -1.3144e-02, -1.3092e-02],
          [-1.0215e-02, -1.4861e-02,  2.6932e-03],
          [ 1.6201e-02,  3.0428e-02,  1.8525e-02]]],


        ...,


        [[[-1.9974e-02,  1.1504e-02, -2.6985e-02],
          [ 4.8373e-03, -1.1448e-02, -2.9714e-02],
          [ 1.0161e-02,  4.0713e-04, -5.4723e-03]],

         [[-1.0206e-02, -2.4139e-02, -1.9440e-03],
          [ 1.3549e-02,  1.6065e-02,  7.1622e-03],
          [-1.7702e-02, -2.6147e-02, -1.3642e-02]],

         [[ 3.8849e-03, -1.7403e-02,  1.2938e-02],
          [-1.9256e-02,  6.1334e-04,  2.1614e-02],
          [ 1.6667e-02,  5.0436e-03, -1.7543e-02]],

         ...,

         [[ 1.5507e-02, -3.7510e-02,  1.2881e-02],
          [-1.2808e-02, -2.3361e-03,  6.2863e-03],
          [-1.3173e-02, -2.3897e-02,  3.5782e-03]],

         [[ 1.5828e-02, -1.1075e-03,  2.9502e-03],
          [-1.5395e-02,  1.1555e-02,  8.1211e-04],
          [-2.1754e-02,  1.8892e-03,  4.4568e-03]],

         [[-2.0833e-02, -1.2777e-02, -2.3997e-02],
          [ 7.8807e-03,  1.1869e-02, -5.6520e-03],
          [-2.2399e-02,  2.5414e-02,  3.1647e-03]]],


        [[[ 7.6862e-03, -4.8828e-03,  3.5852e-02],
          [ 5.9241e-03,  1.9418e-02,  1.1669e-02],
          [ 2.6099e-02, -5.3945e-03,  3.4313e-02]],

         [[ 4.2977e-03,  2.0862e-02, -1.6685e-02],
          [-1.3569e-02,  3.6116e-03,  2.2099e-02],
          [ 4.4421e-03,  1.0453e-02,  2.7379e-02]],

         [[-1.1417e-02,  2.8744e-03,  1.4235e-02],
          [-1.3374e-02,  8.5883e-03, -2.2370e-02],
          [ 2.7222e-03,  1.8432e-02,  1.0435e-02]],

         ...,

         [[ 1.0895e-02,  1.2586e-02, -1.3950e-02],
          [ 1.3981e-02,  1.7701e-02, -1.8112e-02],
          [ 2.4933e-04,  1.7855e-02,  3.1495e-02]],

         [[ 4.2770e-03, -9.7476e-03,  1.8774e-02],
          [ 1.0385e-02,  1.6169e-02,  2.0675e-02],
          [-1.3269e-02,  7.8275e-03,  1.4175e-02]],

         [[-1.3816e-02, -1.0523e-02,  4.0005e-03],
          [ 6.8564e-03, -4.5785e-03,  2.6445e-03],
          [-1.6965e-02, -3.6317e-03,  1.8466e-02]]],


        [[[ 2.8643e-02,  4.4541e-03, -6.1324e-03],
          [ 9.4501e-03, -4.7134e-03,  8.1492e-03],
          [-2.5110e-03, -1.3132e-02, -7.8182e-03]],

         [[-1.0141e-02, -6.7989e-03,  1.4324e-02],
          [ 1.7699e-02, -1.9448e-03,  2.4596e-02],
          [-2.5443e-03,  2.5294e-02,  1.0399e-02]],

         [[-1.1870e-02, -2.0901e-02, -2.7057e-03],
          [-1.3125e-02, -2.6513e-02, -1.3134e-02],
          [ 1.7488e-03,  1.2341e-03,  4.6122e-03]],

         ...,

         [[ 9.4898e-03,  7.6372e-03,  1.7142e-02],
          [ 1.6129e-02,  1.4261e-02, -1.2147e-02],
          [-1.2023e-04, -1.3359e-02, -1.5372e-02]],

         [[ 1.3057e-02, -1.0468e-02, -1.3956e-02],
          [-7.3180e-04, -2.3067e-02, -1.5019e-02],
          [-1.6924e-05, -2.1441e-02, -1.9900e-02]],

         [[ 4.9479e-03, -3.3567e-03, -1.1717e-02],
          [-1.5554e-03, -9.1754e-03, -1.6225e-02],
          [ 2.0390e-02,  2.1853e-03,  1.4138e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.0513e+07, 7.1305e+05, 6.8947e+05,  ..., 1.5754e+01, 1.5279e+01,
        1.2455e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1206]) 

NULL SPACE BASIS :  tensor([[-0.0317, -0.0214, -0.0059,  ..., -0.0060, -0.0017, -0.0024],
        [ 0.0018, -0.0185,  0.0048,  ...,  0.0024,  0.0043,  0.0051],
        [ 0.0175, -0.0149, -0.0359,  ..., -0.0017, -0.0034, -0.0024],
        ...,
        [ 0.0054,  0.0366,  0.0114,  ...,  0.0254,  0.0116, -0.0016],
        [ 0.0040, -0.0176,  0.0143,  ..., -0.0261, -0.0047, -0.0003],
        [ 0.0192,  0.0134,  0.0307,  ...,  0.0033, -0.0024, -0.0011]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.0419e-02, -4.5217e-03, -3.1563e-04,  ...,  2.0208e-04,
         -1.8489e-04,  9.2020e-05],
        [-4.5217e-03,  1.1795e-02, -4.6832e-03,  ..., -2.4559e-04,
          1.9318e-04, -2.6037e-04],
        [-3.1563e-04, -4.6832e-03,  1.0313e-02,  ..., -1.1205e-04,
         -2.7647e-04,  1.8831e-04],
        ...,
        [ 2.0208e-04, -2.4559e-04, -1.1205e-04,  ...,  2.1987e-02,
         -7.5506e-04, -5.9038e-04],
        [-1.8489e-04,  1.9318e-04, -2.7647e-04,  ..., -7.5506e-04,
          2.1151e-02, -6.7230e-04],
        [ 9.2020e-05, -2.6037e-04,  1.8831e-04,  ..., -5.9038e-04,
         -6.7230e-04,  2.1030e-02]], device='cuda:0') 

reserving basis 895/2304; cond: 1353939.5, radio:0.00251783337444067
PARAMETER       :  Parameter containing:
tensor([[[[-0.0137, -0.0071, -0.0153],
          [-0.0045, -0.0296,  0.0065],
          [-0.0154, -0.0130, -0.0047]],

         [[ 0.0245,  0.0278,  0.0296],
          [-0.0100,  0.0123, -0.0199],
          [-0.0075,  0.0248,  0.0255]],

         [[-0.0034, -0.0101, -0.0270],
          [-0.0128, -0.0162, -0.0030],
          [ 0.0165, -0.0200, -0.0151]],

         ...,

         [[-0.0181,  0.0224, -0.0192],
          [ 0.0050,  0.0024, -0.0226],
          [ 0.0006,  0.0019,  0.0091]],

         [[ 0.0202,  0.0020, -0.0205],
          [-0.0124,  0.0030, -0.0088],
          [-0.0267,  0.0153,  0.0099]],

         [[ 0.0040,  0.0254,  0.0078],
          [-0.0105,  0.0150, -0.0181],
          [ 0.0090, -0.0050,  0.0161]]],


        [[[ 0.0006,  0.0208,  0.0330],
          [-0.0243,  0.0191,  0.0083],
          [ 0.0030, -0.0047,  0.0079]],

         [[ 0.0207,  0.0163, -0.0203],
          [ 0.0115,  0.0027, -0.0037],
          [-0.0285, -0.0110, -0.0167]],

         [[ 0.0029, -0.0028,  0.0105],
          [ 0.0045, -0.0268, -0.0164],
          [ 0.0157, -0.0089,  0.0167]],

         ...,

         [[-0.0185, -0.0037, -0.0142],
          [ 0.0078,  0.0128, -0.0002],
          [-0.0022,  0.0141, -0.0093]],

         [[ 0.0027, -0.0073,  0.0210],
          [-0.0007, -0.0042,  0.0206],
          [ 0.0063,  0.0099,  0.0144]],

         [[-0.0228, -0.0162, -0.0225],
          [-0.0186, -0.0156, -0.0175],
          [-0.0190, -0.0168,  0.0027]]],


        [[[-0.0272, -0.0197, -0.0156],
          [ 0.0013,  0.0171,  0.0083],
          [ 0.0107, -0.0094, -0.0164]],

         [[-0.0165,  0.0149,  0.0043],
          [-0.0095,  0.0035,  0.0031],
          [-0.0041, -0.0142,  0.0179]],

         [[-0.0071,  0.0079,  0.0018],
          [-0.0002, -0.0162, -0.0241],
          [ 0.0097, -0.0054,  0.0022]],

         ...,

         [[-0.0051,  0.0275, -0.0089],
          [-0.0181,  0.0018, -0.0158],
          [-0.0209, -0.0238,  0.0076]],

         [[-0.0001,  0.0024, -0.0004],
          [-0.0030,  0.0129,  0.0091],
          [-0.0184, -0.0169,  0.0010]],

         [[-0.0201, -0.0106, -0.0005],
          [ 0.0108, -0.0087,  0.0193],
          [-0.0062, -0.0019, -0.0127]]],


        ...,


        [[[-0.0141, -0.0161,  0.0025],
          [-0.0020,  0.0106,  0.0127],
          [-0.0127, -0.0115,  0.0060]],

         [[-0.0006,  0.0209, -0.0025],
          [ 0.0144, -0.0030,  0.0284],
          [-0.0016, -0.0127, -0.0100]],

         [[ 0.0003, -0.0177,  0.0080],
          [ 0.0006,  0.0200, -0.0028],
          [-0.0355, -0.0059,  0.0238]],

         ...,

         [[ 0.0238,  0.0044,  0.0137],
          [ 0.0089,  0.0172,  0.0161],
          [-0.0021, -0.0106,  0.0069]],

         [[ 0.0035, -0.0142, -0.0239],
          [-0.0047,  0.0188, -0.0058],
          [ 0.0112, -0.0128,  0.0164]],

         [[ 0.0118,  0.0138, -0.0066],
          [-0.0119, -0.0024,  0.0182],
          [ 0.0129, -0.0017,  0.0065]]],


        [[[ 0.0081, -0.0176,  0.0072],
          [ 0.0259,  0.0043, -0.0100],
          [-0.0039,  0.0021, -0.0177]],

         [[ 0.0045, -0.0076,  0.0146],
          [ 0.0196,  0.0210,  0.0353],
          [ 0.0324,  0.0234,  0.0351]],

         [[-0.0127,  0.0164, -0.0078],
          [ 0.0280,  0.0297,  0.0113],
          [-0.0177,  0.0002,  0.0234]],

         ...,

         [[-0.0138,  0.0063,  0.0203],
          [ 0.0172,  0.0097,  0.0116],
          [-0.0141,  0.0093, -0.0152]],

         [[-0.0214, -0.0207,  0.0087],
          [-0.0109,  0.0092, -0.0003],
          [ 0.0167, -0.0149, -0.0084]],

         [[-0.0186,  0.0021,  0.0226],
          [ 0.0130,  0.0207, -0.0113],
          [ 0.0129,  0.0123, -0.0071]]],


        [[[ 0.0152, -0.0118, -0.0026],
          [-0.0068, -0.0108, -0.0028],
          [-0.0217, -0.0183, -0.0009]],

         [[ 0.0044, -0.0063, -0.0135],
          [-0.0070, -0.0024, -0.0176],
          [ 0.0014, -0.0204, -0.0094]],

         [[ 0.0316,  0.0025,  0.0023],
          [ 0.0259,  0.0054,  0.0089],
          [-0.0025, -0.0062, -0.0140]],

         ...,

         [[ 0.0147,  0.0127,  0.0019],
          [ 0.0198, -0.0071,  0.0056],
          [-0.0063, -0.0037,  0.0002]],

         [[-0.0068,  0.0039, -0.0199],
          [ 0.0211,  0.0187,  0.0126],
          [ 0.0007,  0.0383,  0.0332]],

         [[ 0.0208, -0.0009, -0.0117],
          [ 0.0079,  0.0079, -0.0124],
          [-0.0310, -0.0042, -0.0175]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([9.5177e+06, 6.5456e+05, 6.2077e+05,  ..., 8.3889e+00, 8.0433e+00,
        7.0296e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 895]) 

NULL SPACE BASIS :  tensor([[-0.0125,  0.0558, -0.0177,  ...,  0.0038,  0.0008,  0.0050],
        [ 0.0329,  0.0369,  0.0010,  ..., -0.0051, -0.0030, -0.0006],
        [ 0.0185, -0.0058,  0.0021,  ..., -0.0015, -0.0004, -0.0039],
        ...,
        [-0.0250, -0.0152, -0.0097,  ..., -0.0019,  0.0050,  0.0007],
        [ 0.0050,  0.0320,  0.0195,  ..., -0.0019, -0.0031,  0.0004],
        [ 0.0173, -0.0144, -0.0078,  ...,  0.0007,  0.0008, -0.0002]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.6414e-02, -5.6717e-03, -8.2925e-04,  ...,  1.6700e-04,
         -2.0919e-04,  3.5657e-04],
        [-5.6717e-03,  1.6227e-02, -6.1624e-03,  ..., -8.7613e-06,
         -6.0376e-06, -3.8141e-04],
        [-8.2925e-04, -6.1624e-03,  1.9043e-02,  ...,  3.2510e-04,
         -1.9700e-05,  1.1464e-04],
        ...,
        [ 1.6700e-04, -8.7613e-06,  3.2510e-04,  ...,  2.8376e-03,
         -1.0465e-03, -8.5381e-05],
        [-2.0919e-04, -6.0376e-06, -1.9700e-05,  ..., -1.0465e-03,
          2.9325e-03, -9.4916e-04],
        [ 3.5657e-04, -3.8141e-04,  1.1464e-04,  ..., -8.5381e-05,
         -9.4916e-04,  2.6251e-03]], device='cuda:0') 

reserving basis 1186/2304; cond: 1134544.125, radio:0.0038884119130671024
PARAMETER       :  Parameter containing:
tensor([[[[-6.5094e-03, -8.7223e-03, -6.9744e-03],
          [-7.9433e-03,  1.5449e-02,  1.3074e-02],
          [-2.3433e-02, -4.3671e-03, -2.7999e-03]],

         [[ 1.8199e-03,  1.7999e-03, -1.1976e-02],
          [-7.3791e-03,  1.0861e-02, -5.8130e-03],
          [ 2.0179e-02, -2.4578e-02,  6.0927e-03]],

         [[ 1.4120e-02, -4.7098e-03,  8.3808e-03],
          [ 1.2329e-02, -1.0379e-02,  1.6508e-03],
          [-1.8713e-02, -1.2086e-02, -7.9681e-03]],

         ...,

         [[ 1.5159e-02, -1.5716e-02,  1.9858e-03],
          [ 1.0666e-02,  1.0543e-02,  1.3968e-02],
          [-1.2491e-03, -2.6182e-02,  1.3100e-02]],

         [[ 2.1000e-02,  1.1413e-02, -1.5882e-03],
          [-1.0251e-02,  2.0663e-03, -1.0846e-02],
          [-2.2772e-03,  1.8463e-02, -1.6889e-02]],

         [[-6.0168e-03, -4.1743e-03,  7.8244e-03],
          [-3.0524e-02,  3.9347e-03,  7.6122e-04],
          [ 5.6357e-03, -1.0487e-02,  1.9853e-02]]],


        [[[-1.1970e-02,  7.3850e-03,  7.5796e-03],
          [ 1.0193e-02, -7.1028e-03,  1.5042e-02],
          [ 1.5467e-02, -6.7457e-03, -8.8484e-04]],

         [[ 1.8659e-02, -1.4729e-03, -1.8891e-02],
          [ 2.6295e-03, -3.4790e-03,  8.9618e-03],
          [ 1.3544e-02, -2.1449e-02, -1.5119e-02]],

         [[ 1.6535e-02, -2.4535e-02,  3.6523e-03],
          [-2.1977e-02,  3.0563e-04,  9.1629e-03],
          [-1.0320e-02, -1.8257e-02,  4.3335e-03]],

         ...,

         [[-2.1522e-02, -1.8040e-02, -8.2798e-03],
          [ 1.7553e-04, -1.2640e-02, -2.6018e-02],
          [ 4.2602e-03, -5.0936e-03,  2.9443e-06]],

         [[-8.7933e-03,  6.2795e-03, -5.8085e-03],
          [ 8.3461e-03,  2.4306e-02, -1.0373e-02],
          [-1.5100e-02,  1.1745e-02, -9.2163e-03]],

         [[ 1.0824e-02,  2.1246e-02,  4.8389e-03],
          [ 7.4149e-04,  6.9256e-03, -8.9340e-03],
          [-7.3423e-03,  2.7153e-02, -2.2055e-03]]],


        [[[ 3.1838e-03, -2.4718e-02, -4.2544e-03],
          [-2.8814e-03,  1.0908e-02, -5.1823e-03],
          [ 6.6933e-03,  1.0517e-02,  3.3138e-03]],

         [[ 2.1972e-02,  1.7579e-02,  2.0816e-02],
          [ 2.0407e-02,  9.6129e-03,  5.3314e-03],
          [ 8.8369e-03, -4.8073e-03,  8.2908e-03]],

         [[-1.7694e-02, -1.4462e-02,  2.8272e-03],
          [ 6.1705e-03, -1.9496e-02, -6.1746e-03],
          [-1.3509e-02, -9.2126e-03, -3.1369e-02]],

         ...,

         [[ 1.4920e-03, -4.0812e-03, -3.1046e-02],
          [-5.4443e-03,  2.8839e-03, -8.2972e-03],
          [ 1.8415e-02,  8.2986e-04, -3.2881e-03]],

         [[ 1.2981e-02,  5.2331e-03, -4.7719e-03],
          [-5.6485e-03, -1.6294e-03,  6.7950e-03],
          [-2.1646e-02, -2.1833e-02, -1.4938e-02]],

         [[ 1.2403e-03,  5.3521e-03, -7.0222e-03],
          [ 1.4650e-03,  1.7001e-02,  2.9325e-04],
          [ 1.1594e-02,  1.1014e-02, -9.7250e-03]]],


        ...,


        [[[ 4.0588e-05,  1.2003e-03, -1.3810e-02],
          [-1.4268e-02,  2.5235e-03,  4.3596e-04],
          [ 7.8513e-03, -1.0836e-02, -8.0594e-03]],

         [[ 1.7344e-02, -3.8378e-03, -1.1489e-02],
          [ 2.1698e-02,  1.5423e-02, -1.5550e-02],
          [-5.9905e-03,  2.9860e-03, -1.5348e-02]],

         [[-2.0892e-02, -2.3490e-03,  8.1284e-03],
          [-7.0219e-03, -1.5516e-02,  6.0753e-03],
          [ 5.0286e-03,  3.6070e-03, -2.4923e-02]],

         ...,

         [[ 8.7868e-03, -1.3015e-02,  7.6711e-03],
          [ 1.1629e-03,  4.2137e-03, -5.7352e-03],
          [-6.2770e-03, -7.7021e-03, -2.9761e-02]],

         [[-1.5721e-02,  6.6167e-03, -3.0095e-03],
          [ 7.9337e-03, -2.3483e-02, -2.1299e-02],
          [-7.3911e-03, -1.3231e-02, -1.1694e-02]],

         [[ 1.0128e-02,  1.8320e-02,  7.1047e-03],
          [-1.0224e-02,  1.2001e-02, -9.8444e-03],
          [-1.8717e-03,  9.8164e-03, -5.5338e-03]]],


        [[[-1.8967e-02, -3.8377e-03, -1.0101e-03],
          [-9.6354e-03,  2.7831e-03,  7.4588e-03],
          [ 1.6551e-03,  1.1608e-02,  8.2001e-03]],

         [[-1.1741e-02,  2.1523e-02,  1.0973e-02],
          [-3.2939e-03,  3.0482e-03,  1.1858e-02],
          [ 8.0156e-03, -1.1455e-02,  1.1559e-02]],

         [[-8.8745e-03,  3.9171e-03,  1.7604e-02],
          [ 1.3259e-02, -4.8454e-03,  1.5105e-02],
          [-5.2814e-03, -1.0878e-02, -1.1763e-02]],

         ...,

         [[ 1.1184e-02, -1.9161e-02, -1.6151e-03],
          [-2.1881e-03, -1.5139e-02,  1.0930e-02],
          [-1.6203e-02, -1.2800e-02,  3.5958e-03]],

         [[-3.1828e-03,  6.4035e-03, -2.6313e-02],
          [-1.5515e-02, -5.9686e-03, -3.3756e-02],
          [-1.4546e-02, -2.0601e-02, -5.6379e-03]],

         [[ 1.1367e-02,  1.7915e-02,  2.7424e-02],
          [ 1.3802e-02,  2.4712e-03,  3.2643e-03],
          [ 7.5097e-03, -1.4336e-03,  1.2451e-02]]],


        [[[ 2.2492e-03,  7.4031e-03,  2.3384e-02],
          [-3.9326e-03, -3.0847e-03, -1.6948e-02],
          [ 6.1437e-03,  4.6731e-03, -8.1317e-03]],

         [[-2.2329e-04, -4.2614e-03, -1.0907e-02],
          [-9.8455e-03, -9.6933e-03, -1.6815e-02],
          [ 4.5134e-03, -2.6917e-02,  1.7831e-03]],

         [[ 1.8512e-03, -1.7503e-02,  8.8324e-03],
          [-1.3551e-02,  9.2246e-03,  4.3280e-03],
          [-2.0026e-02,  6.2693e-04, -9.2270e-03]],

         ...,

         [[-1.4691e-02, -1.6248e-02,  1.9765e-02],
          [-1.2561e-02, -1.2546e-02, -2.3426e-02],
          [ 8.3347e-03, -1.3275e-02,  1.8460e-02]],

         [[ 2.1293e-02,  8.9022e-03, -8.1552e-03],
          [ 1.3959e-02,  1.1514e-02,  1.6151e-02],
          [-9.6823e-03, -2.1951e-02, -1.2206e-03]],

         [[-9.8945e-03, -9.4375e-03,  2.1094e-03],
          [-1.7904e-02,  2.8153e-02,  1.5587e-02],
          [-2.0400e-02, -3.7830e-03, -1.6621e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.9467e+06, 3.3058e+05, 3.1949e+05,  ..., 3.1712e+00, 2.7390e+00,
        2.5973e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1186]) 

NULL SPACE BASIS :  tensor([[-3.2065e-03,  1.9961e-03, -1.0464e-02,  ..., -4.4838e-03,
          2.9642e-03, -8.2725e-03],
        [ 2.2717e-03, -3.7491e-03, -1.2748e-02,  ..., -3.7230e-03,
         -5.5865e-03,  4.3901e-03],
        [-2.1817e-03,  5.2717e-03, -7.3254e-03,  ...,  8.2162e-03,
         -1.5008e-03, -6.0585e-03],
        ...,
        [-4.3715e-04,  1.6785e-02, -6.8789e-05,  ...,  2.8325e-02,
         -4.2675e-03, -4.1445e-03],
        [ 4.2677e-03,  1.5315e-02, -1.7702e-02,  ..., -1.8218e-02,
          1.8267e-02,  1.6966e-03],
        [ 1.9545e-03,  2.2677e-02, -1.5288e-02,  ...,  1.0673e-02,
         -1.3890e-02,  6.4223e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 5.8780e-03, -2.9143e-03,  3.6143e-04,  ...,  1.3957e-04,
         -1.0590e-04, -2.3455e-04],
        [-2.9143e-03,  6.4604e-03, -2.6891e-03,  ..., -9.6657e-05,
          2.2937e-04,  8.1168e-05],
        [ 3.6143e-04, -2.6891e-03,  4.1817e-03,  ...,  1.9224e-04,
          1.7319e-04, -1.5474e-05],
        ...,
        [ 1.3957e-04, -9.6657e-05,  1.9224e-04,  ...,  2.2490e-02,
         -2.5209e-03, -9.8869e-04],
        [-1.0590e-04,  2.2937e-04,  1.7319e-04,  ..., -2.5209e-03,
          2.1144e-02, -3.1124e-03],
        [-2.3455e-04,  8.1168e-05, -1.5474e-05,  ..., -9.8869e-04,
         -3.1124e-03,  2.0323e-02]], device='cuda:0') 

reserving basis 1394/4608; cond: 5610101.5, radio:0.0008409680449403822
PARAMETER       :  Parameter containing:
tensor([[[[ 4.4713e-03,  3.9361e-03,  1.3243e-02],
          [-7.2816e-03, -8.2268e-03,  1.3167e-02],
          [-4.1277e-03, -4.8121e-03,  1.0251e-02]],

         [[ 4.4671e-03, -6.3678e-03,  1.6916e-03],
          [ 5.8176e-03,  1.0017e-02,  2.2939e-03],
          [ 1.7486e-02, -7.0450e-03,  8.7169e-03]],

         [[-2.6287e-03, -1.4052e-02, -3.7239e-04],
          [-5.2036e-03, -6.6862e-03, -2.3340e-02],
          [-1.1380e-02,  9.5348e-03, -2.1185e-02]],

         ...,

         [[-5.2577e-03,  2.1214e-03,  3.6766e-03],
          [-1.7178e-03, -1.1755e-03, -1.7243e-02],
          [-1.0327e-02, -1.0057e-03, -2.5452e-02]],

         [[-4.3180e-03,  1.0982e-02,  1.4349e-02],
          [ 2.9489e-03,  1.6310e-02,  3.4558e-03],
          [-3.8728e-04,  5.4207e-03,  5.1675e-03]],

         [[-1.4069e-02, -3.0964e-02, -2.8176e-02],
          [-6.1216e-03,  1.8266e-04, -1.9755e-02],
          [ 7.9346e-03,  1.4552e-03, -8.3036e-04]]],


        [[[ 8.5369e-03, -1.6671e-02,  8.4760e-03],
          [-3.2254e-03,  5.6582e-03,  6.7557e-03],
          [ 5.1970e-03, -1.2866e-02, -1.1188e-02]],

         [[-1.4957e-02,  7.3014e-03, -9.5889e-03],
          [-3.1076e-03, -2.5670e-03, -3.5980e-03],
          [ 7.0863e-03,  4.4332e-03, -1.0857e-02]],

         [[-1.5428e-03, -1.7811e-04,  1.0905e-02],
          [ 5.7127e-04,  1.3200e-02,  1.5393e-03],
          [ 1.0777e-02,  2.3022e-02,  1.8352e-02]],

         ...,

         [[-2.9685e-03,  2.9584e-03, -9.1481e-03],
          [ 6.0822e-04, -6.4700e-03, -2.2522e-03],
          [-1.7074e-03,  7.1829e-03,  1.3605e-02]],

         [[ 4.8075e-03, -3.6900e-03, -1.2047e-02],
          [-4.9598e-03, -3.6318e-03,  3.2160e-03],
          [-5.4112e-03,  1.4102e-04, -1.2009e-02]],

         [[ 5.2131e-04,  1.7746e-03, -4.4573e-03],
          [-1.4841e-02, -1.8009e-03,  2.7428e-04],
          [ 1.1403e-02, -6.1501e-03, -4.2372e-03]]],


        [[[ 2.9305e-03,  1.0458e-02,  2.8972e-03],
          [ 7.4050e-03,  1.4803e-02,  1.8692e-02],
          [-7.0797e-03, -2.3269e-03,  3.9198e-03]],

         [[ 8.3899e-04,  1.3342e-02,  8.8447e-03],
          [-1.1904e-02, -7.1703e-03,  3.7463e-03],
          [-9.8515e-03, -7.5919e-03, -5.9409e-03]],

         [[-3.8992e-03, -7.2620e-03,  6.0351e-03],
          [-5.9638e-03, -1.1403e-02,  9.6857e-03],
          [-1.2803e-03, -5.0859e-03, -1.1414e-03]],

         ...,

         [[ 7.7551e-03,  2.2792e-03, -9.9077e-03],
          [ 6.6015e-03, -2.1153e-03, -1.1808e-02],
          [ 8.6140e-03,  3.7248e-03,  1.4517e-03]],

         [[ 9.0638e-03,  4.8416e-03,  1.9741e-02],
          [-6.8490e-03, -6.4513e-03,  1.8014e-02],
          [-5.2945e-03,  2.7489e-03, -4.9087e-03]],

         [[-4.7040e-03,  4.3436e-03, -1.3064e-02],
          [-1.2686e-02,  2.7001e-03, -7.2921e-03],
          [-7.6856e-03,  1.0478e-02,  8.2013e-03]]],


        ...,


        [[[ 5.7993e-03, -4.2008e-03, -1.6247e-02],
          [-1.0635e-02,  5.5820e-03, -3.2995e-03],
          [ 1.6422e-03,  1.1029e-02, -1.3507e-02]],

         [[-6.4417e-03,  4.7971e-04,  3.6371e-03],
          [-8.1972e-03,  1.5575e-02, -1.0834e-04],
          [-7.2116e-03,  9.2058e-03, -7.8739e-03]],

         [[ 8.8855e-04, -9.5116e-03,  9.8701e-04],
          [-3.3073e-03, -2.6361e-02, -1.1543e-02],
          [ 1.2082e-02, -2.0022e-02, -1.7889e-02]],

         ...,

         [[-1.7843e-02,  6.1157e-03, -7.7393e-03],
          [-7.0536e-03, -5.1387e-03, -1.1834e-02],
          [-5.9894e-03,  1.1275e-02,  6.5639e-03]],

         [[-1.0225e-03, -3.8260e-03, -3.4112e-03],
          [-6.1166e-03,  5.3313e-03,  1.3607e-02],
          [ 9.2397e-03, -5.7343e-03, -9.6190e-03]],

         [[ 1.4349e-02,  1.3910e-02,  5.5147e-03],
          [-1.0887e-03,  4.8186e-03,  1.6729e-03],
          [-5.9199e-04,  4.8285e-04, -1.0905e-03]]],


        [[[-1.3696e-02, -3.7791e-03,  3.3772e-03],
          [-1.3313e-02,  8.5710e-03, -1.8462e-02],
          [-6.6268e-03, -2.5353e-03,  2.6027e-03]],

         [[ 1.1887e-03, -1.3970e-02, -2.2501e-02],
          [ 9.6232e-03,  4.1242e-03, -1.6268e-02],
          [ 8.4646e-03,  4.0215e-03,  4.7755e-03]],

         [[-6.7532e-03,  5.8790e-04,  1.0736e-02],
          [ 1.6241e-03, -1.1176e-03, -6.1480e-03],
          [-7.1075e-03, -5.0265e-03, -7.4026e-03]],

         ...,

         [[ 2.7619e-03,  1.4282e-03, -6.4575e-03],
          [-2.4195e-02, -1.7766e-02, -1.6644e-02],
          [-1.4755e-04, -1.9764e-03,  1.3845e-02]],

         [[ 4.7131e-03, -4.0293e-03,  3.7854e-03],
          [-2.9036e-03,  7.5359e-04,  5.2911e-05],
          [ 1.1008e-02,  1.0067e-02,  5.3875e-03]],

         [[ 8.2152e-03, -1.4078e-02, -1.3189e-02],
          [ 1.8584e-03, -3.2615e-03,  1.3927e-02],
          [-1.4221e-02,  2.9921e-03, -4.8145e-03]]],


        [[[ 8.8664e-03, -6.4636e-03, -3.4978e-03],
          [-1.4267e-02, -6.9746e-04,  4.7940e-03],
          [ 1.0941e-03, -9.7837e-03, -1.1726e-02]],

         [[-4.2767e-03,  1.4072e-02,  2.5511e-03],
          [ 5.7102e-03, -1.0088e-02,  5.0276e-04],
          [ 4.1665e-03,  7.0060e-03, -5.4714e-03]],

         [[-1.2480e-02, -3.5906e-03,  1.6663e-02],
          [-2.6804e-03,  4.5601e-03,  1.8349e-02],
          [-8.7549e-03, -1.7220e-03,  2.4622e-03]],

         ...,

         [[ 1.3955e-02,  5.8151e-03, -3.2835e-04],
          [ 1.0996e-02, -1.4292e-02,  5.5677e-03],
          [ 1.8861e-03, -1.5108e-02,  1.0256e-02]],

         [[ 2.0037e-02,  2.3464e-02,  2.2966e-03],
          [ 1.5866e-02,  1.4835e-03, -5.7150e-03],
          [-9.9890e-03,  1.1568e-02, -1.3212e-02]],

         [[-4.3264e-03, -1.3424e-02,  8.4753e-03],
          [-1.1709e-02, -6.3550e-03, -5.2907e-03],
          [ 1.1262e-02, -2.5227e-03,  2.3783e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.4492e+06, 5.4740e+05, 5.3397e+05,  ..., 7.0400e-01, 6.9674e-01,
        6.1481e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1394]) 

NULL SPACE BASIS :  tensor([[ 2.2743e-02, -2.5782e-02, -1.9324e-02,  ...,  4.6087e-03,
          4.5397e-03,  8.2484e-04],
        [-2.1509e-03, -1.8259e-02, -1.1749e-02,  ..., -1.2639e-03,
         -8.9832e-05,  9.4640e-04],
        [ 1.3550e-02, -2.7289e-02,  1.5198e-02,  ...,  5.2528e-04,
          4.3047e-03, -9.9587e-05],
        ...,
        [-1.5907e-02,  2.6859e-02, -3.8736e-03,  ..., -2.2602e-03,
         -1.4767e-03, -7.6195e-04],
        [-3.5413e-03,  1.9233e-02, -1.5425e-02,  ..., -5.6011e-03,
          1.6184e-03, -9.4380e-04],
        [-5.6047e-03, -3.6942e-03, -1.3408e-02,  ..., -1.4698e-04,
          6.1383e-04, -8.1631e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.0971e-03, -3.2769e-04, -2.9645e-04,  ..., -4.6016e-05,
         -2.7033e-04, -8.6362e-05],
        [-3.2769e-04,  4.4152e-03, -3.7922e-05,  ..., -1.1488e-04,
          1.8010e-04, -1.8681e-04],
        [-2.9645e-04, -3.7922e-05,  5.8615e-03,  ...,  1.5528e-04,
         -2.1618e-04,  1.0434e-04],
        ...,
        [-4.6016e-05, -1.1488e-04,  1.5528e-04,  ...,  1.0241e-02,
         -6.5315e-04, -1.3372e-04],
        [-2.7033e-04,  1.8010e-04, -2.1618e-04,  ..., -6.5315e-04,
          6.4963e-03, -5.6133e-04],
        [-8.6362e-05, -1.8681e-04,  1.0434e-04,  ..., -1.3372e-04,
         -5.6133e-04,  7.7566e-03]], device='cuda:0') 

reserving basis 189/256; cond: 53996.51953125, radio:0.011564877815544605
PARAMETER       :  Parameter containing:
tensor([[[[-0.0161]],

         [[-0.0354]],

         [[-0.0345]],

         ...,

         [[ 0.0119]],

         [[-0.0474]],

         [[-0.0601]]],


        [[[ 0.0618]],

         [[-0.0592]],

         [[ 0.0175]],

         ...,

         [[ 0.0076]],

         [[ 0.0102]],

         [[ 0.0174]]],


        [[[ 0.0423]],

         [[ 0.0229]],

         [[-0.0083]],

         ...,

         [[ 0.0180]],

         [[ 0.0403]],

         [[-0.0246]]],


        ...,


        [[[-0.0274]],

         [[ 0.0100]],

         [[ 0.0433]],

         ...,

         [[ 0.0195]],

         [[-0.0175]],

         [[-0.0191]]],


        [[[ 0.0167]],

         [[ 0.0210]],

         [[-0.0294]],

         ...,

         [[ 0.0404]],

         [[ 0.0080]],

         [[ 0.0420]]],


        [[[-0.0363]],

         [[ 0.0256]],

         [[ 0.0393]],

         ...,

         [[ 0.0524]],

         [[-0.0608]],

         [[ 0.0432]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([5.8462e+05, 3.5032e+04, 2.9515e+04, 1.0465e+04, 8.1979e+03, 6.3436e+03,
        5.3902e+03, 2.8256e+03, 1.9937e+03, 1.7609e+03, 1.4614e+03, 1.2542e+03,
        1.1854e+03, 1.0312e+03, 1.0047e+03, 9.3482e+02, 8.8640e+02, 8.4345e+02,
        7.9349e+02, 7.2221e+02, 6.7148e+02, 6.6576e+02, 6.0151e+02, 5.6116e+02,
        5.1184e+02, 4.7469e+02, 4.5006e+02, 3.9263e+02, 3.7641e+02, 3.7173e+02,
        3.5984e+02, 3.2895e+02, 3.2481e+02, 3.2378e+02, 2.9461e+02, 2.8167e+02,
        2.6952e+02, 2.5150e+02, 2.4237e+02, 2.3829e+02, 2.2353e+02, 2.1738e+02,
        2.0965e+02, 2.0252e+02, 1.9365e+02, 1.9107e+02, 1.8361e+02, 1.7844e+02,
        1.7418e+02, 1.6634e+02, 1.6025e+02, 1.5543e+02, 1.4869e+02, 1.4619e+02,
        1.4479e+02, 1.4316e+02, 1.4118e+02, 1.3596e+02, 1.3418e+02, 1.2996e+02,
        1.2671e+02, 1.2356e+02, 1.2194e+02, 1.1641e+02, 1.1484e+02, 1.1391e+02,
        1.0912e+02, 1.0726e+02, 1.0620e+02, 1.0455e+02, 1.0334e+02, 1.0240e+02,
        9.8509e+01, 9.5944e+01, 9.5572e+01, 9.4926e+01, 9.2345e+01, 9.1342e+01,
        8.9462e+01, 8.8470e+01, 8.5506e+01, 8.4378e+01, 8.3598e+01, 8.3069e+01,
        8.2706e+01, 8.0237e+01, 7.9888e+01, 7.8759e+01, 7.7041e+01, 7.6597e+01,
        7.5475e+01, 7.4424e+01, 7.3866e+01, 7.2968e+01, 7.2302e+01, 7.1524e+01,
        7.1224e+01, 6.9367e+01, 6.8543e+01, 6.7694e+01, 6.7193e+01, 6.6270e+01,
        6.5021e+01, 6.4677e+01, 6.4297e+01, 6.3417e+01, 6.2877e+01, 6.2038e+01,
        6.1407e+01, 6.0513e+01, 6.0148e+01, 5.9233e+01, 5.8856e+01, 5.7652e+01,
        5.6955e+01, 5.6667e+01, 5.6066e+01, 5.5627e+01, 5.4771e+01, 5.4077e+01,
        5.3353e+01, 5.3096e+01, 5.2719e+01, 5.2379e+01, 5.1872e+01, 5.1513e+01,
        5.0878e+01, 5.0398e+01, 5.0250e+01, 4.9776e+01, 4.8873e+01, 4.8845e+01,
        4.8203e+01, 4.7554e+01, 4.6838e+01, 4.6514e+01, 4.6428e+01, 4.5976e+01,
        4.5650e+01, 4.5257e+01, 4.5000e+01, 4.4618e+01, 4.4324e+01, 4.3895e+01,
        4.3376e+01, 4.2987e+01, 4.2743e+01, 4.2119e+01, 4.1888e+01, 4.1594e+01,
        4.1137e+01, 4.0766e+01, 4.0554e+01, 4.0152e+01, 3.9862e+01, 3.9588e+01,
        3.9256e+01, 3.9097e+01, 3.8844e+01, 3.8622e+01, 3.8248e+01, 3.8147e+01,
        3.7701e+01, 3.7457e+01, 3.6759e+01, 3.6427e+01, 3.6174e+01, 3.5893e+01,
        3.5780e+01, 3.5589e+01, 3.5249e+01, 3.4944e+01, 3.4713e+01, 3.4254e+01,
        3.4178e+01, 3.4082e+01, 3.3587e+01, 3.3312e+01, 3.3115e+01, 3.2680e+01,
        3.2617e+01, 3.2257e+01, 3.1938e+01, 3.1637e+01, 3.1482e+01, 3.1155e+01,
        3.0879e+01, 3.0550e+01, 3.0435e+01, 3.0055e+01, 2.9729e+01, 2.9637e+01,
        2.9294e+01, 2.9006e+01, 2.8506e+01, 2.8291e+01, 2.8269e+01, 2.7947e+01,
        2.7726e+01, 2.7619e+01, 2.7521e+01, 2.7265e+01, 2.7156e+01, 2.6834e+01,
        2.6606e+01, 2.6246e+01, 2.6186e+01, 2.5922e+01, 2.5749e+01, 2.5392e+01,
        2.5347e+01, 2.5158e+01, 2.4816e+01, 2.4484e+01, 2.4357e+01, 2.3932e+01,
        2.3824e+01, 2.3468e+01, 2.3341e+01, 2.3243e+01, 2.2867e+01, 2.2793e+01,
        2.2499e+01, 2.2234e+01, 2.1995e+01, 2.1741e+01, 2.1001e+01, 2.0969e+01,
        2.0833e+01, 2.0621e+01, 2.0420e+01, 2.0208e+01, 2.0068e+01, 1.9853e+01,
        1.9650e+01, 1.9437e+01, 1.8949e+01, 1.8693e+01, 1.8396e+01, 1.7999e+01,
        1.7768e+01, 1.7465e+01, 1.7374e+01, 1.6947e+01, 1.6447e+01, 1.5936e+01,
        1.5842e+01, 1.4967e+01, 1.4864e+01, 1.4362e+01, 1.4236e+01, 1.4051e+01,
        1.3209e+01, 1.2976e+01, 1.2427e+01, 1.0827e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 189]) 

NULL SPACE BASIS :  tensor([[ 0.0678,  0.0599, -0.0374,  ...,  0.0016,  0.0110,  0.0025],
        [-0.0140,  0.1110,  0.0311,  ..., -0.0173,  0.0106,  0.0411],
        [ 0.0397, -0.0728,  0.0051,  ...,  0.0447,  0.0387,  0.0139],
        ...,
        [-0.0005, -0.0578, -0.0278,  ..., -0.0316, -0.0177,  0.0078],
        [-0.0376, -0.0201, -0.0586,  ...,  0.0191, -0.0021, -0.0042],
        [-0.0231,  0.0045,  0.0083,  ..., -0.0286, -0.3242,  0.0281]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.1542e-02, -6.1063e-06, -1.7020e-03,  ...,  3.3114e-03,
         -3.0392e-03,  1.2224e-03],
        [-6.1063e-06,  5.8035e-02, -3.2430e-03,  ..., -4.9494e-03,
          2.7188e-03,  7.4361e-04],
        [-1.7020e-03, -3.2430e-03,  6.1516e-02,  ..., -2.9456e-03,
          3.4902e-03, -5.1863e-04],
        ...,
        [ 3.3114e-03, -4.9494e-03, -2.9456e-03,  ...,  5.5267e-02,
         -2.1909e-03,  1.6112e-03],
        [-3.0392e-03,  2.7188e-03,  3.4902e-03,  ..., -2.1909e-03,
          4.4391e-02, -1.1495e-03],
        [ 1.2224e-03,  7.4361e-04, -5.1863e-04,  ...,  1.6112e-03,
         -1.1495e-03,  6.6987e-02]], device='cuda:0') 

reserving basis 1573/4608; cond: 6429037.0, radio:0.0008123407606035471
PARAMETER       :  Parameter containing:
tensor([[[[-4.1395e-03,  4.5780e-03, -1.2615e-02],
          [-8.4637e-03,  5.3435e-03, -1.6105e-02],
          [ 9.6813e-03, -3.0349e-03,  6.6919e-03]],

         [[-7.4282e-03, -2.6026e-03,  3.7544e-03],
          [ 9.0958e-03,  4.5155e-03, -4.1916e-03],
          [ 5.4224e-03, -5.1719e-03,  6.3069e-03]],

         [[ 1.0127e-03, -1.3547e-02, -1.4264e-02],
          [-4.5236e-03, -1.8276e-03, -1.2181e-02],
          [ 1.6535e-03,  1.4116e-02, -4.4824e-03]],

         ...,

         [[-3.5743e-03,  9.5251e-03, -1.1950e-03],
          [ 1.0016e-02,  4.2432e-04,  6.7699e-03],
          [ 5.3134e-03,  1.0760e-02, -5.1819e-03]],

         [[-1.1756e-02,  1.7748e-04, -1.0761e-02],
          [ 1.1675e-03, -6.8570e-03, -1.9888e-03],
          [-4.0074e-04, -1.0158e-02,  1.0668e-02]],

         [[ 8.6142e-05,  1.0225e-02,  8.2070e-03],
          [ 6.7010e-03,  9.0110e-03,  1.2846e-02],
          [-1.2844e-02,  2.9088e-03, -9.3007e-03]]],


        [[[ 5.8666e-03, -3.8132e-03,  7.8558e-03],
          [-1.3410e-02, -8.9636e-03,  5.7645e-03],
          [-2.2990e-02,  9.8329e-04,  6.8792e-03]],

         [[-7.5274e-03,  1.7871e-03,  2.6356e-03],
          [-6.6931e-03,  1.6661e-02, -1.6623e-03],
          [ 1.1403e-02,  7.9485e-03,  1.5566e-02]],

         [[ 5.3711e-03, -6.9897e-03,  5.9979e-03],
          [ 1.2813e-03, -1.1236e-02,  4.8948e-03],
          [ 1.1622e-02, -9.6323e-05,  7.1716e-03]],

         ...,

         [[ 8.9054e-03,  3.6322e-03,  8.0215e-03],
          [ 1.1520e-02, -3.5880e-03, -4.5145e-03],
          [-6.3372e-03, -1.0520e-03,  2.5910e-03]],

         [[ 2.0363e-03, -1.9808e-03, -9.6925e-03],
          [ 1.0616e-03, -4.7297e-03, -1.9998e-03],
          [-2.8590e-03,  1.2515e-02,  2.7637e-03]],

         [[ 1.2167e-02, -7.3726e-03, -9.4902e-03],
          [ 1.2847e-02, -5.1262e-03,  1.8356e-03],
          [-1.8006e-03,  6.0333e-03, -1.0444e-02]]],


        [[[-5.9478e-03, -1.8207e-02, -8.3193e-03],
          [-2.2688e-03,  8.0177e-03, -1.4820e-02],
          [ 1.5558e-02,  1.7973e-02,  5.7917e-03]],

         [[ 7.2356e-03, -1.0195e-02,  9.2319e-04],
          [-5.6778e-03, -2.8895e-03, -2.1446e-02],
          [-4.5613e-03, -4.1400e-03, -2.0954e-02]],

         [[-1.1426e-03, -1.9204e-03,  5.5474e-03],
          [ 1.4482e-03,  2.0559e-03, -4.7011e-04],
          [-5.7810e-03,  1.0154e-02, -1.9605e-03]],

         ...,

         [[ 7.4531e-03,  1.3845e-02,  8.1467e-03],
          [-5.5744e-03,  9.5342e-03, -8.6429e-03],
          [ 4.4052e-04, -1.2732e-02, -2.2682e-03]],

         [[ 5.2627e-03, -1.1255e-02, -5.9067e-03],
          [ 4.0028e-03, -6.4204e-03, -1.9651e-02],
          [-3.7589e-03,  1.6531e-03, -1.3848e-02]],

         [[-1.2837e-02, -1.9052e-02, -3.6428e-03],
          [ 7.5173e-03,  6.0028e-03, -1.1661e-02],
          [-4.9725e-03, -1.0966e-02,  6.0539e-03]]],


        ...,


        [[[-1.1682e-02,  8.4157e-03, -8.2987e-03],
          [ 8.8082e-03,  7.0055e-03,  1.0376e-02],
          [-1.2743e-02, -1.7563e-02,  9.6552e-04]],

         [[-1.6339e-02, -1.7950e-03, -1.2517e-03],
          [ 4.2553e-03,  2.5882e-03, -1.7578e-03],
          [-9.9159e-03,  4.5113e-03, -3.6436e-03]],

         [[-1.5798e-02,  8.5419e-04,  1.2659e-03],
          [-1.3023e-02, -5.6013e-03, -1.5803e-02],
          [-8.1738e-03, -1.4315e-02, -3.1926e-03]],

         ...,

         [[ 1.0093e-02, -1.4799e-02,  5.8039e-03],
          [-3.0395e-03,  4.2150e-03, -1.0632e-02],
          [-7.9099e-03, -1.2860e-03,  5.6345e-03]],

         [[-1.7091e-03, -2.6535e-03, -1.5056e-03],
          [-1.2516e-02, -8.1326e-03, -1.9361e-02],
          [-1.4218e-02, -4.1482e-03, -3.8914e-03]],

         [[-3.3502e-03, -4.0613e-03, -1.0707e-02],
          [ 8.7797e-03,  9.5669e-03, -1.3511e-03],
          [ 1.0778e-02,  5.9142e-03,  1.0544e-02]]],


        [[[-1.4441e-02, -2.5837e-03,  9.8928e-03],
          [ 6.7612e-03, -7.7822e-04,  2.2724e-02],
          [-6.1397e-03, -7.4708e-03,  2.0822e-02]],

         [[ 1.0703e-02, -7.6741e-03, -1.8342e-03],
          [-3.5033e-03, -3.9534e-03,  9.0982e-03],
          [-1.9183e-02, -7.0930e-03, -1.8964e-02]],

         [[-1.5787e-02, -1.4371e-02, -1.3275e-02],
          [ 1.3453e-02,  1.2650e-03, -1.5372e-02],
          [ 8.5691e-03, -1.2032e-02, -2.9939e-03]],

         ...,

         [[ 1.2671e-02,  7.4167e-03, -2.5552e-03],
          [ 1.2906e-02,  9.9473e-03,  1.5679e-02],
          [-6.0933e-03,  9.6564e-03, -1.5054e-02]],

         [[-8.2852e-03,  2.1562e-03, -4.4481e-03],
          [ 4.9050e-03, -4.8693e-03,  1.4357e-03],
          [ 1.5599e-02,  5.3659e-03,  6.4319e-03]],

         [[ 3.6640e-03,  1.1700e-02,  1.0121e-02],
          [-8.3048e-04,  4.3637e-03,  3.2196e-03],
          [-1.7479e-03,  1.6705e-02,  2.1437e-03]]],


        [[[ 1.9950e-02,  4.9775e-03, -1.6952e-03],
          [ 2.8651e-03, -5.5966e-03, -7.2941e-03],
          [-1.0232e-02,  2.4653e-03,  3.7271e-03]],

         [[ 1.5688e-02,  5.8055e-04, -3.1816e-03],
          [-1.0531e-03,  1.7502e-02, -2.7373e-03],
          [ 6.6347e-03,  2.5814e-03,  1.4112e-02]],

         [[ 1.8279e-02,  2.5722e-03,  5.9050e-03],
          [-5.7094e-03, -9.6941e-03, -9.6799e-03],
          [-1.0810e-02, -7.7897e-04, -9.5253e-03]],

         ...,

         [[ 1.0406e-02, -1.4481e-02,  1.9691e-03],
          [ 3.6668e-03,  9.2350e-03,  4.4952e-03],
          [ 1.9386e-02,  2.3774e-02,  2.6576e-02]],

         [[-9.3732e-04,  1.1237e-02, -8.2529e-03],
          [-1.3051e-02,  1.2996e-03, -1.7537e-02],
          [ 7.8115e-03,  7.3989e-04, -1.5558e-03]],

         [[-2.2428e-05,  3.7721e-03, -1.3559e-02],
          [-1.0241e-02,  6.4702e-03, -1.0719e-02],
          [-5.2083e-03, -1.1696e-02,  1.1053e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.5384e+06, 8.9368e+05, 8.6897e+05,  ..., 7.6970e-01, 7.4574e-01,
        7.0592e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1573]) 

NULL SPACE BASIS :  tensor([[-0.0264, -0.0104, -0.0204,  ...,  0.0046,  0.0014,  0.0006],
        [-0.0053, -0.0014, -0.0047,  ...,  0.0020,  0.0041, -0.0048],
        [ 0.0006, -0.0053, -0.0126,  ..., -0.0016, -0.0044,  0.0063],
        ...,
        [-0.0035,  0.0190,  0.0066,  ...,  0.0016,  0.0048,  0.0041],
        [ 0.0109,  0.0155,  0.0036,  ..., -0.0039, -0.0054, -0.0031],
        [-0.0123,  0.0091,  0.0346,  ..., -0.0033,  0.0028,  0.0021]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 8.9098e-03, -2.6113e-03, -9.3500e-05,  ...,  2.9831e-04,
         -5.4212e-05,  4.7843e-05],
        [-2.6113e-03,  7.2005e-03, -2.3256e-03,  ...,  6.5830e-05,
          2.2459e-05, -2.6331e-05],
        [-9.3500e-05, -2.3256e-03,  7.8522e-03,  ...,  2.0850e-05,
          1.1148e-05,  2.6402e-05],
        ...,
        [ 2.9831e-04,  6.5830e-05,  2.0850e-05,  ...,  4.0825e-03,
         -3.4454e-04, -2.7687e-05],
        [-5.4212e-05,  2.2459e-05,  1.1148e-05,  ..., -3.4454e-04,
          3.0590e-03, -3.0623e-04],
        [ 4.7843e-05, -2.6331e-05,  2.6402e-05,  ..., -2.7687e-05,
         -3.0623e-04,  3.4462e-03]], device='cuda:0') 

reserving basis 579/4608; cond: 44793248.0, radio:3.5765224311035126e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 7.1135e-03,  5.3916e-03, -4.3322e-03],
          [-2.8804e-03, -8.9917e-03, -2.0424e-03],
          [ 7.5581e-04, -7.9883e-03,  6.0303e-03]],

         [[-4.0955e-03, -1.1737e-02,  9.5641e-04],
          [-3.8014e-03, -1.2387e-03, -2.4110e-03],
          [ 3.1172e-03,  9.2369e-03, -1.7118e-03]],

         [[-1.0965e-02,  6.3326e-03,  1.1996e-02],
          [ 1.5342e-02,  1.3423e-02,  3.1375e-03],
          [-6.6205e-04,  1.7574e-02,  5.1269e-03]],

         ...,

         [[ 1.1883e-02,  3.4662e-03,  5.2054e-04],
          [ 2.5117e-03, -8.4667e-03,  1.4453e-03],
          [ 5.8151e-03, -5.7461e-03,  7.6093e-03]],

         [[ 7.5723e-03, -1.4139e-03,  2.9900e-03],
          [ 3.7208e-03,  1.2214e-03,  7.3459e-03],
          [ 7.8033e-03,  8.3296e-03, -2.0077e-04]],

         [[-7.9539e-03,  1.2913e-02, -3.4609e-04],
          [-1.0097e-02, -1.2977e-02,  1.7509e-03],
          [-1.2293e-02,  5.2935e-03, -1.4396e-02]]],


        [[[-2.4480e-03,  7.0270e-03, -2.2446e-03],
          [ 2.1877e-03, -2.7613e-03, -9.3776e-03],
          [-3.6315e-03, -1.5530e-02,  5.7850e-03]],

         [[ 4.0074e-03,  6.8590e-03,  3.1921e-03],
          [ 9.6803e-03,  1.4165e-02, -1.2751e-02],
          [ 1.6512e-02,  7.7819e-03, -1.0150e-02]],

         [[ 7.7900e-03,  5.7549e-03,  2.7695e-03],
          [ 3.9512e-03, -4.5086e-03,  6.8452e-03],
          [-2.8272e-03, -7.4532e-03,  5.0769e-04]],

         ...,

         [[-1.8375e-03,  9.3105e-03,  4.6431e-03],
          [-7.7285e-03, -8.4187e-03,  7.1954e-03],
          [-7.6204e-03,  6.4388e-03, -3.6007e-03]],

         [[ 4.6539e-03, -2.8131e-03,  9.4068e-03],
          [-2.9686e-03, -1.3952e-02, -4.5384e-03],
          [ 4.8370e-03, -2.0954e-03, -1.4461e-02]],

         [[ 1.6155e-02,  1.9178e-02, -1.9995e-03],
          [ 4.0829e-04, -9.7236e-05,  6.0079e-03],
          [ 4.5142e-03, -2.8967e-03, -9.8267e-04]]],


        [[[-3.7780e-03, -5.6728e-03, -7.5487e-03],
          [-5.1191e-03, -1.2635e-02, -8.6474e-03],
          [-8.1370e-03, -4.2898e-03, -2.7343e-03]],

         [[-3.3495e-03,  5.2417e-03, -1.2376e-02],
          [ 1.9153e-03, -1.0354e-02, -1.3671e-02],
          [-5.4328e-03, -4.0662e-03, -8.6011e-03]],

         [[-2.2501e-03, -1.2463e-02,  4.1633e-03],
          [-5.5255e-03, -7.6151e-03, -1.3407e-02],
          [-4.8615e-03,  1.3630e-02, -6.2574e-03]],

         ...,

         [[ 3.3241e-03, -1.9139e-02, -2.9358e-04],
          [ 1.6020e-02,  1.0349e-03, -5.0430e-03],
          [-4.7677e-03,  8.3455e-03, -2.5675e-03]],

         [[ 3.9685e-03,  1.8512e-02,  4.6293e-03],
          [-2.6152e-03, -4.9883e-03,  8.8602e-03],
          [ 2.0731e-04, -4.3431e-03,  6.3648e-04]],

         [[-1.4033e-02, -1.5088e-02, -1.6575e-02],
          [-1.2285e-02, -4.5046e-04,  1.3417e-03],
          [-1.2707e-02, -1.0981e-02, -1.7013e-02]]],


        ...,


        [[[-4.4090e-04,  1.8542e-03,  9.2347e-03],
          [ 9.8692e-04,  8.6628e-03, -6.5619e-03],
          [ 6.9601e-03, -1.8019e-04,  5.3551e-03]],

         [[-8.4386e-03, -3.4257e-03,  9.0668e-03],
          [ 9.2718e-03,  6.2245e-03, -7.5794e-03],
          [ 1.2558e-02, -3.8457e-03, -3.1788e-03]],

         [[-1.4602e-02, -1.6003e-03, -1.1391e-02],
          [-1.2502e-02, -1.1380e-02, -3.7344e-03],
          [ 1.1667e-02,  1.6661e-02, -7.2565e-03]],

         ...,

         [[-3.9510e-03,  6.4059e-03,  5.6608e-03],
          [ 9.5903e-03,  2.0168e-03,  5.6364e-03],
          [-4.8081e-03, -3.5754e-03, -2.8184e-03]],

         [[-8.7108e-03,  1.1808e-02, -9.0753e-03],
          [ 1.0029e-02, -1.2291e-02, -8.2462e-03],
          [ 5.2327e-04,  1.3692e-03, -1.6843e-02]],

         [[ 1.6900e-02,  2.0616e-02,  2.3809e-02],
          [ 1.0995e-02, -2.3821e-03,  4.0087e-03],
          [ 1.0333e-02,  1.6066e-02,  6.6909e-03]]],


        [[[ 3.9705e-03, -1.5483e-03,  7.5852e-04],
          [-1.2548e-03, -1.0910e-02, -8.5969e-03],
          [-9.4629e-03,  1.5573e-03, -2.0331e-03]],

         [[-4.2695e-03,  9.6904e-03,  2.9427e-03],
          [-1.3683e-03,  5.9784e-03,  1.3080e-02],
          [-6.3952e-03, -1.0281e-02,  1.1922e-03]],

         [[ 4.8340e-03,  7.7782e-04,  4.5939e-03],
          [-1.2318e-02, -8.5332e-03,  9.7723e-03],
          [ 3.1258e-03,  9.9091e-03, -5.8836e-03]],

         ...,

         [[ 4.3815e-03,  3.7964e-03, -4.0496e-03],
          [ 8.4296e-03,  3.5667e-03,  3.1823e-03],
          [ 2.7159e-03,  1.9784e-03, -1.4462e-02]],

         [[-8.8793e-03, -3.3695e-03, -5.0362e-03],
          [ 2.4741e-03,  1.5732e-03, -1.4586e-02],
          [-2.2266e-04, -8.0355e-03,  2.4597e-03]],

         [[-1.3036e-02,  6.5721e-04,  1.8062e-04],
          [ 3.0529e-03, -4.7420e-03,  4.8697e-03],
          [-1.1433e-02, -1.1732e-02,  1.3529e-03]]],


        [[[-6.3874e-03, -1.0788e-02, -8.4033e-03],
          [ 2.9018e-03, -6.6907e-03,  8.4263e-05],
          [ 2.3560e-03,  1.5764e-03, -4.8437e-03]],

         [[-6.0440e-03,  9.9966e-03,  1.0682e-03],
          [ 2.0873e-03,  8.7527e-03,  2.2275e-03],
          [ 3.0138e-03,  1.1801e-02,  2.2679e-03]],

         [[-1.8185e-03,  5.8113e-03,  6.1419e-03],
          [-7.0246e-03,  1.0286e-02, -1.2544e-02],
          [ 5.9690e-04,  1.9656e-03,  2.7938e-03]],

         ...,

         [[ 5.0807e-03, -7.5418e-04,  1.3592e-02],
          [ 1.0633e-02,  3.9020e-03,  1.4515e-02],
          [ 7.9694e-03,  1.1034e-02,  4.2193e-03]],

         [[-8.0237e-03, -1.9271e-02, -7.0331e-04],
          [-1.1413e-02, -1.2163e-02,  4.4062e-03],
          [ 5.6743e-03, -3.6574e-04,  4.7786e-03]],

         [[-6.7856e-03,  4.9233e-03, -1.5055e-02],
          [-4.8846e-03,  1.0206e-02, -1.2395e-02],
          [ 6.7441e-03, -3.5091e-03, -5.9405e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.0275e+06, 2.2553e+05, 2.0253e+05,  ..., 3.4350e-02, 3.0555e-02,
        2.2939e-02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 579]) 

NULL SPACE BASIS :  tensor([[-1.4890e-02, -1.0241e-02, -2.1433e-02,  ..., -1.8528e-02,
         -1.0439e-03,  1.6251e-02],
        [ 2.0838e-02,  3.3868e-04, -2.4162e-02,  ...,  1.5122e-02,
          1.9313e-02, -1.8804e-02],
        [ 1.1847e-02, -1.2987e-02, -1.8174e-02,  ...,  1.0387e-02,
         -1.5375e-02, -1.3389e-02],
        ...,
        [ 1.3585e-02, -3.4418e-04, -2.2944e-02,  ...,  7.5472e-04,
          2.5635e-03,  9.0435e-04],
        [ 7.7803e-03,  1.1564e-02,  1.5213e-02,  ..., -3.8823e-03,
         -1.6038e-03,  4.9105e-05],
        [-6.2098e-03, -7.9802e-04, -1.3230e-02,  ...,  1.6003e-03,
          3.8938e-03, -8.8923e-04]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2324e-02, -4.4542e-03, -1.0503e-04,  ..., -1.1885e-04,
          2.1919e-04, -1.9144e-05],
        [-4.4542e-03,  1.8449e-02, -4.1936e-03,  ...,  2.3277e-04,
         -7.1741e-06,  4.7526e-05],
        [-1.0503e-04, -4.1936e-03,  2.1265e-02,  ..., -4.0667e-05,
          2.3010e-04, -2.0090e-04],
        ...,
        [-1.1885e-04,  2.3277e-04, -4.0667e-05,  ...,  6.7070e-04,
         -4.0093e-04,  1.7927e-04],
        [ 2.1919e-04, -7.1741e-06,  2.3010e-04,  ..., -4.0093e-04,
          7.9072e-04, -4.0951e-04],
        [-1.9144e-05,  4.7526e-05, -2.0090e-04,  ...,  1.7927e-04,
         -4.0951e-04,  6.6666e-04]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 83.900, Total time 0.56
 * Val loss 0.835, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 97.360, Total time 3.14
 * Val loss 0.080, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 70.100, Total time 0.57
 * Val loss 0.916, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 71.220, Total time 3.17
 * Val loss 0.849, Total time 0.00
**************************************************
validation split name: 3
 * Val Acc 64.000, Total time 0.57
 * Val loss 1.029, Total time 0.00
**************************************************
training split name: 3
 * Val Acc 69.860, Total time 3.17
 * Val loss 0.878, Total time 0.00
**************************************************
validation split name: 4
 * Val Acc 68.300, Total time 0.57
 * Val loss 0.912, Total time 0.00
**************************************************
training split name: 4
 * Val Acc 71.280, Total time 3.18
 * Val loss 0.831, Total time 0.00
**************************************************
validation split name: 5
 * Val Acc 72.800, Total time 0.57
 * Val loss 0.882, Total time 0.00
**************************************************
training split name: 5
 * Val Acc 74.260, Total time 3.22
 * Val loss 0.747, Total time 0.00
**************************************************
validation split name: 6
 * Val Acc 70.600, Total time 0.57
 * Val loss 0.837, Total time 0.00
**************************************************
training split name: 6
 * Val Acc 73.240, Total time 3.24
 * Val loss 0.741, Total time 0.00
**************************************************
validation split name: 7
 * Val Acc 72.100, Total time 0.61
 * Val loss 0.793, Total time 0.00
**************************************************
training split name: 7
 * Val Acc 75.140, Total time 3.28
 * Val loss 0.717, Total time 0.00
**************************************************
====================== 8 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0083 (0.0083)	2.427 (2.427)	15.62 (15.62)
[10/157]	0.0988 (0.0915)	0.0596 (0.0540)	2.009 (2.227)	50.00 (29.83)
[20/157]	0.0966 (0.0939)	0.0583 (0.0564)	1.925 (2.109)	34.38 (34.23)
[30/157]	0.0981 (0.0949)	0.0598 (0.0574)	1.644 (2.020)	40.62 (36.39)
[40/157]	0.0950 (0.0953)	0.0580 (0.0578)	1.599 (1.936)	68.75 (39.41)
[50/157]	0.0981 (0.0957)	0.0590 (0.0582)	1.626 (1.869)	56.25 (42.40)
[60/157]	0.0962 (0.0959)	0.0576 (0.0584)	1.480 (1.811)	59.38 (44.21)
[70/157]	0.0971 (0.0961)	0.0584 (0.0584)	1.415 (1.750)	59.38 (46.43)
[80/157]	0.0978 (0.0963)	0.0581 (0.0585)	1.403 (1.709)	59.38 (47.61)
[90/157]	0.0969 (0.0963)	0.0583 (0.0585)	1.515 (1.681)	53.12 (48.32)
[100/157]	0.0978 (0.0964)	0.0588 (0.0585)	1.278 (1.659)	65.62 (48.86)
[110/157]	0.0956 (0.0965)	0.0579 (0.0586)	1.223 (1.644)	62.50 (49.07)
[120/157]	0.0976 (0.0965)	0.0587 (0.0586)	1.644 (1.631)	37.50 (49.35)
[130/157]	0.0976 (0.0965)	0.0594 (0.0586)	1.248 (1.613)	62.50 (49.86)
[140/157]	0.0975 (0.0966)	0.0583 (0.0587)	1.119 (1.593)	75.00 (50.55)
[150/157]	0.0978 (0.0966)	0.0586 (0.0587)	1.441 (1.574)	59.38 (51.10)
[156/157]	0.0809 (0.0965)	0.0547 (0.0587)	1.182 (1.568)	50.00 (51.20)
 * Train Acc 51.200
 * Val Acc 57.100, Total time 0.59
 * Val loss 1.213, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0414 (0.0414)	0.0083 (0.0083)	1.166 (1.166)	62.50 (62.50)
[10/157]	0.0976 (0.0913)	0.0593 (0.0543)	1.399 (1.363)	62.50 (55.40)
[20/157]	0.0970 (0.0938)	0.0593 (0.0567)	1.797 (1.374)	40.62 (55.80)
[30/157]	0.0991 (0.0948)	0.0595 (0.0576)	1.506 (1.366)	53.12 (56.55)
[40/157]	0.0971 (0.0953)	0.0587 (0.0580)	1.271 (1.348)	62.50 (57.62)
[50/157]	0.0980 (0.0956)	0.0591 (0.0582)	1.619 (1.338)	46.88 (57.41)
[60/157]	0.0960 (0.0958)	0.0585 (0.0584)	1.539 (1.344)	37.50 (57.17)
[70/157]	0.0978 (0.0960)	0.0589 (0.0585)	1.423 (1.343)	62.50 (57.17)
[80/157]	0.0946 (0.0961)	0.0564 (0.0586)	1.226 (1.332)	68.75 (57.83)
[90/157]	0.0981 (0.0961)	0.0599 (0.0586)	1.542 (1.326)	46.88 (58.00)
[100/157]	0.0984 (0.0962)	0.0588 (0.0587)	1.343 (1.321)	59.38 (58.38)
[110/157]	0.0972 (0.0964)	0.0594 (0.0588)	1.157 (1.322)	62.50 (58.08)
[120/157]	0.0982 (0.0965)	0.0583 (0.0588)	1.161 (1.320)	59.38 (58.08)
[130/157]	0.0960 (0.0965)	0.0574 (0.0589)	1.361 (1.313)	56.25 (58.35)
[140/157]	0.0985 (0.0966)	0.0590 (0.0589)	1.712 (1.320)	40.62 (58.22)
[150/157]	0.0959 (0.0966)	0.0581 (0.0588)	1.438 (1.315)	65.62 (58.42)
[156/157]	0.0794 (0.0965)	0.0541 (0.0588)	2.267 (1.315)	37.50 (58.36)
 * Train Acc 58.360
 * Val Acc 60.600, Total time 0.59
 * Val loss 1.128, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0085 (0.0085)	1.216 (1.216)	62.50 (62.50)
[10/157]	0.0971 (0.0915)	0.0580 (0.0539)	1.286 (1.293)	53.12 (57.39)
[20/157]	0.0963 (0.0944)	0.0586 (0.0567)	1.295 (1.254)	62.50 (60.57)
[30/157]	0.0999 (0.0976)	0.0616 (0.0597)	1.424 (1.267)	53.12 (60.48)
[40/157]	0.0961 (0.0980)	0.0579 (0.0600)	0.920 (1.267)	87.50 (60.82)
[50/157]	0.1001 (0.0982)	0.0608 (0.0600)	1.079 (1.255)	59.38 (61.21)
[60/157]	0.0990 (0.0984)	0.0603 (0.0602)	1.272 (1.262)	65.62 (60.76)
[70/157]	0.0953 (0.0985)	0.0560 (0.0601)	1.212 (1.272)	65.62 (60.43)
[80/157]	0.0994 (0.0986)	0.0608 (0.0602)	1.238 (1.265)	53.12 (60.38)
[90/157]	0.0995 (0.0986)	0.0603 (0.0603)	1.413 (1.269)	56.25 (60.34)
[100/157]	0.1010 (0.0988)	0.0610 (0.0603)	1.310 (1.267)	50.00 (60.24)
[110/157]	0.0996 (0.0988)	0.0598 (0.0603)	1.167 (1.268)	65.62 (60.33)
[120/157]	0.0992 (0.0989)	0.0604 (0.0604)	1.201 (1.273)	75.00 (60.18)
[130/157]	0.1009 (0.0989)	0.0606 (0.0604)	1.485 (1.280)	56.25 (59.80)
[140/157]	0.0989 (0.0989)	0.0593 (0.0605)	1.162 (1.273)	62.50 (59.97)
[150/157]	0.0985 (0.0989)	0.0598 (0.0605)	1.280 (1.267)	56.25 (60.20)
[156/157]	0.0817 (0.0988)	0.0554 (0.0605)	1.316 (1.266)	62.50 (60.16)
 * Train Acc 60.160
 * Val Acc 60.700, Total time 0.60
 * Val loss 1.115, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0086 (0.0086)	1.197 (1.197)	65.62 (65.62)
[10/157]	0.0997 (0.0936)	0.0607 (0.0557)	1.065 (1.250)	65.62 (57.10)
[20/157]	0.0994 (0.0961)	0.0608 (0.0581)	1.362 (1.212)	50.00 (59.08)
[30/157]	0.0991 (0.0970)	0.0595 (0.0589)	1.302 (1.245)	53.12 (58.47)
[40/157]	0.0996 (0.0974)	0.0615 (0.0593)	1.133 (1.255)	65.62 (58.31)
[50/157]	0.0994 (0.0978)	0.0604 (0.0597)	1.021 (1.250)	68.75 (58.64)
[60/157]	0.1000 (0.0981)	0.0603 (0.0597)	1.425 (1.243)	53.12 (59.22)
[70/157]	0.0993 (0.0983)	0.0606 (0.0599)	1.549 (1.245)	56.25 (59.60)
[80/157]	0.1013 (0.0984)	0.0621 (0.0600)	1.172 (1.238)	56.25 (59.72)
[90/157]	0.0994 (0.0985)	0.0597 (0.0601)	0.940 (1.232)	71.88 (60.10)
[100/157]	0.0987 (0.0986)	0.0593 (0.0601)	1.241 (1.226)	56.25 (60.12)
[110/157]	0.0990 (0.0986)	0.0608 (0.0601)	1.128 (1.234)	56.25 (59.66)
[120/157]	0.1001 (0.0986)	0.0615 (0.0602)	1.213 (1.229)	65.62 (60.12)
[130/157]	0.0989 (0.0987)	0.0601 (0.0602)	0.993 (1.229)	68.75 (60.21)
[140/157]	0.1003 (0.0987)	0.0609 (0.0603)	1.114 (1.232)	62.50 (59.97)
[150/157]	0.0997 (0.0987)	0.0606 (0.0603)	1.152 (1.222)	62.50 (60.45)
[156/157]	0.0804 (0.0986)	0.0546 (0.0603)	1.403 (1.224)	50.00 (60.42)
 * Train Acc 60.420
 * Val Acc 62.200, Total time 0.60
 * Val loss 1.071, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0087 (0.0087)	1.217 (1.217)	65.62 (65.62)
[10/157]	0.1005 (0.0939)	0.0600 (0.0554)	1.364 (1.124)	59.38 (64.77)
[20/157]	0.0992 (0.0965)	0.0600 (0.0576)	1.150 (1.131)	59.38 (64.14)
[30/157]	0.0988 (0.0973)	0.0599 (0.0586)	1.371 (1.149)	56.25 (64.21)
[40/157]	0.0996 (0.0978)	0.0596 (0.0590)	1.413 (1.161)	59.38 (64.02)
[50/157]	0.0997 (0.0981)	0.0609 (0.0593)	1.637 (1.175)	50.00 (63.30)
[60/157]	0.0985 (0.0982)	0.0602 (0.0596)	1.332 (1.187)	65.62 (62.65)
[70/157]	0.0999 (0.0983)	0.0612 (0.0597)	1.170 (1.189)	56.25 (62.32)
[80/157]	0.0986 (0.0984)	0.0599 (0.0599)	1.168 (1.212)	68.75 (61.77)
[90/157]	0.0994 (0.0985)	0.0607 (0.0600)	1.183 (1.212)	62.50 (61.81)
[100/157]	0.1000 (0.0986)	0.0612 (0.0600)	1.069 (1.210)	62.50 (61.66)
[110/157]	0.0995 (0.0986)	0.0607 (0.0601)	1.101 (1.222)	65.62 (61.12)
[120/157]	0.0984 (0.0987)	0.0607 (0.0601)	1.131 (1.222)	56.25 (61.18)
[130/157]	0.1000 (0.0987)	0.0607 (0.0602)	1.360 (1.217)	53.12 (61.50)
[140/157]	0.1005 (0.0988)	0.0616 (0.0603)	1.142 (1.209)	65.62 (61.64)
[150/157]	0.0990 (0.0988)	0.0606 (0.0603)	0.923 (1.204)	71.88 (61.88)
[156/157]	0.0845 (0.0987)	0.0574 (0.0603)	1.088 (1.210)	62.50 (61.64)
 * Train Acc 61.640
 * Val Acc 63.000, Total time 0.59
 * Val loss 1.053, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0085 (0.0085)	1.098 (1.098)	62.50 (62.50)
[10/157]	0.0985 (0.0932)	0.0603 (0.0553)	1.356 (1.110)	59.38 (63.35)
[20/157]	0.0997 (0.0962)	0.0609 (0.0583)	1.203 (1.165)	62.50 (62.65)
[30/157]	0.1001 (0.0972)	0.0607 (0.0591)	1.080 (1.195)	71.88 (61.49)
[40/157]	0.0998 (0.0977)	0.0614 (0.0595)	1.018 (1.207)	68.75 (60.82)
[50/157]	0.0994 (0.0981)	0.0611 (0.0599)	1.446 (1.213)	50.00 (60.91)
[60/157]	0.0990 (0.0984)	0.0595 (0.0601)	1.404 (1.204)	46.88 (61.12)
[70/157]	0.1023 (0.0985)	0.0607 (0.0602)	1.119 (1.187)	65.62 (61.88)
[80/157]	0.1004 (0.0987)	0.0602 (0.0602)	1.455 (1.180)	53.12 (62.23)
[90/157]	0.0996 (0.0988)	0.0595 (0.0602)	1.195 (1.179)	62.50 (62.19)
[100/157]	0.0996 (0.0989)	0.0593 (0.0603)	1.285 (1.191)	56.25 (61.82)
[110/157]	0.0998 (0.0990)	0.0611 (0.0603)	1.852 (1.197)	37.50 (61.66)
[120/157]	0.1209 (0.0994)	0.0800 (0.0607)	1.099 (1.190)	75.00 (61.91)
[130/157]	0.1044 (0.1000)	0.0640 (0.0613)	1.446 (1.191)	53.12 (61.83)
[140/157]	0.1056 (0.1004)	0.0665 (0.0616)	1.208 (1.196)	59.38 (62.08)
[150/157]	0.1038 (0.1007)	0.0639 (0.0619)	1.266 (1.192)	62.50 (62.23)
[156/157]	0.0886 (0.1007)	0.0615 (0.0620)	1.364 (1.191)	62.50 (62.36)
 * Train Acc 62.360
 * Val Acc 64.500, Total time 0.62
 * Val loss 1.030, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0085 (0.0085)	1.153 (1.153)	65.62 (65.62)
[10/157]	0.1028 (0.0986)	0.0652 (0.0598)	0.919 (1.173)	81.25 (64.77)
[20/157]	0.0961 (0.0969)	0.0585 (0.0591)	1.673 (1.226)	43.75 (61.01)
[30/157]	0.0942 (0.0963)	0.0569 (0.0589)	1.198 (1.195)	71.88 (63.10)
[40/157]	0.0966 (0.0961)	0.0583 (0.0588)	1.767 (1.193)	46.88 (63.19)
[50/157]	0.1206 (0.0980)	0.0802 (0.0604)	0.970 (1.171)	71.88 (64.22)
[60/157]	0.0985 (0.0983)	0.0600 (0.0606)	1.381 (1.173)	46.88 (63.78)
[70/157]	0.0992 (0.0983)	0.0607 (0.0605)	1.409 (1.178)	62.50 (63.38)
[80/157]	0.0992 (0.0982)	0.0601 (0.0605)	0.900 (1.180)	65.62 (62.96)
[90/157]	0.0992 (0.0982)	0.0604 (0.0604)	1.512 (1.182)	43.75 (62.81)
[100/157]	0.0982 (0.0982)	0.0599 (0.0604)	1.318 (1.193)	56.25 (62.22)
[110/157]	0.0972 (0.0982)	0.0592 (0.0603)	1.212 (1.191)	65.62 (62.36)
[120/157]	0.0995 (0.0983)	0.0601 (0.0604)	1.284 (1.179)	62.50 (62.91)
[130/157]	0.1031 (0.0985)	0.0636 (0.0605)	1.367 (1.172)	50.00 (63.17)
[140/157]	0.1019 (0.0987)	0.0628 (0.0607)	1.144 (1.168)	56.25 (63.25)
[150/157]	0.1006 (0.0989)	0.0616 (0.0608)	1.171 (1.173)	50.00 (63.08)
[156/157]	0.0860 (0.0989)	0.0587 (0.0609)	1.575 (1.175)	25.00 (63.02)
 * Train Acc 63.020
 * Val Acc 64.600, Total time 0.60
 * Val loss 1.020, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0087 (0.0087)	1.180 (1.180)	68.75 (68.75)
[10/157]	0.1005 (0.0956)	0.0616 (0.0569)	0.667 (1.062)	87.50 (67.05)
[20/157]	0.1029 (0.0985)	0.0630 (0.0599)	1.578 (1.137)	59.38 (63.54)
[30/157]	0.1010 (0.0992)	0.0623 (0.0607)	1.141 (1.135)	71.88 (64.11)
[40/157]	0.0999 (0.0997)	0.0608 (0.0612)	1.193 (1.156)	59.38 (63.03)
[50/157]	0.1013 (0.1000)	0.0623 (0.0615)	1.253 (1.154)	50.00 (62.87)
[60/157]	0.1004 (0.1002)	0.0615 (0.0617)	0.753 (1.153)	81.25 (63.11)
[70/157]	0.1014 (0.1002)	0.0623 (0.0618)	1.019 (1.142)	81.25 (63.73)
[80/157]	0.0998 (0.1004)	0.0608 (0.0619)	1.330 (1.144)	56.25 (63.27)
[90/157]	0.1019 (0.1004)	0.0625 (0.0620)	1.067 (1.149)	75.00 (63.12)
[100/157]	0.1001 (0.1005)	0.0606 (0.0620)	1.238 (1.151)	68.75 (63.15)
[110/157]	0.1028 (0.1005)	0.0632 (0.0620)	0.854 (1.160)	75.00 (62.95)
[120/157]	0.1002 (0.1006)	0.0610 (0.0619)	1.319 (1.155)	59.38 (63.35)
[130/157]	0.1002 (0.1007)	0.0612 (0.0620)	1.353 (1.153)	50.00 (63.43)
[140/157]	0.1025 (0.1007)	0.0630 (0.0620)	1.128 (1.150)	71.88 (63.65)
[150/157]	0.0993 (0.1007)	0.0602 (0.0620)	1.455 (1.153)	50.00 (63.62)
[156/157]	0.0860 (0.1006)	0.0575 (0.0620)	1.255 (1.146)	37.50 (63.84)
 * Train Acc 63.840
 * Val Acc 64.500, Total time 0.61
 * Val loss 1.008, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0085 (0.0085)	1.103 (1.103)	78.12 (78.12)
[10/157]	0.1010 (0.0955)	0.0622 (0.0564)	0.805 (1.106)	78.12 (66.19)
[20/157]	0.1009 (0.0978)	0.0622 (0.0591)	1.213 (1.107)	56.25 (65.77)
[30/157]	0.1016 (0.0990)	0.0627 (0.0604)	1.012 (1.114)	68.75 (65.62)
[40/157]	0.1016 (0.0994)	0.0627 (0.0608)	1.258 (1.137)	62.50 (64.71)
[50/157]	0.1005 (0.0999)	0.0617 (0.0613)	0.982 (1.118)	65.62 (65.44)
[60/157]	0.1026 (0.1001)	0.0629 (0.0615)	1.202 (1.128)	59.38 (64.65)
[70/157]	0.0998 (0.1001)	0.0612 (0.0616)	1.521 (1.137)	59.38 (64.44)
[80/157]	0.1012 (0.1003)	0.0616 (0.0617)	0.904 (1.127)	71.88 (65.01)
[90/157]	0.1005 (0.1003)	0.0617 (0.0617)	1.006 (1.129)	71.88 (64.77)
[100/157]	0.1027 (0.1004)	0.0625 (0.0618)	0.718 (1.120)	81.25 (65.13)
[110/157]	0.1006 (0.1005)	0.0610 (0.0618)	1.156 (1.125)	50.00 (64.89)
[120/157]	0.1021 (0.1006)	0.0624 (0.0619)	1.007 (1.121)	71.88 (65.06)
[130/157]	0.1024 (0.1007)	0.0622 (0.0620)	1.019 (1.117)	68.75 (65.08)
[140/157]	0.1011 (0.1007)	0.0624 (0.0620)	1.293 (1.117)	53.12 (65.03)
[150/157]	0.1009 (0.1007)	0.0620 (0.0620)	1.413 (1.129)	56.25 (64.49)
[156/157]	0.0846 (0.1006)	0.0573 (0.0620)	1.202 (1.128)	62.50 (64.56)
 * Train Acc 64.560
 * Val Acc 65.400, Total time 0.58
 * Val loss 0.997, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0415 (0.0415)	0.0086 (0.0086)	1.239 (1.239)	65.62 (65.62)
[10/157]	0.0955 (0.0924)	0.0580 (0.0541)	1.099 (1.051)	62.50 (67.05)
[20/157]	0.0964 (0.0939)	0.0579 (0.0561)	0.873 (1.034)	78.12 (67.11)
[30/157]	0.0953 (0.0944)	0.0570 (0.0568)	1.199 (1.049)	50.00 (66.73)
[40/157]	0.1090 (0.0950)	0.0682 (0.0574)	0.959 (1.074)	68.75 (66.08)
[50/157]	0.0987 (0.0971)	0.0591 (0.0593)	1.369 (1.100)	59.38 (65.38)
[60/157]	0.0987 (0.0973)	0.0596 (0.0594)	1.220 (1.102)	59.38 (65.47)
[70/157]	0.0986 (0.0975)	0.0603 (0.0595)	1.057 (1.108)	84.38 (65.27)
[80/157]	0.0986 (0.0977)	0.0594 (0.0595)	0.995 (1.112)	65.62 (64.39)
[90/157]	0.0999 (0.0977)	0.0603 (0.0596)	1.340 (1.114)	59.38 (64.46)
[100/157]	0.0966 (0.0978)	0.0593 (0.0596)	0.902 (1.117)	71.88 (64.45)
[110/157]	0.0966 (0.0978)	0.0573 (0.0597)	1.027 (1.123)	71.88 (64.22)
[120/157]	0.0978 (0.0979)	0.0587 (0.0597)	1.152 (1.126)	62.50 (64.05)
[130/157]	0.0978 (0.0979)	0.0592 (0.0597)	0.859 (1.122)	75.00 (64.36)
[140/157]	0.0971 (0.0979)	0.0583 (0.0597)	0.973 (1.119)	68.75 (64.49)
[150/157]	0.0984 (0.0979)	0.0593 (0.0597)	1.318 (1.123)	50.00 (64.36)
[156/157]	0.0840 (0.0978)	0.0554 (0.0597)	1.233 (1.124)	62.50 (64.40)
 * Train Acc 64.400
 * Val Acc 66.700, Total time 0.58
 * Val loss 0.966, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0086 (0.0086)	1.072 (1.072)	68.75 (68.75)
[10/157]	0.0987 (0.0932)	0.0604 (0.0551)	1.259 (1.156)	56.25 (62.50)
[20/157]	0.0945 (0.0945)	0.0573 (0.0569)	1.054 (1.131)	71.88 (64.73)
[30/157]	0.1083 (0.0974)	0.0683 (0.0595)	1.381 (1.123)	62.50 (65.42)
[40/157]	0.1088 (0.1000)	0.0685 (0.0618)	1.022 (1.118)	71.88 (65.17)
[50/157]	0.1178 (0.1002)	0.0778 (0.0621)	1.155 (1.105)	65.62 (65.62)
[60/157]	0.1033 (0.1027)	0.0636 (0.0644)	1.190 (1.101)	56.25 (65.16)
[70/157]	0.1005 (0.1024)	0.0614 (0.0641)	1.306 (1.109)	53.12 (65.01)
[80/157]	0.1022 (0.1023)	0.0622 (0.0639)	1.354 (1.116)	59.38 (65.12)
[90/157]	0.1011 (0.1021)	0.0616 (0.0637)	1.129 (1.106)	62.50 (65.59)
[100/157]	0.1022 (0.1021)	0.0627 (0.0636)	1.135 (1.101)	68.75 (65.81)
[110/157]	0.1003 (0.1019)	0.0612 (0.0635)	0.918 (1.107)	71.88 (65.51)
[120/157]	0.1037 (0.1018)	0.0638 (0.0634)	1.340 (1.110)	62.50 (65.26)
[130/157]	0.1003 (0.1018)	0.0617 (0.0633)	1.010 (1.112)	62.50 (65.46)
[140/157]	0.1016 (0.1017)	0.0627 (0.0632)	1.186 (1.112)	65.62 (65.20)
[150/157]	0.1003 (0.1017)	0.0609 (0.0632)	1.251 (1.110)	53.12 (65.15)
[156/157]	0.0866 (0.1015)	0.0572 (0.0631)	1.210 (1.110)	62.50 (65.18)
 * Train Acc 65.180
 * Val Acc 65.100, Total time 0.61
 * Val loss 0.995, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0087 (0.0087)	1.226 (1.226)	65.62 (65.62)
[10/157]	0.1025 (0.0958)	0.0628 (0.0569)	1.600 (1.205)	53.12 (62.22)
[20/157]	0.1041 (0.0982)	0.0631 (0.0594)	0.915 (1.173)	78.12 (63.10)
[30/157]	0.1008 (0.0992)	0.0616 (0.0603)	1.051 (1.173)	65.62 (63.81)
[40/157]	0.1019 (0.0996)	0.0632 (0.0608)	1.260 (1.153)	62.50 (64.18)
[50/157]	0.1007 (0.0999)	0.0614 (0.0611)	1.084 (1.124)	59.38 (65.44)
[60/157]	0.1005 (0.0999)	0.0617 (0.0612)	1.248 (1.123)	53.12 (64.96)
[70/157]	0.1021 (0.1001)	0.0623 (0.0613)	1.129 (1.113)	59.38 (65.18)
[80/157]	0.0993 (0.1002)	0.0605 (0.0615)	1.341 (1.119)	56.25 (64.89)
[90/157]	0.1017 (0.1002)	0.0630 (0.0616)	1.465 (1.120)	56.25 (64.94)
[100/157]	0.1003 (0.1003)	0.0610 (0.0617)	0.955 (1.118)	65.62 (64.79)
[110/157]	0.1023 (0.1005)	0.0629 (0.0618)	1.108 (1.112)	65.62 (64.92)
[120/157]	0.1022 (0.1005)	0.0623 (0.0618)	1.130 (1.106)	68.75 (64.93)
[130/157]	0.1012 (0.1006)	0.0613 (0.0618)	1.107 (1.109)	71.88 (64.91)
[140/157]	0.1030 (0.1006)	0.0632 (0.0619)	1.226 (1.112)	65.62 (64.96)
[150/157]	0.1013 (0.1006)	0.0612 (0.0619)	1.409 (1.119)	53.12 (64.78)
[156/157]	0.0876 (0.1005)	0.0583 (0.0619)	0.954 (1.116)	62.50 (64.96)
 * Train Acc 64.960
 * Val Acc 67.200, Total time 0.60
 * Val loss 0.955, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0087 (0.0087)	1.077 (1.077)	65.62 (65.62)
[10/157]	0.1021 (0.0958)	0.0623 (0.0569)	0.896 (1.065)	78.12 (65.34)
[20/157]	0.1025 (0.0983)	0.0619 (0.0592)	1.273 (1.116)	53.12 (62.50)
[30/157]	0.1013 (0.0992)	0.0615 (0.0600)	0.883 (1.101)	81.25 (64.11)
[40/157]	0.1010 (0.0998)	0.0616 (0.0606)	1.266 (1.090)	59.38 (65.02)
[50/157]	0.1022 (0.1000)	0.0625 (0.0610)	1.267 (1.106)	65.62 (64.46)
[60/157]	0.1011 (0.1002)	0.0613 (0.0611)	1.285 (1.106)	50.00 (64.55)
[70/157]	0.1020 (0.1004)	0.0621 (0.0613)	1.368 (1.102)	56.25 (64.74)
[80/157]	0.1003 (0.1004)	0.0614 (0.0614)	0.775 (1.092)	71.88 (65.08)
[90/157]	0.1023 (0.1005)	0.0629 (0.0616)	1.207 (1.087)	50.00 (65.18)
[100/157]	0.1020 (0.1006)	0.0627 (0.0616)	1.161 (1.088)	65.62 (65.47)
[110/157]	0.1032 (0.1007)	0.0606 (0.0616)	1.053 (1.089)	71.88 (65.48)
[120/157]	0.0995 (0.1007)	0.0608 (0.0616)	0.999 (1.094)	62.50 (65.37)
[130/157]	0.1015 (0.1007)	0.0620 (0.0617)	0.830 (1.098)	68.75 (65.12)
[140/157]	0.1014 (0.1008)	0.0623 (0.0618)	1.308 (1.096)	65.62 (65.16)
[150/157]	0.1024 (0.1008)	0.0631 (0.0618)	1.000 (1.093)	68.75 (65.27)
[156/157]	0.0853 (0.1007)	0.0577 (0.0618)	1.119 (1.094)	62.50 (65.32)
 * Train Acc 65.320
 * Val Acc 67.800, Total time 0.60
 * Val loss 0.940, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0089 (0.0089)	1.091 (1.091)	65.62 (65.62)
[10/157]	0.1037 (0.0959)	0.0621 (0.0571)	0.880 (1.097)	78.12 (65.34)
[20/157]	0.1015 (0.0980)	0.0621 (0.0593)	1.042 (1.050)	62.50 (65.33)
[30/157]	0.1000 (0.0992)	0.0609 (0.0604)	1.107 (1.090)	68.75 (64.72)
[40/157]	0.1015 (0.0995)	0.0624 (0.0608)	0.786 (1.094)	78.12 (64.25)
[50/157]	0.1015 (0.0999)	0.0620 (0.0613)	1.123 (1.113)	71.88 (64.22)
[60/157]	0.1014 (0.1000)	0.0621 (0.0613)	1.205 (1.115)	59.38 (64.19)
[70/157]	0.1027 (0.1002)	0.0630 (0.0615)	0.904 (1.107)	75.00 (64.08)
[80/157]	0.1018 (0.1003)	0.0626 (0.0616)	1.153 (1.110)	59.38 (64.16)
[90/157]	0.1027 (0.1004)	0.0623 (0.0617)	1.154 (1.101)	59.38 (64.56)
[100/157]	0.1016 (0.1005)	0.0620 (0.0617)	1.223 (1.096)	59.38 (64.91)
[110/157]	0.1017 (0.1005)	0.0624 (0.0617)	0.906 (1.092)	90.62 (65.23)
[120/157]	0.1013 (0.1006)	0.0614 (0.0618)	1.059 (1.093)	62.50 (65.21)
[130/157]	0.1014 (0.1006)	0.0617 (0.0618)	1.341 (1.089)	59.38 (65.48)
[140/157]	0.1007 (0.1007)	0.0612 (0.0619)	1.120 (1.093)	68.75 (65.43)
[150/157]	0.1009 (0.1007)	0.0610 (0.0619)	1.124 (1.089)	68.75 (65.56)
[156/157]	0.0859 (0.1006)	0.0573 (0.0619)	1.951 (1.095)	25.00 (65.30)
 * Train Acc 65.300
 * Val Acc 68.200, Total time 0.57
 * Val loss 0.943, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0088 (0.0088)	1.226 (1.226)	62.50 (62.50)
[10/157]	0.0971 (0.0929)	0.0579 (0.0545)	1.131 (1.061)	62.50 (64.20)
[20/157]	0.1037 (0.0977)	0.0648 (0.0592)	1.011 (1.048)	71.88 (66.37)
[30/157]	0.1055 (0.1000)	0.0657 (0.0613)	1.328 (1.048)	56.25 (66.73)
[40/157]	0.1193 (0.1001)	0.0792 (0.0614)	0.942 (1.048)	65.62 (66.39)
[50/157]	0.0967 (0.1009)	0.0589 (0.0624)	1.177 (1.052)	62.50 (66.48)
[60/157]	0.0963 (0.1001)	0.0576 (0.0617)	1.036 (1.048)	68.75 (66.65)
[70/157]	0.0976 (0.0995)	0.0590 (0.0612)	1.017 (1.038)	71.88 (67.34)
[80/157]	0.1062 (0.1010)	0.0655 (0.0624)	0.917 (1.030)	71.88 (68.06)
[90/157]	0.1050 (0.1016)	0.0648 (0.0629)	0.922 (1.035)	71.88 (67.69)
[100/157]	0.0975 (0.1012)	0.0581 (0.0625)	1.001 (1.036)	81.25 (67.79)
[110/157]	0.0969 (0.1007)	0.0584 (0.0621)	1.215 (1.043)	56.25 (67.37)
[120/157]	0.0965 (0.1003)	0.0579 (0.0618)	1.045 (1.052)	68.75 (66.89)
[130/157]	0.0959 (0.1000)	0.0577 (0.0615)	1.014 (1.059)	65.62 (66.53)
[140/157]	0.0989 (0.1006)	0.0603 (0.0621)	1.379 (1.062)	53.12 (66.31)
[150/157]	0.0982 (0.1004)	0.0600 (0.0619)	1.196 (1.060)	59.38 (66.41)
[156/157]	0.0801 (0.1001)	0.0550 (0.0618)	1.965 (1.061)	0.00 (66.40)
 * Train Acc 66.400
 * Val Acc 67.700, Total time 0.59
 * Val loss 0.929, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0086 (0.0086)	1.364 (1.364)	56.25 (56.25)
[10/157]	0.0970 (0.0931)	0.0583 (0.0547)	1.240 (1.031)	59.38 (69.32)
[20/157]	0.0987 (0.0955)	0.0598 (0.0573)	1.202 (1.049)	62.50 (68.75)
[30/157]	0.0978 (0.0962)	0.0595 (0.0582)	1.020 (1.049)	71.88 (67.94)
[40/157]	0.0987 (0.0967)	0.0595 (0.0585)	0.972 (1.047)	75.00 (67.91)
[50/157]	0.0989 (0.0970)	0.0600 (0.0588)	1.356 (1.054)	62.50 (67.65)
[60/157]	0.1006 (0.0975)	0.0615 (0.0592)	1.092 (1.081)	62.50 (66.19)
[70/157]	0.1021 (0.0979)	0.0629 (0.0596)	1.109 (1.073)	59.38 (66.20)
[80/157]	0.1011 (0.0984)	0.0616 (0.0600)	1.269 (1.076)	65.62 (65.74)
[90/157]	0.1013 (0.0986)	0.0624 (0.0602)	1.260 (1.067)	62.50 (66.21)
[100/157]	0.1014 (0.0989)	0.0618 (0.0604)	1.107 (1.055)	62.50 (66.65)
[110/157]	0.1000 (0.0991)	0.0599 (0.0606)	0.857 (1.054)	68.75 (66.61)
[120/157]	0.1010 (0.0993)	0.0617 (0.0607)	0.909 (1.053)	65.62 (66.58)
[130/157]	0.1002 (0.0994)	0.0600 (0.0608)	0.859 (1.052)	71.88 (66.75)
[140/157]	0.1008 (0.0995)	0.0617 (0.0609)	1.053 (1.058)	81.25 (66.56)
[150/157]	0.0999 (0.0996)	0.0605 (0.0610)	1.309 (1.062)	59.38 (66.39)
[156/157]	0.0857 (0.0996)	0.0583 (0.0610)	1.122 (1.064)	50.00 (66.46)
 * Train Acc 66.460
 * Val Acc 67.200, Total time 0.60
 * Val loss 0.930, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0451 (0.0451)	0.0092 (0.0092)	0.990 (0.990)	68.75 (68.75)
[10/157]	0.1001 (0.0956)	0.0614 (0.0568)	1.177 (1.086)	56.25 (63.92)
[20/157]	0.1012 (0.0979)	0.0621 (0.0595)	0.989 (1.076)	71.88 (65.48)
[30/157]	0.1006 (0.0990)	0.0611 (0.0605)	1.062 (1.093)	62.50 (64.01)
[40/157]	0.1007 (0.0993)	0.0616 (0.0609)	0.739 (1.091)	87.50 (64.71)
[50/157]	0.1037 (0.0996)	0.0639 (0.0612)	0.822 (1.067)	71.88 (65.99)
[60/157]	0.1016 (0.0999)	0.0626 (0.0612)	1.247 (1.052)	56.25 (66.70)
[70/157]	0.1007 (0.1001)	0.0612 (0.0615)	0.985 (1.060)	75.00 (66.64)
[80/157]	0.1013 (0.1002)	0.0617 (0.0615)	1.227 (1.060)	62.50 (66.82)
[90/157]	0.1038 (0.1004)	0.0605 (0.0617)	0.918 (1.059)	78.12 (66.76)
[100/157]	0.1007 (0.1003)	0.0622 (0.0617)	1.122 (1.063)	65.62 (66.68)
[110/157]	0.1003 (0.1004)	0.0604 (0.0617)	1.175 (1.066)	65.62 (66.19)
[120/157]	0.1019 (0.1004)	0.0630 (0.0617)	1.056 (1.069)	68.75 (66.09)
[130/157]	0.1006 (0.1005)	0.0610 (0.0618)	0.788 (1.061)	71.88 (66.39)
[140/157]	0.1012 (0.1005)	0.0616 (0.0618)	1.218 (1.064)	62.50 (66.25)
[150/157]	0.0991 (0.1006)	0.0603 (0.0618)	1.137 (1.067)	59.38 (66.12)
[156/157]	0.0865 (0.1005)	0.0588 (0.0619)	0.923 (1.063)	75.00 (66.38)
 * Train Acc 66.380
 * Val Acc 67.600, Total time 0.61
 * Val loss 0.930, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0087 (0.0087)	1.012 (1.012)	68.75 (68.75)
[10/157]	0.1017 (0.0956)	0.0625 (0.0563)	1.449 (1.054)	59.38 (69.03)
[20/157]	0.1034 (0.0983)	0.0634 (0.0595)	1.039 (1.064)	68.75 (66.96)
[30/157]	0.1008 (0.0991)	0.0614 (0.0603)	1.107 (1.046)	71.88 (67.54)
[40/157]	0.1007 (0.0997)	0.0611 (0.0609)	1.351 (1.036)	50.00 (67.38)
[50/157]	0.1015 (0.0999)	0.0620 (0.0611)	0.791 (1.041)	84.38 (67.22)
[60/157]	0.1002 (0.1002)	0.0610 (0.0613)	1.212 (1.038)	59.38 (67.32)
[70/157]	0.1010 (0.1002)	0.0618 (0.0614)	0.949 (1.056)	68.75 (66.55)
[80/157]	0.0984 (0.1003)	0.0580 (0.0614)	0.743 (1.049)	81.25 (66.98)
[90/157]	0.1030 (0.1004)	0.0637 (0.0615)	0.968 (1.046)	78.12 (67.24)
[100/157]	0.1014 (0.1005)	0.0616 (0.0616)	0.957 (1.051)	68.75 (66.92)
[110/157]	0.1034 (0.1005)	0.0630 (0.0616)	0.803 (1.058)	71.88 (66.67)
[120/157]	0.1005 (0.1006)	0.0601 (0.0617)	0.732 (1.058)	81.25 (66.74)
[130/157]	0.1016 (0.1006)	0.0625 (0.0617)	0.834 (1.050)	78.12 (67.03)
[140/157]	0.0997 (0.1006)	0.0601 (0.0618)	1.021 (1.046)	65.62 (67.35)
[150/157]	0.1023 (0.1007)	0.0612 (0.0618)	1.140 (1.046)	56.25 (67.32)
[156/157]	0.0884 (0.1006)	0.0580 (0.0618)	1.232 (1.047)	50.00 (67.30)
 * Train Acc 67.300
 * Val Acc 69.000, Total time 0.58
 * Val loss 0.922, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0406 (0.0406)	0.0075 (0.0075)	1.070 (1.070)	71.88 (71.88)
[10/157]	0.0959 (0.0921)	0.0585 (0.0540)	0.781 (1.066)	81.25 (69.89)
[20/157]	0.0964 (0.0938)	0.0587 (0.0562)	0.758 (1.021)	71.88 (68.30)
[30/157]	0.1038 (0.0965)	0.0641 (0.0585)	1.418 (1.042)	43.75 (68.25)
[40/157]	0.1039 (0.0983)	0.0632 (0.0599)	0.837 (1.037)	71.88 (68.22)
[50/157]	0.1045 (0.0994)	0.0645 (0.0608)	1.203 (1.059)	56.25 (66.67)
[60/157]	0.0959 (0.1000)	0.0570 (0.0613)	1.080 (1.063)	62.50 (66.60)
[70/157]	0.0953 (0.0994)	0.0565 (0.0609)	0.998 (1.066)	71.88 (66.24)
[80/157]	0.0973 (0.0991)	0.0579 (0.0605)	1.160 (1.073)	65.62 (65.70)
[90/157]	0.1090 (0.1002)	0.0675 (0.0615)	0.825 (1.065)	78.12 (65.93)
[100/157]	0.1034 (0.1011)	0.0653 (0.0622)	1.374 (1.077)	59.38 (65.59)
[110/157]	0.0960 (0.1006)	0.0578 (0.0619)	0.907 (1.069)	75.00 (65.88)
[120/157]	0.0961 (0.1003)	0.0572 (0.0616)	1.047 (1.067)	68.75 (65.93)
[130/157]	0.0980 (0.1000)	0.0587 (0.0613)	1.078 (1.062)	68.75 (66.25)
[140/157]	0.0980 (0.0998)	0.0589 (0.0611)	0.999 (1.061)	71.88 (66.31)
[150/157]	0.1038 (0.0998)	0.0643 (0.0612)	1.093 (1.059)	78.12 (66.27)
[156/157]	0.0863 (0.0998)	0.0587 (0.0612)	1.043 (1.062)	75.00 (66.22)
 * Train Acc 66.220
 * Val Acc 66.800, Total time 0.61
 * Val loss 0.960, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0086 (0.0086)	0.983 (0.983)	62.50 (62.50)
[10/157]	0.1015 (0.0962)	0.0627 (0.0575)	1.557 (1.071)	43.75 (65.34)
[20/157]	0.1028 (0.0990)	0.0640 (0.0605)	1.390 (1.078)	50.00 (64.88)
[30/157]	0.1017 (0.1000)	0.0622 (0.0613)	1.503 (1.078)	43.75 (65.52)
[40/157]	0.1019 (0.1005)	0.0624 (0.0617)	1.023 (1.071)	62.50 (65.47)
[50/157]	0.1052 (0.1009)	0.0645 (0.0619)	0.946 (1.037)	71.88 (66.91)
[60/157]	0.1030 (0.1011)	0.0631 (0.0621)	0.772 (1.033)	71.88 (66.91)
[70/157]	0.1056 (0.1012)	0.0648 (0.0623)	0.984 (1.027)	78.12 (67.34)
[80/157]	0.1010 (0.1014)	0.0609 (0.0623)	1.083 (1.019)	71.88 (68.02)
[90/157]	0.1011 (0.1015)	0.0624 (0.0625)	0.694 (1.002)	81.25 (68.58)
[100/157]	0.0988 (0.1016)	0.0612 (0.0625)	1.104 (1.013)	62.50 (68.13)
[110/157]	0.0973 (0.1010)	0.0595 (0.0622)	1.374 (1.021)	59.38 (67.88)
[120/157]	0.0961 (0.1006)	0.0578 (0.0618)	1.217 (1.017)	65.62 (67.92)
[130/157]	0.0966 (0.1002)	0.0586 (0.0616)	1.157 (1.027)	68.75 (67.75)
[140/157]	0.1094 (0.1001)	0.0691 (0.0615)	0.824 (1.037)	75.00 (67.40)
[150/157]	0.0958 (0.1005)	0.0576 (0.0618)	1.234 (1.041)	56.25 (66.99)
[156/157]	0.0800 (0.1002)	0.0535 (0.0616)	1.144 (1.042)	62.50 (66.94)
 * Train Acc 66.940
 * Val Acc 69.000, Total time 0.59
 * Val loss 0.909, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0457 (0.0457)	0.0092 (0.0092)	0.794 (0.794)	78.12 (78.12)
[10/157]	0.1090 (0.1025)	0.0683 (0.0627)	1.257 (0.935)	59.38 (70.17)
[20/157]	0.0956 (0.1018)	0.0576 (0.0628)	1.121 (0.983)	68.75 (68.75)
[30/157]	0.0978 (0.0999)	0.0582 (0.0614)	0.972 (1.008)	71.88 (68.04)
[40/157]	0.1021 (0.0994)	0.0629 (0.0609)	1.308 (1.008)	68.75 (68.67)
[50/157]	0.0988 (0.0996)	0.0593 (0.0608)	0.870 (0.992)	75.00 (69.06)
[60/157]	0.0991 (0.0996)	0.0593 (0.0608)	0.963 (1.007)	59.38 (68.29)
[70/157]	0.1007 (0.0996)	0.0612 (0.0608)	1.117 (1.018)	59.38 (67.91)
[80/157]	0.0996 (0.0997)	0.0608 (0.0609)	1.059 (1.031)	65.62 (67.63)
[90/157]	0.1001 (0.0997)	0.0605 (0.0609)	0.873 (1.025)	65.62 (67.72)
[100/157]	0.1006 (0.0997)	0.0605 (0.0609)	1.255 (1.021)	56.25 (67.95)
[110/157]	0.1001 (0.0998)	0.0614 (0.0609)	0.872 (1.024)	75.00 (68.30)
[120/157]	0.1003 (0.0998)	0.0609 (0.0609)	1.287 (1.025)	62.50 (68.16)
[130/157]	0.1004 (0.0998)	0.0605 (0.0609)	0.739 (1.021)	81.25 (68.42)
[140/157]	0.1008 (0.0998)	0.0609 (0.0609)	0.686 (1.019)	81.25 (68.24)
[150/157]	0.0999 (0.0998)	0.0607 (0.0609)	1.347 (1.026)	43.75 (67.90)
[156/157]	0.0859 (0.0997)	0.0557 (0.0609)	1.003 (1.029)	75.00 (67.88)
 * Train Acc 67.880
 * Val Acc 68.900, Total time 0.61
 * Val loss 0.897, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0089 (0.0089)	1.002 (1.002)	68.75 (68.75)
[10/157]	0.1001 (0.0947)	0.0604 (0.0558)	1.047 (1.052)	71.88 (66.76)
[20/157]	0.0991 (0.0973)	0.0601 (0.0581)	1.110 (1.028)	65.62 (68.60)
[30/157]	0.0998 (0.0982)	0.0601 (0.0590)	1.556 (1.079)	46.88 (67.14)
[40/157]	0.0989 (0.0986)	0.0598 (0.0593)	0.917 (1.038)	65.62 (68.37)
[50/157]	0.1002 (0.0988)	0.0606 (0.0596)	0.791 (1.028)	65.62 (67.77)
[60/157]	0.0990 (0.0990)	0.0598 (0.0599)	1.163 (1.028)	68.75 (68.24)
[70/157]	0.1000 (0.0992)	0.0602 (0.0600)	0.956 (1.030)	71.88 (68.00)
[80/157]	0.1001 (0.0993)	0.0608 (0.0602)	1.286 (1.027)	50.00 (67.90)
[90/157]	0.1001 (0.0993)	0.0609 (0.0603)	0.920 (1.030)	65.62 (67.72)
[100/157]	0.0999 (0.0994)	0.0607 (0.0604)	0.964 (1.041)	65.62 (67.23)
[110/157]	0.1009 (0.0995)	0.0624 (0.0604)	0.730 (1.041)	81.25 (67.31)
[120/157]	0.1002 (0.0995)	0.0605 (0.0605)	1.252 (1.046)	59.38 (66.97)
[130/157]	0.1005 (0.0995)	0.0616 (0.0605)	1.064 (1.045)	65.62 (66.87)
[140/157]	0.1003 (0.0996)	0.0613 (0.0606)	0.881 (1.044)	68.75 (66.87)
[150/157]	0.1007 (0.0996)	0.0610 (0.0606)	1.188 (1.046)	56.25 (66.66)
[156/157]	0.0843 (0.0995)	0.0563 (0.0606)	1.400 (1.043)	62.50 (66.72)
 * Train Acc 66.720
 * Val Acc 68.800, Total time 0.59
 * Val loss 0.888, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0454 (0.0454)	0.0093 (0.0093)	1.332 (1.332)	56.25 (56.25)
[10/157]	0.1000 (0.0947)	0.0617 (0.0562)	1.453 (1.203)	50.00 (61.36)
[20/157]	0.0997 (0.0972)	0.0608 (0.0591)	1.363 (1.151)	59.38 (63.99)
[30/157]	0.0998 (0.0983)	0.0609 (0.0599)	0.973 (1.096)	68.75 (66.03)
[40/157]	0.1009 (0.0988)	0.0610 (0.0603)	0.788 (1.087)	75.00 (65.70)
[50/157]	0.1000 (0.0990)	0.0606 (0.0604)	1.057 (1.077)	68.75 (65.87)
[60/157]	0.1008 (0.0992)	0.0609 (0.0604)	0.873 (1.078)	78.12 (65.62)
[70/157]	0.1015 (0.0993)	0.0605 (0.0604)	1.024 (1.074)	68.75 (65.58)
[80/157]	0.1009 (0.0994)	0.0607 (0.0605)	0.924 (1.071)	71.88 (65.86)
[90/157]	0.1009 (0.0995)	0.0606 (0.0606)	0.814 (1.045)	84.38 (66.79)
[100/157]	0.0995 (0.0995)	0.0614 (0.0606)	0.959 (1.048)	65.62 (66.80)
[110/157]	0.1000 (0.0995)	0.0606 (0.0607)	1.024 (1.042)	65.62 (67.12)
[120/157]	0.1013 (0.0996)	0.0612 (0.0607)	0.945 (1.043)	78.12 (67.23)
[130/157]	0.1012 (0.0996)	0.0612 (0.0607)	0.924 (1.042)	68.75 (67.32)
[140/157]	0.1003 (0.0996)	0.0601 (0.0607)	0.966 (1.040)	65.62 (67.42)
[150/157]	0.1005 (0.0996)	0.0607 (0.0607)	1.097 (1.041)	62.50 (67.32)
[156/157]	0.0867 (0.0996)	0.0573 (0.0607)	1.548 (1.040)	50.00 (67.50)
 * Train Acc 67.500
 * Val Acc 68.500, Total time 0.60
 * Val loss 0.908, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0093 (0.0093)	0.634 (0.634)	84.38 (84.38)
[10/157]	0.1001 (0.0948)	0.0604 (0.0554)	1.048 (1.026)	65.62 (68.47)
[20/157]	0.0993 (0.0972)	0.0598 (0.0581)	0.780 (0.998)	75.00 (69.35)
[30/157]	0.1004 (0.0981)	0.0611 (0.0590)	1.080 (1.007)	59.38 (68.04)
[40/157]	0.1008 (0.0986)	0.0608 (0.0595)	1.309 (1.014)	62.50 (67.91)
[50/157]	0.0968 (0.0989)	0.0573 (0.0596)	1.051 (1.031)	75.00 (67.71)
[60/157]	0.1008 (0.0990)	0.0601 (0.0598)	1.324 (1.036)	56.25 (67.16)
[70/157]	0.1006 (0.0992)	0.0612 (0.0599)	0.947 (1.033)	68.75 (67.12)
[80/157]	0.0999 (0.0992)	0.0605 (0.0601)	0.782 (1.034)	71.88 (67.01)
[90/157]	0.1005 (0.0993)	0.0609 (0.0602)	1.059 (1.018)	65.62 (67.48)
[100/157]	0.1002 (0.0994)	0.0608 (0.0603)	1.005 (1.011)	71.88 (68.07)
[110/157]	0.1016 (0.0995)	0.0605 (0.0604)	0.994 (1.011)	65.62 (68.22)
[120/157]	0.1016 (0.0995)	0.0608 (0.0604)	1.005 (1.014)	68.75 (68.34)
[130/157]	0.1000 (0.0995)	0.0605 (0.0604)	0.767 (1.013)	84.38 (68.32)
[140/157]	0.0980 (0.0996)	0.0588 (0.0604)	1.232 (1.015)	59.38 (68.33)
[150/157]	0.1005 (0.0996)	0.0606 (0.0605)	0.640 (1.010)	87.50 (68.36)
[156/157]	0.0875 (0.0995)	0.0568 (0.0604)	1.629 (1.013)	50.00 (68.30)
 * Train Acc 68.300
 * Val Acc 68.900, Total time 0.61
 * Val loss 0.885, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0090 (0.0090)	1.100 (1.100)	71.88 (71.88)
[10/157]	0.0998 (0.0946)	0.0593 (0.0554)	0.992 (0.975)	68.75 (69.03)
[20/157]	0.0972 (0.0971)	0.0582 (0.0579)	0.818 (0.988)	65.62 (68.90)
[30/157]	0.1018 (0.0981)	0.0608 (0.0589)	0.999 (0.995)	71.88 (69.56)
[40/157]	0.0990 (0.0985)	0.0599 (0.0593)	0.736 (0.971)	71.88 (69.97)
[50/157]	0.1011 (0.0988)	0.0613 (0.0596)	0.991 (0.977)	71.88 (70.22)
[60/157]	0.1020 (0.0990)	0.0613 (0.0599)	0.857 (0.972)	78.12 (70.34)
[70/157]	0.0990 (0.0991)	0.0599 (0.0601)	1.260 (0.982)	59.38 (69.85)
[80/157]	0.0971 (0.0992)	0.0576 (0.0602)	1.095 (0.989)	68.75 (69.44)
[90/157]	0.1000 (0.0993)	0.0608 (0.0603)	1.025 (0.990)	59.38 (69.30)
[100/157]	0.1004 (0.0994)	0.0610 (0.0604)	1.029 (0.994)	68.75 (69.18)
[110/157]	0.0999 (0.0994)	0.0606 (0.0605)	1.013 (0.993)	71.88 (69.09)
[120/157]	0.1011 (0.0995)	0.0612 (0.0606)	1.172 (1.003)	53.12 (68.75)
[130/157]	0.1000 (0.0995)	0.0612 (0.0606)	0.953 (0.997)	68.75 (69.01)
[140/157]	0.1001 (0.0995)	0.0611 (0.0607)	0.843 (0.996)	68.75 (68.88)
[150/157]	0.0998 (0.0996)	0.0604 (0.0607)	1.201 (1.006)	62.50 (68.67)
[156/157]	0.0834 (0.0994)	0.0566 (0.0607)	0.670 (1.011)	75.00 (68.46)
 * Train Acc 68.460
 * Val Acc 70.500, Total time 0.61
 * Val loss 0.880, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0088 (0.0088)	0.961 (0.961)	71.88 (71.88)
[10/157]	0.1001 (0.0946)	0.0594 (0.0558)	0.825 (1.037)	68.75 (66.48)
[20/157]	0.0998 (0.0970)	0.0606 (0.0580)	0.957 (1.030)	65.62 (68.15)
[30/157]	0.1001 (0.0980)	0.0601 (0.0589)	0.804 (1.023)	75.00 (68.25)
[40/157]	0.1015 (0.0985)	0.0621 (0.0594)	1.037 (1.035)	68.75 (68.22)
[50/157]	0.1007 (0.0988)	0.0616 (0.0596)	0.914 (1.012)	71.88 (69.12)
[60/157]	0.0997 (0.0990)	0.0604 (0.0598)	0.806 (1.012)	75.00 (69.06)
[70/157]	0.1010 (0.0991)	0.0618 (0.0600)	1.107 (1.015)	62.50 (68.88)
[80/157]	0.1002 (0.0993)	0.0604 (0.0601)	0.533 (1.009)	84.38 (69.10)
[90/157]	0.0995 (0.0993)	0.0605 (0.0602)	0.894 (1.000)	68.75 (69.30)
[100/157]	0.1006 (0.0994)	0.0616 (0.0603)	1.345 (1.010)	56.25 (69.00)
[110/157]	0.0999 (0.0994)	0.0608 (0.0604)	0.840 (1.011)	75.00 (68.98)
[120/157]	0.0991 (0.0995)	0.0597 (0.0604)	1.204 (1.016)	56.25 (68.67)
[130/157]	0.1024 (0.0995)	0.0608 (0.0605)	1.001 (1.025)	65.62 (68.13)
[140/157]	0.1004 (0.0995)	0.0610 (0.0605)	1.028 (1.025)	53.12 (67.95)
[150/157]	0.0998 (0.0996)	0.0599 (0.0606)	0.948 (1.016)	68.75 (68.00)
[156/157]	0.0822 (0.0994)	0.0550 (0.0605)	0.968 (1.018)	75.00 (68.00)
 * Train Acc 68.000
 * Val Acc 70.200, Total time 0.60
 * Val loss 0.871, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0082 (0.0082)	0.959 (0.959)	78.12 (78.12)
[10/157]	0.1002 (0.0949)	0.0609 (0.0561)	1.396 (1.113)	65.62 (65.62)
[20/157]	0.1013 (0.0974)	0.0618 (0.0585)	1.094 (1.033)	75.00 (69.20)
[30/157]	0.1005 (0.0982)	0.0610 (0.0595)	1.118 (1.027)	65.62 (68.95)
[40/157]	0.1014 (0.0987)	0.0603 (0.0600)	1.034 (1.012)	62.50 (69.59)
[50/157]	0.1000 (0.0989)	0.0607 (0.0602)	0.906 (1.007)	71.88 (69.91)
[60/157]	0.1002 (0.0991)	0.0608 (0.0603)	0.874 (1.024)	75.00 (69.21)
[70/157]	0.0997 (0.0992)	0.0608 (0.0605)	0.876 (1.018)	75.00 (69.28)
[80/157]	0.1005 (0.0993)	0.0612 (0.0606)	0.877 (1.009)	81.25 (69.52)
[90/157]	0.0989 (0.0994)	0.0597 (0.0607)	0.739 (1.007)	78.12 (69.33)
[100/157]	0.1018 (0.0994)	0.0614 (0.0607)	0.764 (1.003)	81.25 (69.68)
[110/157]	0.1011 (0.0995)	0.0619 (0.0608)	1.319 (1.002)	59.38 (69.62)
[120/157]	0.0993 (0.0995)	0.0607 (0.0608)	1.214 (0.998)	65.62 (69.73)
[130/157]	0.1000 (0.0996)	0.0603 (0.0608)	1.217 (0.996)	68.75 (69.54)
[140/157]	0.1016 (0.0996)	0.0613 (0.0609)	0.838 (0.996)	71.88 (69.41)
[150/157]	0.1003 (0.0996)	0.0611 (0.0609)	0.820 (1.006)	81.25 (69.16)
[156/157]	0.0849 (0.0995)	0.0575 (0.0608)	0.838 (1.004)	75.00 (69.10)
 * Train Acc 69.100
 * Val Acc 69.100, Total time 0.61
 * Val loss 0.891, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0087 (0.0087)	1.183 (1.183)	59.38 (59.38)
[10/157]	0.1003 (0.0944)	0.0606 (0.0557)	0.877 (1.027)	65.62 (69.03)
[20/157]	0.1002 (0.0971)	0.0611 (0.0585)	0.918 (1.045)	71.88 (68.30)
[30/157]	0.1007 (0.0980)	0.0610 (0.0593)	0.899 (1.022)	75.00 (68.35)
[40/157]	0.1009 (0.0985)	0.0614 (0.0598)	1.092 (1.019)	65.62 (68.14)
[50/157]	0.1005 (0.0988)	0.0607 (0.0601)	1.120 (1.009)	71.88 (68.87)
[60/157]	0.1005 (0.0990)	0.0610 (0.0603)	0.848 (1.008)	75.00 (68.65)
[70/157]	0.1004 (0.0991)	0.0609 (0.0604)	0.746 (0.999)	75.00 (68.66)
[80/157]	0.1001 (0.0992)	0.0607 (0.0605)	0.843 (0.994)	81.25 (69.14)
[90/157]	0.1008 (0.0993)	0.0616 (0.0606)	1.114 (1.005)	65.62 (68.68)
[100/157]	0.1007 (0.0994)	0.0616 (0.0607)	0.838 (1.010)	75.00 (68.75)
[110/157]	0.1000 (0.0994)	0.0607 (0.0607)	1.135 (1.013)	62.50 (68.58)
[120/157]	0.0998 (0.0995)	0.0605 (0.0608)	0.877 (1.012)	78.12 (68.52)
[130/157]	0.0999 (0.0995)	0.0610 (0.0608)	1.081 (1.011)	71.88 (68.77)
[140/157]	0.1008 (0.0995)	0.0615 (0.0609)	0.831 (1.006)	62.50 (68.91)
[150/157]	0.0995 (0.0996)	0.0607 (0.0609)	1.088 (1.003)	59.38 (68.89)
[156/157]	0.0849 (0.0995)	0.0568 (0.0609)	0.773 (0.996)	87.50 (69.08)
 * Train Acc 69.080
 * Val Acc 71.000, Total time 0.60
 * Val loss 0.861, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0083 (0.0083)	1.080 (1.080)	62.50 (62.50)
[10/157]	0.1000 (0.0948)	0.0605 (0.0559)	0.956 (0.925)	75.00 (73.30)
[20/157]	0.1005 (0.0973)	0.0613 (0.0584)	1.197 (0.931)	65.62 (72.02)
[30/157]	0.1004 (0.0981)	0.0611 (0.0592)	0.857 (0.944)	75.00 (71.67)
[40/157]	0.1019 (0.0987)	0.0604 (0.0597)	1.015 (0.971)	75.00 (70.58)
[50/157]	0.1011 (0.0990)	0.0610 (0.0599)	0.735 (0.972)	81.25 (70.22)
[60/157]	0.0998 (0.0992)	0.0615 (0.0602)	0.669 (0.967)	87.50 (70.75)
[70/157]	0.1003 (0.0993)	0.0620 (0.0605)	0.857 (0.977)	75.00 (70.42)
[80/157]	0.1015 (0.0995)	0.0629 (0.0607)	1.037 (0.970)	71.88 (70.56)
[90/157]	0.0999 (0.0996)	0.0607 (0.0607)	1.210 (0.973)	62.50 (70.64)
[100/157]	0.1002 (0.0996)	0.0604 (0.0608)	1.137 (0.980)	65.62 (70.45)
[110/157]	0.1004 (0.0996)	0.0604 (0.0608)	0.868 (0.983)	75.00 (70.19)
[120/157]	0.1001 (0.0997)	0.0609 (0.0608)	0.990 (0.987)	75.00 (69.99)
[130/157]	0.0994 (0.0997)	0.0601 (0.0608)	0.966 (0.987)	71.88 (69.80)
[140/157]	0.1016 (0.0998)	0.0625 (0.0609)	0.847 (0.983)	81.25 (69.97)
[150/157]	0.1018 (0.0998)	0.0616 (0.0609)	0.866 (0.984)	78.12 (69.95)
[156/157]	0.0838 (0.0997)	0.0557 (0.0609)	1.198 (0.985)	62.50 (69.88)
 * Train Acc 69.880
 * Val Acc 70.600, Total time 0.61
 * Val loss 0.857, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0087 (0.0087)	1.192 (1.192)	53.12 (53.12)
[10/157]	0.1019 (0.0949)	0.0625 (0.0559)	1.147 (1.145)	65.62 (61.65)
[20/157]	0.0997 (0.0972)	0.0608 (0.0585)	0.658 (1.041)	84.38 (65.48)
[30/157]	0.1003 (0.0981)	0.0617 (0.0594)	0.740 (1.028)	81.25 (66.53)
[40/157]	0.1013 (0.0985)	0.0615 (0.0599)	0.855 (1.013)	68.75 (67.53)
[50/157]	0.1015 (0.0989)	0.0609 (0.0601)	0.911 (0.990)	71.88 (68.63)
[60/157]	0.1004 (0.0990)	0.0611 (0.0602)	0.876 (0.982)	71.88 (68.80)
[70/157]	0.1004 (0.0992)	0.0600 (0.0602)	0.967 (0.978)	65.62 (68.93)
[80/157]	0.1013 (0.0993)	0.0620 (0.0603)	1.409 (0.999)	53.12 (68.25)
[90/157]	0.0999 (0.0994)	0.0608 (0.0604)	1.109 (1.008)	59.38 (67.96)
[100/157]	0.1000 (0.0994)	0.0604 (0.0604)	1.094 (1.009)	71.88 (68.38)
[110/157]	0.1001 (0.0995)	0.0607 (0.0605)	1.016 (1.010)	68.75 (68.41)
[120/157]	0.1004 (0.0995)	0.0614 (0.0606)	0.847 (1.016)	71.88 (68.08)
[130/157]	0.1005 (0.0995)	0.0612 (0.0606)	0.924 (1.014)	71.88 (68.18)
[140/157]	0.1012 (0.0996)	0.0606 (0.0607)	0.748 (1.009)	81.25 (68.57)
[150/157]	0.0998 (0.0996)	0.0604 (0.0607)	1.163 (1.004)	62.50 (68.52)
[156/157]	0.0831 (0.0995)	0.0562 (0.0607)	1.039 (1.008)	62.50 (68.44)
 * Train Acc 68.440
 * Val Acc 69.600, Total time 0.60
 * Val loss 0.869, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0087 (0.0087)	1.304 (1.304)	59.38 (59.38)
[10/157]	0.0991 (0.0947)	0.0608 (0.0566)	1.066 (1.038)	65.62 (66.76)
[20/157]	0.1003 (0.0972)	0.0620 (0.0591)	1.127 (0.968)	59.38 (68.60)
[30/157]	0.1003 (0.0983)	0.0607 (0.0600)	1.030 (0.992)	65.62 (69.15)
[40/157]	0.1006 (0.0987)	0.0612 (0.0603)	0.939 (0.989)	68.75 (68.75)
[50/157]	0.1005 (0.0990)	0.0611 (0.0605)	0.885 (0.974)	65.62 (69.30)
[60/157]	0.1003 (0.0991)	0.0608 (0.0606)	0.876 (0.976)	78.12 (69.52)
[70/157]	0.1006 (0.0993)	0.0617 (0.0607)	0.870 (0.969)	75.00 (69.81)
[80/157]	0.0996 (0.0993)	0.0600 (0.0607)	0.985 (0.977)	75.00 (69.68)
[90/157]	0.1006 (0.0994)	0.0609 (0.0608)	1.070 (0.979)	65.62 (69.47)
[100/157]	0.1003 (0.0995)	0.0607 (0.0608)	0.823 (0.978)	81.25 (69.55)
[110/157]	0.1001 (0.0995)	0.0604 (0.0608)	0.719 (0.978)	75.00 (69.26)
[120/157]	0.1011 (0.0995)	0.0616 (0.0609)	1.115 (0.985)	68.75 (69.03)
[130/157]	0.1004 (0.0996)	0.0607 (0.0609)	0.903 (0.983)	65.62 (69.08)
[140/157]	0.1003 (0.0996)	0.0602 (0.0609)	1.065 (0.984)	62.50 (68.84)
[150/157]	0.1006 (0.0996)	0.0604 (0.0609)	0.810 (0.983)	75.00 (68.79)
[156/157]	0.0813 (0.0995)	0.0544 (0.0608)	0.883 (0.981)	75.00 (68.98)
 * Train Acc 68.980
 * Val Acc 70.800, Total time 0.61
 * Val loss 0.855, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0089 (0.0089)	1.063 (1.063)	65.62 (65.62)
[10/157]	0.1005 (0.0950)	0.0610 (0.0553)	1.044 (1.031)	68.75 (63.92)
[20/157]	0.1005 (0.0974)	0.0615 (0.0581)	0.762 (0.968)	78.12 (68.30)
[30/157]	0.0997 (0.0982)	0.0601 (0.0590)	1.127 (0.983)	62.50 (68.25)
[40/157]	0.1019 (0.0986)	0.0616 (0.0595)	1.145 (0.967)	59.38 (69.21)
[50/157]	0.1001 (0.0989)	0.0610 (0.0598)	0.952 (0.970)	75.00 (69.30)
[60/157]	0.0994 (0.0991)	0.0592 (0.0599)	1.024 (0.974)	62.50 (68.60)
[70/157]	0.0998 (0.0992)	0.0600 (0.0599)	0.770 (0.975)	71.88 (68.84)
[80/157]	0.1007 (0.0993)	0.0606 (0.0600)	0.887 (0.968)	65.62 (69.25)
[90/157]	0.0993 (0.0993)	0.0599 (0.0601)	1.164 (0.973)	65.62 (68.99)
[100/157]	0.1003 (0.0994)	0.0610 (0.0602)	0.845 (0.978)	71.88 (68.78)
[110/157]	0.1015 (0.0995)	0.0621 (0.0602)	1.149 (0.973)	71.88 (69.12)
[120/157]	0.1006 (0.0995)	0.0611 (0.0603)	1.117 (0.973)	71.88 (69.34)
[130/157]	0.1004 (0.0995)	0.0614 (0.0604)	0.897 (0.973)	71.88 (69.32)
[140/157]	0.0995 (0.0996)	0.0607 (0.0604)	1.105 (0.975)	62.50 (69.19)
[150/157]	0.1016 (0.0996)	0.0620 (0.0605)	1.118 (0.975)	62.50 (69.23)
[156/157]	0.0830 (0.0995)	0.0554 (0.0605)	0.546 (0.972)	87.50 (69.34)
 * Train Acc 69.340
 * Val Acc 71.600, Total time 0.60
 * Val loss 0.854, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0463 (0.0463)	0.0083 (0.0083)	0.722 (0.722)	84.38 (84.38)
[10/157]	0.1012 (0.0951)	0.0611 (0.0552)	0.610 (0.892)	87.50 (73.58)
[20/157]	0.1007 (0.0974)	0.0604 (0.0580)	1.183 (0.902)	71.88 (73.36)
[30/157]	0.1006 (0.0982)	0.0610 (0.0590)	0.860 (0.915)	71.88 (72.08)
[40/157]	0.1010 (0.0988)	0.0576 (0.0595)	0.929 (0.932)	71.88 (71.57)
[50/157]	0.1009 (0.0989)	0.0614 (0.0597)	1.041 (0.935)	71.88 (71.20)
[60/157]	0.0998 (0.0991)	0.0601 (0.0600)	1.091 (0.939)	62.50 (71.06)
[70/157]	0.0998 (0.0992)	0.0606 (0.0602)	0.776 (0.941)	68.75 (70.77)
[80/157]	0.1016 (0.0993)	0.0621 (0.0603)	1.079 (0.949)	62.50 (70.29)
[90/157]	0.0998 (0.0994)	0.0599 (0.0603)	0.888 (0.949)	78.12 (70.54)
[100/157]	0.0999 (0.0994)	0.0605 (0.0604)	0.542 (0.951)	87.50 (70.54)
[110/157]	0.0984 (0.0995)	0.0592 (0.0604)	1.026 (0.954)	68.75 (70.50)
[120/157]	0.0999 (0.0995)	0.0596 (0.0604)	0.757 (0.954)	81.25 (70.43)
[130/157]	0.1003 (0.0996)	0.0604 (0.0605)	1.168 (0.958)	68.75 (70.30)
[140/157]	0.1011 (0.0996)	0.0608 (0.0606)	0.697 (0.961)	84.38 (70.06)
[150/157]	0.1003 (0.0996)	0.0609 (0.0606)	1.066 (0.962)	62.50 (69.99)
[156/157]	0.0828 (0.0995)	0.0554 (0.0606)	0.919 (0.964)	87.50 (69.98)
 * Train Acc 69.980
 * Val Acc 69.900, Total time 0.60
 * Val loss 0.860, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0089 (0.0089)	0.844 (0.844)	75.00 (75.00)
[10/157]	0.0999 (0.0951)	0.0608 (0.0558)	0.987 (0.872)	68.75 (74.43)
[20/157]	0.1000 (0.0974)	0.0606 (0.0584)	1.138 (0.921)	71.88 (71.43)
[30/157]	0.1011 (0.0983)	0.0620 (0.0593)	1.117 (0.936)	68.75 (70.36)
[40/157]	0.0993 (0.0987)	0.0595 (0.0598)	0.713 (0.930)	75.00 (69.89)
[50/157]	0.1006 (0.0989)	0.0606 (0.0600)	1.141 (0.942)	56.25 (69.49)
[60/157]	0.1043 (0.0992)	0.0613 (0.0602)	0.912 (0.943)	81.25 (70.13)
[70/157]	0.1020 (0.0992)	0.0615 (0.0603)	0.890 (0.956)	65.62 (69.85)
[80/157]	0.1001 (0.0993)	0.0610 (0.0603)	0.742 (0.958)	75.00 (69.83)
[90/157]	0.1013 (0.0994)	0.0610 (0.0603)	0.784 (0.956)	81.25 (69.81)
[100/157]	0.1016 (0.0994)	0.0614 (0.0603)	0.814 (0.954)	68.75 (69.93)
[110/157]	0.1007 (0.0995)	0.0615 (0.0604)	1.189 (0.956)	59.38 (70.13)
[120/157]	0.1013 (0.0995)	0.0614 (0.0604)	1.109 (0.963)	56.25 (69.83)
[130/157]	0.1003 (0.0996)	0.0604 (0.0604)	0.906 (0.972)	71.88 (69.56)
[140/157]	0.1007 (0.0996)	0.0607 (0.0605)	1.025 (0.973)	62.50 (69.48)
[150/157]	0.1013 (0.0996)	0.0614 (0.0605)	0.954 (0.969)	68.75 (69.70)
[156/157]	0.0830 (0.0995)	0.0564 (0.0605)	0.668 (0.970)	87.50 (69.70)
 * Train Acc 69.700
 * Val Acc 70.600, Total time 0.60
 * Val loss 0.866, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0086 (0.0086)	0.693 (0.693)	75.00 (75.00)
[10/157]	0.1002 (0.0946)	0.0608 (0.0558)	0.913 (0.978)	71.88 (69.03)
[20/157]	0.1003 (0.0972)	0.0600 (0.0583)	1.081 (0.948)	59.38 (70.68)
[30/157]	0.1006 (0.0982)	0.0610 (0.0592)	0.853 (0.962)	75.00 (70.77)
[40/157]	0.1009 (0.0986)	0.0613 (0.0597)	0.974 (0.940)	62.50 (71.49)
[50/157]	0.1005 (0.0989)	0.0612 (0.0600)	0.917 (0.959)	65.62 (70.28)
[60/157]	0.1004 (0.0992)	0.0619 (0.0604)	0.913 (0.959)	71.88 (70.24)
[70/157]	0.1003 (0.0994)	0.0601 (0.0606)	0.985 (0.959)	65.62 (70.16)
[80/157]	0.0998 (0.0995)	0.0605 (0.0606)	1.229 (0.966)	65.62 (70.18)
[90/157]	0.1002 (0.0995)	0.0610 (0.0607)	0.908 (0.959)	71.88 (70.33)
[100/157]	0.1012 (0.0996)	0.0617 (0.0607)	1.267 (0.974)	59.38 (69.74)
[110/157]	0.1000 (0.0996)	0.0603 (0.0607)	1.338 (0.977)	65.62 (69.79)
[120/157]	0.1001 (0.0997)	0.0605 (0.0607)	1.060 (0.974)	71.88 (69.86)
[130/157]	0.0995 (0.0997)	0.0594 (0.0607)	0.971 (0.981)	65.62 (69.51)
[140/157]	0.0997 (0.0997)	0.0600 (0.0606)	0.870 (0.979)	68.75 (69.57)
[150/157]	0.1002 (0.0997)	0.0609 (0.0607)	0.623 (0.972)	87.50 (69.99)
[156/157]	0.0835 (0.0996)	0.0562 (0.0607)	1.471 (0.973)	62.50 (70.02)
 * Train Acc 70.020
 * Val Acc 70.800, Total time 0.60
 * Val loss 0.850, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0083 (0.0083)	1.281 (1.281)	56.25 (56.25)
[10/157]	0.1010 (0.0952)	0.0611 (0.0559)	0.687 (0.885)	81.25 (72.44)
[20/157]	0.1003 (0.0975)	0.0607 (0.0585)	0.836 (0.890)	71.88 (72.77)
[30/157]	0.0986 (0.0983)	0.0604 (0.0594)	0.951 (0.917)	65.62 (72.38)
[40/157]	0.0991 (0.0987)	0.0602 (0.0598)	1.044 (0.931)	65.62 (71.72)
[50/157]	0.1005 (0.0990)	0.0609 (0.0602)	0.694 (0.943)	81.25 (71.08)
[60/157]	0.0999 (0.0992)	0.0607 (0.0603)	0.676 (0.945)	78.12 (70.44)
[70/157]	0.1000 (0.0993)	0.0603 (0.0603)	1.260 (0.958)	56.25 (69.81)
[80/157]	0.1003 (0.0994)	0.0608 (0.0604)	0.779 (0.964)	75.00 (69.60)
[90/157]	0.1002 (0.0994)	0.0610 (0.0604)	1.106 (0.971)	65.62 (69.47)
[100/157]	0.0993 (0.0995)	0.0597 (0.0605)	0.866 (0.977)	81.25 (69.28)
[110/157]	0.1006 (0.0995)	0.0611 (0.0605)	0.738 (0.982)	78.12 (69.20)
[120/157]	0.1002 (0.0996)	0.0605 (0.0606)	0.739 (0.970)	84.38 (69.65)
[130/157]	0.1011 (0.0996)	0.0618 (0.0605)	0.988 (0.967)	56.25 (69.47)
[140/157]	0.1008 (0.0996)	0.0610 (0.0606)	0.846 (0.969)	81.25 (69.48)
[150/157]	0.0996 (0.0996)	0.0602 (0.0606)	1.009 (0.971)	59.38 (69.47)
[156/157]	0.0829 (0.0995)	0.0550 (0.0605)	0.951 (0.972)	62.50 (69.52)
 * Train Acc 69.520
 * Val Acc 70.700, Total time 0.62
 * Val loss 0.846, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0091 (0.0091)	1.278 (1.278)	53.12 (53.12)
[10/157]	0.1002 (0.0941)	0.0602 (0.0557)	0.892 (0.960)	75.00 (69.60)
[20/157]	0.1003 (0.0969)	0.0602 (0.0582)	0.751 (0.964)	81.25 (70.09)
[30/157]	0.1002 (0.0979)	0.0608 (0.0592)	0.774 (0.970)	81.25 (69.96)
[40/157]	0.1005 (0.0984)	0.0611 (0.0595)	1.139 (0.965)	59.38 (69.82)
[50/157]	0.1000 (0.0987)	0.0603 (0.0598)	0.867 (0.957)	75.00 (70.16)
[60/157]	0.0999 (0.0989)	0.0608 (0.0600)	0.761 (0.947)	78.12 (70.70)
[70/157]	0.1004 (0.0991)	0.0605 (0.0602)	1.134 (0.945)	59.38 (70.69)
[80/157]	0.0999 (0.0992)	0.0607 (0.0603)	0.755 (0.944)	71.88 (70.91)
[90/157]	0.1004 (0.0992)	0.0605 (0.0604)	0.843 (0.941)	78.12 (71.22)
[100/157]	0.0998 (0.0993)	0.0604 (0.0605)	1.096 (0.954)	56.25 (70.58)
[110/157]	0.1001 (0.0994)	0.0602 (0.0606)	1.087 (0.954)	65.62 (70.58)
[120/157]	0.1003 (0.0994)	0.0605 (0.0606)	0.949 (0.951)	65.62 (70.66)
[130/157]	0.0998 (0.0995)	0.0603 (0.0606)	1.070 (0.950)	62.50 (70.71)
[140/157]	0.0995 (0.0995)	0.0606 (0.0607)	1.212 (0.952)	62.50 (70.50)
[150/157]	0.0985 (0.0995)	0.0591 (0.0607)	0.759 (0.960)	81.25 (70.10)
[156/157]	0.0882 (0.0995)	0.0573 (0.0607)	0.899 (0.958)	87.50 (70.20)
 * Train Acc 70.200
 * Val Acc 72.200, Total time 0.61
 * Val loss 0.836, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0465 (0.0465)	0.0084 (0.0084)	1.144 (1.144)	65.62 (65.62)
[10/157]	0.1032 (0.0947)	0.0616 (0.0558)	1.137 (1.077)	62.50 (65.34)
[20/157]	0.1006 (0.0971)	0.0613 (0.0583)	0.993 (1.000)	68.75 (68.90)
[30/157]	0.1001 (0.0980)	0.0611 (0.0593)	0.698 (0.972)	71.88 (69.56)
[40/157]	0.1003 (0.0985)	0.0610 (0.0598)	0.807 (0.971)	78.12 (70.12)
[50/157]	0.1010 (0.0988)	0.0613 (0.0600)	0.790 (0.950)	78.12 (70.71)
[60/157]	0.1000 (0.0990)	0.0606 (0.0602)	0.893 (0.954)	68.75 (70.70)
[70/157]	0.0997 (0.0991)	0.0606 (0.0604)	0.965 (0.951)	71.88 (70.73)
[80/157]	0.0996 (0.0992)	0.0604 (0.0604)	1.182 (0.960)	59.38 (70.41)
[90/157]	0.1010 (0.0993)	0.0620 (0.0605)	1.127 (0.962)	68.75 (70.47)
[100/157]	0.0999 (0.0994)	0.0601 (0.0605)	0.824 (0.964)	71.88 (70.45)
[110/157]	0.1004 (0.0994)	0.0611 (0.0606)	1.030 (0.963)	65.62 (70.50)
[120/157]	0.0997 (0.0995)	0.0602 (0.0607)	0.965 (0.961)	71.88 (70.43)
[130/157]	0.0993 (0.0995)	0.0601 (0.0607)	0.805 (0.959)	71.88 (70.61)
[140/157]	0.1000 (0.0995)	0.0609 (0.0607)	0.733 (0.959)	78.12 (70.48)
[150/157]	0.0998 (0.0996)	0.0603 (0.0607)	0.890 (0.961)	75.00 (70.34)
[156/157]	0.0835 (0.0995)	0.0565 (0.0607)	0.722 (0.963)	87.50 (70.34)
 * Train Acc 70.340
 * Val Acc 70.600, Total time 0.60
 * Val loss 0.837, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0086 (0.0086)	0.945 (0.945)	65.62 (65.62)
[10/157]	0.1014 (0.0945)	0.0622 (0.0566)	1.095 (1.005)	65.62 (66.76)
[20/157]	0.1005 (0.0974)	0.0618 (0.0591)	1.339 (1.020)	71.88 (68.01)
[30/157]	0.1002 (0.0982)	0.0607 (0.0599)	0.871 (0.997)	78.12 (69.35)
[40/157]	0.0999 (0.0986)	0.0609 (0.0602)	1.100 (0.992)	65.62 (69.44)
[50/157]	0.1003 (0.0989)	0.0602 (0.0605)	0.925 (0.995)	65.62 (69.49)
[60/157]	0.0994 (0.0991)	0.0601 (0.0606)	0.966 (0.981)	71.88 (70.18)
[70/157]	0.1004 (0.0992)	0.0606 (0.0607)	0.816 (0.964)	84.38 (70.69)
[80/157]	0.0999 (0.0993)	0.0598 (0.0607)	0.850 (0.959)	68.75 (70.99)
[90/157]	0.1020 (0.0994)	0.0598 (0.0607)	0.802 (0.963)	68.75 (70.57)
[100/157]	0.1005 (0.0994)	0.0606 (0.0607)	0.839 (0.961)	71.88 (70.39)
[110/157]	0.1005 (0.0995)	0.0606 (0.0607)	0.741 (0.955)	81.25 (70.69)
[120/157]	0.1010 (0.0995)	0.0607 (0.0607)	0.848 (0.957)	78.12 (70.53)
[130/157]	0.0988 (0.0996)	0.0601 (0.0608)	0.712 (0.957)	81.25 (70.56)
[140/157]	0.1002 (0.0996)	0.0609 (0.0608)	0.708 (0.955)	75.00 (70.52)
[150/157]	0.1003 (0.0996)	0.0602 (0.0608)	0.950 (0.956)	65.62 (70.36)
[156/157]	0.0846 (0.0995)	0.0578 (0.0608)	0.938 (0.952)	62.50 (70.48)
 * Train Acc 70.480
 * Val Acc 71.100, Total time 0.60
 * Val loss 0.839, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0466 (0.0466)	0.0085 (0.0085)	0.879 (0.879)	71.88 (71.88)
[10/157]	0.1010 (0.0951)	0.0606 (0.0557)	0.835 (1.027)	75.00 (65.06)
[20/157]	0.1006 (0.0974)	0.0610 (0.0583)	0.810 (0.961)	81.25 (69.20)
[30/157]	0.1008 (0.0983)	0.0611 (0.0593)	1.071 (0.941)	59.38 (70.56)
[40/157]	0.0998 (0.0988)	0.0605 (0.0598)	0.845 (0.953)	75.00 (69.82)
[50/157]	0.1016 (0.0992)	0.0617 (0.0603)	0.784 (0.935)	75.00 (70.59)
[60/157]	0.1008 (0.0994)	0.0616 (0.0604)	0.780 (0.945)	78.12 (70.49)
[70/157]	0.0998 (0.0995)	0.0606 (0.0605)	0.894 (0.946)	65.62 (70.38)
[80/157]	0.0995 (0.0996)	0.0597 (0.0607)	0.542 (0.944)	87.50 (70.45)
[90/157]	0.1008 (0.0996)	0.0621 (0.0607)	1.185 (0.945)	59.38 (70.54)
[100/157]	0.1023 (0.0997)	0.0617 (0.0608)	0.869 (0.950)	65.62 (70.24)
[110/157]	0.1006 (0.0997)	0.0616 (0.0609)	1.011 (0.954)	65.62 (70.13)
[120/157]	0.1009 (0.0997)	0.0626 (0.0610)	1.178 (0.956)	65.62 (70.14)
[130/157]	0.1010 (0.0998)	0.0611 (0.0611)	1.132 (0.963)	65.62 (70.01)
[140/157]	0.1001 (0.0999)	0.0610 (0.0611)	1.022 (0.965)	71.88 (70.26)
[150/157]	0.0994 (0.0999)	0.0602 (0.0611)	0.880 (0.960)	65.62 (70.28)
[156/157]	0.0839 (0.0997)	0.0569 (0.0611)	1.112 (0.962)	62.50 (70.20)
 * Train Acc 70.200
 * Val Acc 71.500, Total time 0.61
 * Val loss 0.839, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0455 (0.0455)	0.0089 (0.0089)	1.135 (1.135)	71.88 (71.88)
[10/157]	0.1014 (0.0944)	0.0614 (0.0556)	0.985 (0.922)	78.12 (71.02)
[20/157]	0.1009 (0.0971)	0.0612 (0.0583)	1.027 (0.906)	59.38 (71.43)
[30/157]	0.1004 (0.0980)	0.0608 (0.0591)	1.040 (0.924)	65.62 (70.77)
[40/157]	0.1000 (0.0985)	0.0602 (0.0595)	0.684 (0.934)	81.25 (70.50)
[50/157]	0.0998 (0.0988)	0.0604 (0.0598)	0.966 (0.946)	68.75 (70.34)
[60/157]	0.1006 (0.0990)	0.0603 (0.0599)	0.821 (0.951)	75.00 (70.39)
[70/157]	0.1006 (0.0991)	0.0600 (0.0600)	1.141 (0.952)	62.50 (70.64)
[80/157]	0.1011 (0.0992)	0.0600 (0.0601)	1.000 (0.955)	68.75 (70.52)
[90/157]	0.1012 (0.0993)	0.0611 (0.0602)	0.945 (0.953)	68.75 (70.67)
[100/157]	0.0994 (0.0993)	0.0597 (0.0603)	0.592 (0.947)	84.38 (70.79)
[110/157]	0.1015 (0.0994)	0.0619 (0.0603)	1.117 (0.948)	68.75 (70.86)
[120/157]	0.1004 (0.0995)	0.0605 (0.0604)	1.130 (0.947)	53.12 (70.79)
[130/157]	0.1007 (0.0995)	0.0608 (0.0604)	1.061 (0.949)	68.75 (70.78)
[140/157]	0.0996 (0.0995)	0.0604 (0.0605)	0.862 (0.951)	81.25 (70.81)
[150/157]	0.1008 (0.0996)	0.0612 (0.0605)	0.893 (0.950)	75.00 (70.67)
[156/157]	0.0842 (0.0994)	0.0559 (0.0605)	1.082 (0.949)	75.00 (70.68)
 * Train Acc 70.680
 * Val Acc 70.900, Total time 0.61
 * Val loss 0.842, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0086 (0.0086)	1.065 (1.065)	65.62 (65.62)
[10/157]	0.1004 (0.0944)	0.0609 (0.0557)	0.922 (0.997)	68.75 (67.90)
[20/157]	0.1002 (0.0971)	0.0610 (0.0585)	0.992 (0.924)	68.75 (70.24)
[30/157]	0.1004 (0.0980)	0.0611 (0.0593)	1.022 (0.939)	68.75 (70.26)
[40/157]	0.1009 (0.0985)	0.0610 (0.0597)	0.792 (0.941)	75.00 (70.66)
[50/157]	0.1006 (0.0988)	0.0611 (0.0599)	0.879 (0.949)	75.00 (70.53)
[60/157]	0.0997 (0.0990)	0.0597 (0.0600)	0.789 (0.938)	71.88 (70.70)
[70/157]	0.0996 (0.0991)	0.0604 (0.0601)	1.054 (0.943)	75.00 (70.69)
[80/157]	0.0987 (0.0992)	0.0589 (0.0602)	1.217 (0.942)	50.00 (70.52)
[90/157]	0.0992 (0.0993)	0.0592 (0.0603)	1.190 (0.952)	62.50 (70.30)
[100/157]	0.0996 (0.0994)	0.0603 (0.0604)	0.584 (0.951)	90.62 (70.58)
[110/157]	0.1008 (0.0995)	0.0618 (0.0604)	0.701 (0.944)	81.25 (70.92)
[120/157]	0.1017 (0.0996)	0.0617 (0.0606)	1.073 (0.950)	62.50 (70.64)
[130/157]	0.0996 (0.0997)	0.0596 (0.0606)	1.279 (0.954)	65.62 (70.21)
[140/157]	0.0996 (0.0997)	0.0605 (0.0606)	0.642 (0.952)	81.25 (70.35)
[150/157]	0.1005 (0.0997)	0.0605 (0.0606)	1.065 (0.948)	65.62 (70.49)
[156/157]	0.0842 (0.0996)	0.0560 (0.0606)	0.927 (0.946)	62.50 (70.52)
 * Train Acc 70.520
 * Val Acc 71.900, Total time 0.61
 * Val loss 0.827, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0458 (0.0458)	0.0090 (0.0090)	1.011 (1.011)	75.00 (75.00)
[10/157]	0.1014 (0.0951)	0.0616 (0.0565)	1.046 (1.044)	68.75 (69.32)
[20/157]	0.1024 (0.0978)	0.0624 (0.0590)	0.865 (1.036)	71.88 (67.86)
[30/157]	0.1003 (0.0985)	0.0612 (0.0597)	1.016 (0.993)	62.50 (68.85)
[40/157]	0.1002 (0.0989)	0.0610 (0.0601)	0.757 (0.960)	75.00 (70.12)
[50/157]	0.1026 (0.0991)	0.0597 (0.0603)	1.230 (0.975)	56.25 (69.12)
[60/157]	0.1001 (0.0992)	0.0605 (0.0604)	1.276 (0.965)	59.38 (69.47)
[70/157]	0.1003 (0.0993)	0.0605 (0.0605)	0.905 (0.979)	62.50 (68.84)
[80/157]	0.1008 (0.0994)	0.0613 (0.0606)	0.736 (0.968)	75.00 (69.37)
[90/157]	0.1000 (0.0995)	0.0604 (0.0606)	0.821 (0.961)	75.00 (69.61)
[100/157]	0.0995 (0.0995)	0.0602 (0.0607)	0.992 (0.956)	81.25 (70.05)
[110/157]	0.1004 (0.0996)	0.0613 (0.0608)	0.858 (0.959)	75.00 (70.05)
[120/157]	0.0999 (0.0996)	0.0607 (0.0608)	0.759 (0.956)	78.12 (70.12)
[130/157]	0.0996 (0.0996)	0.0601 (0.0608)	1.041 (0.959)	68.75 (70.09)
[140/157]	0.1004 (0.0996)	0.0610 (0.0609)	1.172 (0.959)	56.25 (69.90)
[150/157]	0.1000 (0.0997)	0.0606 (0.0609)	0.943 (0.961)	78.12 (69.97)
[156/157]	0.0842 (0.0996)	0.0569 (0.0609)	1.235 (0.962)	50.00 (70.04)
 * Train Acc 70.040
 * Val Acc 71.300, Total time 0.59
 * Val loss 0.839, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0414 (0.0414)	0.0084 (0.0084)	1.072 (1.072)	68.75 (68.75)
[10/157]	0.1008 (0.0962)	0.0617 (0.0574)	1.153 (0.981)	59.38 (67.33)
[20/157]	0.0985 (0.0991)	0.0594 (0.0602)	1.076 (1.027)	65.62 (66.67)
[30/157]	0.1050 (0.1002)	0.0637 (0.0613)	0.757 (1.023)	81.25 (67.44)
[40/157]	0.0965 (0.1005)	0.0584 (0.0617)	1.282 (0.995)	56.25 (68.37)
[50/157]	0.1049 (0.1000)	0.0651 (0.0614)	1.022 (0.988)	62.50 (68.93)
[60/157]	0.1030 (0.1006)	0.0635 (0.0620)	0.675 (0.968)	81.25 (69.83)
[70/157]	0.1030 (0.1011)	0.0631 (0.0624)	0.917 (0.969)	71.88 (69.81)
[80/157]	0.0964 (0.1012)	0.0581 (0.0625)	0.935 (0.962)	68.75 (69.98)
[90/157]	0.0969 (0.1006)	0.0576 (0.0619)	0.984 (0.956)	65.62 (70.26)
[100/157]	0.0951 (0.1001)	0.0570 (0.0615)	1.021 (0.950)	75.00 (70.58)
[110/157]	0.0953 (0.0997)	0.0567 (0.0612)	1.470 (0.957)	50.00 (70.24)
[120/157]	0.1080 (0.0997)	0.0671 (0.0613)	1.233 (0.956)	59.38 (70.30)
[130/157]	0.1035 (0.1002)	0.0633 (0.0616)	0.749 (0.950)	78.12 (70.28)
[140/157]	0.0977 (0.1003)	0.0574 (0.0617)	0.991 (0.945)	65.62 (70.61)
[150/157]	0.0961 (0.1000)	0.0586 (0.0615)	1.362 (0.946)	53.12 (70.74)
[156/157]	0.0801 (0.0997)	0.0543 (0.0613)	1.160 (0.945)	62.50 (70.90)
 * Train Acc 70.900
 * Val Acc 72.000, Total time 0.59
 * Val loss 0.830, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0091 (0.0091)	0.869 (0.869)	78.12 (78.12)
[10/157]	0.1069 (0.1002)	0.0667 (0.0608)	0.772 (0.908)	78.12 (72.44)
[20/157]	0.1041 (0.1030)	0.0644 (0.0636)	1.045 (0.991)	62.50 (68.01)
[30/157]	0.0949 (0.1007)	0.0570 (0.0619)	0.897 (0.969)	65.62 (69.86)
[40/157]	0.0967 (0.0996)	0.0579 (0.0609)	1.066 (0.983)	59.38 (68.90)
[50/157]	0.1008 (0.1008)	0.0608 (0.0621)	0.815 (0.948)	78.12 (70.53)
[60/157]	0.1019 (0.1010)	0.0619 (0.0622)	0.880 (0.951)	71.88 (70.34)
[70/157]	0.1023 (0.1011)	0.0630 (0.0622)	0.884 (0.953)	78.12 (70.42)
[80/157]	0.0996 (0.1012)	0.0618 (0.0624)	0.929 (0.962)	65.62 (69.98)
[90/157]	0.0969 (0.1006)	0.0587 (0.0618)	0.857 (0.971)	81.25 (69.92)
[100/157]	0.0963 (0.1001)	0.0584 (0.0615)	0.869 (0.968)	68.75 (69.89)
[110/157]	0.0977 (0.1007)	0.0607 (0.0621)	0.822 (0.973)	75.00 (69.65)
[120/157]	0.1004 (0.1006)	0.0605 (0.0619)	1.087 (0.974)	68.75 (69.58)
[130/157]	0.0999 (0.1004)	0.0611 (0.0618)	0.844 (0.966)	81.25 (69.92)
[140/157]	0.0996 (0.1003)	0.0611 (0.0617)	0.919 (0.958)	78.12 (70.30)
[150/157]	0.0990 (0.1002)	0.0601 (0.0616)	0.838 (0.956)	68.75 (70.32)
[156/157]	0.0825 (0.1001)	0.0562 (0.0615)	1.200 (0.957)	62.50 (70.20)
 * Train Acc 70.200
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.822, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0086 (0.0086)	1.080 (1.080)	65.62 (65.62)
[10/157]	0.0994 (0.0934)	0.0601 (0.0551)	0.825 (1.060)	68.75 (63.07)
[20/157]	0.0988 (0.0957)	0.0601 (0.0576)	1.094 (0.993)	62.50 (67.11)
[30/157]	0.0985 (0.0965)	0.0605 (0.0584)	0.848 (0.992)	68.75 (68.55)
[40/157]	0.0990 (0.0972)	0.0598 (0.0589)	1.061 (0.982)	71.88 (69.13)
[50/157]	0.1001 (0.0975)	0.0607 (0.0592)	0.858 (0.956)	71.88 (70.34)
[60/157]	0.0995 (0.0977)	0.0605 (0.0595)	0.779 (0.954)	75.00 (70.39)
[70/157]	0.0992 (0.0979)	0.0600 (0.0596)	0.856 (0.942)	78.12 (70.95)
[80/157]	0.0993 (0.0979)	0.0617 (0.0597)	0.677 (0.937)	78.12 (71.18)
[90/157]	0.0982 (0.0980)	0.0598 (0.0597)	1.080 (0.935)	65.62 (71.22)
[100/157]	0.1012 (0.0981)	0.0620 (0.0598)	0.908 (0.937)	71.88 (71.35)
[110/157]	0.1029 (0.0985)	0.0639 (0.0602)	1.013 (0.942)	65.62 (71.20)
[120/157]	0.1061 (0.0989)	0.0646 (0.0604)	1.005 (0.944)	68.75 (70.84)
[130/157]	0.1028 (0.0991)	0.0636 (0.0607)	1.144 (0.947)	68.75 (70.75)
[140/157]	0.1010 (0.0993)	0.0620 (0.0609)	0.932 (0.945)	62.50 (70.81)
[150/157]	0.1026 (0.0996)	0.0634 (0.0611)	0.902 (0.945)	81.25 (70.82)
[156/157]	0.0860 (0.0996)	0.0586 (0.0611)	1.689 (0.946)	37.50 (70.74)
 * Train Acc 70.740
 * Val Acc 72.200, Total time 0.60
 * Val loss 0.824, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0083 (0.0083)	0.714 (0.714)	75.00 (75.00)
[10/157]	0.1025 (0.0964)	0.0635 (0.0579)	0.737 (0.838)	75.00 (73.86)
[20/157]	0.1052 (0.0994)	0.0646 (0.0605)	0.898 (0.913)	62.50 (72.32)
[30/157]	0.1028 (0.1005)	0.0624 (0.0615)	1.140 (0.917)	68.75 (71.88)
[40/157]	0.1031 (0.1009)	0.0643 (0.0620)	1.062 (0.902)	62.50 (72.64)
[50/157]	0.1030 (0.1013)	0.0632 (0.0623)	0.890 (0.898)	75.00 (72.49)
[60/157]	0.1023 (0.1015)	0.0628 (0.0625)	0.971 (0.917)	78.12 (71.88)
[70/157]	0.0950 (0.1015)	0.0571 (0.0625)	0.681 (0.925)	84.38 (71.65)
[80/157]	0.0966 (0.1008)	0.0576 (0.0619)	0.912 (0.930)	68.75 (71.41)
[90/157]	0.0946 (0.1002)	0.0572 (0.0615)	0.921 (0.930)	68.75 (71.15)
[100/157]	0.0963 (0.0998)	0.0588 (0.0612)	0.922 (0.939)	71.88 (70.98)
[110/157]	0.0971 (0.0994)	0.0589 (0.0609)	0.855 (0.943)	75.00 (71.03)
[120/157]	0.0972 (0.0991)	0.0591 (0.0607)	0.944 (0.937)	65.62 (71.07)
[130/157]	0.0972 (0.0989)	0.0594 (0.0606)	1.077 (0.942)	68.75 (70.90)
[140/157]	0.0977 (0.0986)	0.0593 (0.0604)	1.135 (0.948)	71.88 (70.70)
[150/157]	0.1027 (0.0989)	0.0635 (0.0606)	1.114 (0.945)	59.38 (70.78)
[156/157]	0.0875 (0.0989)	0.0604 (0.0607)	1.035 (0.942)	50.00 (71.00)
 * Train Acc 71.000
 * Val Acc 71.000, Total time 0.61
 * Val loss 0.844, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0088 (0.0088)	0.885 (0.885)	78.12 (78.12)
[10/157]	0.1035 (0.0976)	0.0642 (0.0590)	1.204 (0.984)	68.75 (69.03)
[20/157]	0.1023 (0.1000)	0.0632 (0.0614)	0.832 (0.935)	71.88 (70.68)
[30/157]	0.1026 (0.1011)	0.0636 (0.0624)	0.985 (0.958)	59.38 (69.46)
[40/157]	0.1040 (0.1015)	0.0643 (0.0628)	1.021 (0.946)	68.75 (70.43)
[50/157]	0.1026 (0.1019)	0.0634 (0.0631)	0.958 (0.952)	68.75 (70.28)
[60/157]	0.1046 (0.1021)	0.0647 (0.0633)	0.724 (0.953)	78.12 (69.93)
[70/157]	0.1018 (0.1022)	0.0621 (0.0633)	0.981 (0.956)	65.62 (70.11)
[80/157]	0.1036 (0.1024)	0.0635 (0.0634)	0.793 (0.936)	78.12 (71.10)
[90/157]	0.1041 (0.1025)	0.0640 (0.0635)	0.963 (0.928)	75.00 (71.15)
[100/157]	0.0939 (0.1024)	0.0565 (0.0634)	0.918 (0.928)	71.88 (71.32)
[110/157]	0.1042 (0.1021)	0.0643 (0.0632)	0.877 (0.934)	68.75 (71.09)
[120/157]	0.1039 (0.1022)	0.0645 (0.0633)	1.153 (0.940)	56.25 (70.89)
[130/157]	0.1045 (0.1023)	0.0637 (0.0633)	0.799 (0.938)	65.62 (70.78)
[140/157]	0.1020 (0.1024)	0.0630 (0.0634)	0.923 (0.945)	68.75 (70.48)
[150/157]	0.0953 (0.1021)	0.0578 (0.0631)	0.740 (0.950)	87.50 (70.41)
[156/157]	0.0798 (0.1017)	0.0551 (0.0629)	1.024 (0.949)	62.50 (70.40)
 * Train Acc 70.400
 * Val Acc 71.100, Total time 0.58
 * Val loss 0.833, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0084 (0.0084)	1.133 (1.133)	75.00 (75.00)
[10/157]	0.0996 (0.0952)	0.0606 (0.0559)	1.033 (0.962)	71.88 (71.59)
[20/157]	0.1028 (0.0976)	0.0617 (0.0586)	1.197 (0.959)	59.38 (70.09)
[30/157]	0.1009 (0.0984)	0.0624 (0.0596)	0.800 (0.960)	75.00 (70.46)
[40/157]	0.1016 (0.0990)	0.0616 (0.0604)	0.819 (0.973)	81.25 (69.44)
[50/157]	0.1013 (0.0994)	0.0616 (0.0607)	0.699 (0.968)	71.88 (69.55)
[60/157]	0.1017 (0.0995)	0.0625 (0.0609)	1.003 (0.965)	65.62 (69.01)
[70/157]	0.0992 (0.0998)	0.0605 (0.0610)	1.056 (0.959)	59.38 (69.41)
[80/157]	0.1002 (0.0998)	0.0610 (0.0610)	0.847 (0.959)	75.00 (69.33)
[90/157]	0.1001 (0.0998)	0.0612 (0.0611)	1.013 (0.959)	68.75 (69.40)
[100/157]	0.1002 (0.0998)	0.0610 (0.0611)	0.578 (0.964)	87.50 (69.28)
[110/157]	0.0994 (0.0999)	0.0604 (0.0612)	1.038 (0.961)	68.75 (69.45)
[120/157]	0.1001 (0.0999)	0.0611 (0.0612)	0.929 (0.955)	71.88 (69.76)
[130/157]	0.1011 (0.0999)	0.0619 (0.0612)	0.814 (0.949)	71.88 (70.06)
[140/157]	0.1004 (0.1000)	0.0608 (0.0613)	1.124 (0.956)	65.62 (69.73)
[150/157]	0.1001 (0.1000)	0.0609 (0.0613)	0.955 (0.957)	68.75 (69.72)
[156/157]	0.0848 (0.0999)	0.0566 (0.0612)	0.731 (0.953)	75.00 (69.94)
 * Train Acc 69.940
 * Val Acc 71.400, Total time 0.59
 * Val loss 0.837, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0408 (0.0408)	0.0080 (0.0080)	1.001 (1.001)	62.50 (62.50)
[10/157]	0.1202 (0.0948)	0.0797 (0.0563)	0.965 (0.984)	75.00 (69.32)
[20/157]	0.0935 (0.0991)	0.0569 (0.0611)	0.812 (0.997)	71.88 (68.75)
[30/157]	0.0981 (0.0986)	0.0585 (0.0604)	0.969 (0.954)	62.50 (70.36)
[40/157]	0.0987 (0.0984)	0.0595 (0.0603)	0.841 (0.968)	71.88 (69.97)
[50/157]	0.0981 (0.0983)	0.0598 (0.0602)	1.196 (0.969)	56.25 (69.98)
[60/157]	0.0958 (0.0983)	0.0574 (0.0602)	0.801 (0.975)	71.88 (69.77)
[70/157]	0.0985 (0.0985)	0.0589 (0.0603)	0.663 (0.957)	81.25 (70.91)
[80/157]	0.1012 (0.0994)	0.0607 (0.0611)	0.941 (0.952)	68.75 (70.99)
[90/157]	0.1002 (0.0995)	0.0610 (0.0611)	0.766 (0.958)	84.38 (70.71)
[100/157]	0.1006 (0.0995)	0.0615 (0.0612)	0.844 (0.954)	71.88 (70.73)
[110/157]	0.1006 (0.0996)	0.0611 (0.0612)	0.675 (0.947)	81.25 (70.83)
[120/157]	0.1010 (0.0996)	0.0614 (0.0611)	1.193 (0.946)	62.50 (70.82)
[130/157]	0.1004 (0.0997)	0.0611 (0.0612)	1.035 (0.947)	65.62 (70.83)
[140/157]	0.0990 (0.0997)	0.0602 (0.0612)	0.968 (0.949)	75.00 (70.70)
[150/157]	0.1016 (0.0998)	0.0620 (0.0613)	0.969 (0.944)	65.62 (70.78)
[156/157]	0.0830 (0.0996)	0.0563 (0.0612)	1.416 (0.946)	50.00 (70.72)
 * Train Acc 70.720
 * Val Acc 71.300, Total time 0.61
 * Val loss 0.819, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0089 (0.0089)	0.997 (0.997)	68.75 (68.75)
[10/157]	0.1009 (0.0949)	0.0608 (0.0557)	0.815 (0.898)	81.25 (73.86)
[20/157]	0.1043 (0.0976)	0.0629 (0.0582)	0.916 (0.920)	78.12 (71.88)
[30/157]	0.1022 (0.0987)	0.0616 (0.0595)	0.899 (0.942)	71.88 (70.36)
[40/157]	0.1012 (0.0991)	0.0617 (0.0600)	0.921 (0.944)	68.75 (70.50)
[50/157]	0.1008 (0.0993)	0.0611 (0.0601)	1.132 (0.936)	59.38 (70.65)
[60/157]	0.1014 (0.0994)	0.0604 (0.0602)	0.863 (0.944)	71.88 (70.65)
[70/157]	0.1046 (0.0996)	0.0610 (0.0603)	1.097 (0.948)	68.75 (70.38)
[80/157]	0.1003 (0.0996)	0.0616 (0.0605)	0.782 (0.952)	75.00 (70.25)
[90/157]	0.1012 (0.0997)	0.0621 (0.0607)	1.197 (0.957)	53.12 (69.99)
[100/157]	0.1010 (0.0998)	0.0612 (0.0608)	0.926 (0.956)	65.62 (70.17)
[110/157]	0.1002 (0.0998)	0.0609 (0.0609)	0.736 (0.953)	84.38 (70.33)
[120/157]	0.0999 (0.0998)	0.0607 (0.0609)	1.044 (0.947)	62.50 (70.58)
[130/157]	0.1003 (0.0998)	0.0611 (0.0609)	0.786 (0.944)	75.00 (70.78)
[140/157]	0.1003 (0.0998)	0.0612 (0.0610)	0.765 (0.934)	78.12 (70.99)
[150/157]	0.1006 (0.0999)	0.0613 (0.0610)	0.776 (0.937)	87.50 (70.80)
[156/157]	0.0841 (0.0998)	0.0565 (0.0610)	0.814 (0.936)	62.50 (70.94)
 * Train Acc 70.940
 * Val Acc 71.400, Total time 0.60
 * Val loss 0.820, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0089 (0.0089)	0.921 (0.921)	65.62 (65.62)
[10/157]	0.1002 (0.0951)	0.0615 (0.0564)	0.756 (0.896)	68.75 (69.89)
[20/157]	0.1000 (0.0974)	0.0608 (0.0589)	0.795 (0.900)	78.12 (71.13)
[30/157]	0.0998 (0.0982)	0.0604 (0.0596)	0.888 (0.895)	68.75 (72.08)
[40/157]	0.0998 (0.0987)	0.0607 (0.0601)	0.879 (0.910)	68.75 (71.72)
[50/157]	0.1006 (0.0989)	0.0609 (0.0603)	0.841 (0.917)	71.88 (71.32)
[60/157]	0.1003 (0.0991)	0.0613 (0.0605)	0.896 (0.912)	68.75 (71.52)
[70/157]	0.0995 (0.0992)	0.0604 (0.0605)	0.904 (0.918)	75.00 (71.08)
[80/157]	0.0996 (0.0993)	0.0603 (0.0606)	1.041 (0.930)	59.38 (70.79)
[90/157]	0.1003 (0.0994)	0.0608 (0.0607)	0.886 (0.928)	71.88 (70.95)
[100/157]	0.1002 (0.0994)	0.0608 (0.0607)	1.012 (0.933)	71.88 (70.92)
[110/157]	0.1008 (0.0995)	0.0617 (0.0608)	0.782 (0.931)	81.25 (71.17)
[120/157]	0.1003 (0.0995)	0.0613 (0.0609)	0.971 (0.938)	71.88 (70.74)
[130/157]	0.1011 (0.0996)	0.0611 (0.0609)	0.804 (0.939)	68.75 (70.75)
[140/157]	0.1001 (0.0996)	0.0607 (0.0609)	0.881 (0.938)	65.62 (70.68)
[150/157]	0.1006 (0.0996)	0.0611 (0.0609)	1.370 (0.940)	56.25 (70.65)
[156/157]	0.0841 (0.0995)	0.0563 (0.0609)	0.983 (0.943)	75.00 (70.68)
 * Train Acc 70.680
 * Val Acc 72.600, Total time 0.60
 * Val loss 0.817, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0088 (0.0088)	1.188 (1.188)	62.50 (62.50)
[10/157]	0.0997 (0.0949)	0.0603 (0.0559)	0.559 (0.932)	87.50 (71.31)
[20/157]	0.1007 (0.0975)	0.0613 (0.0586)	0.878 (0.916)	75.00 (71.88)
[30/157]	0.1019 (0.0986)	0.0621 (0.0598)	0.681 (0.915)	81.25 (71.88)
[40/157]	0.1008 (0.0991)	0.0609 (0.0601)	0.814 (0.922)	75.00 (71.88)
[50/157]	0.1013 (0.0993)	0.0616 (0.0603)	0.811 (0.928)	84.38 (71.75)
[60/157]	0.1001 (0.0994)	0.0607 (0.0604)	0.909 (0.937)	75.00 (71.31)
[70/157]	0.1004 (0.0995)	0.0611 (0.0605)	1.224 (0.939)	56.25 (70.55)
[80/157]	0.1005 (0.0996)	0.0611 (0.0607)	1.039 (0.935)	71.88 (70.76)
[90/157]	0.1010 (0.0997)	0.0618 (0.0608)	0.831 (0.923)	71.88 (71.57)
[100/157]	0.1011 (0.0998)	0.0620 (0.0609)	0.904 (0.926)	68.75 (71.35)
[110/157]	0.0997 (0.0998)	0.0605 (0.0609)	0.857 (0.930)	68.75 (71.14)
[120/157]	0.1018 (0.0999)	0.0624 (0.0610)	0.680 (0.930)	81.25 (71.13)
[130/157]	0.1007 (0.1000)	0.0611 (0.0611)	0.839 (0.927)	71.88 (71.25)
[140/157]	0.0997 (0.1000)	0.0605 (0.0611)	0.986 (0.932)	71.88 (71.17)
[150/157]	0.0996 (0.1000)	0.0608 (0.0611)	0.773 (0.929)	78.12 (71.40)
[156/157]	0.0833 (0.0999)	0.0563 (0.0611)	1.224 (0.933)	50.00 (71.14)
 * Train Acc 71.140
 * Val Acc 70.800, Total time 0.60
 * Val loss 0.834, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0085 (0.0085)	0.820 (0.820)	78.12 (78.12)
[10/157]	0.1018 (0.0953)	0.0616 (0.0565)	0.806 (0.835)	75.00 (74.43)
[20/157]	0.0987 (0.0976)	0.0597 (0.0589)	1.109 (0.890)	65.62 (73.36)
[30/157]	0.0999 (0.0983)	0.0608 (0.0596)	1.301 (0.921)	56.25 (71.88)
[40/157]	0.0994 (0.0988)	0.0600 (0.0600)	0.815 (0.926)	71.88 (71.19)
[50/157]	0.0992 (0.0990)	0.0599 (0.0602)	1.001 (0.927)	71.88 (71.26)
[60/157]	0.1001 (0.0992)	0.0605 (0.0603)	0.630 (0.916)	87.50 (71.77)
[70/157]	0.1000 (0.0992)	0.0607 (0.0605)	1.108 (0.920)	59.38 (71.30)
[80/157]	0.1000 (0.0993)	0.0605 (0.0606)	0.699 (0.931)	81.25 (71.14)
[90/157]	0.1002 (0.0994)	0.0608 (0.0606)	0.601 (0.928)	81.25 (71.36)
[100/157]	0.1001 (0.0995)	0.0610 (0.0607)	0.881 (0.926)	68.75 (71.13)
[110/157]	0.1003 (0.0995)	0.0607 (0.0608)	0.887 (0.923)	71.88 (71.34)
[120/157]	0.0995 (0.0995)	0.0606 (0.0608)	0.894 (0.920)	71.88 (71.36)
[130/157]	0.1009 (0.0996)	0.0617 (0.0608)	1.252 (0.918)	59.38 (71.56)
[140/157]	0.1000 (0.0996)	0.0606 (0.0609)	0.784 (0.923)	78.12 (71.43)
[150/157]	0.1001 (0.0996)	0.0602 (0.0609)	1.343 (0.926)	59.38 (71.25)
[156/157]	0.0860 (0.0995)	0.0567 (0.0609)	0.976 (0.932)	62.50 (70.94)
 * Train Acc 70.940
 * Val Acc 71.900, Total time 0.61
 * Val loss 0.817, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0082 (0.0082)	0.712 (0.712)	81.25 (81.25)
[10/157]	0.0997 (0.0947)	0.0605 (0.0561)	1.021 (0.930)	78.12 (73.01)
[20/157]	0.1004 (0.0972)	0.0613 (0.0586)	1.049 (0.938)	65.62 (71.58)
[30/157]	0.1000 (0.0981)	0.0613 (0.0595)	0.755 (0.926)	78.12 (71.67)
[40/157]	0.1001 (0.0986)	0.0606 (0.0600)	0.994 (0.922)	75.00 (71.95)
[50/157]	0.0999 (0.0988)	0.0607 (0.0601)	0.759 (0.920)	71.88 (71.63)
[60/157]	0.1005 (0.0990)	0.0615 (0.0603)	0.760 (0.908)	75.00 (72.13)
[70/157]	0.0997 (0.0992)	0.0610 (0.0604)	0.840 (0.901)	68.75 (72.36)
[80/157]	0.1008 (0.0994)	0.0620 (0.0606)	0.940 (0.909)	62.50 (72.38)
[90/157]	0.1007 (0.0994)	0.0618 (0.0607)	0.913 (0.908)	68.75 (72.29)
[100/157]	0.1016 (0.0996)	0.0612 (0.0609)	0.986 (0.914)	65.62 (72.15)
[110/157]	0.0999 (0.0996)	0.0608 (0.0609)	1.027 (0.925)	65.62 (71.82)
[120/157]	0.1004 (0.0997)	0.0609 (0.0609)	1.022 (0.934)	68.75 (71.54)
[130/157]	0.1008 (0.0997)	0.0613 (0.0609)	0.799 (0.932)	71.88 (71.47)
[140/157]	0.0993 (0.0997)	0.0601 (0.0609)	0.828 (0.932)	84.38 (71.37)
[150/157]	0.1006 (0.0997)	0.0610 (0.0609)	0.875 (0.932)	78.12 (71.44)
[156/157]	0.0847 (0.0996)	0.0560 (0.0609)	1.621 (0.932)	75.00 (71.38)
 * Train Acc 71.380
 * Val Acc 72.300, Total time 0.61
 * Val loss 0.820, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0456 (0.0456)	0.0092 (0.0092)	0.770 (0.770)	75.00 (75.00)
[10/157]	0.1006 (0.0955)	0.0610 (0.0564)	1.299 (1.009)	65.62 (69.03)
[20/157]	0.1012 (0.0977)	0.0619 (0.0586)	1.016 (0.998)	68.75 (69.20)
[30/157]	0.0983 (0.0984)	0.0587 (0.0593)	0.881 (0.999)	75.00 (68.65)
[40/157]	0.0990 (0.0988)	0.0593 (0.0597)	0.870 (0.981)	62.50 (69.36)
[50/157]	0.1003 (0.0990)	0.0599 (0.0599)	1.059 (0.971)	62.50 (69.73)
[60/157]	0.1003 (0.0992)	0.0609 (0.0601)	1.026 (0.973)	59.38 (69.52)
[70/157]	0.1006 (0.0993)	0.0609 (0.0603)	1.101 (0.970)	68.75 (69.85)
[80/157]	0.1005 (0.0994)	0.0607 (0.0604)	0.912 (0.968)	78.12 (69.98)
[90/157]	0.1008 (0.0994)	0.0608 (0.0605)	0.908 (0.961)	62.50 (70.12)
[100/157]	0.1002 (0.0995)	0.0610 (0.0606)	0.578 (0.954)	87.50 (70.36)
[110/157]	0.1000 (0.0995)	0.0608 (0.0606)	0.930 (0.946)	68.75 (70.78)
[120/157]	0.1006 (0.0996)	0.0616 (0.0607)	0.980 (0.944)	71.88 (70.89)
[130/157]	0.1003 (0.0996)	0.0606 (0.0607)	0.759 (0.938)	81.25 (70.99)
[140/157]	0.1003 (0.0996)	0.0611 (0.0608)	0.722 (0.938)	78.12 (71.01)
[150/157]	0.1009 (0.0996)	0.0611 (0.0608)	0.877 (0.939)	78.12 (70.99)
[156/157]	0.0823 (0.0995)	0.0558 (0.0608)	1.340 (0.936)	62.50 (71.10)
 * Train Acc 71.100
 * Val Acc 71.000, Total time 0.60
 * Val loss 0.823, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0087 (0.0087)	0.889 (0.889)	65.62 (65.62)
[10/157]	0.1003 (0.0953)	0.0610 (0.0564)	1.051 (0.917)	71.88 (74.15)
[20/157]	0.1003 (0.0977)	0.0587 (0.0588)	0.998 (0.956)	68.75 (70.68)
[30/157]	0.1000 (0.0984)	0.0613 (0.0598)	0.714 (0.928)	78.12 (71.67)
[40/157]	0.0998 (0.0987)	0.0607 (0.0601)	0.583 (0.943)	81.25 (70.50)
[50/157]	0.1000 (0.0990)	0.0602 (0.0603)	0.984 (0.942)	78.12 (70.89)
[60/157]	0.1005 (0.0991)	0.0611 (0.0604)	0.849 (0.928)	75.00 (71.52)
[70/157]	0.0995 (0.0993)	0.0602 (0.0605)	0.746 (0.927)	78.12 (71.35)
[80/157]	0.1037 (0.0994)	0.0607 (0.0606)	0.966 (0.925)	75.00 (71.41)
[90/157]	0.1000 (0.0995)	0.0613 (0.0607)	0.925 (0.934)	78.12 (71.19)
[100/157]	0.1009 (0.0996)	0.0614 (0.0608)	0.824 (0.927)	78.12 (71.60)
[110/157]	0.1004 (0.0997)	0.0614 (0.0609)	1.079 (0.938)	62.50 (71.14)
[120/157]	0.1025 (0.0997)	0.0611 (0.0609)	0.720 (0.940)	71.88 (71.23)
[130/157]	0.1009 (0.0997)	0.0604 (0.0609)	0.940 (0.940)	68.75 (71.02)
[140/157]	0.0999 (0.0998)	0.0608 (0.0609)	0.771 (0.941)	71.88 (70.97)
[150/157]	0.1001 (0.0998)	0.0606 (0.0610)	1.003 (0.944)	71.88 (70.84)
[156/157]	0.0880 (0.0997)	0.0575 (0.0610)	0.731 (0.944)	62.50 (70.84)
 * Train Acc 70.840
 * Val Acc 71.800, Total time 0.61
 * Val loss 0.819, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0092 (0.0092)	0.869 (0.869)	68.75 (68.75)
[10/157]	0.1015 (0.0952)	0.0622 (0.0565)	0.724 (0.939)	87.50 (70.74)
[20/157]	0.1000 (0.0976)	0.0608 (0.0589)	0.926 (0.914)	59.38 (71.28)
[30/157]	0.1002 (0.0984)	0.0616 (0.0597)	0.742 (0.906)	81.25 (72.18)
[40/157]	0.1002 (0.0988)	0.0607 (0.0601)	0.627 (0.903)	78.12 (72.03)
[50/157]	0.0990 (0.0990)	0.0603 (0.0603)	0.929 (0.922)	75.00 (71.26)
[60/157]	0.1012 (0.0992)	0.0614 (0.0605)	1.351 (0.923)	62.50 (71.26)
[70/157]	0.1006 (0.0993)	0.0615 (0.0607)	0.767 (0.926)	75.00 (71.21)
[80/157]	0.1002 (0.0994)	0.0613 (0.0608)	0.695 (0.915)	84.38 (71.72)
[90/157]	0.1001 (0.0995)	0.0610 (0.0608)	1.144 (0.924)	56.25 (71.29)
[100/157]	0.1010 (0.0996)	0.0619 (0.0610)	0.870 (0.935)	71.88 (71.07)
[110/157]	0.0995 (0.0997)	0.0606 (0.0610)	1.078 (0.937)	68.75 (70.92)
[120/157]	0.1003 (0.0998)	0.0615 (0.0611)	0.747 (0.924)	75.00 (71.49)
[130/157]	0.0997 (0.0998)	0.0607 (0.0611)	0.953 (0.929)	65.62 (71.23)
[140/157]	0.1018 (0.0998)	0.0628 (0.0612)	0.694 (0.929)	84.38 (70.99)
[150/157]	0.1014 (0.0999)	0.0615 (0.0612)	0.693 (0.927)	81.25 (71.07)
[156/157]	0.0840 (0.0998)	0.0569 (0.0612)	0.798 (0.928)	75.00 (71.12)
 * Train Acc 71.120
 * Val Acc 71.400, Total time 0.61
 * Val loss 0.825, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0090 (0.0090)	0.765 (0.765)	81.25 (81.25)
[10/157]	0.0998 (0.0945)	0.0610 (0.0560)	0.840 (0.962)	78.12 (69.32)
[20/157]	0.1018 (0.0974)	0.0627 (0.0590)	0.671 (0.962)	81.25 (69.05)
[30/157]	0.1009 (0.0986)	0.0614 (0.0598)	0.732 (0.948)	81.25 (69.76)
[40/157]	0.1007 (0.0989)	0.0614 (0.0602)	0.879 (0.917)	78.12 (71.11)
[50/157]	0.0994 (0.0991)	0.0601 (0.0604)	0.790 (0.911)	78.12 (71.57)
[60/157]	0.1004 (0.0993)	0.0612 (0.0605)	0.924 (0.915)	65.62 (71.67)
[70/157]	0.1000 (0.0994)	0.0608 (0.0606)	0.980 (0.916)	62.50 (71.39)
[80/157]	0.1002 (0.0994)	0.0608 (0.0608)	1.120 (0.929)	59.38 (70.60)
[90/157]	0.0999 (0.0995)	0.0617 (0.0609)	0.824 (0.926)	75.00 (70.91)
[100/157]	0.1001 (0.0996)	0.0608 (0.0609)	0.703 (0.932)	71.88 (70.51)
[110/157]	0.1002 (0.0996)	0.0609 (0.0610)	0.556 (0.928)	87.50 (70.86)
[120/157]	0.1002 (0.0996)	0.0613 (0.0610)	0.940 (0.927)	65.62 (70.69)
[130/157]	0.0996 (0.0997)	0.0606 (0.0611)	0.827 (0.922)	75.00 (70.94)
[140/157]	0.1034 (0.0998)	0.0630 (0.0612)	0.870 (0.923)	81.25 (71.05)
[150/157]	0.0998 (0.0998)	0.0607 (0.0612)	0.883 (0.922)	71.88 (70.94)
[156/157]	0.0861 (0.0997)	0.0573 (0.0611)	0.778 (0.919)	75.00 (71.04)
 * Train Acc 71.040
 * Val Acc 71.200, Total time 0.59
 * Val loss 0.841, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0419 (0.0419)	0.0084 (0.0084)	0.742 (0.742)	81.25 (81.25)
[10/157]	0.1140 (0.0954)	0.0740 (0.0576)	1.083 (0.929)	68.75 (72.44)
[20/157]	0.0981 (0.0985)	0.0598 (0.0608)	0.751 (0.931)	81.25 (72.32)
[30/157]	0.0990 (0.0983)	0.0611 (0.0606)	0.834 (0.927)	78.12 (72.18)
[40/157]	0.0987 (0.0982)	0.0599 (0.0606)	0.758 (0.913)	75.00 (71.72)
[50/157]	0.0977 (0.0983)	0.0593 (0.0604)	1.143 (0.935)	62.50 (70.83)
[60/157]	0.0985 (0.0983)	0.0597 (0.0605)	0.911 (0.953)	75.00 (70.54)
[70/157]	0.0988 (0.0984)	0.0602 (0.0603)	1.128 (0.948)	71.88 (70.55)
[80/157]	0.0986 (0.0984)	0.0600 (0.0602)	0.642 (0.937)	81.25 (70.76)
[90/157]	0.0990 (0.0985)	0.0595 (0.0602)	1.003 (0.937)	68.75 (70.74)
[100/157]	0.0995 (0.0985)	0.0600 (0.0602)	1.203 (0.936)	62.50 (71.07)
[110/157]	0.0972 (0.0984)	0.0600 (0.0601)	0.909 (0.924)	75.00 (71.54)
[120/157]	0.0987 (0.0984)	0.0595 (0.0601)	0.773 (0.926)	71.88 (71.33)
[130/157]	0.0995 (0.0985)	0.0598 (0.0601)	0.934 (0.926)	71.88 (71.37)
[140/157]	0.0990 (0.0985)	0.0602 (0.0601)	1.041 (0.927)	68.75 (71.28)
[150/157]	0.0992 (0.0985)	0.0587 (0.0600)	0.721 (0.926)	87.50 (71.52)
[156/157]	0.0807 (0.0984)	0.0536 (0.0600)	0.763 (0.926)	87.50 (71.50)
 * Train Acc 71.500
 * Val Acc 71.600, Total time 0.60
 * Val loss 0.820, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0082 (0.0082)	0.892 (0.892)	71.88 (71.88)
[10/157]	0.0989 (0.0925)	0.0604 (0.0548)	0.693 (0.881)	75.00 (72.16)
[20/157]	0.0986 (0.0951)	0.0596 (0.0573)	0.869 (0.844)	81.25 (73.36)
[30/157]	0.0991 (0.0961)	0.0602 (0.0582)	0.978 (0.868)	68.75 (72.48)
[40/157]	0.1202 (0.0981)	0.0802 (0.0601)	1.024 (0.881)	68.75 (71.88)
[50/157]	0.1018 (0.0992)	0.0622 (0.0611)	0.730 (0.887)	81.25 (71.75)
[60/157]	0.1014 (0.0997)	0.0613 (0.0614)	1.043 (0.909)	68.75 (71.26)
[70/157]	0.1030 (0.1001)	0.0635 (0.0617)	0.959 (0.908)	62.50 (71.52)
[80/157]	0.1029 (0.1003)	0.0632 (0.0618)	0.915 (0.908)	62.50 (71.06)
[90/157]	0.1014 (0.1004)	0.0609 (0.0619)	0.960 (0.903)	68.75 (71.53)
[100/157]	0.0992 (0.1005)	0.0598 (0.0620)	0.964 (0.905)	68.75 (71.44)
[110/157]	0.1024 (0.1006)	0.0626 (0.0621)	0.883 (0.904)	81.25 (71.26)
[120/157]	0.1023 (0.1008)	0.0630 (0.0622)	1.229 (0.913)	59.38 (71.07)
[130/157]	0.1024 (0.1008)	0.0630 (0.0623)	0.802 (0.913)	68.75 (70.97)
[140/157]	0.1032 (0.1009)	0.0638 (0.0623)	0.741 (0.909)	78.12 (71.30)
[150/157]	0.1014 (0.1009)	0.0626 (0.0624)	0.961 (0.918)	75.00 (71.03)
[156/157]	0.0872 (0.1008)	0.0585 (0.0624)	0.762 (0.916)	87.50 (71.10)
 * Train Acc 71.100
 * Val Acc 72.300, Total time 0.61
 * Val loss 0.809, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0088 (0.0088)	1.020 (1.020)	71.88 (71.88)
[10/157]	0.1019 (0.0959)	0.0630 (0.0574)	1.339 (0.978)	59.38 (71.59)
[20/157]	0.1205 (0.0977)	0.0803 (0.0593)	1.186 (0.931)	62.50 (71.58)
[30/157]	0.0946 (0.0996)	0.0570 (0.0614)	0.673 (0.949)	78.12 (70.67)
[40/157]	0.0960 (0.0986)	0.0586 (0.0607)	0.774 (0.926)	78.12 (71.49)
[50/157]	0.0967 (0.0980)	0.0582 (0.0603)	0.773 (0.928)	71.88 (71.14)
[60/157]	0.0972 (0.0980)	0.0591 (0.0601)	0.734 (0.915)	71.88 (71.77)
[70/157]	0.0938 (0.0979)	0.0563 (0.0600)	0.974 (0.922)	71.88 (71.61)
[80/157]	0.1208 (0.0996)	0.0799 (0.0616)	0.989 (0.921)	71.88 (71.80)
[90/157]	0.0975 (0.0998)	0.0586 (0.0617)	1.041 (0.918)	71.88 (72.05)
[100/157]	0.0980 (0.0994)	0.0595 (0.0614)	0.977 (0.920)	65.62 (71.78)
[110/157]	0.1024 (0.0995)	0.0632 (0.0614)	0.719 (0.921)	78.12 (72.07)
[120/157]	0.1029 (0.0998)	0.0636 (0.0616)	0.852 (0.920)	75.00 (72.06)
[130/157]	0.1045 (0.1000)	0.0645 (0.0618)	0.804 (0.915)	68.75 (72.16)
[140/157]	0.1022 (0.1001)	0.0626 (0.0619)	0.993 (0.913)	71.88 (72.03)
[150/157]	0.1033 (0.1003)	0.0635 (0.0620)	1.082 (0.912)	65.62 (72.06)
[156/157]	0.0874 (0.1002)	0.0589 (0.0620)	1.531 (0.913)	62.50 (72.10)
 * Train Acc 72.100
 * Val Acc 71.700, Total time 0.59
 * Val loss 0.802, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0088 (0.0088)	0.732 (0.732)	84.38 (84.38)
[10/157]	0.1050 (0.0983)	0.0653 (0.0590)	0.972 (0.862)	71.88 (73.01)
[20/157]	0.0973 (0.0971)	0.0586 (0.0588)	0.970 (0.895)	78.12 (72.32)
[30/157]	0.0952 (0.0966)	0.0574 (0.0586)	0.864 (0.916)	68.75 (71.27)
[40/157]	0.0968 (0.0980)	0.0587 (0.0598)	0.677 (0.908)	81.25 (71.34)
[50/157]	0.0954 (0.0974)	0.0578 (0.0595)	0.863 (0.912)	75.00 (70.89)
[60/157]	0.0961 (0.0971)	0.0586 (0.0593)	1.194 (0.928)	65.62 (70.59)
[70/157]	0.1116 (0.0995)	0.0712 (0.0615)	1.544 (0.937)	62.50 (70.51)
[80/157]	0.0974 (0.0995)	0.0585 (0.0615)	1.065 (0.932)	65.62 (70.64)
[90/157]	0.0957 (0.0990)	0.0580 (0.0612)	0.753 (0.918)	78.12 (70.91)
[100/157]	0.1004 (0.0997)	0.0620 (0.0617)	0.568 (0.911)	90.62 (71.26)
[110/157]	0.1006 (0.0997)	0.0618 (0.0618)	0.834 (0.914)	71.88 (71.09)
[120/157]	0.1000 (0.0998)	0.0613 (0.0617)	0.940 (0.911)	68.75 (71.28)
[130/157]	0.1001 (0.0997)	0.0613 (0.0617)	0.773 (0.913)	75.00 (71.28)
[140/157]	0.1016 (0.0998)	0.0607 (0.0617)	0.984 (0.909)	68.75 (71.37)
[150/157]	0.1007 (0.0997)	0.0607 (0.0616)	0.902 (0.908)	81.25 (71.52)
[156/157]	0.0849 (0.0996)	0.0566 (0.0615)	0.791 (0.911)	75.00 (71.44)
 * Train Acc 71.440
 * Val Acc 72.400, Total time 0.60
 * Val loss 0.810, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0088 (0.0088)	1.505 (1.505)	56.25 (56.25)
[10/157]	0.1006 (0.0941)	0.0612 (0.0559)	1.105 (0.929)	65.62 (71.59)
[20/157]	0.1005 (0.0968)	0.0618 (0.0584)	1.027 (0.930)	65.62 (71.73)
[30/157]	0.1007 (0.0978)	0.0613 (0.0595)	0.688 (0.950)	81.25 (70.67)
[40/157]	0.1000 (0.0982)	0.0607 (0.0597)	1.170 (0.947)	62.50 (71.04)
[50/157]	0.1004 (0.0985)	0.0611 (0.0600)	0.792 (0.935)	78.12 (71.26)
[60/157]	0.1001 (0.0987)	0.0603 (0.0601)	0.710 (0.933)	81.25 (71.57)
[70/157]	0.0995 (0.0989)	0.0601 (0.0603)	0.644 (0.920)	78.12 (72.01)
[80/157]	0.1007 (0.0990)	0.0605 (0.0604)	0.810 (0.911)	75.00 (72.30)
[90/157]	0.1018 (0.0991)	0.0613 (0.0604)	0.784 (0.903)	71.88 (72.56)
[100/157]	0.1002 (0.0992)	0.0603 (0.0605)	0.930 (0.905)	71.88 (72.49)
[110/157]	0.1007 (0.0993)	0.0607 (0.0605)	0.997 (0.912)	65.62 (72.10)
[120/157]	0.1007 (0.0993)	0.0602 (0.0605)	0.959 (0.915)	68.75 (71.98)
[130/157]	0.1003 (0.0994)	0.0605 (0.0606)	0.891 (0.922)	81.25 (71.83)
[140/157]	0.0990 (0.0994)	0.0596 (0.0605)	0.838 (0.921)	78.12 (71.94)
[150/157]	0.1010 (0.0995)	0.0614 (0.0606)	0.576 (0.920)	81.25 (71.90)
[156/157]	0.0814 (0.0993)	0.0552 (0.0605)	1.153 (0.918)	50.00 (71.94)
 * Train Acc 71.940
 * Val Acc 72.200, Total time 0.62
 * Val loss 0.805, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0085 (0.0085)	1.195 (1.195)	62.50 (62.50)
[10/157]	0.1001 (0.0938)	0.0606 (0.0557)	0.554 (0.861)	81.25 (75.85)
[20/157]	0.1004 (0.0967)	0.0608 (0.0581)	0.912 (0.880)	71.88 (74.70)
[30/157]	0.1004 (0.0977)	0.0611 (0.0590)	0.745 (0.913)	75.00 (73.69)
[40/157]	0.1018 (0.0982)	0.0617 (0.0595)	0.752 (0.905)	81.25 (73.63)
[50/157]	0.0993 (0.0985)	0.0600 (0.0597)	0.933 (0.919)	71.88 (72.98)
[60/157]	0.1002 (0.0987)	0.0614 (0.0600)	0.813 (0.920)	78.12 (72.59)
[70/157]	0.1005 (0.0989)	0.0610 (0.0601)	0.965 (0.922)	78.12 (72.27)
[80/157]	0.0989 (0.0990)	0.0609 (0.0603)	0.768 (0.915)	75.00 (72.53)
[90/157]	0.0991 (0.0991)	0.0609 (0.0604)	1.155 (0.917)	65.62 (72.36)
[100/157]	0.0998 (0.0991)	0.0612 (0.0606)	0.880 (0.920)	71.88 (72.25)
[110/157]	0.1000 (0.0992)	0.0605 (0.0606)	0.895 (0.915)	71.88 (72.30)
[120/157]	0.0997 (0.0992)	0.0616 (0.0606)	1.072 (0.920)	68.75 (71.95)
[130/157]	0.1009 (0.0992)	0.0622 (0.0607)	0.869 (0.916)	71.88 (71.85)
[140/157]	0.0986 (0.0992)	0.0607 (0.0608)	0.647 (0.916)	81.25 (71.85)
[150/157]	0.1006 (0.0992)	0.0607 (0.0608)	0.780 (0.918)	78.12 (71.85)
[156/157]	0.0868 (0.0992)	0.0562 (0.0607)	0.450 (0.917)	100.00 (71.88)
 * Train Acc 71.880
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.808, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0085 (0.0085)	0.849 (0.849)	71.88 (71.88)
[10/157]	0.0979 (0.0941)	0.0589 (0.0554)	0.668 (0.946)	78.12 (68.18)
[20/157]	0.1001 (0.0966)	0.0600 (0.0579)	0.903 (0.919)	71.88 (70.68)
[30/157]	0.1003 (0.0977)	0.0605 (0.0588)	1.048 (0.915)	65.62 (71.47)
[40/157]	0.1009 (0.0982)	0.0607 (0.0592)	0.821 (0.902)	68.75 (71.95)
[50/157]	0.1011 (0.0986)	0.0598 (0.0595)	1.159 (0.919)	59.38 (71.38)
[60/157]	0.0993 (0.0988)	0.0596 (0.0596)	0.805 (0.909)	78.12 (71.93)
[70/157]	0.0993 (0.0990)	0.0595 (0.0597)	0.830 (0.907)	81.25 (72.45)
[80/157]	0.1008 (0.0991)	0.0608 (0.0598)	0.795 (0.910)	71.88 (72.11)
[90/157]	0.1015 (0.0992)	0.0617 (0.0599)	0.872 (0.906)	65.62 (72.08)
[100/157]	0.1002 (0.0993)	0.0607 (0.0600)	0.963 (0.909)	75.00 (71.72)
[110/157]	0.0999 (0.0993)	0.0601 (0.0600)	0.695 (0.906)	81.25 (71.73)
[120/157]	0.1000 (0.0994)	0.0602 (0.0600)	0.862 (0.909)	75.00 (71.88)
[130/157]	0.0996 (0.0994)	0.0601 (0.0601)	0.896 (0.909)	71.88 (71.80)
[140/157]	0.0999 (0.0995)	0.0601 (0.0602)	0.786 (0.908)	71.88 (71.81)
[150/157]	0.1000 (0.0995)	0.0609 (0.0602)	0.805 (0.906)	81.25 (72.02)
[156/157]	0.0847 (0.0994)	0.0557 (0.0602)	1.644 (0.907)	37.50 (72.12)
 * Train Acc 72.120
 * Val Acc 72.500, Total time 0.61
 * Val loss 0.805, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0087 (0.0087)	1.340 (1.340)	65.62 (65.62)
[10/157]	0.0997 (0.0940)	0.0609 (0.0558)	1.019 (1.012)	65.62 (67.90)
[20/157]	0.1014 (0.0969)	0.0619 (0.0587)	0.710 (0.954)	75.00 (70.24)
[30/157]	0.0991 (0.0979)	0.0604 (0.0596)	0.934 (0.951)	65.62 (69.96)
[40/157]	0.1011 (0.0984)	0.0617 (0.0600)	1.045 (0.944)	68.75 (70.66)
[50/157]	0.0985 (0.0988)	0.0596 (0.0603)	0.850 (0.942)	68.75 (70.65)
[60/157]	0.1020 (0.0991)	0.0608 (0.0603)	0.958 (0.925)	75.00 (71.31)
[70/157]	0.1005 (0.0992)	0.0606 (0.0604)	0.819 (0.925)	71.88 (71.13)
[80/157]	0.0985 (0.0993)	0.0593 (0.0603)	1.138 (0.935)	65.62 (70.95)
[90/157]	0.1004 (0.0993)	0.0612 (0.0603)	1.039 (0.931)	65.62 (70.78)
[100/157]	0.1000 (0.0993)	0.0616 (0.0604)	0.950 (0.921)	71.88 (71.19)
[110/157]	0.1003 (0.0994)	0.0616 (0.0605)	0.974 (0.925)	71.88 (71.28)
[120/157]	0.0991 (0.0994)	0.0605 (0.0606)	0.915 (0.926)	71.88 (71.31)
[130/157]	0.1006 (0.0994)	0.0601 (0.0606)	1.035 (0.927)	65.62 (71.21)
[140/157]	0.1008 (0.0994)	0.0611 (0.0605)	0.816 (0.921)	71.88 (71.43)
[150/157]	0.1012 (0.0995)	0.0614 (0.0605)	1.251 (0.920)	50.00 (71.44)
[156/157]	0.0826 (0.0993)	0.0554 (0.0605)	1.434 (0.921)	50.00 (71.40)
 * Train Acc 71.400
 * Val Acc 71.800, Total time 0.61
 * Val loss 0.810, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0091 (0.0091)	1.025 (1.025)	65.62 (65.62)
[10/157]	0.1011 (0.0947)	0.0603 (0.0556)	0.726 (0.917)	78.12 (69.89)
[20/157]	0.1001 (0.0970)	0.0611 (0.0582)	1.010 (0.900)	65.62 (71.88)
[30/157]	0.1002 (0.0977)	0.0625 (0.0592)	1.101 (0.888)	62.50 (72.08)
[40/157]	0.1005 (0.0982)	0.0616 (0.0598)	0.876 (0.896)	65.62 (71.72)
[50/157]	0.0992 (0.0985)	0.0597 (0.0599)	0.953 (0.910)	65.62 (71.02)
[60/157]	0.0956 (0.0986)	0.0566 (0.0599)	0.847 (0.918)	68.75 (70.65)
[70/157]	0.1005 (0.0988)	0.0615 (0.0601)	0.918 (0.909)	68.75 (71.26)
[80/157]	0.1006 (0.0989)	0.0609 (0.0602)	0.731 (0.906)	78.12 (71.37)
[90/157]	0.0999 (0.0990)	0.0601 (0.0604)	0.895 (0.903)	68.75 (71.57)
[100/157]	0.0997 (0.0991)	0.0605 (0.0604)	0.687 (0.908)	78.12 (71.29)
[110/157]	0.0996 (0.0991)	0.0605 (0.0605)	0.984 (0.906)	62.50 (71.17)
[120/157]	0.1008 (0.0992)	0.0599 (0.0605)	0.937 (0.906)	68.75 (71.23)
[130/157]	0.1008 (0.0992)	0.0613 (0.0605)	0.968 (0.916)	65.62 (70.99)
[140/157]	0.1010 (0.0992)	0.0602 (0.0605)	0.844 (0.912)	78.12 (71.32)
[150/157]	0.1000 (0.0992)	0.0611 (0.0606)	0.913 (0.913)	68.75 (71.25)
[156/157]	0.0847 (0.0991)	0.0555 (0.0605)	1.020 (0.915)	75.00 (71.26)
 * Train Acc 71.260
 * Val Acc 71.700, Total time 0.59
 * Val loss 0.803, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0438 (0.0438)	0.0090 (0.0090)	0.672 (0.672)	84.38 (84.38)
[10/157]	0.0996 (0.0940)	0.0608 (0.0560)	1.079 (0.920)	65.62 (72.16)
[20/157]	0.0997 (0.0965)	0.0602 (0.0584)	0.524 (0.935)	87.50 (71.13)
[30/157]	0.0989 (0.0975)	0.0601 (0.0590)	0.688 (0.908)	78.12 (71.47)
[40/157]	0.1004 (0.0980)	0.0610 (0.0595)	1.016 (0.899)	81.25 (72.33)
[50/157]	0.1001 (0.0983)	0.0606 (0.0599)	0.616 (0.881)	78.12 (73.10)
[60/157]	0.0995 (0.0986)	0.0606 (0.0600)	1.178 (0.887)	59.38 (72.49)
[70/157]	0.0996 (0.0987)	0.0598 (0.0601)	0.624 (0.891)	87.50 (72.62)
[80/157]	0.1004 (0.0989)	0.0603 (0.0602)	1.142 (0.895)	71.88 (72.53)
[90/157]	0.1002 (0.0990)	0.0599 (0.0602)	0.912 (0.899)	78.12 (72.53)
[100/157]	0.0995 (0.0990)	0.0598 (0.0602)	0.676 (0.909)	81.25 (72.03)
[110/157]	0.1007 (0.0991)	0.0611 (0.0603)	0.813 (0.911)	71.88 (71.96)
[120/157]	0.0990 (0.0991)	0.0603 (0.0603)	1.140 (0.909)	65.62 (72.13)
[130/157]	0.0979 (0.0991)	0.0589 (0.0604)	1.036 (0.906)	62.50 (72.07)
[140/157]	0.1014 (0.0992)	0.0611 (0.0604)	0.738 (0.910)	78.12 (72.19)
[150/157]	0.1012 (0.0992)	0.0606 (0.0604)	1.013 (0.917)	75.00 (71.98)
[156/157]	0.0871 (0.0992)	0.0556 (0.0604)	1.081 (0.920)	75.00 (71.96)
 * Train Acc 71.960
 * Val Acc 72.100, Total time 0.60
 * Val loss 0.818, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0085 (0.0085)	0.962 (0.962)	68.75 (68.75)
[10/157]	0.1003 (0.0944)	0.0618 (0.0556)	0.953 (0.907)	68.75 (71.59)
[20/157]	0.1024 (0.0971)	0.0601 (0.0584)	0.851 (0.916)	71.88 (71.43)
[30/157]	0.0998 (0.0978)	0.0603 (0.0592)	0.746 (0.899)	84.38 (71.67)
[40/157]	0.1010 (0.0983)	0.0584 (0.0595)	1.137 (0.934)	62.50 (69.36)
[50/157]	0.1021 (0.0987)	0.0600 (0.0596)	0.944 (0.899)	65.62 (70.89)
[60/157]	0.1042 (0.0989)	0.0611 (0.0598)	1.028 (0.908)	65.62 (70.49)
[70/157]	0.0990 (0.0990)	0.0596 (0.0598)	0.974 (0.910)	65.62 (70.60)
[80/157]	0.1006 (0.0990)	0.0612 (0.0599)	0.829 (0.912)	81.25 (70.87)
[90/157]	0.0996 (0.0991)	0.0610 (0.0601)	1.024 (0.904)	68.75 (71.33)
[100/157]	0.0995 (0.0991)	0.0606 (0.0602)	1.166 (0.905)	62.50 (71.50)
[110/157]	0.0995 (0.0992)	0.0600 (0.0602)	0.694 (0.902)	78.12 (71.62)
[120/157]	0.1010 (0.0992)	0.0612 (0.0603)	1.160 (0.898)	53.12 (71.95)
[130/157]	0.1002 (0.0993)	0.0606 (0.0603)	0.765 (0.900)	71.88 (71.76)
[140/157]	0.1006 (0.0993)	0.0612 (0.0604)	0.641 (0.906)	81.25 (71.32)
[150/157]	0.1001 (0.0994)	0.0603 (0.0604)	1.040 (0.907)	59.38 (71.30)
[156/157]	0.0816 (0.0993)	0.0554 (0.0604)	1.378 (0.912)	62.50 (71.18)
 * Train Acc 71.180
 * Val Acc 72.400, Total time 0.59
 * Val loss 0.810, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0086 (0.0086)	0.729 (0.729)	78.12 (78.12)
[10/157]	0.0994 (0.0941)	0.0594 (0.0549)	0.763 (0.902)	75.00 (71.59)
[20/157]	0.0998 (0.0968)	0.0604 (0.0576)	0.779 (0.885)	81.25 (72.77)
[30/157]	0.0998 (0.0977)	0.0612 (0.0587)	0.579 (0.855)	87.50 (73.49)
[40/157]	0.0994 (0.0983)	0.0603 (0.0591)	0.630 (0.853)	78.12 (73.63)
[50/157]	0.0987 (0.0984)	0.0606 (0.0595)	0.947 (0.871)	71.88 (73.10)
[60/157]	0.0998 (0.0986)	0.0604 (0.0598)	1.071 (0.884)	68.75 (72.85)
[70/157]	0.0993 (0.0988)	0.0598 (0.0598)	0.821 (0.896)	78.12 (72.32)
[80/157]	0.1005 (0.0989)	0.0600 (0.0599)	0.821 (0.892)	81.25 (72.42)
[90/157]	0.0993 (0.0990)	0.0595 (0.0599)	0.907 (0.882)	71.88 (72.84)
[100/157]	0.1003 (0.0991)	0.0607 (0.0600)	0.987 (0.896)	68.75 (72.25)
[110/157]	0.0995 (0.0992)	0.0599 (0.0600)	0.711 (0.895)	78.12 (72.33)
[120/157]	0.1010 (0.0992)	0.0608 (0.0600)	0.869 (0.895)	71.88 (72.26)
[130/157]	0.1003 (0.0993)	0.0601 (0.0601)	1.005 (0.899)	68.75 (72.14)
[140/157]	0.1012 (0.0994)	0.0606 (0.0601)	0.805 (0.902)	75.00 (72.01)
[150/157]	0.1008 (0.0994)	0.0605 (0.0601)	0.993 (0.899)	75.00 (72.19)
[156/157]	0.0894 (0.0993)	0.0563 (0.0601)	1.156 (0.903)	62.50 (71.94)
 * Train Acc 71.940
 * Val Acc 71.900, Total time 0.61
 * Val loss 0.808, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0454 (0.0454)	0.0090 (0.0090)	1.320 (1.320)	62.50 (62.50)
[10/157]	0.1011 (0.0953)	0.0614 (0.0563)	0.835 (1.009)	75.00 (69.60)
[20/157]	0.1012 (0.0983)	0.0618 (0.0592)	1.083 (0.975)	68.75 (70.39)
[30/157]	0.1041 (0.0993)	0.0636 (0.0603)	0.953 (0.934)	65.62 (70.87)
[40/157]	0.1014 (0.0998)	0.0626 (0.0608)	1.166 (0.946)	68.75 (70.58)
[50/157]	0.1005 (0.1000)	0.0614 (0.0611)	0.981 (0.932)	68.75 (71.14)
[60/157]	0.1034 (0.1003)	0.0633 (0.0613)	0.711 (0.911)	81.25 (71.88)
[70/157]	0.1007 (0.1003)	0.0620 (0.0614)	0.799 (0.909)	75.00 (72.10)
[80/157]	0.1025 (0.1004)	0.0620 (0.0616)	0.834 (0.908)	78.12 (72.15)
[90/157]	0.1022 (0.1005)	0.0612 (0.0616)	1.027 (0.907)	65.62 (72.01)
[100/157]	0.1004 (0.1006)	0.0614 (0.0616)	1.014 (0.908)	68.75 (72.09)
[110/157]	0.0999 (0.1007)	0.0590 (0.0616)	1.011 (0.910)	71.88 (72.07)
[120/157]	0.1028 (0.1008)	0.0628 (0.0617)	0.941 (0.912)	71.88 (71.82)
[130/157]	0.1003 (0.1008)	0.0604 (0.0617)	1.051 (0.908)	65.62 (71.85)
[140/157]	0.1017 (0.1008)	0.0622 (0.0618)	0.856 (0.913)	78.12 (71.72)
[150/157]	0.0946 (0.1007)	0.0570 (0.0617)	0.685 (0.917)	71.88 (71.34)
[156/157]	0.0988 (0.1005)	0.0724 (0.0616)	0.519 (0.910)	87.50 (71.60)
 * Train Acc 71.600
 * Val Acc 72.000, Total time 0.63
 * Val loss 0.812, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0096 (0.0096)	0.878 (0.878)	81.25 (81.25)
[10/157]	0.0976 (0.0920)	0.0583 (0.0541)	0.986 (0.907)	65.62 (71.02)
[20/157]	0.0971 (0.0944)	0.0591 (0.0565)	1.043 (0.876)	68.75 (73.36)
[30/157]	0.0980 (0.0954)	0.0587 (0.0574)	0.935 (0.902)	59.38 (72.08)
[40/157]	0.0966 (0.0958)	0.0583 (0.0578)	0.985 (0.895)	65.62 (71.49)
[50/157]	0.0982 (0.0961)	0.0595 (0.0582)	1.037 (0.907)	71.88 (71.26)
[60/157]	0.0980 (0.0963)	0.0591 (0.0584)	0.786 (0.910)	71.88 (71.06)
[70/157]	0.0978 (0.0964)	0.0599 (0.0584)	0.801 (0.909)	78.12 (71.21)
[80/157]	0.0973 (0.0965)	0.0595 (0.0585)	0.962 (0.901)	71.88 (71.45)
[90/157]	0.0987 (0.0966)	0.0595 (0.0587)	0.964 (0.905)	71.88 (71.33)
[100/157]	0.0985 (0.0966)	0.0593 (0.0587)	0.975 (0.906)	62.50 (71.19)
[110/157]	0.0973 (0.0967)	0.0582 (0.0587)	0.737 (0.899)	71.88 (71.34)
[120/157]	0.0988 (0.0968)	0.0596 (0.0587)	0.689 (0.892)	78.12 (71.80)
[130/157]	0.0975 (0.0968)	0.0587 (0.0587)	0.816 (0.890)	81.25 (71.92)
[140/157]	0.1209 (0.0970)	0.0802 (0.0589)	0.839 (0.887)	68.75 (72.05)
[150/157]	0.1079 (0.0980)	0.0678 (0.0598)	1.366 (0.893)	62.50 (71.85)
[156/157]	0.0905 (0.0982)	0.0631 (0.0600)	0.661 (0.894)	87.50 (71.92)
 * Train Acc 71.920
 * Val Acc 72.200, Total time 0.59
 * Val loss 0.805, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0087 (0.0087)	0.803 (0.803)	78.12 (78.12)
[10/157]	0.0946 (0.0920)	0.0573 (0.0541)	1.120 (0.897)	56.25 (72.44)
[20/157]	0.1020 (0.0954)	0.0625 (0.0572)	0.903 (0.878)	75.00 (72.47)
[30/157]	0.1014 (0.0975)	0.0622 (0.0590)	0.770 (0.876)	84.38 (72.88)
[40/157]	0.1022 (0.0986)	0.0625 (0.0599)	0.605 (0.869)	87.50 (73.40)
[50/157]	0.1018 (0.0992)	0.0625 (0.0605)	0.819 (0.864)	84.38 (73.90)
[60/157]	0.1015 (0.0994)	0.0621 (0.0608)	0.848 (0.871)	81.25 (74.13)
[70/157]	0.1013 (0.0998)	0.0624 (0.0611)	0.772 (0.885)	71.88 (73.06)
[80/157]	0.1012 (0.1000)	0.0611 (0.0613)	1.069 (0.899)	68.75 (72.53)
[90/157]	0.1017 (0.1003)	0.0621 (0.0614)	0.937 (0.888)	65.62 (72.87)
[100/157]	0.0960 (0.0999)	0.0573 (0.0612)	1.534 (0.894)	46.88 (72.62)
[110/157]	0.0952 (0.0995)	0.0572 (0.0609)	1.070 (0.902)	68.75 (72.13)
[120/157]	0.0987 (0.0994)	0.0602 (0.0608)	0.809 (0.907)	78.12 (72.00)
[130/157]	0.0986 (0.0992)	0.0594 (0.0607)	0.762 (0.906)	84.38 (72.14)
[140/157]	0.0977 (0.0991)	0.0594 (0.0606)	0.944 (0.908)	71.88 (72.12)
[150/157]	0.0988 (0.0990)	0.0600 (0.0605)	0.925 (0.909)	75.00 (72.08)
[156/157]	0.0793 (0.0988)	0.0528 (0.0604)	1.353 (0.909)	50.00 (72.14)
 * Train Acc 72.140
 * Val Acc 72.100, Total time 0.60
 * Val loss 0.804, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0087 (0.0087)	1.180 (1.180)	56.25 (56.25)
[10/157]	0.0972 (0.0925)	0.0579 (0.0542)	1.033 (1.016)	71.88 (65.34)
[20/157]	0.0978 (0.0949)	0.0593 (0.0568)	0.887 (0.943)	75.00 (69.20)
[30/157]	0.0989 (0.0957)	0.0595 (0.0575)	0.786 (0.913)	68.75 (70.46)
[40/157]	0.0984 (0.0961)	0.0586 (0.0579)	1.154 (0.912)	65.62 (70.35)
[50/157]	0.0986 (0.0965)	0.0597 (0.0580)	0.926 (0.905)	71.88 (70.65)
[60/157]	0.0974 (0.0966)	0.0585 (0.0583)	0.997 (0.906)	68.75 (71.06)
[70/157]	0.0976 (0.0968)	0.0586 (0.0584)	0.974 (0.890)	68.75 (71.74)
[80/157]	0.0968 (0.0969)	0.0584 (0.0585)	0.953 (0.897)	71.88 (71.45)
[90/157]	0.0985 (0.0970)	0.0597 (0.0586)	1.383 (0.915)	56.25 (70.78)
[100/157]	0.0974 (0.0971)	0.0589 (0.0587)	0.942 (0.911)	71.88 (71.26)
[110/157]	0.1013 (0.0971)	0.0609 (0.0588)	0.876 (0.913)	65.62 (71.20)
[120/157]	0.0995 (0.0972)	0.0597 (0.0588)	1.031 (0.910)	65.62 (71.44)
[130/157]	0.0990 (0.0972)	0.0596 (0.0588)	1.074 (0.907)	75.00 (71.64)
[140/157]	0.0984 (0.0973)	0.0590 (0.0588)	1.012 (0.903)	65.62 (71.79)
[150/157]	0.1007 (0.0974)	0.0606 (0.0589)	1.163 (0.907)	62.50 (71.69)
[156/157]	0.0796 (0.0974)	0.0539 (0.0589)	2.216 (0.908)	12.50 (71.66)
 * Train Acc 71.660
 * Val Acc 71.700, Total time 0.59
 * Val loss 0.798, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0084 (0.0084)	0.508 (0.508)	84.38 (84.38)
[10/157]	0.1003 (0.0941)	0.0616 (0.0551)	0.850 (0.783)	81.25 (76.14)
[20/157]	0.0989 (0.0964)	0.0604 (0.0579)	1.037 (0.846)	68.75 (74.70)
[30/157]	0.0994 (0.0972)	0.0611 (0.0588)	0.878 (0.840)	75.00 (74.19)
[40/157]	0.0996 (0.0976)	0.0603 (0.0593)	1.177 (0.874)	59.38 (72.79)
[50/157]	0.0988 (0.0980)	0.0593 (0.0596)	0.923 (0.881)	65.62 (72.73)
[60/157]	0.0993 (0.0983)	0.0607 (0.0598)	1.128 (0.903)	62.50 (72.28)
[70/157]	0.1003 (0.0984)	0.0607 (0.0600)	0.857 (0.902)	71.88 (71.92)
[80/157]	0.0995 (0.0985)	0.0589 (0.0600)	0.888 (0.903)	78.12 (71.88)
[90/157]	0.0994 (0.0986)	0.0605 (0.0600)	1.069 (0.903)	68.75 (72.05)
[100/157]	0.0996 (0.0987)	0.0612 (0.0601)	0.860 (0.907)	71.88 (71.84)
[110/157]	0.1001 (0.0988)	0.0613 (0.0602)	0.990 (0.909)	78.12 (71.93)
[120/157]	0.1008 (0.0988)	0.0604 (0.0603)	0.935 (0.909)	71.88 (72.18)
[130/157]	0.0999 (0.0989)	0.0600 (0.0603)	0.984 (0.912)	75.00 (71.97)
[140/157]	0.1007 (0.0990)	0.0607 (0.0603)	0.968 (0.910)	65.62 (71.94)
[150/157]	0.0996 (0.0990)	0.0612 (0.0603)	1.074 (0.913)	68.75 (71.77)
[156/157]	0.0833 (0.0989)	0.0554 (0.0603)	0.997 (0.915)	62.50 (71.74)
 * Train Acc 71.740
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.800, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0088 (0.0088)	1.063 (1.063)	68.75 (68.75)
[10/157]	0.1008 (0.0941)	0.0609 (0.0552)	0.968 (0.914)	65.62 (71.88)
[20/157]	0.1001 (0.0965)	0.0613 (0.0579)	0.969 (0.881)	75.00 (73.07)
[30/157]	0.0995 (0.0975)	0.0611 (0.0591)	1.049 (0.889)	65.62 (72.58)
[40/157]	0.1001 (0.0979)	0.0605 (0.0595)	0.735 (0.893)	81.25 (72.41)
[50/157]	0.1003 (0.0982)	0.0614 (0.0598)	0.979 (0.885)	68.75 (72.24)
[60/157]	0.0992 (0.0984)	0.0602 (0.0600)	1.154 (0.892)	59.38 (71.98)
[70/157]	0.1008 (0.0985)	0.0605 (0.0601)	0.797 (0.892)	75.00 (71.79)
[80/157]	0.1006 (0.0986)	0.0615 (0.0601)	0.929 (0.904)	71.88 (71.30)
[90/157]	0.1016 (0.0987)	0.0618 (0.0602)	0.707 (0.907)	71.88 (71.05)
[100/157]	0.1006 (0.0988)	0.0613 (0.0602)	0.717 (0.901)	78.12 (71.47)
[110/157]	0.0998 (0.0989)	0.0598 (0.0602)	0.784 (0.897)	75.00 (71.79)
[120/157]	0.0998 (0.0989)	0.0612 (0.0603)	0.939 (0.897)	75.00 (72.03)
[130/157]	0.0995 (0.0989)	0.0601 (0.0603)	0.905 (0.897)	68.75 (72.11)
[140/157]	0.0993 (0.0989)	0.0596 (0.0603)	0.799 (0.899)	71.88 (72.05)
[150/157]	0.1005 (0.0990)	0.0616 (0.0603)	1.098 (0.899)	59.38 (71.92)
[156/157]	0.0843 (0.0989)	0.0564 (0.0603)	0.906 (0.903)	75.00 (71.78)
 * Train Acc 71.780
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.807, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0448 (0.0448)	0.0084 (0.0084)	0.943 (0.943)	65.62 (65.62)
[10/157]	0.0986 (0.0939)	0.0595 (0.0548)	0.733 (0.922)	81.25 (69.89)
[20/157]	0.1001 (0.0963)	0.0612 (0.0576)	0.922 (0.933)	68.75 (69.35)
[30/157]	0.0991 (0.0973)	0.0597 (0.0586)	0.664 (0.905)	84.38 (70.87)
[40/157]	0.1005 (0.0976)	0.0615 (0.0592)	0.735 (0.886)	71.88 (71.19)
[50/157]	0.1009 (0.0979)	0.0602 (0.0595)	0.702 (0.894)	84.38 (71.45)
[60/157]	0.0994 (0.0981)	0.0603 (0.0596)	1.009 (0.912)	71.88 (70.59)
[70/157]	0.0973 (0.0982)	0.0589 (0.0597)	0.839 (0.909)	68.75 (70.86)
[80/157]	0.1020 (0.0984)	0.0603 (0.0598)	0.872 (0.903)	81.25 (71.22)
[90/157]	0.1002 (0.0984)	0.0615 (0.0598)	1.079 (0.909)	68.75 (71.05)
[100/157]	0.0987 (0.0985)	0.0593 (0.0599)	1.160 (0.910)	62.50 (71.16)
[110/157]	0.0997 (0.0986)	0.0605 (0.0599)	1.120 (0.915)	65.62 (71.14)
[120/157]	0.0986 (0.0986)	0.0596 (0.0599)	0.851 (0.915)	71.88 (71.02)
[130/157]	0.1001 (0.0987)	0.0619 (0.0599)	0.811 (0.911)	75.00 (71.18)
[140/157]	0.1001 (0.0987)	0.0606 (0.0600)	0.683 (0.905)	78.12 (71.45)
[150/157]	0.0995 (0.0987)	0.0606 (0.0601)	0.566 (0.905)	87.50 (71.48)
[156/157]	0.0878 (0.0986)	0.0566 (0.0601)	1.882 (0.907)	50.00 (71.40)
 * Train Acc 71.400
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.806, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0088 (0.0088)	0.913 (0.913)	71.88 (71.88)
[10/157]	0.0984 (0.0934)	0.0584 (0.0554)	0.843 (0.956)	71.88 (69.32)
[20/157]	0.1002 (0.0961)	0.0616 (0.0577)	0.781 (0.929)	75.00 (68.45)
[30/157]	0.0990 (0.0972)	0.0599 (0.0588)	0.845 (0.924)	59.38 (68.85)
[40/157]	0.0998 (0.0977)	0.0599 (0.0593)	1.024 (0.917)	81.25 (70.12)
[50/157]	0.0998 (0.0980)	0.0602 (0.0596)	0.779 (0.922)	75.00 (70.59)
[60/157]	0.0998 (0.0982)	0.0609 (0.0599)	0.634 (0.925)	81.25 (70.75)
[70/157]	0.0989 (0.0984)	0.0592 (0.0600)	0.779 (0.932)	71.88 (70.99)
[80/157]	0.0997 (0.0985)	0.0605 (0.0600)	0.922 (0.928)	78.12 (71.41)
[90/157]	0.0990 (0.0986)	0.0598 (0.0601)	1.142 (0.929)	71.88 (71.57)
[100/157]	0.0999 (0.0987)	0.0594 (0.0601)	1.077 (0.920)	68.75 (71.72)
[110/157]	0.0997 (0.0987)	0.0608 (0.0601)	0.799 (0.909)	71.88 (72.16)
[120/157]	0.0998 (0.0987)	0.0602 (0.0601)	0.974 (0.917)	68.75 (71.98)
[130/157]	0.0999 (0.0988)	0.0603 (0.0601)	1.107 (0.917)	62.50 (71.85)
[140/157]	0.1014 (0.0988)	0.0625 (0.0602)	0.823 (0.914)	75.00 (72.01)
[150/157]	0.0991 (0.0988)	0.0595 (0.0602)	1.048 (0.913)	59.38 (71.90)
[156/157]	0.0819 (0.0987)	0.0548 (0.0601)	1.253 (0.911)	75.00 (71.94)
 * Train Acc 71.940
 * Val Acc 72.400, Total time 0.60
 * Val loss 0.807, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0086 (0.0086)	1.131 (1.131)	59.38 (59.38)
[10/157]	0.0979 (0.0936)	0.0587 (0.0548)	1.125 (0.968)	65.62 (69.89)
[20/157]	0.0996 (0.0961)	0.0603 (0.0575)	0.816 (0.926)	75.00 (71.73)
[30/157]	0.1003 (0.0969)	0.0612 (0.0584)	0.685 (0.911)	81.25 (72.38)
[40/157]	0.1001 (0.0975)	0.0603 (0.0589)	0.886 (0.920)	71.88 (72.10)
[50/157]	0.1006 (0.0979)	0.0606 (0.0592)	0.831 (0.919)	68.75 (72.37)
[60/157]	0.1006 (0.0981)	0.0610 (0.0594)	0.941 (0.922)	65.62 (71.77)
[70/157]	0.0990 (0.0982)	0.0602 (0.0595)	1.129 (0.921)	68.75 (72.14)
[80/157]	0.0979 (0.0983)	0.0590 (0.0596)	0.750 (0.913)	75.00 (72.18)
[90/157]	0.0997 (0.0984)	0.0607 (0.0597)	0.837 (0.904)	84.38 (72.63)
[100/157]	0.0988 (0.0985)	0.0595 (0.0596)	0.653 (0.900)	78.12 (72.52)
[110/157]	0.0994 (0.0985)	0.0604 (0.0597)	1.039 (0.901)	71.88 (72.58)
[120/157]	0.1001 (0.0986)	0.0602 (0.0598)	0.862 (0.900)	84.38 (72.78)
[130/157]	0.0994 (0.0986)	0.0604 (0.0598)	0.804 (0.901)	68.75 (72.66)
[140/157]	0.0993 (0.0987)	0.0591 (0.0599)	0.649 (0.901)	84.38 (72.85)
[150/157]	0.1019 (0.0987)	0.0623 (0.0599)	0.948 (0.902)	71.88 (72.70)
[156/157]	0.0824 (0.0986)	0.0555 (0.0599)	1.100 (0.905)	62.50 (72.64)
 * Train Acc 72.640
 * Val Acc 73.000, Total time 0.60
 * Val loss 0.788, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 6/27; cond: 447476.46875, radio:5.0608679885044694e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0040,  0.1072, -0.1578],
          [-0.1378, -0.0745,  0.0497],
          [ 0.0050,  0.1572, -0.0147]],

         [[ 0.0549, -0.0551, -0.0377],
          [-0.1813, -0.1279, -0.0787],
          [ 0.0138,  0.0788,  0.1179]],

         [[-0.1299, -0.0832,  0.0676],
          [ 0.1594, -0.0410,  0.1448],
          [-0.0280,  0.0217,  0.1756]]],


        [[[-0.1680, -0.1048, -0.0262],
          [-0.0665,  0.1732, -0.1105],
          [-0.0792, -0.1349, -0.1716]],

         [[-0.1061,  0.1790,  0.1032],
          [ 0.0968,  0.0176, -0.0889],
          [ 0.0345, -0.1830, -0.1368]],

         [[-0.0980,  0.1278,  0.1214],
          [-0.0828, -0.0011,  0.1280],
          [ 0.1913,  0.0731,  0.0269]]],


        [[[ 0.1331, -0.1134,  0.0363],
          [-0.1476, -0.1350, -0.0988],
          [ 0.0870,  0.0753, -0.1128]],

         [[ 0.0592,  0.1009, -0.0278],
          [ 0.0060,  0.0401,  0.1155],
          [ 0.1801, -0.1532, -0.0733]],

         [[ 0.0775,  0.1566,  0.1639],
          [ 0.1683,  0.0345, -0.1717],
          [ 0.0119, -0.1273, -0.1839]]],


        ...,


        [[[ 0.0683, -0.0763, -0.1208],
          [ 0.0759,  0.0337, -0.1409],
          [-0.1320,  0.0943,  0.1445]],

         [[ 0.0184, -0.0978,  0.1005],
          [ 0.0361, -0.1679, -0.1244],
          [ 0.0341,  0.1232, -0.0630]],

         [[ 0.0805, -0.1304,  0.1821],
          [ 0.1278,  0.0265,  0.0758],
          [ 0.0452, -0.1181, -0.0129]]],


        [[[ 0.0625, -0.0404,  0.1923],
          [-0.0630, -0.1658, -0.0865],
          [-0.1705,  0.1685, -0.0841]],

         [[-0.1290, -0.0213, -0.1254],
          [-0.0925,  0.1034, -0.1427],
          [ 0.0977,  0.1512,  0.0007]],

         [[ 0.0849, -0.1125,  0.1497],
          [ 0.1868,  0.0454,  0.1722],
          [-0.1953, -0.0194,  0.0318]]],


        [[[-0.1668,  0.0944, -0.0725],
          [-0.0709, -0.1546,  0.0822],
          [-0.1406, -0.0456,  0.0851]],

         [[ 0.1450,  0.0468, -0.0711],
          [-0.0833,  0.1101,  0.0388],
          [ 0.1594,  0.0672,  0.1518]],

         [[-0.1558,  0.1528, -0.0939],
          [-0.0556, -0.0039, -0.0431],
          [ 0.0337,  0.1604, -0.1260]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([4.6039e+06, 4.0257e+05, 3.9744e+05, 1.7227e+05, 1.4823e+05, 1.2568e+05,
        2.3449e+04, 1.5004e+04, 4.9077e+03, 4.6971e+03, 4.5909e+03, 4.3810e+03,
        1.6561e+03, 1.0050e+03, 1.0005e+03, 8.3278e+02, 7.7078e+02, 4.6336e+02,
        2.0495e+02, 1.8501e+02, 1.5548e+02, 1.0082e+02, 9.1290e+01, 3.7410e+01,
        3.3909e+01, 2.5563e+01, 1.0289e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 6]) 

NULL SPACE BASIS :  tensor([[ 2.1339e-01, -2.0429e-01, -1.2818e-01,  1.1331e-01, -1.2734e-01,
          8.2190e-02],
        [-3.9401e-01, -6.8996e-03,  2.1895e-01,  1.7512e-02,  2.3517e-01,
         -1.4815e-01],
        [ 2.0318e-01,  2.1162e-01, -1.1329e-01, -1.3235e-01, -1.2365e-01,
          8.1847e-02],
        [-6.6127e-03,  3.9376e-01,  6.3385e-03, -2.1893e-01,  2.3339e-01,
         -1.4775e-01],
        [ 5.1270e-03, -8.9337e-04,  1.4482e-02, -7.2225e-03, -4.2812e-01,
          2.6524e-01],
        [ 7.4759e-03, -3.9281e-01, -2.1178e-02,  2.2631e-01,  2.2662e-01,
         -1.4764e-01],
        [-2.0668e-01, -2.1681e-01,  1.2060e-01,  1.2801e-01, -1.2559e-01,
          8.2509e-02],
        [ 3.8976e-01,  7.4522e-03, -2.3382e-01, -9.9088e-03,  2.3022e-01,
         -1.4953e-01],
        [-2.1139e-01,  2.0872e-01,  1.3621e-01, -1.1678e-01, -1.2181e-01,
          8.2585e-02],
        [-2.0414e-02,  2.2724e-02,  2.5845e-01, -2.2756e-01,  1.6972e-02,
         -1.5341e-01],
        [ 3.3570e-02,  4.2188e-05, -4.3976e-01, -2.9173e-02, -2.3922e-02,
          2.7463e-01],
        [-1.6104e-02, -2.2952e-02,  2.2721e-01,  2.5926e-01,  1.0316e-02,
         -1.5034e-01],
        [ 3.3528e-03, -3.7358e-02, -2.7652e-02,  4.3907e-01, -1.7203e-02,
          2.7390e-01],
        [-4.6383e-03,  1.0434e-03, -1.9136e-03,  2.6428e-03,  1.6618e-02,
         -4.8928e-01],
        [ 3.3602e-03,  3.6202e-02,  2.8204e-02, -4.4122e-01, -3.4262e-03,
          2.7059e-01],
        [ 1.8124e-02,  1.5062e-02, -2.2759e-01, -2.5807e-01,  2.6468e-03,
         -1.5196e-01],
        [-3.0857e-02, -2.9894e-04,  4.4146e-01,  2.7118e-02,  3.7644e-03,
          2.7444e-01],
        [ 1.2900e-02, -1.4208e-02, -2.5837e-01,  2.2783e-01, -5.9343e-03,
         -1.5103e-01],
        [-2.2032e-01,  2.0526e-01, -1.5372e-01,  1.3477e-01,  1.2541e-01,
          8.3952e-02],
        [ 4.1088e-01,  7.9342e-03,  2.6076e-01,  1.4008e-02, -2.3960e-01,
         -1.4933e-01],
        [-2.1409e-01, -2.1352e-01, -1.3483e-01, -1.5000e-01,  1.2899e-01,
          8.1025e-02],
        [ 9.2853e-03, -4.0071e-01,  2.5012e-02, -2.5937e-01, -2.4413e-01,
         -1.4880e-01],
        [-1.0627e-02, -6.2620e-04, -1.4773e-02,  4.9327e-03,  4.6420e-01,
          2.6467e-01],
        [-6.0721e-03,  4.0151e-01, -7.8173e-03,  2.5374e-01, -2.5271e-01,
         -1.4538e-01],
        [ 2.0976e-01,  2.2706e-01,  1.2633e-01,  1.5290e-01,  1.3882e-01,
          8.1878e-02],
        [-3.9870e-01, -7.7074e-03, -2.4522e-01, -1.9899e-02, -2.6442e-01,
         -1.4746e-01],
        [ 2.2095e-01, -2.1934e-01,  1.4395e-01, -1.3096e-01,  1.4475e-01,
          8.0833e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0570, -0.0616,  0.0090, -0.0610,  0.0306,  0.0283,  0.0090,  0.0281,
         -0.0392, -0.0338,  0.0350, -0.0050,  0.0353, -0.0176, -0.0154, -0.0049,
         -0.0155,  0.0216, -0.0257,  0.0296, -0.0047,  0.0286, -0.0151, -0.0136,
         -0.0047, -0.0134,  0.0191],
        [-0.0616,  0.1146, -0.0612,  0.0303, -0.0567,  0.0303,  0.0285, -0.0525,
          0.0281,  0.0356, -0.0638,  0.0349, -0.0180,  0.0318, -0.0180, -0.0157,
          0.0284, -0.0149,  0.0290, -0.0567,  0.0294, -0.0144,  0.0290, -0.0145,
         -0.0136,  0.0256, -0.0140],
        [ 0.0090, -0.0612,  0.0565,  0.0283,  0.0305, -0.0609, -0.0393,  0.0277,
          0.0094, -0.0054,  0.0351, -0.0334, -0.0154, -0.0175,  0.0352,  0.0221,
         -0.0155, -0.0053, -0.0042,  0.0291, -0.0257, -0.0136, -0.0151,  0.0287,
          0.0186, -0.0129, -0.0048],
        [-0.0610,  0.0303,  0.0283,  0.1140, -0.0563, -0.0529, -0.0624,  0.0314,
          0.0283,  0.0356, -0.0175, -0.0162, -0.0635,  0.0310,  0.0287,  0.0343,
         -0.0174, -0.0148,  0.0280, -0.0142, -0.0133, -0.0555,  0.0277,  0.0266,
          0.0309, -0.0153, -0.0149],
        [ 0.0306, -0.0567,  0.0305, -0.0563,  0.1037, -0.0562,  0.0309, -0.0570,
          0.0309, -0.0174,  0.0315, -0.0175,  0.0312, -0.0559,  0.0314, -0.0175,
          0.0315, -0.0175, -0.0147,  0.0281, -0.0145,  0.0276, -0.0526,  0.0275,
         -0.0147,  0.0280, -0.0148],
        [ 0.0283,  0.0303, -0.0609, -0.0529, -0.0562,  0.1140,  0.0283,  0.0314,
         -0.0623, -0.0161, -0.0176,  0.0356,  0.0287,  0.0311, -0.0634, -0.0148,
         -0.0176,  0.0342, -0.0133, -0.0141,  0.0279,  0.0265,  0.0276, -0.0555,
         -0.0148, -0.0153,  0.0310],
        [ 0.0090,  0.0285, -0.0393, -0.0624,  0.0309,  0.0283,  0.0585, -0.0624,
          0.0090, -0.0055, -0.0155,  0.0225,  0.0347, -0.0170, -0.0159, -0.0328,
          0.0348, -0.0054, -0.0037, -0.0145,  0.0186,  0.0299, -0.0144, -0.0141,
         -0.0279,  0.0298, -0.0037],
        [ 0.0281, -0.0525,  0.0277,  0.0314, -0.0570,  0.0314, -0.0624,  0.1152,
         -0.0620, -0.0160,  0.0284, -0.0152, -0.0171,  0.0309, -0.0171,  0.0352,
         -0.0636,  0.0344, -0.0136,  0.0270, -0.0141, -0.0149,  0.0272, -0.0149,
          0.0294, -0.0558,  0.0299],
        [-0.0392,  0.0281,  0.0094,  0.0283,  0.0309, -0.0623,  0.0090, -0.0620,
          0.0580,  0.0229, -0.0155, -0.0059, -0.0159, -0.0171,  0.0347, -0.0059,
          0.0350, -0.0324,  0.0181, -0.0141, -0.0037, -0.0141, -0.0144,  0.0299,
         -0.0032,  0.0292, -0.0280],
        [-0.0338,  0.0356, -0.0054,  0.0356, -0.0174, -0.0161, -0.0055, -0.0160,
          0.0229,  0.0585, -0.0613,  0.0093, -0.0614,  0.0304,  0.0273,  0.0095,
          0.0272, -0.0392, -0.0294,  0.0306, -0.0047,  0.0306, -0.0153, -0.0133,
         -0.0047, -0.0134,  0.0194],
        [ 0.0350, -0.0638,  0.0351, -0.0175,  0.0315, -0.0176, -0.0155,  0.0284,
         -0.0155, -0.0613,  0.1108, -0.0611,  0.0307, -0.0548,  0.0306,  0.0271,
         -0.0493,  0.0270,  0.0312, -0.0558,  0.0309, -0.0156,  0.0276, -0.0155,
         -0.0138,  0.0248, -0.0136],
        [-0.0050,  0.0349, -0.0334, -0.0162, -0.0175,  0.0356,  0.0225, -0.0152,
         -0.0059,  0.0093, -0.0611,  0.0581,  0.0274,  0.0302, -0.0611, -0.0393,
          0.0272,  0.0094, -0.0051,  0.0311, -0.0294, -0.0133, -0.0151,  0.0303,
          0.0200, -0.0142, -0.0043],
        [ 0.0353, -0.0180, -0.0154, -0.0635,  0.0312,  0.0287,  0.0347, -0.0171,
         -0.0159, -0.0614,  0.0307,  0.0274,  0.1103, -0.0544, -0.0497, -0.0609,
          0.0305,  0.0271,  0.0310, -0.0150, -0.0142, -0.0556,  0.0274,  0.0250,
          0.0310, -0.0159, -0.0134],
        [-0.0176,  0.0318, -0.0175,  0.0310, -0.0559,  0.0311, -0.0170,  0.0309,
         -0.0171,  0.0304, -0.0548,  0.0302, -0.0544,  0.0979, -0.0546,  0.0302,
         -0.0551,  0.0305, -0.0151,  0.0272, -0.0151,  0.0276, -0.0497,  0.0278,
         -0.0156,  0.0286, -0.0159],
        [-0.0154, -0.0180,  0.0352,  0.0287,  0.0314, -0.0634, -0.0159, -0.0171,
          0.0347,  0.0273,  0.0306, -0.0611, -0.0497, -0.0546,  0.1102,  0.0273,
          0.0305, -0.0609, -0.0142, -0.0150,  0.0308,  0.0250,  0.0275, -0.0556,
         -0.0136, -0.0158,  0.0310],
        [-0.0049, -0.0157,  0.0221,  0.0343, -0.0175, -0.0148, -0.0328,  0.0352,
         -0.0059,  0.0095,  0.0271, -0.0393, -0.0609,  0.0302,  0.0273,  0.0580,
         -0.0611,  0.0094, -0.0054, -0.0136,  0.0205,  0.0316, -0.0151, -0.0148,
         -0.0298,  0.0307, -0.0041],
        [-0.0155,  0.0284, -0.0155, -0.0174,  0.0315, -0.0176,  0.0348, -0.0636,
          0.0350,  0.0272, -0.0493,  0.0272,  0.0305, -0.0551,  0.0305, -0.0611,
          0.1110, -0.0611, -0.0139,  0.0249, -0.0140, -0.0155,  0.0279, -0.0152,
          0.0312, -0.0563,  0.0310],
        [ 0.0216, -0.0149, -0.0053, -0.0148, -0.0175,  0.0342, -0.0054,  0.0344,
         -0.0324, -0.0392,  0.0270,  0.0094,  0.0271,  0.0305, -0.0609,  0.0094,
         -0.0611,  0.0579,  0.0209, -0.0143, -0.0049, -0.0146, -0.0155,  0.0316,
         -0.0047,  0.0317, -0.0303],
        [-0.0257,  0.0290, -0.0042,  0.0280, -0.0147, -0.0133, -0.0037, -0.0136,
          0.0181, -0.0294,  0.0312, -0.0051,  0.0310, -0.0151, -0.0142, -0.0054,
         -0.0139,  0.0209,  0.0634, -0.0693,  0.0110, -0.0679,  0.0349,  0.0307,
          0.0106,  0.0309, -0.0443],
        [ 0.0296, -0.0567,  0.0291, -0.0142,  0.0281, -0.0141, -0.0145,  0.0270,
         -0.0141,  0.0306, -0.0558,  0.0311, -0.0150,  0.0272, -0.0150, -0.0136,
          0.0249, -0.0143, -0.0693,  0.1293, -0.0694,  0.0344, -0.0649,  0.0345,
          0.0317, -0.0583,  0.0318],
        [-0.0047,  0.0294, -0.0257, -0.0133, -0.0145,  0.0279,  0.0186, -0.0141,
         -0.0037, -0.0047,  0.0309, -0.0294, -0.0142, -0.0151,  0.0308,  0.0205,
         -0.0140, -0.0049,  0.0110, -0.0694,  0.0634,  0.0308,  0.0347, -0.0677,
         -0.0444,  0.0314,  0.0102],
        [ 0.0286, -0.0144, -0.0136, -0.0555,  0.0276,  0.0265,  0.0299, -0.0149,
         -0.0141,  0.0306, -0.0156, -0.0133, -0.0556,  0.0276,  0.0250,  0.0316,
         -0.0155, -0.0146, -0.0679,  0.0344,  0.0308,  0.1267, -0.0630, -0.0586,
         -0.0701,  0.0347,  0.0327],
        [-0.0151,  0.0290, -0.0151,  0.0277, -0.0526,  0.0276, -0.0144,  0.0272,
         -0.0144, -0.0153,  0.0276, -0.0151,  0.0274, -0.0497,  0.0275, -0.0151,
          0.0279, -0.0155,  0.0349, -0.0649,  0.0347, -0.0630,  0.1167, -0.0631,
          0.0337, -0.0629,  0.0341],
        [-0.0136, -0.0145,  0.0287,  0.0266,  0.0275, -0.0555, -0.0141, -0.0149,
          0.0299, -0.0133, -0.0155,  0.0303,  0.0250,  0.0278, -0.0556, -0.0148,
         -0.0152,  0.0316,  0.0307,  0.0345, -0.0677, -0.0586, -0.0631,  0.1268,
          0.0330,  0.0345, -0.0703],
        [-0.0047, -0.0136,  0.0186,  0.0309, -0.0147, -0.0148, -0.0279,  0.0294,
         -0.0032, -0.0047, -0.0138,  0.0200,  0.0310, -0.0156, -0.0136, -0.0298,
          0.0312, -0.0047,  0.0106,  0.0317, -0.0444, -0.0701,  0.0337,  0.0330,
          0.0657, -0.0687,  0.0087],
        [-0.0134,  0.0256, -0.0129, -0.0153,  0.0280, -0.0153,  0.0298, -0.0558,
          0.0292, -0.0134,  0.0248, -0.0142, -0.0159,  0.0286, -0.0158,  0.0307,
         -0.0563,  0.0317,  0.0309, -0.0583,  0.0314,  0.0347, -0.0629,  0.0345,
         -0.0687,  0.1271, -0.0691],
        [ 0.0191, -0.0140, -0.0048, -0.0149, -0.0148,  0.0310, -0.0037,  0.0299,
         -0.0280,  0.0194, -0.0136, -0.0043, -0.0134, -0.0159,  0.0310, -0.0041,
          0.0310, -0.0303, -0.0443,  0.0318,  0.0102,  0.0327,  0.0341, -0.0703,
          0.0087, -0.0691,  0.0663]], device='cuda:0') 

reserving basis 80/576; cond: 9537839.0, radio:3.853463567793369e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1531e-02, -3.9708e-02, -7.4769e-03],
          [ 3.8286e-02,  2.6514e-02,  3.1884e-02],
          [ 2.2172e-02, -1.7845e-03,  8.6749e-03]],

         [[-1.4064e-02,  3.7979e-02,  1.8392e-02],
          [-2.9860e-02,  3.6716e-02, -3.1358e-03],
          [-7.0753e-03, -1.7031e-02, -2.0422e-02]],

         [[-3.1800e-03, -2.0672e-02, -3.6671e-02],
          [-2.9165e-02, -1.0459e-02,  3.3264e-02],
          [ 2.0553e-02, -3.1919e-02, -9.4750e-03]],

         ...,

         [[-2.5535e-02,  1.1655e-02,  4.3023e-05],
          [ 1.4977e-02, -6.5295e-03, -9.4964e-03],
          [-2.0355e-03,  8.6564e-03, -3.1072e-02]],

         [[-4.1517e-02,  1.3732e-02, -3.4941e-02],
          [-6.7101e-03,  1.2050e-02,  1.1445e-03],
          [ 2.0814e-02,  1.1833e-04,  4.0826e-03]],

         [[-1.5898e-02,  1.8838e-03,  1.1371e-02],
          [ 2.3044e-02, -3.9628e-02, -1.6091e-02],
          [-4.1528e-02, -1.1371e-02,  2.9614e-02]]],


        [[[-4.2301e-02, -4.7068e-02,  1.3940e-02],
          [-2.3491e-02,  1.9782e-02,  2.9307e-03],
          [ 8.0442e-03,  2.3406e-02, -4.2336e-02]],

         [[ 3.2932e-02,  7.8613e-03, -3.0227e-02],
          [ 1.1968e-02, -1.1387e-02, -1.7366e-02],
          [ 3.3011e-03, -1.1581e-02,  4.0668e-02]],

         [[ 2.6498e-02, -1.6425e-02, -4.4845e-02],
          [-1.7281e-02, -1.0959e-02,  1.0055e-02],
          [-1.2346e-02, -2.3545e-03, -6.3182e-03]],

         ...,

         [[-5.4902e-03, -3.8139e-03,  8.7049e-03],
          [ 1.5800e-02, -3.4124e-02,  1.7237e-02],
          [ 3.3102e-02,  7.4937e-03,  3.0864e-02]],

         [[-2.1724e-02,  6.7264e-03, -4.3358e-04],
          [-1.1097e-02,  2.3854e-02, -4.6510e-02],
          [-1.0923e-03, -1.4434e-02, -2.0034e-02]],

         [[-1.4830e-02, -3.2040e-02,  4.2879e-03],
          [-6.7517e-03, -3.1414e-02, -8.2191e-03],
          [-8.7765e-03,  1.5876e-02, -4.1574e-02]]],


        [[[-3.0094e-03,  1.8512e-02,  2.0563e-02],
          [ 7.9637e-03,  1.2935e-02,  4.1812e-02],
          [-1.3441e-02,  2.5664e-02,  3.2349e-02]],

         [[-3.1185e-02,  2.0026e-02,  2.9033e-02],
          [-2.5307e-03,  3.2259e-02,  6.2604e-03],
          [ 3.5460e-02,  6.7315e-03, -2.1006e-02]],

         [[ 3.8709e-02,  1.5353e-02,  3.4330e-02],
          [ 1.8291e-02,  2.5603e-02,  2.0271e-02],
          [ 1.6788e-02, -5.9319e-03,  9.4116e-03]],

         ...,

         [[ 2.1056e-02, -1.6858e-02,  4.4097e-02],
          [-3.1410e-03,  3.8393e-02, -3.4262e-03],
          [ 2.4367e-02,  3.3242e-02, -3.6228e-02]],

         [[-2.0653e-02, -3.6410e-02,  3.6826e-02],
          [ 4.5186e-02, -1.3419e-02, -1.8947e-02],
          [ 1.2016e-02,  3.0951e-02,  1.3908e-02]],

         [[-1.4881e-02, -2.3233e-02,  3.0080e-02],
          [-2.9717e-02, -4.1818e-02, -2.1362e-02],
          [ 3.6191e-03,  6.0135e-03,  6.1269e-03]]],


        ...,


        [[[-4.4239e-02, -1.8245e-02,  9.4446e-03],
          [ 2.4311e-02, -1.7214e-02, -8.6230e-03],
          [-4.2805e-03,  2.1028e-02, -3.4652e-02]],

         [[-4.1709e-02,  2.7597e-02,  5.9062e-03],
          [-1.3620e-02, -1.1777e-02,  8.6806e-03],
          [-1.9306e-02,  2.8316e-02,  1.2310e-02]],

         [[ 2.3944e-02,  3.2035e-02,  1.0455e-03],
          [ 4.7835e-02, -7.7036e-03, -3.4140e-02],
          [ 3.8567e-02, -1.4956e-02, -2.8544e-03]],

         ...,

         [[-2.5786e-02, -5.3309e-02,  2.3106e-02],
          [-2.0458e-02,  2.2707e-02, -4.7258e-02],
          [ 1.6387e-02,  4.0658e-02,  2.7427e-02]],

         [[-4.6437e-02, -5.5599e-02,  1.6263e-02],
          [-1.3043e-02, -1.4848e-02, -6.4819e-03],
          [-2.0928e-02,  9.7785e-03, -9.3095e-03]],

         [[ 3.6283e-02, -2.2412e-02,  4.1208e-02],
          [ 9.5793e-03, -5.8061e-03,  4.7891e-02],
          [ 3.1204e-02,  2.8013e-02,  4.8035e-02]]],


        [[[ 2.5441e-02,  1.4033e-02, -1.3651e-02],
          [-1.3411e-02, -3.2030e-02, -2.0657e-02],
          [ 1.5764e-02, -5.8580e-02, -4.1948e-02]],

         [[-8.2400e-03,  2.0954e-02,  8.8236e-03],
          [ 9.8647e-03,  8.4306e-04, -4.0370e-02],
          [-9.3377e-03,  3.7851e-02,  1.2258e-02]],

         [[-9.3432e-03,  8.2890e-03,  4.0498e-02],
          [ 1.6466e-04,  5.7369e-03,  3.0441e-02],
          [-3.1269e-02,  2.5704e-02,  3.2408e-02]],

         ...,

         [[ 4.0181e-02,  1.9627e-02,  3.8745e-02],
          [-3.5753e-02,  1.0002e-02,  7.6465e-04],
          [-1.2527e-03, -4.1775e-02, -3.9157e-02]],

         [[ 1.6752e-02,  2.3260e-02, -9.1028e-03],
          [ 2.6054e-02,  8.1976e-03,  1.8079e-02],
          [-4.8381e-02, -2.4971e-02, -5.2182e-03]],

         [[ 2.7819e-02,  2.5179e-02, -3.1940e-02],
          [-3.3048e-02, -2.8804e-02, -1.2028e-02],
          [-8.0678e-03, -3.9856e-02,  2.2268e-02]]],


        [[[ 2.7625e-02,  1.0203e-02, -1.6896e-02],
          [-6.1893e-03,  6.4723e-03, -1.2449e-02],
          [-1.6817e-03, -3.5173e-02, -7.7920e-03]],

         [[-4.3861e-02, -2.8637e-02, -2.9450e-02],
          [ 1.0884e-02, -5.6426e-02, -1.2499e-03],
          [ 2.6622e-02, -3.5910e-02, -6.8707e-03]],

         [[-1.6961e-02,  1.3512e-02,  2.2393e-02],
          [ 1.5360e-02, -1.3467e-02,  1.0165e-02],
          [-2.2161e-02,  3.6099e-02,  3.8304e-03]],

         ...,

         [[-2.1326e-02, -9.0812e-03,  1.0005e-02],
          [ 4.0017e-03,  3.7255e-02, -4.2810e-02],
          [-2.2932e-02,  1.5724e-02,  2.4678e-02]],

         [[-3.1370e-02, -3.8710e-02,  5.5561e-03],
          [-1.9637e-02, -1.0645e-02, -4.8757e-02],
          [-2.8324e-02, -5.1250e-02, -3.9134e-02]],

         [[-4.0210e-03, -1.0417e-02,  3.1146e-02],
          [ 3.3860e-02,  4.6372e-03,  3.5759e-02],
          [-2.8755e-02,  4.5807e-02, -3.2631e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([9.0390e+07, 4.0848e+06, 3.2492e+06, 3.0663e+06, 2.3259e+06, 1.6559e+06,
        1.5898e+06, 1.4724e+06, 1.2544e+06, 1.0410e+06, 8.7066e+05, 7.6221e+05,
        6.4272e+05, 4.4210e+05, 3.1276e+05, 2.4154e+05, 2.3426e+05, 2.0722e+05,
        1.7052e+05, 1.6302e+05, 1.4334e+05, 1.2627e+05, 1.0676e+05, 9.4750e+04,
        8.6901e+04, 8.3980e+04, 7.9303e+04, 7.5891e+04, 7.2345e+04, 6.8434e+04,
        6.5338e+04, 5.8715e+04, 5.4964e+04, 5.1291e+04, 4.8061e+04, 4.6399e+04,
        4.3154e+04, 4.1642e+04, 3.7816e+04, 3.3552e+04, 3.3303e+04, 3.0755e+04,
        3.0468e+04, 2.9295e+04, 2.7701e+04, 2.6833e+04, 2.3676e+04, 2.3368e+04,
        2.2061e+04, 2.1608e+04, 2.0430e+04, 2.0134e+04, 1.9093e+04, 1.8912e+04,
        1.8231e+04, 1.6584e+04, 1.5960e+04, 1.5591e+04, 1.5156e+04, 1.4822e+04,
        1.4502e+04, 1.3811e+04, 1.3632e+04, 1.3147e+04, 1.3010e+04, 1.2835e+04,
        1.2267e+04, 1.2135e+04, 1.2061e+04, 1.1381e+04, 1.1110e+04, 1.0898e+04,
        1.0451e+04, 1.0415e+04, 1.0198e+04, 9.6641e+03, 9.4349e+03, 9.1277e+03,
        9.1169e+03, 8.8035e+03, 8.5138e+03, 8.2524e+03, 8.2159e+03, 8.0854e+03,
        7.8846e+03, 7.7120e+03, 7.3270e+03, 7.0801e+03, 6.9950e+03, 6.9756e+03,
        6.7977e+03, 6.6228e+03, 6.4986e+03, 6.1882e+03, 6.0361e+03, 5.7725e+03,
        5.7449e+03, 5.5353e+03, 5.4007e+03, 5.3159e+03, 5.1653e+03, 4.9542e+03,
        4.9295e+03, 4.7599e+03, 4.7074e+03, 4.5730e+03, 4.5124e+03, 4.4343e+03,
        4.3689e+03, 4.3122e+03, 4.2764e+03, 4.1510e+03, 3.9902e+03, 3.9652e+03,
        3.9163e+03, 3.8193e+03, 3.8174e+03, 3.6914e+03, 3.6457e+03, 3.5868e+03,
        3.5412e+03, 3.4848e+03, 3.4072e+03, 3.3161e+03, 3.2780e+03, 3.2167e+03,
        3.2041e+03, 3.1665e+03, 3.0611e+03, 2.9832e+03, 2.9603e+03, 2.9389e+03,
        2.8720e+03, 2.8432e+03, 2.8095e+03, 2.7453e+03, 2.6963e+03, 2.6635e+03,
        2.6286e+03, 2.5784e+03, 2.5258e+03, 2.4996e+03, 2.4764e+03, 2.4531e+03,
        2.4152e+03, 2.3890e+03, 2.3567e+03, 2.3081e+03, 2.2611e+03, 2.2279e+03,
        2.2041e+03, 2.1823e+03, 2.1719e+03, 2.1436e+03, 2.1159e+03, 2.0902e+03,
        2.0616e+03, 2.0231e+03, 1.9650e+03, 1.9624e+03, 1.9484e+03, 1.8939e+03,
        1.8845e+03, 1.8577e+03, 1.8238e+03, 1.7906e+03, 1.7498e+03, 1.7407e+03,
        1.7285e+03, 1.7032e+03, 1.6889e+03, 1.6750e+03, 1.6444e+03, 1.6082e+03,
        1.5767e+03, 1.5658e+03, 1.5470e+03, 1.5341e+03, 1.5137e+03, 1.5083e+03,
        1.4989e+03, 1.4629e+03, 1.4296e+03, 1.4080e+03, 1.3783e+03, 1.3702e+03,
        1.3606e+03, 1.3464e+03, 1.3284e+03, 1.3210e+03, 1.3038e+03, 1.2805e+03,
        1.2771e+03, 1.2585e+03, 1.2519e+03, 1.2415e+03, 1.2199e+03, 1.2091e+03,
        1.1917e+03, 1.1851e+03, 1.1668e+03, 1.1544e+03, 1.1418e+03, 1.1348e+03,
        1.1265e+03, 1.1145e+03, 1.0967e+03, 1.0845e+03, 1.0786e+03, 1.0574e+03,
        1.0542e+03, 1.0356e+03, 1.0290e+03, 1.0117e+03, 1.0077e+03, 9.9480e+02,
        9.8741e+02, 9.7434e+02, 9.5879e+02, 9.5013e+02, 9.3917e+02, 9.2686e+02,
        9.1340e+02, 9.0080e+02, 8.9910e+02, 8.8842e+02, 8.8317e+02, 8.7621e+02,
        8.6512e+02, 8.5181e+02, 8.4772e+02, 8.3742e+02, 8.2464e+02, 8.2314e+02,
        8.0712e+02, 8.0382e+02, 7.9550e+02, 7.8360e+02, 7.7678e+02, 7.7240e+02,
        7.6622e+02, 7.5012e+02, 7.4804e+02, 7.3464e+02, 7.3364e+02, 7.2482e+02,
        7.1302e+02, 7.0889e+02, 7.0115e+02, 6.9760e+02, 6.9253e+02, 6.7227e+02,
        6.7160e+02, 6.6809e+02, 6.6725e+02, 6.5877e+02, 6.5589e+02, 6.5080e+02,
        6.4532e+02, 6.3177e+02, 6.2993e+02, 6.2617e+02, 6.2042e+02, 6.1508e+02,
        6.0836e+02, 6.0037e+02, 5.9682e+02, 5.8897e+02, 5.7997e+02, 5.7384e+02,
        5.7173e+02, 5.6781e+02, 5.6547e+02, 5.5886e+02, 5.5342e+02, 5.4736e+02,
        5.4654e+02, 5.3847e+02, 5.3681e+02, 5.3162e+02, 5.2482e+02, 5.2030e+02,
        5.1640e+02, 5.1173e+02, 5.0570e+02, 5.0100e+02, 4.9915e+02, 4.9592e+02,
        4.9166e+02, 4.8650e+02, 4.8065e+02, 4.7829e+02, 4.7613e+02, 4.7149e+02,
        4.6767e+02, 4.6352e+02, 4.6074e+02, 4.5457e+02, 4.4863e+02, 4.4712e+02,
        4.4114e+02, 4.3748e+02, 4.3517e+02, 4.3182e+02, 4.2399e+02, 4.2351e+02,
        4.1862e+02, 4.1723e+02, 4.1445e+02, 4.1123e+02, 4.0849e+02, 4.0535e+02,
        4.0306e+02, 4.0072e+02, 3.9671e+02, 3.9308e+02, 3.8939e+02, 3.8568e+02,
        3.8180e+02, 3.7856e+02, 3.7590e+02, 3.7161e+02, 3.6588e+02, 3.6391e+02,
        3.6106e+02, 3.5879e+02, 3.5600e+02, 3.5476e+02, 3.5375e+02, 3.5051e+02,
        3.4394e+02, 3.4280e+02, 3.4192e+02, 3.3582e+02, 3.3451e+02, 3.3313e+02,
        3.2857e+02, 3.2700e+02, 3.2445e+02, 3.2169e+02, 3.1835e+02, 3.1514e+02,
        3.1369e+02, 3.1042e+02, 3.0755e+02, 3.0717e+02, 3.0524e+02, 3.0301e+02,
        3.0203e+02, 2.9782e+02, 2.9613e+02, 2.9343e+02, 2.9112e+02, 2.9012e+02,
        2.8813e+02, 2.8671e+02, 2.8368e+02, 2.8279e+02, 2.7911e+02, 2.7832e+02,
        2.7588e+02, 2.7472e+02, 2.7135e+02, 2.6870e+02, 2.6654e+02, 2.6257e+02,
        2.6214e+02, 2.6029e+02, 2.5901e+02, 2.5849e+02, 2.5574e+02, 2.5369e+02,
        2.5139e+02, 2.5094e+02, 2.4756e+02, 2.4494e+02, 2.4387e+02, 2.4139e+02,
        2.4008e+02, 2.3902e+02, 2.3674e+02, 2.3581e+02, 2.3299e+02, 2.3086e+02,
        2.3008e+02, 2.2948e+02, 2.2775e+02, 2.2748e+02, 2.2608e+02, 2.2384e+02,
        2.2164e+02, 2.1906e+02, 2.1818e+02, 2.1657e+02, 2.1536e+02, 2.1397e+02,
        2.1240e+02, 2.1094e+02, 2.0977e+02, 2.0657e+02, 2.0573e+02, 2.0358e+02,
        2.0232e+02, 1.9988e+02, 1.9916e+02, 1.9885e+02, 1.9697e+02, 1.9480e+02,
        1.9398e+02, 1.9306e+02, 1.9174e+02, 1.8991e+02, 1.8967e+02, 1.8843e+02,
        1.8735e+02, 1.8463e+02, 1.8443e+02, 1.8280e+02, 1.8096e+02, 1.8012e+02,
        1.7946e+02, 1.7845e+02, 1.7733e+02, 1.7610e+02, 1.7425e+02, 1.7319e+02,
        1.7185e+02, 1.6954e+02, 1.6816e+02, 1.6690e+02, 1.6543e+02, 1.6352e+02,
        1.6254e+02, 1.6153e+02, 1.6059e+02, 1.5943e+02, 1.5763e+02, 1.5645e+02,
        1.5543e+02, 1.5369e+02, 1.5323e+02, 1.5291e+02, 1.5249e+02, 1.5097e+02,
        1.4903e+02, 1.4827e+02, 1.4697e+02, 1.4497e+02, 1.4380e+02, 1.4242e+02,
        1.4197e+02, 1.4020e+02, 1.3931e+02, 1.3831e+02, 1.3698e+02, 1.3551e+02,
        1.3460e+02, 1.3325e+02, 1.3165e+02, 1.3086e+02, 1.2982e+02, 1.2899e+02,
        1.2817e+02, 1.2663e+02, 1.2584e+02, 1.2496e+02, 1.2444e+02, 1.2231e+02,
        1.2189e+02, 1.2101e+02, 1.2042e+02, 1.1832e+02, 1.1772e+02, 1.1704e+02,
        1.1569e+02, 1.1418e+02, 1.1406e+02, 1.1258e+02, 1.1153e+02, 1.0996e+02,
        1.0921e+02, 1.0734e+02, 1.0715e+02, 1.0608e+02, 1.0450e+02, 1.0406e+02,
        1.0331e+02, 1.0287e+02, 1.0170e+02, 9.9704e+01, 9.9627e+01, 9.9039e+01,
        9.7724e+01, 9.6577e+01, 9.5881e+01, 9.5578e+01, 9.4352e+01, 9.3851e+01,
        9.2704e+01, 9.1820e+01, 9.1205e+01, 8.9510e+01, 8.8470e+01, 8.8423e+01,
        8.7501e+01, 8.7102e+01, 8.5761e+01, 8.4817e+01, 8.4629e+01, 8.2784e+01,
        8.2702e+01, 8.1791e+01, 8.1366e+01, 8.0085e+01, 7.8380e+01, 7.7099e+01,
        7.6865e+01, 7.5815e+01, 7.4694e+01, 7.4042e+01, 7.3285e+01, 7.1866e+01,
        7.1540e+01, 7.0181e+01, 6.9507e+01, 6.9191e+01, 6.8353e+01, 6.6981e+01,
        6.5221e+01, 6.4281e+01, 6.3868e+01, 6.1162e+01, 6.1026e+01, 6.0262e+01,
        5.8938e+01, 5.8233e+01, 5.7275e+01, 5.6953e+01, 5.5245e+01, 5.4301e+01,
        5.3248e+01, 5.2799e+01, 5.2166e+01, 5.0108e+01, 4.9654e+01, 4.9396e+01,
        4.8037e+01, 4.7330e+01, 4.6037e+01, 4.4307e+01, 4.2736e+01, 4.1713e+01,
        4.0826e+01, 3.8789e+01, 3.7685e+01, 3.6084e+01, 3.4450e+01, 3.2964e+01,
        3.2149e+01, 3.0847e+01, 3.0110e+01, 2.9056e+01, 2.8287e+01, 2.7564e+01,
        2.6503e+01, 2.6213e+01, 2.4022e+01, 2.3155e+01, 2.1009e+01, 1.9278e+01,
        1.8649e+01, 1.4834e+01, 1.3384e+01, 1.2553e+01, 1.1161e+01, 9.4770e+00],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 80]) 

NULL SPACE BASIS :  tensor([[-8.8123e-02, -1.9959e-02, -2.7233e-02,  ...,  1.3361e-02,
          4.0772e-03,  5.5514e-03],
        [ 6.9942e-02, -6.2442e-03, -2.5829e-03,  ..., -5.1840e-03,
         -3.1741e-03, -5.2653e-03],
        [ 1.8473e-02,  4.9677e-02,  7.0814e-03,  ..., -3.1007e-03,
          2.0807e-03,  1.6287e-03],
        ...,
        [-1.5538e-02, -7.2106e-03,  6.2676e-03,  ...,  3.6229e-03,
         -2.5516e-04, -2.2504e-03],
        [ 8.5242e-03, -4.0174e-02,  9.4304e-03,  ..., -2.7778e-05,
          2.0193e-04,  1.6106e-03],
        [ 1.2113e-02, -1.7807e-02, -1.8442e-02,  ..., -4.5192e-03,
          4.7219e-04,  2.5618e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 7.3116e-03, -6.8164e-03,  2.8728e-04,  ..., -4.4516e-04,
          5.6047e-04, -2.8459e-04],
        [-6.8164e-03,  1.3478e-02, -7.2673e-03,  ...,  1.3165e-03,
         -2.6552e-04, -3.1159e-04],
        [ 2.8728e-04, -7.2673e-03,  8.4186e-03,  ..., -1.1273e-03,
          6.1297e-05,  4.2009e-04],
        ...,
        [-4.4516e-04,  1.3165e-03, -1.1273e-03,  ...,  2.1745e-03,
         -1.4436e-03, -1.3314e-04],
        [ 5.6047e-04, -2.6552e-04,  6.1297e-05,  ..., -1.4436e-03,
          3.1856e-03, -1.2203e-03],
        [-2.8459e-04, -3.1159e-04,  4.2009e-04,  ..., -1.3314e-04,
         -1.2203e-03,  1.7054e-03]], device='cuda:0') 

reserving basis 139/576; cond: 1323318.125, radio:0.00047327729407697916
PARAMETER       :  Parameter containing:
tensor([[[[-1.3484e-02,  3.7415e-03,  1.4434e-02],
          [ 2.3900e-02,  7.0829e-03,  7.7220e-03],
          [-5.2423e-02, -4.9319e-02, -1.1124e-02]],

         [[ 3.2664e-03, -1.1701e-02, -6.4575e-03],
          [ 2.6698e-02, -2.8945e-02,  2.6118e-02],
          [-6.2278e-02, -7.8839e-03, -5.6770e-02]],

         [[-2.1194e-04,  1.9363e-02,  2.5550e-02],
          [-1.4185e-02, -3.3923e-02,  4.6470e-02],
          [-1.6770e-02, -8.2423e-03,  5.1681e-02]],

         ...,

         [[-3.9639e-02,  2.7512e-02,  2.1362e-02],
          [-1.1190e-02,  7.6527e-03, -6.7556e-03],
          [-3.4924e-02,  1.2329e-02,  2.3761e-02]],

         [[-1.7642e-02,  1.2468e-02,  1.7439e-03],
          [-3.3145e-02,  1.9519e-02, -4.3207e-03],
          [ 1.7546e-02, -3.9495e-02,  3.4575e-02]],

         [[ 2.1114e-02, -6.7156e-04,  2.4128e-02],
          [ 1.0148e-02,  1.0893e-02,  3.9863e-02],
          [-2.9381e-02,  2.0504e-02, -5.6758e-03]]],


        [[[ 3.2387e-02,  1.0996e-02, -2.2631e-02],
          [-7.9794e-03, -8.8110e-05,  1.0437e-02],
          [ 3.7045e-03, -4.7121e-02,  9.3042e-03]],

         [[ 3.4596e-02,  2.4624e-02,  2.2340e-02],
          [-2.0434e-02, -8.8608e-03, -2.3855e-02],
          [ 8.3326e-03, -2.9381e-02,  1.5306e-02]],

         [[-3.0698e-02, -1.4653e-02, -2.7794e-02],
          [-3.9216e-03,  1.3329e-02,  1.0195e-02],
          [ 3.5083e-02,  1.1917e-03, -4.3002e-02]],

         ...,

         [[ 1.7056e-02,  3.2596e-02,  6.1331e-03],
          [ 3.6120e-02,  3.9152e-02, -7.9967e-03],
          [-8.3869e-03, -2.9532e-02,  3.5989e-02]],

         [[-4.5236e-03,  1.2588e-02, -1.8621e-02],
          [-5.3496e-03, -2.0835e-02,  1.5609e-02],
          [-4.0197e-02, -2.4014e-02, -3.4097e-02]],

         [[ 3.4819e-02, -1.7706e-02,  8.9078e-03],
          [ 1.0304e-02,  1.5716e-02,  4.2572e-03],
          [ 2.2228e-02, -1.3257e-02,  2.3929e-02]]],


        [[[ 3.0841e-02, -5.1202e-03, -4.6013e-02],
          [ 3.1229e-02,  3.7622e-02,  3.0198e-02],
          [-1.4860e-02, -6.8807e-03,  3.8915e-02]],

         [[ 1.3937e-02,  3.5155e-03,  9.2985e-03],
          [ 1.9373e-02, -2.9046e-02, -2.9409e-03],
          [ 2.9675e-02,  2.8190e-02, -1.5820e-02]],

         [[ 4.1792e-02,  2.1708e-03,  9.4968e-03],
          [ 1.4150e-02, -1.8375e-02,  3.8972e-02],
          [-3.3736e-02, -3.8382e-02,  8.1537e-03]],

         ...,

         [[-3.6489e-04,  3.0768e-02, -1.2520e-02],
          [-1.6142e-02, -4.4830e-02,  6.1340e-03],
          [-1.0154e-02, -2.5226e-02, -2.5291e-02]],

         [[-8.3018e-03,  2.4528e-02, -4.5658e-02],
          [ 3.2477e-02,  2.3804e-02, -2.2597e-02],
          [-2.7165e-02, -2.3732e-02,  1.0839e-02]],

         [[ 3.7879e-02, -3.2310e-03, -8.6888e-03],
          [-2.5360e-02, -2.5927e-02,  8.9714e-03],
          [ 2.7421e-03,  4.1952e-02,  3.4079e-02]]],


        ...,


        [[[ 3.7213e-02, -3.1025e-02, -3.4955e-02],
          [ 3.6322e-02, -3.4948e-02, -4.7064e-02],
          [-1.4982e-02, -2.2087e-02,  1.4551e-02]],

         [[ 3.8170e-02,  1.6740e-02,  2.8067e-02],
          [-4.1612e-02, -4.2872e-02,  3.4278e-03],
          [-4.7496e-02,  2.4296e-02, -3.8786e-02]],

         [[ 1.2336e-02,  4.5213e-02,  5.1518e-03],
          [-4.4598e-02, -4.2214e-02,  2.0880e-02],
          [-2.1419e-02,  1.7526e-02,  2.7815e-02]],

         ...,

         [[-2.5301e-02, -2.4213e-02,  1.8536e-02],
          [ 2.5237e-03,  2.9371e-02,  3.6635e-02],
          [ 3.4490e-02, -4.8476e-02,  1.1500e-02]],

         [[ 9.4215e-04, -2.3073e-02,  3.3358e-02],
          [-3.8678e-02, -5.8827e-03, -2.6094e-03],
          [-8.2051e-03, -4.4374e-02,  6.3868e-03]],

         [[-3.8607e-03, -2.0714e-02, -7.5638e-03],
          [ 3.1327e-02,  4.8806e-02, -2.6650e-03],
          [-2.2110e-03,  3.0965e-02,  7.6141e-03]]],


        [[[ 1.0610e-05, -4.1708e-02,  1.4435e-02],
          [-4.0779e-02,  2.0911e-02,  3.3495e-02],
          [ 1.3067e-02,  2.7708e-02, -1.1549e-02]],

         [[ 1.2961e-02, -2.0616e-02, -1.9108e-02],
          [ 7.3796e-03, -4.4786e-02,  1.6552e-03],
          [-2.1795e-02, -2.0348e-02, -5.0264e-03]],

         [[ 2.6369e-02,  2.5161e-02,  1.5079e-02],
          [-4.5150e-03,  5.8592e-03, -2.0199e-02],
          [ 3.3106e-02, -3.4994e-02, -1.1279e-03]],

         ...,

         [[-2.1580e-02,  4.2827e-02,  5.0424e-03],
          [ 2.5057e-02,  2.3692e-02, -3.0870e-02],
          [-4.1780e-03, -3.8853e-02,  1.4441e-02]],

         [[-2.3865e-02, -1.3994e-02, -3.5410e-02],
          [-3.6185e-03,  2.0923e-02,  2.7236e-03],
          [ 1.0262e-02, -3.1532e-02,  3.3050e-02]],

         [[ 1.1346e-02,  4.1718e-02,  1.1174e-02],
          [-2.0298e-03,  2.2982e-02,  1.8546e-02],
          [-1.3318e-02,  4.9742e-03,  2.2939e-02]]],


        [[[ 9.0037e-03, -9.5104e-03, -7.5996e-03],
          [-1.6102e-02,  1.6250e-02, -7.6060e-03],
          [-3.9538e-02, -4.3255e-02,  4.2674e-02]],

         [[-2.5034e-02,  4.6229e-03, -1.8471e-02],
          [-3.2005e-02, -2.8388e-02,  7.4790e-04],
          [-3.3205e-02,  1.2140e-02,  1.0581e-02]],

         [[ 5.3864e-04,  1.3943e-02, -1.4991e-02],
          [ 6.4655e-03, -3.7256e-02, -8.0638e-03],
          [-2.3150e-02,  2.7762e-02, -4.7499e-02]],

         ...,

         [[ 4.0186e-02, -3.6252e-02, -3.3652e-02],
          [-1.0883e-02,  1.1819e-02,  4.0240e-02],
          [-1.4266e-02,  3.0615e-02, -3.6979e-02]],

         [[-1.2413e-02, -3.1274e-02, -1.1436e-02],
          [ 1.6741e-02,  2.1795e-02, -2.9640e-02],
          [ 3.0451e-02, -3.1135e-02,  2.9516e-02]],

         [[-2.8726e-03, -3.1696e-02, -2.7762e-02],
          [-2.1256e-02,  3.1941e-02,  6.0536e-04],
          [-1.5299e-02,  3.9546e-02,  6.2616e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([8.0376e+07, 3.1341e+06, 2.5961e+06, 2.4689e+06, 2.3611e+06, 1.4110e+06,
        1.2025e+06, 1.0888e+06, 1.0131e+06, 7.0003e+05, 6.8699e+05, 6.4547e+05,
        5.6468e+05, 5.3780e+05, 4.3549e+05, 3.2177e+05, 2.8823e+05, 2.7118e+05,
        2.4871e+05, 2.1878e+05, 2.0615e+05, 1.6277e+05, 1.5178e+05, 1.4326e+05,
        1.4016e+05, 1.2194e+05, 1.1426e+05, 1.0980e+05, 1.0489e+05, 9.3527e+04,
        8.8041e+04, 8.4483e+04, 7.7855e+04, 7.7606e+04, 6.8044e+04, 6.4307e+04,
        6.2866e+04, 6.0234e+04, 5.2163e+04, 5.0275e+04, 4.8256e+04, 4.3780e+04,
        4.2423e+04, 4.1444e+04, 3.9321e+04, 3.7499e+04, 3.6356e+04, 3.4268e+04,
        3.2980e+04, 3.1774e+04, 3.0767e+04, 3.0252e+04, 2.9476e+04, 2.9006e+04,
        2.8378e+04, 2.6886e+04, 2.6000e+04, 2.5471e+04, 2.5290e+04, 2.4370e+04,
        2.3589e+04, 2.2506e+04, 2.1853e+04, 2.1611e+04, 2.1206e+04, 2.0052e+04,
        1.9659e+04, 1.9187e+04, 1.8455e+04, 1.8162e+04, 1.7947e+04, 1.7232e+04,
        1.7139e+04, 1.6907e+04, 1.6743e+04, 1.6083e+04, 1.5853e+04, 1.5293e+04,
        1.5055e+04, 1.4949e+04, 1.4527e+04, 1.4329e+04, 1.4182e+04, 1.3673e+04,
        1.3400e+04, 1.3298e+04, 1.3139e+04, 1.2873e+04, 1.2555e+04, 1.2425e+04,
        1.2214e+04, 1.2057e+04, 1.1898e+04, 1.1532e+04, 1.1336e+04, 1.1069e+04,
        1.1026e+04, 1.0731e+04, 1.0634e+04, 1.0510e+04, 1.0334e+04, 1.0209e+04,
        1.0070e+04, 9.8632e+03, 9.7575e+03, 9.6867e+03, 9.3767e+03, 9.2690e+03,
        9.1233e+03, 9.0037e+03, 8.9407e+03, 8.8396e+03, 8.5907e+03, 8.5039e+03,
        8.3611e+03, 8.1855e+03, 8.1476e+03, 7.9859e+03, 7.9115e+03, 7.7103e+03,
        7.6659e+03, 7.5998e+03, 7.5530e+03, 7.4383e+03, 7.3406e+03, 7.2696e+03,
        7.1974e+03, 7.0410e+03, 6.8714e+03, 6.8100e+03, 6.7859e+03, 6.7024e+03,
        6.6658e+03, 6.6177e+03, 6.5054e+03, 6.4704e+03, 6.2683e+03, 6.2244e+03,
        6.1094e+03, 6.0498e+03, 5.9333e+03, 5.8675e+03, 5.7976e+03, 5.7067e+03,
        5.6339e+03, 5.5819e+03, 5.5599e+03, 5.4510e+03, 5.3890e+03, 5.3593e+03,
        5.2776e+03, 5.2319e+03, 5.1779e+03, 5.0633e+03, 4.9978e+03, 4.9675e+03,
        4.9378e+03, 4.8766e+03, 4.8006e+03, 4.7153e+03, 4.6944e+03, 4.6612e+03,
        4.6567e+03, 4.5852e+03, 4.4947e+03, 4.4680e+03, 4.4378e+03, 4.3829e+03,
        4.3208e+03, 4.2930e+03, 4.2654e+03, 4.2408e+03, 4.1779e+03, 4.1739e+03,
        4.1409e+03, 4.0976e+03, 4.0576e+03, 3.9976e+03, 3.9402e+03, 3.9081e+03,
        3.8549e+03, 3.8409e+03, 3.7977e+03, 3.7413e+03, 3.6995e+03, 3.6746e+03,
        3.6402e+03, 3.6045e+03, 3.5692e+03, 3.5687e+03, 3.5308e+03, 3.4922e+03,
        3.4561e+03, 3.4218e+03, 3.3848e+03, 3.3621e+03, 3.3296e+03, 3.2913e+03,
        3.2728e+03, 3.2360e+03, 3.2141e+03, 3.2021e+03, 3.1852e+03, 3.1303e+03,
        3.0873e+03, 3.0770e+03, 3.0388e+03, 3.0307e+03, 2.9914e+03, 2.9688e+03,
        2.9636e+03, 2.9308e+03, 2.9005e+03, 2.8840e+03, 2.8620e+03, 2.8081e+03,
        2.8065e+03, 2.7739e+03, 2.7469e+03, 2.7391e+03, 2.6815e+03, 2.6674e+03,
        2.6409e+03, 2.6286e+03, 2.6183e+03, 2.5951e+03, 2.5786e+03, 2.5503e+03,
        2.5268e+03, 2.4883e+03, 2.4769e+03, 2.4551e+03, 2.4459e+03, 2.4198e+03,
        2.3943e+03, 2.3686e+03, 2.3549e+03, 2.3443e+03, 2.3328e+03, 2.3213e+03,
        2.2926e+03, 2.2741e+03, 2.2584e+03, 2.2390e+03, 2.2339e+03, 2.2093e+03,
        2.1985e+03, 2.1813e+03, 2.1605e+03, 2.1243e+03, 2.1161e+03, 2.1060e+03,
        2.0983e+03, 2.0743e+03, 2.0551e+03, 2.0395e+03, 2.0136e+03, 2.0086e+03,
        1.9963e+03, 1.9898e+03, 1.9743e+03, 1.9651e+03, 1.9394e+03, 1.9230e+03,
        1.9161e+03, 1.8984e+03, 1.8826e+03, 1.8707e+03, 1.8563e+03, 1.8286e+03,
        1.8231e+03, 1.8083e+03, 1.7993e+03, 1.7831e+03, 1.7668e+03, 1.7607e+03,
        1.7481e+03, 1.7333e+03, 1.7242e+03, 1.7155e+03, 1.6984e+03, 1.6813e+03,
        1.6799e+03, 1.6566e+03, 1.6508e+03, 1.6430e+03, 1.6341e+03, 1.6239e+03,
        1.6090e+03, 1.5924e+03, 1.5862e+03, 1.5636e+03, 1.5565e+03, 1.5385e+03,
        1.5289e+03, 1.5259e+03, 1.5179e+03, 1.5039e+03, 1.4869e+03, 1.4810e+03,
        1.4757e+03, 1.4644e+03, 1.4529e+03, 1.4490e+03, 1.4280e+03, 1.4223e+03,
        1.4192e+03, 1.4073e+03, 1.4029e+03, 1.3898e+03, 1.3855e+03, 1.3701e+03,
        1.3639e+03, 1.3612e+03, 1.3540e+03, 1.3443e+03, 1.3202e+03, 1.3076e+03,
        1.3037e+03, 1.2952e+03, 1.2875e+03, 1.2815e+03, 1.2739e+03, 1.2678e+03,
        1.2603e+03, 1.2526e+03, 1.2352e+03, 1.2327e+03, 1.2236e+03, 1.2185e+03,
        1.2143e+03, 1.2098e+03, 1.2023e+03, 1.1919e+03, 1.1866e+03, 1.1672e+03,
        1.1635e+03, 1.1599e+03, 1.1456e+03, 1.1365e+03, 1.1357e+03, 1.1312e+03,
        1.1171e+03, 1.1134e+03, 1.1123e+03, 1.0907e+03, 1.0860e+03, 1.0839e+03,
        1.0764e+03, 1.0714e+03, 1.0645e+03, 1.0570e+03, 1.0477e+03, 1.0434e+03,
        1.0395e+03, 1.0280e+03, 1.0198e+03, 1.0130e+03, 1.0052e+03, 1.0017e+03,
        9.9416e+02, 9.9114e+02, 9.8340e+02, 9.7695e+02, 9.6917e+02, 9.6475e+02,
        9.6144e+02, 9.5171e+02, 9.5123e+02, 9.4472e+02, 9.4413e+02, 9.4133e+02,
        9.2838e+02, 9.2297e+02, 9.1513e+02, 9.1346e+02, 9.0708e+02, 8.9962e+02,
        8.9851e+02, 8.9157e+02, 8.8623e+02, 8.7626e+02, 8.7058e+02, 8.6356e+02,
        8.5718e+02, 8.5191e+02, 8.4804e+02, 8.4335e+02, 8.4149e+02, 8.2930e+02,
        8.2561e+02, 8.2327e+02, 8.1151e+02, 8.0742e+02, 8.0497e+02, 7.9444e+02,
        7.9321e+02, 7.8607e+02, 7.8419e+02, 7.7499e+02, 7.7110e+02, 7.6615e+02,
        7.5988e+02, 7.5447e+02, 7.5135e+02, 7.4634e+02, 7.4322e+02, 7.4169e+02,
        7.3503e+02, 7.2813e+02, 7.2629e+02, 7.2108e+02, 7.1549e+02, 7.0946e+02,
        7.0203e+02, 6.9929e+02, 6.9861e+02, 6.9423e+02, 6.9069e+02, 6.8428e+02,
        6.8136e+02, 6.7641e+02, 6.7246e+02, 6.6981e+02, 6.6665e+02, 6.5918e+02,
        6.5514e+02, 6.5157e+02, 6.4767e+02, 6.4200e+02, 6.4089e+02, 6.3805e+02,
        6.3158e+02, 6.2730e+02, 6.2093e+02, 6.1624e+02, 6.0917e+02, 6.0590e+02,
        6.0265e+02, 5.9737e+02, 5.9272e+02, 5.8968e+02, 5.8806e+02, 5.8431e+02,
        5.8075e+02, 5.7934e+02, 5.6842e+02, 5.6525e+02, 5.6340e+02, 5.6297e+02,
        5.5822e+02, 5.5360e+02, 5.5086e+02, 5.4694e+02, 5.4227e+02, 5.3844e+02,
        5.3580e+02, 5.3516e+02, 5.3285e+02, 5.2674e+02, 5.1863e+02, 5.1460e+02,
        5.0981e+02, 5.0899e+02, 5.0372e+02, 5.0059e+02, 4.9494e+02, 4.9333e+02,
        4.8995e+02, 4.8872e+02, 4.8606e+02, 4.8105e+02, 4.8023e+02, 4.7369e+02,
        4.7058e+02, 4.6516e+02, 4.6173e+02, 4.5918e+02, 4.5648e+02, 4.5405e+02,
        4.5302e+02, 4.4713e+02, 4.4424e+02, 4.4146e+02, 4.3769e+02, 4.3507e+02,
        4.2859e+02, 4.2583e+02, 4.2420e+02, 4.2219e+02, 4.1630e+02, 4.1292e+02,
        4.0942e+02, 4.0724e+02, 4.0266e+02, 3.9858e+02, 3.9378e+02, 3.8995e+02,
        3.8813e+02, 3.8421e+02, 3.7936e+02, 3.7189e+02, 3.6979e+02, 3.6543e+02,
        3.6271e+02, 3.5873e+02, 3.5292e+02, 3.5181e+02, 3.5022e+02, 3.4520e+02,
        3.4290e+02, 3.4121e+02, 3.3923e+02, 3.3843e+02, 3.3391e+02, 3.3154e+02,
        3.2607e+02, 3.2030e+02, 3.1769e+02, 3.1465e+02, 3.0751e+02, 3.0482e+02,
        3.0276e+02, 2.9950e+02, 2.9733e+02, 2.9562e+02, 2.9058e+02, 2.8646e+02,
        2.8303e+02, 2.7925e+02, 2.7756e+02, 2.7058e+02, 2.6550e+02, 2.6250e+02,
        2.5849e+02, 2.5671e+02, 2.5324e+02, 2.5230e+02, 2.4900e+02, 2.4598e+02,
        2.4143e+02, 2.3621e+02, 2.3312e+02, 2.3189e+02, 2.2955e+02, 2.2738e+02,
        2.2141e+02, 2.1716e+02, 2.1124e+02, 2.0926e+02, 2.0284e+02, 1.9994e+02,
        1.9627e+02, 1.9397e+02, 1.9026e+02, 1.8741e+02, 1.8170e+02, 1.7831e+02,
        1.7615e+02, 1.7252e+02, 1.6372e+02, 1.6147e+02, 1.5927e+02, 1.4902e+02,
        1.4093e+02, 1.3597e+02, 1.2892e+02, 1.2689e+02, 1.1983e+02, 1.1563e+02,
        1.1429e+02, 1.0979e+02, 1.0055e+02, 9.9060e+01, 8.0509e+01, 6.0739e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 139]) 

NULL SPACE BASIS :  tensor([[-0.0071,  0.0572,  0.0185,  ..., -0.0173,  0.0035, -0.0129],
        [ 0.0231,  0.0284,  0.0551,  ...,  0.0307,  0.0037,  0.0250],
        [-0.0159, -0.0437, -0.0543,  ..., -0.0162,  0.0027, -0.0152],
        ...,
        [-0.0712,  0.0032,  0.0415,  ...,  0.0263, -0.2043, -0.0426],
        [ 0.0143,  0.0188,  0.0089,  ..., -0.0572,  0.3348,  0.0460],
        [ 0.1194,  0.0312,  0.0482,  ...,  0.0420, -0.1575, -0.0163]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.6091e-02, -2.1956e-02, -7.0687e-03,  ...,  8.7994e-05,
         -2.8060e-07, -3.0664e-05],
        [-2.1956e-02,  4.2903e-02, -2.1444e-02,  ..., -1.2626e-03,
         -1.0838e-04,  1.1230e-04],
        [-7.0687e-03, -2.1444e-02,  3.3892e-02,  ..., -1.1888e-03,
          5.0982e-05,  3.4835e-04],
        ...,
        [ 8.7994e-05, -1.2626e-03, -1.1888e-03,  ...,  5.0113e-02,
         -2.3599e-02, -1.0995e-02],
        [-2.8060e-07, -1.0838e-04,  5.0982e-05,  ..., -2.3599e-02,
          5.4578e-02, -2.1803e-02],
        [-3.0664e-05,  1.1230e-04,  3.4835e-04,  ..., -1.0995e-02,
         -2.1803e-02,  4.8775e-02]], device='cuda:0') 

reserving basis 220/576; cond: 1098309.25, radio:0.0008006262360140681
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0185,  0.0400,  0.0323],
          [ 0.0039, -0.0031, -0.0084],
          [-0.0350,  0.0283,  0.0165]],

         [[-0.0421, -0.0197, -0.0226],
          [-0.0472, -0.0463, -0.0273],
          [-0.0237,  0.0054, -0.0062]],

         [[ 0.0180,  0.0240, -0.0498],
          [-0.0147, -0.0304, -0.0191],
          [ 0.0119,  0.0107,  0.0193]],

         ...,

         [[-0.0555,  0.0320,  0.0446],
          [-0.0299, -0.0065, -0.0154],
          [ 0.0238, -0.0232, -0.0335]],

         [[ 0.0111, -0.0285,  0.0348],
          [ 0.0404, -0.0261, -0.0034],
          [-0.0189, -0.0329,  0.0198]],

         [[-0.0267, -0.0208,  0.0058],
          [ 0.0170,  0.0148, -0.0233],
          [-0.0260,  0.0036,  0.0037]]],


        [[[-0.0300, -0.0202, -0.0117],
          [ 0.0360,  0.0209,  0.0291],
          [-0.0137,  0.0462,  0.0014]],

         [[-0.0390, -0.0332, -0.0313],
          [ 0.0003, -0.0403, -0.0240],
          [-0.0149,  0.0200,  0.0223]],

         [[-0.0094,  0.0164, -0.0225],
          [-0.0358, -0.0302,  0.0141],
          [ 0.0295,  0.0116, -0.0190]],

         ...,

         [[ 0.0245, -0.0460,  0.0175],
          [-0.0124,  0.0121, -0.0202],
          [ 0.0271,  0.0253, -0.0072]],

         [[ 0.0118, -0.0402, -0.0077],
          [-0.0134, -0.0188, -0.0413],
          [-0.0041,  0.0056, -0.0005]],

         [[-0.0014, -0.0269, -0.0108],
          [-0.0312, -0.0276, -0.0162],
          [ 0.0345,  0.0212, -0.0177]]],


        [[[-0.0406, -0.0087, -0.0020],
          [-0.0576, -0.0422, -0.0351],
          [-0.0014,  0.0351,  0.0381]],

         [[-0.0188, -0.0097, -0.0040],
          [-0.0232, -0.0404, -0.0388],
          [-0.0172, -0.0180, -0.0100]],

         [[ 0.0073, -0.0146,  0.0048],
          [-0.0355,  0.0502, -0.0089],
          [-0.0053,  0.0268,  0.0384]],

         ...,

         [[-0.0451,  0.0227,  0.0365],
          [-0.0186, -0.0404,  0.0339],
          [-0.0117, -0.0294,  0.0129]],

         [[ 0.0269,  0.0184,  0.0403],
          [ 0.0233, -0.0300, -0.0223],
          [-0.0344, -0.0499,  0.0196]],

         [[-0.0286,  0.0497,  0.0325],
          [ 0.0201,  0.0083, -0.0122],
          [-0.0156, -0.0002, -0.0374]]],


        ...,


        [[[ 0.0344,  0.0055, -0.0061],
          [ 0.0396,  0.0448,  0.0014],
          [-0.0157,  0.0127, -0.0381]],

         [[-0.0307,  0.0016, -0.0282],
          [-0.0108,  0.0341,  0.0142],
          [-0.0044, -0.0177,  0.0208]],

         [[-0.0417, -0.0411, -0.0363],
          [-0.0272,  0.0355,  0.0433],
          [-0.0122, -0.0350, -0.0270]],

         ...,

         [[ 0.0337, -0.0062,  0.0338],
          [-0.0114,  0.0431, -0.0124],
          [-0.0394,  0.0259, -0.0236]],

         [[-0.0182, -0.0424, -0.0142],
          [-0.0117,  0.0188, -0.0350],
          [ 0.0244,  0.0227,  0.0132]],

         [[ 0.0217, -0.0155,  0.0108],
          [ 0.0328, -0.0040, -0.0267],
          [-0.0403,  0.0144, -0.0121]]],


        [[[-0.0377,  0.0238, -0.0476],
          [ 0.0031, -0.0421,  0.0208],
          [-0.0166, -0.0071,  0.0368]],

         [[ 0.0186, -0.0306,  0.0073],
          [-0.0080,  0.0273,  0.0052],
          [ 0.0243, -0.0115,  0.0163]],

         [[-0.0348, -0.0423, -0.0251],
          [-0.0233, -0.0176,  0.0184],
          [ 0.0088, -0.0333, -0.0364]],

         ...,

         [[-0.0091, -0.0164, -0.0306],
          [-0.0206,  0.0285, -0.0235],
          [-0.0409,  0.0344,  0.0319]],

         [[-0.0300, -0.0107, -0.0294],
          [ 0.0367,  0.0024, -0.0162],
          [ 0.0301,  0.0324, -0.0206]],

         [[ 0.0152, -0.0264, -0.0073],
          [-0.0402, -0.0345,  0.0386],
          [ 0.0382,  0.0224,  0.0391]]],


        [[[-0.0389,  0.0326, -0.0044],
          [-0.0339,  0.0353, -0.0080],
          [-0.0381, -0.0227,  0.0298]],

         [[-0.0035, -0.0260,  0.0090],
          [ 0.0187,  0.0020, -0.0097],
          [-0.0152, -0.0024,  0.0313]],

         [[-0.0222, -0.0370, -0.0067],
          [-0.0013, -0.0007,  0.0026],
          [ 0.0350, -0.0264,  0.0025]],

         ...,

         [[ 0.0123, -0.0165, -0.0195],
          [-0.0200,  0.0308, -0.0447],
          [ 0.0001,  0.0155, -0.0298]],

         [[ 0.0360, -0.0064,  0.0315],
          [ 0.0050, -0.0212, -0.0383],
          [-0.0207,  0.0085, -0.0363]],

         [[ 0.0102,  0.0416, -0.0181],
          [-0.0315,  0.0423,  0.0362],
          [ 0.0266,  0.0158,  0.0076]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([8.1636e+07, 3.1730e+06, 2.7638e+06, 2.2981e+06, 1.9192e+06, 1.3666e+06,
        1.2478e+06, 9.9128e+05, 9.4432e+05, 7.3055e+05, 6.5976e+05, 6.2758e+05,
        4.8747e+05, 3.7140e+05, 2.0373e+05, 1.8545e+05, 1.6898e+05, 1.5677e+05,
        1.4830e+05, 1.3357e+05, 1.2786e+05, 1.1255e+05, 1.0570e+05, 9.7841e+04,
        9.0505e+04, 8.0888e+04, 8.0110e+04, 7.7216e+04, 7.3453e+04, 7.1391e+04,
        6.8689e+04, 6.2158e+04, 6.1026e+04, 5.9884e+04, 5.3575e+04, 5.0934e+04,
        4.9443e+04, 4.7720e+04, 4.5514e+04, 4.4278e+04, 4.3058e+04, 4.0721e+04,
        4.0349e+04, 3.8350e+04, 3.7590e+04, 3.5750e+04, 3.4430e+04, 3.2732e+04,
        3.2166e+04, 3.1402e+04, 2.9601e+04, 2.9066e+04, 2.6774e+04, 2.5717e+04,
        2.4948e+04, 2.3306e+04, 2.2808e+04, 2.2232e+04, 2.1789e+04, 2.1172e+04,
        1.9856e+04, 1.9461e+04, 1.8836e+04, 1.8592e+04, 1.8078e+04, 1.7670e+04,
        1.7307e+04, 1.7098e+04, 1.6420e+04, 1.6276e+04, 1.6204e+04, 1.5091e+04,
        1.4748e+04, 1.4465e+04, 1.3980e+04, 1.3718e+04, 1.3259e+04, 1.3221e+04,
        1.3133e+04, 1.2848e+04, 1.2573e+04, 1.2293e+04, 1.1908e+04, 1.1707e+04,
        1.1553e+04, 1.1259e+04, 1.1144e+04, 1.0926e+04, 1.0686e+04, 1.0489e+04,
        1.0470e+04, 1.0270e+04, 1.0161e+04, 9.9678e+03, 9.5189e+03, 9.4097e+03,
        8.9897e+03, 8.8639e+03, 8.8232e+03, 8.7295e+03, 8.5057e+03, 8.4255e+03,
        8.2323e+03, 8.0948e+03, 7.9931e+03, 7.8779e+03, 7.7053e+03, 7.6170e+03,
        7.3641e+03, 7.3370e+03, 7.1661e+03, 7.1148e+03, 6.9489e+03, 6.7749e+03,
        6.7499e+03, 6.6016e+03, 6.5574e+03, 6.3914e+03, 6.3126e+03, 6.2213e+03,
        6.0977e+03, 5.9619e+03, 5.9445e+03, 5.8123e+03, 5.7349e+03, 5.6948e+03,
        5.5839e+03, 5.5510e+03, 5.5092e+03, 5.3840e+03, 5.2961e+03, 5.2184e+03,
        5.1295e+03, 5.0386e+03, 5.0305e+03, 4.9328e+03, 4.8288e+03, 4.6988e+03,
        4.6965e+03, 4.5680e+03, 4.5296e+03, 4.5109e+03, 4.4468e+03, 4.4344e+03,
        4.3877e+03, 4.3183e+03, 4.2486e+03, 4.1718e+03, 4.1505e+03, 4.0863e+03,
        4.0682e+03, 4.0037e+03, 3.9144e+03, 3.8597e+03, 3.8348e+03, 3.8143e+03,
        3.7434e+03, 3.6902e+03, 3.6527e+03, 3.6056e+03, 3.5641e+03, 3.5432e+03,
        3.5101e+03, 3.5052e+03, 3.4817e+03, 3.4784e+03, 3.4433e+03, 3.4200e+03,
        3.3082e+03, 3.2914e+03, 3.2417e+03, 3.2111e+03, 3.1851e+03, 3.1700e+03,
        3.1413e+03, 3.1321e+03, 3.0921e+03, 3.0522e+03, 3.0078e+03, 2.9628e+03,
        2.9376e+03, 2.9244e+03, 2.8896e+03, 2.8759e+03, 2.8143e+03, 2.8099e+03,
        2.7966e+03, 2.7815e+03, 2.7449e+03, 2.7278e+03, 2.6972e+03, 2.6678e+03,
        2.6337e+03, 2.6272e+03, 2.5474e+03, 2.5332e+03, 2.4881e+03, 2.4660e+03,
        2.4355e+03, 2.4210e+03, 2.3852e+03, 2.3630e+03, 2.3511e+03, 2.3340e+03,
        2.3315e+03, 2.3040e+03, 2.2632e+03, 2.2352e+03, 2.2219e+03, 2.2092e+03,
        2.1790e+03, 2.1669e+03, 2.1481e+03, 2.1414e+03, 2.1292e+03, 2.1145e+03,
        2.0946e+03, 2.0813e+03, 2.0627e+03, 2.0497e+03, 2.0339e+03, 2.0182e+03,
        1.9925e+03, 1.9822e+03, 1.9603e+03, 1.9245e+03, 1.9219e+03, 1.8916e+03,
        1.8705e+03, 1.8517e+03, 1.8445e+03, 1.8319e+03, 1.8167e+03, 1.8003e+03,
        1.8000e+03, 1.7730e+03, 1.7616e+03, 1.7420e+03, 1.7362e+03, 1.7204e+03,
        1.7183e+03, 1.7061e+03, 1.6816e+03, 1.6574e+03, 1.6538e+03, 1.6372e+03,
        1.6289e+03, 1.6126e+03, 1.6046e+03, 1.5832e+03, 1.5808e+03, 1.5464e+03,
        1.5410e+03, 1.5298e+03, 1.5170e+03, 1.5117e+03, 1.4937e+03, 1.4923e+03,
        1.4815e+03, 1.4583e+03, 1.4558e+03, 1.4528e+03, 1.4272e+03, 1.4137e+03,
        1.4040e+03, 1.3942e+03, 1.3838e+03, 1.3794e+03, 1.3703e+03, 1.3554e+03,
        1.3423e+03, 1.3354e+03, 1.3342e+03, 1.3174e+03, 1.3130e+03, 1.2973e+03,
        1.2915e+03, 1.2791e+03, 1.2666e+03, 1.2597e+03, 1.2495e+03, 1.2476e+03,
        1.2427e+03, 1.2303e+03, 1.2071e+03, 1.2051e+03, 1.1959e+03, 1.1862e+03,
        1.1755e+03, 1.1735e+03, 1.1703e+03, 1.1580e+03, 1.1566e+03, 1.1423e+03,
        1.1395e+03, 1.1329e+03, 1.1287e+03, 1.1202e+03, 1.1060e+03, 1.0934e+03,
        1.0893e+03, 1.0826e+03, 1.0734e+03, 1.0647e+03, 1.0556e+03, 1.0510e+03,
        1.0471e+03, 1.0381e+03, 1.0292e+03, 1.0181e+03, 1.0143e+03, 1.0080e+03,
        9.9572e+02, 9.8284e+02, 9.8110e+02, 9.8006e+02, 9.7183e+02, 9.7124e+02,
        9.6645e+02, 9.5130e+02, 9.3984e+02, 9.3707e+02, 9.2710e+02, 9.1737e+02,
        9.0646e+02, 9.0398e+02, 8.9796e+02, 8.8397e+02, 8.8300e+02, 8.7722e+02,
        8.7278e+02, 8.6414e+02, 8.5959e+02, 8.5580e+02, 8.4819e+02, 8.4662e+02,
        8.3710e+02, 8.3395e+02, 8.2844e+02, 8.2626e+02, 8.2096e+02, 8.1026e+02,
        8.0518e+02, 8.0133e+02, 7.9821e+02, 7.9219e+02, 7.8738e+02, 7.8020e+02,
        7.6730e+02, 7.6534e+02, 7.6077e+02, 7.5997e+02, 7.5405e+02, 7.5151e+02,
        7.4697e+02, 7.4379e+02, 7.3941e+02, 7.3768e+02, 7.2587e+02, 7.2248e+02,
        7.2009e+02, 7.1760e+02, 7.1427e+02, 6.9987e+02, 6.9711e+02, 6.9296e+02,
        6.8581e+02, 6.7943e+02, 6.7505e+02, 6.7371e+02, 6.6856e+02, 6.6283e+02,
        6.5485e+02, 6.5198e+02, 6.4921e+02, 6.4353e+02, 6.4142e+02, 6.3571e+02,
        6.3299e+02, 6.2930e+02, 6.2760e+02, 6.2290e+02, 6.1903e+02, 6.1532e+02,
        6.1191e+02, 6.0557e+02, 6.0380e+02, 5.9673e+02, 5.9387e+02, 5.9136e+02,
        5.8358e+02, 5.8014e+02, 5.7640e+02, 5.7150e+02, 5.6882e+02, 5.6817e+02,
        5.6639e+02, 5.6258e+02, 5.5945e+02, 5.5300e+02, 5.5083e+02, 5.4778e+02,
        5.4273e+02, 5.3883e+02, 5.3753e+02, 5.3303e+02, 5.2826e+02, 5.2707e+02,
        5.2362e+02, 5.2201e+02, 5.1932e+02, 5.1388e+02, 5.1089e+02, 5.0680e+02,
        5.0597e+02, 5.0117e+02, 4.9840e+02, 4.9521e+02, 4.9350e+02, 4.8852e+02,
        4.8228e+02, 4.7828e+02, 4.7332e+02, 4.7241e+02, 4.6754e+02, 4.6534e+02,
        4.6455e+02, 4.6091e+02, 4.5764e+02, 4.5591e+02, 4.5285e+02, 4.5176e+02,
        4.4806e+02, 4.4582e+02, 4.4260e+02, 4.4014e+02, 4.3713e+02, 4.3198e+02,
        4.2830e+02, 4.2512e+02, 4.2379e+02, 4.2172e+02, 4.1986e+02, 4.1474e+02,
        4.1306e+02, 4.1193e+02, 4.1021e+02, 4.0396e+02, 4.0314e+02, 3.9947e+02,
        3.9886e+02, 3.9580e+02, 3.9498e+02, 3.9081e+02, 3.8906e+02, 3.8598e+02,
        3.8392e+02, 3.7829e+02, 3.7724e+02, 3.7385e+02, 3.7246e+02, 3.6965e+02,
        3.6531e+02, 3.6336e+02, 3.6113e+02, 3.5712e+02, 3.5477e+02, 3.5070e+02,
        3.4898e+02, 3.4560e+02, 3.4318e+02, 3.4169e+02, 3.3847e+02, 3.3549e+02,
        3.3474e+02, 3.3255e+02, 3.3155e+02, 3.2844e+02, 3.2488e+02, 3.2314e+02,
        3.1839e+02, 3.1606e+02, 3.1481e+02, 3.1341e+02, 3.0966e+02, 3.0867e+02,
        3.0698e+02, 3.0402e+02, 3.0206e+02, 2.9865e+02, 2.9691e+02, 2.9487e+02,
        2.9011e+02, 2.8969e+02, 2.8642e+02, 2.8456e+02, 2.8394e+02, 2.8139e+02,
        2.7932e+02, 2.7734e+02, 2.7321e+02, 2.7184e+02, 2.6831e+02, 2.6697e+02,
        2.6602e+02, 2.6026e+02, 2.5861e+02, 2.5642e+02, 2.5401e+02, 2.5182e+02,
        2.4991e+02, 2.4820e+02, 2.4628e+02, 2.4465e+02, 2.4173e+02, 2.3938e+02,
        2.3686e+02, 2.3391e+02, 2.3206e+02, 2.3115e+02, 2.2752e+02, 2.2501e+02,
        2.2315e+02, 2.1972e+02, 2.1761e+02, 2.1717e+02, 2.1604e+02, 2.1508e+02,
        2.1402e+02, 2.1214e+02, 2.0861e+02, 2.0702e+02, 2.0496e+02, 2.0410e+02,
        1.9954e+02, 1.9736e+02, 1.9524e+02, 1.9249e+02, 1.9037e+02, 1.9012e+02,
        1.8773e+02, 1.8671e+02, 1.8310e+02, 1.8185e+02, 1.7883e+02, 1.7724e+02,
        1.7628e+02, 1.7543e+02, 1.7468e+02, 1.6989e+02, 1.6947e+02, 1.6810e+02,
        1.6334e+02, 1.6234e+02, 1.6075e+02, 1.6010e+02, 1.5545e+02, 1.5294e+02,
        1.5008e+02, 1.4443e+02, 1.4348e+02, 1.4097e+02, 1.3745e+02, 1.3570e+02,
        1.3088e+02, 1.2977e+02, 1.2392e+02, 1.2042e+02, 1.1682e+02, 1.1331e+02,
        1.0859e+02, 1.0532e+02, 1.0438e+02, 1.0003e+02, 7.8807e+01, 7.4329e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 220]) 

NULL SPACE BASIS :  tensor([[ 8.0063e-02, -7.2986e-02,  1.3928e-02,  ..., -6.7165e-04,
          2.0735e-02,  9.1298e-05],
        [-7.0526e-02, -7.7429e-03, -2.2720e-02,  ...,  3.0076e-03,
         -2.8715e-02, -9.5333e-05],
        [-1.0792e-01,  2.9367e-02,  1.3942e-02,  ..., -1.4830e-03,
          9.5065e-03,  1.1463e-03],
        ...,
        [ 5.5151e-02,  3.6149e-02,  2.7738e-02,  ..., -2.8865e-03,
          7.8823e-04,  1.3823e-02],
        [-2.8162e-02,  1.5572e-02, -1.2568e-02,  ...,  2.4527e-02,
         -2.2172e-03, -1.7197e-02],
        [-1.1940e-02, -1.1518e-02, -5.0312e-02,  ..., -2.0314e-02,
         -3.8552e-03,  1.3015e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0211, -0.0164, -0.0002,  ...,  0.0003,  0.0002,  0.0008],
        [-0.0164,  0.0317, -0.0136,  ..., -0.0018,  0.0013, -0.0011],
        [-0.0002, -0.0136,  0.0171,  ...,  0.0011, -0.0014, -0.0001],
        ...,
        [ 0.0003, -0.0018,  0.0011,  ...,  0.0197, -0.0109, -0.0025],
        [ 0.0002,  0.0013, -0.0014,  ..., -0.0109,  0.0259, -0.0113],
        [ 0.0008, -0.0011, -0.0001,  ..., -0.0025, -0.0113,  0.0214]],
       device='cuda:0') 

reserving basis 297/576; cond: 307607.875, radio:0.003718053922057152
PARAMETER       :  Parameter containing:
tensor([[[[-0.0271, -0.0042,  0.0188],
          [-0.0239, -0.0281, -0.0134],
          [ 0.0114,  0.0122,  0.0080]],

         [[ 0.0011,  0.0155,  0.0060],
          [-0.0050,  0.0004,  0.0176],
          [-0.0197, -0.0166, -0.0482]],

         [[ 0.0018,  0.0432,  0.0432],
          [-0.0042, -0.0370, -0.0035],
          [-0.0351, -0.0049, -0.0249]],

         ...,

         [[-0.0182,  0.0436,  0.0039],
          [-0.0271, -0.0099,  0.0422],
          [-0.0142, -0.0400, -0.0175]],

         [[ 0.0291,  0.0160, -0.0454],
          [ 0.0286,  0.0068,  0.0456],
          [-0.0337,  0.0220, -0.0209]],

         [[ 0.0349,  0.0215,  0.0215],
          [-0.0364, -0.0097,  0.0258],
          [ 0.0307, -0.0150, -0.0263]]],


        [[[-0.0270, -0.0237, -0.0468],
          [ 0.0089, -0.0092,  0.0083],
          [-0.0534,  0.0076,  0.0018]],

         [[-0.0337,  0.0190,  0.0530],
          [ 0.0011,  0.0223, -0.0120],
          [-0.0132,  0.0298,  0.0456]],

         [[-0.0020, -0.0167,  0.0308],
          [-0.0015, -0.0324,  0.0355],
          [ 0.0349,  0.0344, -0.0039]],

         ...,

         [[-0.0073,  0.0214, -0.0223],
          [-0.0275,  0.0231,  0.0145],
          [-0.0390,  0.0276, -0.0426]],

         [[-0.0097,  0.0215, -0.0207],
          [-0.0194, -0.0295, -0.0358],
          [-0.0439, -0.0103, -0.0400]],

         [[ 0.0231, -0.0375, -0.0348],
          [-0.0205, -0.0339, -0.0165],
          [-0.0226,  0.0015,  0.0095]]],


        [[[-0.0058, -0.0052, -0.0169],
          [-0.0005, -0.0448, -0.0391],
          [-0.0411, -0.0017, -0.0142]],

         [[-0.0254, -0.0224, -0.0199],
          [ 0.0002, -0.0274, -0.0125],
          [ 0.0169, -0.0263,  0.0015]],

         [[ 0.0469,  0.0422,  0.0025],
          [ 0.0294,  0.0049, -0.0399],
          [ 0.0118,  0.0081, -0.0012]],

         ...,

         [[ 0.0133, -0.0136, -0.0014],
          [ 0.0152,  0.0172, -0.0401],
          [-0.0344,  0.0099, -0.0132]],

         [[ 0.0112,  0.0059, -0.0249],
          [-0.0281,  0.0095,  0.0257],
          [ 0.0053, -0.0214, -0.0003]],

         [[ 0.0185, -0.0118, -0.0312],
          [ 0.0114, -0.0342, -0.0330],
          [ 0.0310, -0.0158,  0.0010]]],


        ...,


        [[[ 0.0118,  0.0317,  0.0151],
          [-0.0010, -0.0181, -0.0183],
          [ 0.0485,  0.0233, -0.0549]],

         [[ 0.0042, -0.0256,  0.0029],
          [-0.0317,  0.0268, -0.0213],
          [ 0.0161, -0.0259,  0.0437]],

         [[ 0.0252, -0.0211,  0.0426],
          [ 0.0334, -0.0394, -0.0400],
          [ 0.0170, -0.0104, -0.0032]],

         ...,

         [[-0.0144, -0.0139,  0.0048],
          [ 0.0145,  0.0091,  0.0139],
          [ 0.0020, -0.0105,  0.0024]],

         [[ 0.0140,  0.0213, -0.0468],
          [-0.0310, -0.0193, -0.0034],
          [ 0.0109,  0.0075,  0.0138]],

         [[ 0.0314,  0.0065,  0.0299],
          [ 0.0175,  0.0434,  0.0031],
          [ 0.0403, -0.0101,  0.0212]]],


        [[[-0.0132,  0.0204,  0.0343],
          [ 0.0162,  0.0251,  0.0071],
          [-0.0045,  0.0277, -0.0520]],

         [[ 0.0356, -0.0146, -0.0272],
          [-0.0374,  0.0060, -0.0355],
          [-0.0398, -0.0040,  0.0395]],

         [[ 0.0209, -0.0426, -0.0235],
          [-0.0344,  0.0272, -0.0160],
          [-0.0346,  0.0156, -0.0029]],

         ...,

         [[-0.0169, -0.0285,  0.0150],
          [ 0.0138,  0.0385,  0.0094],
          [-0.0208, -0.0341, -0.0068]],

         [[-0.0206,  0.0375, -0.0241],
          [ 0.0063, -0.0153,  0.0381],
          [ 0.0466, -0.0026, -0.0107]],

         [[-0.0232,  0.0065,  0.0287],
          [-0.0335, -0.0222,  0.0101],
          [ 0.0115,  0.0113, -0.0025]]],


        [[[ 0.0375,  0.0431,  0.0005],
          [ 0.0207, -0.0117,  0.0249],
          [ 0.0081, -0.0346, -0.0497]],

         [[ 0.0465, -0.0050,  0.0322],
          [ 0.0204,  0.0345,  0.0092],
          [ 0.0001, -0.0092,  0.0373]],

         [[-0.0224,  0.0001, -0.0436],
          [-0.0025, -0.0093,  0.0394],
          [ 0.0215, -0.0189,  0.0272]],

         ...,

         [[-0.0087,  0.0372, -0.0351],
          [-0.0095, -0.0379,  0.0278],
          [-0.0373,  0.0078,  0.0013]],

         [[ 0.0035, -0.0349, -0.0163],
          [-0.0365,  0.0263, -0.0218],
          [-0.0238, -0.0403,  0.0070]],

         [[ 0.0411, -0.0047,  0.0353],
          [ 0.0348, -0.0143,  0.0240],
          [ 0.0002,  0.0250,  0.0236]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([9.7906e+07, 2.6640e+06, 2.3193e+06, 2.0370e+06, 1.7890e+06, 1.2504e+06,
        1.2131e+06, 1.0430e+06, 9.0848e+05, 7.7279e+05, 7.4081e+05, 5.7982e+05,
        5.5160e+05, 4.4734e+05, 3.7557e+05, 3.1346e+05, 2.8069e+05, 2.2602e+05,
        2.1278e+05, 2.0173e+05, 1.9050e+05, 1.7937e+05, 1.7785e+05, 1.5654e+05,
        1.4857e+05, 1.3928e+05, 1.3182e+05, 1.2704e+05, 1.1697e+05, 1.0733e+05,
        1.0220e+05, 9.9680e+04, 9.2509e+04, 8.9891e+04, 7.8576e+04, 7.6170e+04,
        7.4255e+04, 6.9494e+04, 6.6589e+04, 6.5829e+04, 6.4827e+04, 6.1616e+04,
        5.8817e+04, 5.4590e+04, 5.2267e+04, 5.0277e+04, 4.8475e+04, 4.8211e+04,
        4.7006e+04, 4.5988e+04, 4.5042e+04, 4.2205e+04, 4.0501e+04, 3.9976e+04,
        3.8375e+04, 3.7866e+04, 3.6895e+04, 3.6507e+04, 3.5875e+04, 3.5260e+04,
        3.4181e+04, 3.3247e+04, 3.2128e+04, 2.9943e+04, 2.9723e+04, 2.9068e+04,
        2.8444e+04, 2.8327e+04, 2.7892e+04, 2.6249e+04, 2.6171e+04, 2.5853e+04,
        2.5486e+04, 2.5104e+04, 2.4438e+04, 2.4260e+04, 2.3641e+04, 2.3231e+04,
        2.2801e+04, 2.1901e+04, 2.1787e+04, 2.1631e+04, 2.0727e+04, 2.0551e+04,
        2.0354e+04, 1.9902e+04, 1.9694e+04, 1.9214e+04, 1.8596e+04, 1.8552e+04,
        1.8236e+04, 1.7860e+04, 1.7712e+04, 1.7119e+04, 1.6724e+04, 1.6522e+04,
        1.6200e+04, 1.6169e+04, 1.5567e+04, 1.5501e+04, 1.5417e+04, 1.5160e+04,
        1.4909e+04, 1.4829e+04, 1.4688e+04, 1.4343e+04, 1.4046e+04, 1.3764e+04,
        1.3748e+04, 1.3637e+04, 1.3270e+04, 1.3161e+04, 1.2920e+04, 1.2649e+04,
        1.2564e+04, 1.2497e+04, 1.2225e+04, 1.2175e+04, 1.2076e+04, 1.1990e+04,
        1.1691e+04, 1.1624e+04, 1.1516e+04, 1.1405e+04, 1.1271e+04, 1.1106e+04,
        1.1042e+04, 1.0841e+04, 1.0653e+04, 1.0549e+04, 1.0415e+04, 1.0317e+04,
        1.0287e+04, 1.0103e+04, 1.0037e+04, 9.9394e+03, 9.7720e+03, 9.6383e+03,
        9.5500e+03, 9.4908e+03, 9.3967e+03, 9.3417e+03, 9.2709e+03, 9.1236e+03,
        9.0044e+03, 8.8626e+03, 8.7715e+03, 8.7667e+03, 8.6271e+03, 8.5894e+03,
        8.5137e+03, 8.3539e+03, 8.2667e+03, 8.1566e+03, 8.1136e+03, 8.0328e+03,
        7.8647e+03, 7.8252e+03, 7.7733e+03, 7.6287e+03, 7.6062e+03, 7.5352e+03,
        7.4785e+03, 7.4318e+03, 7.3965e+03, 7.3687e+03, 7.2605e+03, 7.2145e+03,
        7.0873e+03, 7.0501e+03, 7.0334e+03, 6.9510e+03, 6.8848e+03, 6.8527e+03,
        6.8230e+03, 6.7582e+03, 6.7393e+03, 6.6864e+03, 6.6065e+03, 6.5767e+03,
        6.5093e+03, 6.4607e+03, 6.4410e+03, 6.3913e+03, 6.3789e+03, 6.3038e+03,
        6.2384e+03, 6.1277e+03, 6.0869e+03, 6.0241e+03, 5.9956e+03, 5.9757e+03,
        5.8553e+03, 5.8139e+03, 5.7969e+03, 5.7651e+03, 5.7032e+03, 5.6299e+03,
        5.5849e+03, 5.5520e+03, 5.5462e+03, 5.4887e+03, 5.4551e+03, 5.4124e+03,
        5.3324e+03, 5.3174e+03, 5.2894e+03, 5.2275e+03, 5.2174e+03, 5.1846e+03,
        5.1613e+03, 5.0872e+03, 5.0310e+03, 4.9742e+03, 4.9607e+03, 4.9086e+03,
        4.8926e+03, 4.8517e+03, 4.8198e+03, 4.8000e+03, 4.7151e+03, 4.6918e+03,
        4.6663e+03, 4.5999e+03, 4.5682e+03, 4.5196e+03, 4.5111e+03, 4.4676e+03,
        4.4288e+03, 4.4105e+03, 4.3973e+03, 4.3713e+03, 4.3282e+03, 4.2985e+03,
        4.2742e+03, 4.2363e+03, 4.1745e+03, 4.1665e+03, 4.1542e+03, 4.1265e+03,
        4.1011e+03, 4.0925e+03, 4.0563e+03, 4.0519e+03, 4.0283e+03, 3.9596e+03,
        3.9471e+03, 3.9107e+03, 3.8903e+03, 3.8764e+03, 3.8372e+03, 3.7949e+03,
        3.7728e+03, 3.7575e+03, 3.7443e+03, 3.7095e+03, 3.6901e+03, 3.6396e+03,
        3.6280e+03, 3.6031e+03, 3.5876e+03, 3.5613e+03, 3.5483e+03, 3.5300e+03,
        3.4990e+03, 3.4851e+03, 3.4472e+03, 3.4214e+03, 3.4057e+03, 3.3779e+03,
        3.3537e+03, 3.3472e+03, 3.3310e+03, 3.3105e+03, 3.2690e+03, 3.2647e+03,
        3.2473e+03, 3.2199e+03, 3.2066e+03, 3.1800e+03, 3.1458e+03, 3.1413e+03,
        3.1257e+03, 3.1028e+03, 3.0764e+03, 3.0533e+03, 3.0488e+03, 3.0189e+03,
        3.0061e+03, 2.9973e+03, 2.9776e+03, 2.9553e+03, 2.9526e+03, 2.9350e+03,
        2.9258e+03, 2.8857e+03, 2.8690e+03, 2.8613e+03, 2.8332e+03, 2.8192e+03,
        2.7984e+03, 2.7917e+03, 2.7456e+03, 2.7452e+03, 2.7305e+03, 2.7231e+03,
        2.7044e+03, 2.6909e+03, 2.6775e+03, 2.6627e+03, 2.6500e+03, 2.6375e+03,
        2.6231e+03, 2.6085e+03, 2.5821e+03, 2.5631e+03, 2.5531e+03, 2.5335e+03,
        2.5252e+03, 2.5087e+03, 2.4952e+03, 2.4892e+03, 2.4674e+03, 2.4510e+03,
        2.4458e+03, 2.4216e+03, 2.4190e+03, 2.4105e+03, 2.3943e+03, 2.3802e+03,
        2.3691e+03, 2.3400e+03, 2.3328e+03, 2.3275e+03, 2.3220e+03, 2.3009e+03,
        2.2944e+03, 2.2802e+03, 2.2731e+03, 2.2492e+03, 2.2404e+03, 2.2279e+03,
        2.2116e+03, 2.1980e+03, 2.1889e+03, 2.1783e+03, 2.1707e+03, 2.1535e+03,
        2.1345e+03, 2.1301e+03, 2.1213e+03, 2.1112e+03, 2.1011e+03, 2.0885e+03,
        2.0754e+03, 2.0694e+03, 2.0577e+03, 2.0445e+03, 2.0302e+03, 2.0191e+03,
        2.0144e+03, 1.9905e+03, 1.9864e+03, 1.9814e+03, 1.9716e+03, 1.9608e+03,
        1.9549e+03, 1.9372e+03, 1.9209e+03, 1.9128e+03, 1.9085e+03, 1.8893e+03,
        1.8828e+03, 1.8796e+03, 1.8686e+03, 1.8596e+03, 1.8506e+03, 1.8363e+03,
        1.8244e+03, 1.8201e+03, 1.8137e+03, 1.7918e+03, 1.7782e+03, 1.7739e+03,
        1.7637e+03, 1.7618e+03, 1.7572e+03, 1.7397e+03, 1.7300e+03, 1.7158e+03,
        1.7123e+03, 1.7038e+03, 1.6887e+03, 1.6807e+03, 1.6723e+03, 1.6668e+03,
        1.6580e+03, 1.6534e+03, 1.6397e+03, 1.6344e+03, 1.6301e+03, 1.6206e+03,
        1.6107e+03, 1.6056e+03, 1.5984e+03, 1.5976e+03, 1.5792e+03, 1.5627e+03,
        1.5548e+03, 1.5528e+03, 1.5441e+03, 1.5318e+03, 1.5221e+03, 1.5122e+03,
        1.5038e+03, 1.5031e+03, 1.4924e+03, 1.4819e+03, 1.4771e+03, 1.4654e+03,
        1.4581e+03, 1.4496e+03, 1.4347e+03, 1.4262e+03, 1.4197e+03, 1.4117e+03,
        1.4025e+03, 1.3992e+03, 1.3912e+03, 1.3889e+03, 1.3826e+03, 1.3774e+03,
        1.3613e+03, 1.3581e+03, 1.3500e+03, 1.3418e+03, 1.3284e+03, 1.3247e+03,
        1.3230e+03, 1.3100e+03, 1.2977e+03, 1.2948e+03, 1.2938e+03, 1.2779e+03,
        1.2707e+03, 1.2645e+03, 1.2565e+03, 1.2498e+03, 1.2452e+03, 1.2326e+03,
        1.2260e+03, 1.2167e+03, 1.2139e+03, 1.2084e+03, 1.2034e+03, 1.1984e+03,
        1.1967e+03, 1.1859e+03, 1.1813e+03, 1.1719e+03, 1.1654e+03, 1.1501e+03,
        1.1465e+03, 1.1373e+03, 1.1346e+03, 1.1298e+03, 1.1215e+03, 1.1154e+03,
        1.1054e+03, 1.0985e+03, 1.0900e+03, 1.0831e+03, 1.0764e+03, 1.0725e+03,
        1.0685e+03, 1.0609e+03, 1.0551e+03, 1.0484e+03, 1.0389e+03, 1.0312e+03,
        1.0284e+03, 1.0246e+03, 1.0180e+03, 1.0133e+03, 1.0029e+03, 9.9407e+02,
        9.8946e+02, 9.8316e+02, 9.7652e+02, 9.6907e+02, 9.6094e+02, 9.5138e+02,
        9.4019e+02, 9.3609e+02, 9.3183e+02, 9.2581e+02, 9.2028e+02, 9.1763e+02,
        9.0010e+02, 8.9657e+02, 8.9492e+02, 8.8931e+02, 8.8269e+02, 8.8125e+02,
        8.7582e+02, 8.6293e+02, 8.6019e+02, 8.5447e+02, 8.4900e+02, 8.4083e+02,
        8.3433e+02, 8.2900e+02, 8.1376e+02, 8.1149e+02, 8.0470e+02, 7.9977e+02,
        7.9710e+02, 7.8158e+02, 7.7505e+02, 7.7184e+02, 7.6671e+02, 7.6138e+02,
        7.5675e+02, 7.4695e+02, 7.3733e+02, 7.3055e+02, 7.2519e+02, 7.1973e+02,
        7.1005e+02, 7.0354e+02, 6.9877e+02, 6.9673e+02, 6.9103e+02, 6.8262e+02,
        6.7911e+02, 6.7436e+02, 6.6172e+02, 6.5876e+02, 6.5752e+02, 6.4576e+02,
        6.3935e+02, 6.3734e+02, 6.2965e+02, 6.2642e+02, 6.1559e+02, 6.1409e+02,
        6.0509e+02, 6.0323e+02, 5.9375e+02, 5.9099e+02, 5.7328e+02, 5.6460e+02,
        5.5886e+02, 5.5429e+02, 5.4796e+02, 5.3890e+02, 5.3700e+02, 5.2985e+02,
        5.1924e+02, 5.0868e+02, 5.0207e+02, 4.9088e+02, 4.8118e+02, 4.7752e+02,
        4.6998e+02, 4.6027e+02, 4.4991e+02, 4.4373e+02, 4.2913e+02, 4.2288e+02,
        4.1011e+02, 3.9691e+02, 3.8516e+02, 3.7356e+02, 3.4799e+02, 3.1828e+02],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 297]) 

NULL SPACE BASIS :  tensor([[ 0.0520,  0.0658,  0.0603,  ...,  0.0130,  0.0025,  0.0088],
        [ 0.0404,  0.0209,  0.0197,  ..., -0.0143, -0.0179, -0.0150],
        [ 0.0352, -0.0668,  0.0180,  ...,  0.0101,  0.0129,  0.0098],
        ...,
        [ 0.0173, -0.0189,  0.0366,  ..., -0.0173, -0.0310, -0.0058],
        [ 0.0224, -0.0726,  0.0465,  ...,  0.0163,  0.0522, -0.0065],
        [-0.0433, -0.0516,  0.0115,  ..., -0.0067, -0.0169,  0.0068]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0260, -0.0100, -0.0016,  ...,  0.0008, -0.0011, -0.0010],
        [-0.0100,  0.0309, -0.0093,  ..., -0.0010, -0.0008, -0.0007],
        [-0.0016, -0.0093,  0.0213,  ...,  0.0008, -0.0005, -0.0004],
        ...,
        [ 0.0008, -0.0010,  0.0008,  ...,  0.0315, -0.0086, -0.0040],
        [-0.0011, -0.0008, -0.0005,  ..., -0.0086,  0.0365, -0.0091],
        [-0.0010, -0.0007, -0.0004,  ..., -0.0040, -0.0091,  0.0292]],
       device='cuda:0') 

reserving basis 273/576; cond: 512282.34375, radio:0.0020967477466911077
PARAMETER       :  Parameter containing:
tensor([[[[-0.0280, -0.0124, -0.0245],
          [ 0.0324, -0.0164, -0.0239],
          [ 0.0163, -0.0062,  0.0453]],

         [[ 0.0356, -0.0075, -0.0259],
          [ 0.0314,  0.0064,  0.0354],
          [ 0.0087, -0.0200,  0.0029]],

         [[-0.0077, -0.0223, -0.0189],
          [ 0.0256, -0.0457,  0.0225],
          [-0.0335,  0.0032, -0.0109]],

         ...,

         [[-0.0171,  0.0153,  0.0251],
          [ 0.0146, -0.0136, -0.0102],
          [ 0.0146, -0.0143, -0.0327]],

         [[ 0.0024,  0.0313,  0.0428],
          [ 0.0033,  0.0248,  0.0243],
          [ 0.0066,  0.0100,  0.0340]],

         [[-0.0400, -0.0224, -0.0139],
          [-0.0215, -0.0224,  0.0237],
          [-0.0361, -0.0026, -0.0377]]],


        [[[ 0.0058, -0.0175, -0.0501],
          [-0.0118, -0.0239, -0.0448],
          [-0.0053, -0.0185,  0.0122]],

         [[-0.0142, -0.0054,  0.0090],
          [ 0.0204,  0.0399,  0.0254],
          [-0.0119, -0.0045,  0.0085]],

         [[-0.0206,  0.0113, -0.0197],
          [ 0.0289, -0.0330,  0.0383],
          [-0.0128,  0.0072, -0.0012]],

         ...,

         [[ 0.0042,  0.0421, -0.0138],
          [-0.0015, -0.0007, -0.0204],
          [-0.0261, -0.0105, -0.0122]],

         [[-0.0516,  0.0034,  0.0151],
          [ 0.0139, -0.0018, -0.0111],
          [ 0.0483, -0.0094,  0.0004]],

         [[-0.0426,  0.0094,  0.0144],
          [-0.0146,  0.0280, -0.0098],
          [ 0.0226,  0.0362,  0.0374]]],


        [[[-0.0387,  0.0232,  0.0133],
          [ 0.0300,  0.0174,  0.0310],
          [ 0.0333,  0.0157,  0.0226]],

         [[-0.0059,  0.0047, -0.0075],
          [ 0.0232, -0.0215,  0.0200],
          [-0.0083, -0.0369, -0.0262]],

         [[ 0.0425, -0.0158, -0.0096],
          [ 0.0116,  0.0364, -0.0061],
          [-0.0203,  0.0126,  0.0381]],

         ...,

         [[ 0.0191, -0.0245,  0.0235],
          [ 0.0432,  0.0125, -0.0199],
          [ 0.0343,  0.0249, -0.0136]],

         [[ 0.0209, -0.0401, -0.0174],
          [-0.0298,  0.0278,  0.0040],
          [ 0.0031,  0.0294,  0.0218]],

         [[-0.0237,  0.0076,  0.0081],
          [-0.0244,  0.0037, -0.0341],
          [ 0.0042,  0.0168,  0.0048]]],


        ...,


        [[[-0.0153, -0.0268, -0.0358],
          [-0.0013,  0.0123, -0.0494],
          [-0.0103, -0.0097, -0.0387]],

         [[-0.0215,  0.0269,  0.0383],
          [-0.0154, -0.0303, -0.0334],
          [-0.0116,  0.0386,  0.0426]],

         [[-0.0022, -0.0184,  0.0398],
          [-0.0055,  0.0302,  0.0395],
          [ 0.0012,  0.0422,  0.0177]],

         ...,

         [[ 0.0306, -0.0437, -0.0358],
          [ 0.0160,  0.0212, -0.0091],
          [ 0.0015, -0.0049,  0.0086]],

         [[ 0.0224,  0.0002,  0.0087],
          [ 0.0264,  0.0100, -0.0412],
          [ 0.0455,  0.0370, -0.0384]],

         [[ 0.0027,  0.0426, -0.0004],
          [ 0.0306, -0.0338, -0.0070],
          [ 0.0028, -0.0333,  0.0184]]],


        [[[ 0.0367, -0.0115,  0.0454],
          [-0.0484,  0.0198, -0.0141],
          [ 0.0310, -0.0135, -0.0207]],

         [[ 0.0266, -0.0238, -0.0115],
          [ 0.0236,  0.0088, -0.0223],
          [ 0.0515, -0.0233,  0.0455]],

         [[-0.0302, -0.0362,  0.0276],
          [ 0.0165, -0.0374, -0.0538],
          [-0.0383,  0.0001, -0.0115]],

         ...,

         [[-0.0200,  0.0189,  0.0319],
          [-0.0350,  0.0095, -0.0090],
          [ 0.0312, -0.0338,  0.0364]],

         [[-0.0049, -0.0165, -0.0319],
          [ 0.0232, -0.0107,  0.0124],
          [-0.0003,  0.0348,  0.0129]],

         [[-0.0308, -0.0153, -0.0417],
          [ 0.0278, -0.0417, -0.0295],
          [-0.0365,  0.0436,  0.0171]]],


        [[[-0.0010, -0.0054,  0.0164],
          [-0.0391, -0.0058, -0.0074],
          [ 0.0248,  0.0044,  0.0359]],

         [[-0.0236,  0.0278,  0.0152],
          [ 0.0396,  0.0263,  0.0189],
          [-0.0048,  0.0237,  0.0121]],

         [[-0.0150, -0.0260,  0.0346],
          [-0.0375,  0.0202,  0.0167],
          [ 0.0021, -0.0269, -0.0332]],

         ...,

         [[ 0.0515, -0.0201,  0.0275],
          [ 0.0330, -0.0391,  0.0273],
          [-0.0080, -0.0417, -0.0198]],

         [[-0.0164, -0.0005, -0.0185],
          [-0.0310, -0.0486, -0.0391],
          [ 0.0224,  0.0186, -0.0146]],

         [[-0.0093, -0.0135,  0.0471],
          [ 0.0237,  0.0185,  0.0026],
          [ 0.0116,  0.0308, -0.0014]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.8893e+07, 9.5593e+05, 8.7075e+05, 5.6313e+05, 4.0014e+05, 2.2802e+05,
        2.1299e+05, 1.4292e+05, 1.2729e+05, 9.8485e+04, 6.3403e+04, 5.2691e+04,
        4.9178e+04, 4.4458e+04, 4.3749e+04, 4.0532e+04, 3.9174e+04, 3.7031e+04,
        3.3482e+04, 3.0742e+04, 2.9144e+04, 2.7030e+04, 2.5960e+04, 2.3344e+04,
        2.2858e+04, 2.1611e+04, 2.0402e+04, 1.9748e+04, 1.7832e+04, 1.7058e+04,
        1.6300e+04, 1.5905e+04, 1.5027e+04, 1.4231e+04, 1.3293e+04, 1.2755e+04,
        1.1707e+04, 1.1569e+04, 1.0576e+04, 1.0109e+04, 9.3079e+03, 8.8792e+03,
        8.8068e+03, 8.5328e+03, 8.3440e+03, 8.0395e+03, 7.7687e+03, 7.2177e+03,
        7.0341e+03, 6.9415e+03, 6.6896e+03, 6.2227e+03, 6.0553e+03, 5.9796e+03,
        5.7917e+03, 5.6642e+03, 5.4856e+03, 5.2638e+03, 5.1984e+03, 5.0864e+03,
        5.0042e+03, 4.9046e+03, 4.8103e+03, 4.7243e+03, 4.6753e+03, 4.4538e+03,
        4.4076e+03, 4.2529e+03, 4.1860e+03, 4.0657e+03, 3.9208e+03, 3.8738e+03,
        3.8034e+03, 3.7705e+03, 3.6129e+03, 3.5537e+03, 3.5134e+03, 3.4843e+03,
        3.3877e+03, 3.2855e+03, 3.2586e+03, 3.1626e+03, 3.0674e+03, 2.9919e+03,
        2.9668e+03, 2.8967e+03, 2.8679e+03, 2.8208e+03, 2.7675e+03, 2.7195e+03,
        2.6490e+03, 2.6317e+03, 2.5776e+03, 2.5585e+03, 2.4944e+03, 2.4630e+03,
        2.3968e+03, 2.3643e+03, 2.3203e+03, 2.3105e+03, 2.2685e+03, 2.1869e+03,
        2.1354e+03, 2.1105e+03, 2.0979e+03, 2.0836e+03, 2.0358e+03, 2.0279e+03,
        1.9870e+03, 1.9754e+03, 1.9371e+03, 1.9110e+03, 1.8949e+03, 1.8729e+03,
        1.8335e+03, 1.7838e+03, 1.7742e+03, 1.7489e+03, 1.7234e+03, 1.7046e+03,
        1.6840e+03, 1.6727e+03, 1.6560e+03, 1.6341e+03, 1.6210e+03, 1.6023e+03,
        1.5905e+03, 1.5630e+03, 1.5537e+03, 1.5347e+03, 1.5052e+03, 1.4825e+03,
        1.4704e+03, 1.4548e+03, 1.4304e+03, 1.4279e+03, 1.4021e+03, 1.3838e+03,
        1.3729e+03, 1.3520e+03, 1.3436e+03, 1.3359e+03, 1.3028e+03, 1.2889e+03,
        1.2832e+03, 1.2792e+03, 1.2443e+03, 1.2413e+03, 1.2276e+03, 1.2232e+03,
        1.2037e+03, 1.1853e+03, 1.1827e+03, 1.1608e+03, 1.1484e+03, 1.1382e+03,
        1.1347e+03, 1.1173e+03, 1.1111e+03, 1.1019e+03, 1.0950e+03, 1.0796e+03,
        1.0591e+03, 1.0484e+03, 1.0458e+03, 1.0278e+03, 1.0217e+03, 1.0137e+03,
        9.9846e+02, 9.9310e+02, 9.8600e+02, 9.7665e+02, 9.7046e+02, 9.6530e+02,
        9.5389e+02, 9.3408e+02, 9.2877e+02, 9.1889e+02, 9.0999e+02, 9.0322e+02,
        8.9686e+02, 8.8794e+02, 8.8729e+02, 8.7974e+02, 8.6758e+02, 8.6242e+02,
        8.5496e+02, 8.4589e+02, 8.4076e+02, 8.3218e+02, 8.1605e+02, 8.1042e+02,
        8.0823e+02, 7.9690e+02, 7.9420e+02, 7.7971e+02, 7.7497e+02, 7.6543e+02,
        7.5964e+02, 7.5181e+02, 7.4994e+02, 7.4434e+02, 7.3984e+02, 7.3465e+02,
        7.2618e+02, 7.2329e+02, 7.1679e+02, 7.1041e+02, 7.0603e+02, 6.9463e+02,
        6.9300e+02, 6.8784e+02, 6.8336e+02, 6.7854e+02, 6.7539e+02, 6.7021e+02,
        6.6209e+02, 6.5727e+02, 6.5629e+02, 6.4967e+02, 6.4836e+02, 6.4426e+02,
        6.4356e+02, 6.4020e+02, 6.3701e+02, 6.2431e+02, 6.2316e+02, 6.1814e+02,
        6.1144e+02, 6.0721e+02, 6.0022e+02, 5.9722e+02, 5.8944e+02, 5.8730e+02,
        5.8347e+02, 5.7854e+02, 5.7125e+02, 5.6942e+02, 5.6674e+02, 5.6437e+02,
        5.6012e+02, 5.5658e+02, 5.5451e+02, 5.4972e+02, 5.4267e+02, 5.3711e+02,
        5.3438e+02, 5.3080e+02, 5.2870e+02, 5.2560e+02, 5.1968e+02, 5.1784e+02,
        5.1233e+02, 5.0579e+02, 5.0294e+02, 4.9834e+02, 4.9671e+02, 4.9582e+02,
        4.9102e+02, 4.8426e+02, 4.8116e+02, 4.8021e+02, 4.7472e+02, 4.7031e+02,
        4.6891e+02, 4.6531e+02, 4.6241e+02, 4.5952e+02, 4.5857e+02, 4.5549e+02,
        4.5390e+02, 4.4945e+02, 4.4901e+02, 4.4439e+02, 4.4346e+02, 4.4194e+02,
        4.3659e+02, 4.3209e+02, 4.3109e+02, 4.2983e+02, 4.2728e+02, 4.2390e+02,
        4.2170e+02, 4.1907e+02, 4.1567e+02, 4.1393e+02, 4.1100e+02, 4.0607e+02,
        4.0276e+02, 4.0155e+02, 4.0070e+02, 3.9713e+02, 3.9481e+02, 3.9227e+02,
        3.8837e+02, 3.8429e+02, 3.8352e+02, 3.8286e+02, 3.8186e+02, 3.7836e+02,
        3.7589e+02, 3.7125e+02, 3.7069e+02, 3.6769e+02, 3.6681e+02, 3.6248e+02,
        3.6091e+02, 3.5989e+02, 3.5844e+02, 3.5594e+02, 3.5224e+02, 3.5127e+02,
        3.4883e+02, 3.4677e+02, 3.4481e+02, 3.4277e+02, 3.4063e+02, 3.3785e+02,
        3.3774e+02, 3.3680e+02, 3.3467e+02, 3.3253e+02, 3.3058e+02, 3.2935e+02,
        3.2700e+02, 3.2570e+02, 3.2304e+02, 3.1863e+02, 3.1638e+02, 3.1438e+02,
        3.1361e+02, 3.1301e+02, 3.1065e+02, 3.0942e+02, 3.0777e+02, 3.0549e+02,
        3.0194e+02, 3.0027e+02, 2.9922e+02, 2.9786e+02, 2.9680e+02, 2.9561e+02,
        2.9327e+02, 2.9145e+02, 2.8949e+02, 2.8543e+02, 2.8477e+02, 2.8339e+02,
        2.8243e+02, 2.7986e+02, 2.7846e+02, 2.7790e+02, 2.7431e+02, 2.7331e+02,
        2.7217e+02, 2.6989e+02, 2.6889e+02, 2.6849e+02, 2.6540e+02, 2.6507e+02,
        2.6305e+02, 2.6259e+02, 2.6105e+02, 2.5961e+02, 2.5841e+02, 2.5724e+02,
        2.5554e+02, 2.5377e+02, 2.5251e+02, 2.5128e+02, 2.4970e+02, 2.4934e+02,
        2.4652e+02, 2.4472e+02, 2.4239e+02, 2.4131e+02, 2.3962e+02, 2.3885e+02,
        2.3626e+02, 2.3552e+02, 2.3545e+02, 2.3460e+02, 2.3266e+02, 2.3087e+02,
        2.3057e+02, 2.2903e+02, 2.2818e+02, 2.2757e+02, 2.2667e+02, 2.2619e+02,
        2.2365e+02, 2.2142e+02, 2.2043e+02, 2.1896e+02, 2.1870e+02, 2.1843e+02,
        2.1578e+02, 2.1473e+02, 2.1362e+02, 2.1330e+02, 2.1202e+02, 2.0994e+02,
        2.0815e+02, 2.0773e+02, 2.0648e+02, 2.0527e+02, 2.0324e+02, 2.0229e+02,
        2.0077e+02, 2.0043e+02, 1.9965e+02, 1.9918e+02, 1.9802e+02, 1.9642e+02,
        1.9559e+02, 1.9532e+02, 1.9471e+02, 1.9277e+02, 1.9195e+02, 1.9047e+02,
        1.8978e+02, 1.8844e+02, 1.8706e+02, 1.8666e+02, 1.8530e+02, 1.8463e+02,
        1.8350e+02, 1.8244e+02, 1.8095e+02, 1.8011e+02, 1.7920e+02, 1.7809e+02,
        1.7728e+02, 1.7589e+02, 1.7569e+02, 1.7491e+02, 1.7444e+02, 1.7257e+02,
        1.7219e+02, 1.7178e+02, 1.7045e+02, 1.6982e+02, 1.6841e+02, 1.6796e+02,
        1.6737e+02, 1.6677e+02, 1.6578e+02, 1.6506e+02, 1.6405e+02, 1.6172e+02,
        1.6156e+02, 1.6021e+02, 1.5907e+02, 1.5870e+02, 1.5682e+02, 1.5660e+02,
        1.5518e+02, 1.5293e+02, 1.5284e+02, 1.5181e+02, 1.5125e+02, 1.5020e+02,
        1.4978e+02, 1.4923e+02, 1.4757e+02, 1.4613e+02, 1.4548e+02, 1.4491e+02,
        1.4410e+02, 1.4371e+02, 1.4293e+02, 1.4164e+02, 1.4088e+02, 1.3935e+02,
        1.3887e+02, 1.3739e+02, 1.3691e+02, 1.3558e+02, 1.3550e+02, 1.3474e+02,
        1.3401e+02, 1.3301e+02, 1.3186e+02, 1.3089e+02, 1.3039e+02, 1.2844e+02,
        1.2766e+02, 1.2665e+02, 1.2583e+02, 1.2506e+02, 1.2437e+02, 1.2338e+02,
        1.2216e+02, 1.2093e+02, 1.2010e+02, 1.1953e+02, 1.1838e+02, 1.1715e+02,
        1.1633e+02, 1.1559e+02, 1.1536e+02, 1.1422e+02, 1.1310e+02, 1.1274e+02,
        1.1128e+02, 1.1058e+02, 1.1004e+02, 1.0892e+02, 1.0816e+02, 1.0694e+02,
        1.0618e+02, 1.0590e+02, 1.0527e+02, 1.0444e+02, 1.0327e+02, 1.0292e+02,
        1.0244e+02, 1.0138e+02, 1.0050e+02, 1.0024e+02, 9.8715e+01, 9.8075e+01,
        9.7344e+01, 9.6995e+01, 9.6032e+01, 9.4867e+01, 9.4650e+01, 9.3828e+01,
        9.3156e+01, 9.1909e+01, 9.1007e+01, 9.0375e+01, 8.9792e+01, 8.9406e+01,
        8.8703e+01, 8.7909e+01, 8.6993e+01, 8.6219e+01, 8.5239e+01, 8.4752e+01,
        8.4102e+01, 8.2634e+01, 8.2195e+01, 8.1359e+01, 8.0242e+01, 7.9543e+01,
        7.9034e+01, 7.7277e+01, 7.6979e+01, 7.6529e+01, 7.5567e+01, 7.3645e+01,
        7.2537e+01, 7.1925e+01, 7.1073e+01, 6.9230e+01, 6.8104e+01, 6.7528e+01,
        6.6329e+01, 6.4928e+01, 6.3535e+01, 6.2857e+01, 6.0609e+01, 5.9369e+01,
        5.9155e+01, 5.6746e+01, 5.5808e+01, 5.5249e+01, 5.4084e+01, 5.2273e+01,
        5.0710e+01, 5.0170e+01, 4.9007e+01, 4.7474e+01, 4.0311e+01, 3.6880e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 273]) 

NULL SPACE BASIS :  tensor([[-0.0242,  0.0170, -0.0372,  ...,  0.0025, -0.0003,  0.0114],
        [-0.0165, -0.0352,  0.0527,  ..., -0.0052, -0.0021, -0.0126],
        [ 0.0167,  0.0302, -0.0546,  ...,  0.0043,  0.0061,  0.0023],
        ...,
        [-0.0717, -0.0155,  0.0088,  ...,  0.0089, -0.0046,  0.0023],
        [-0.0076,  0.0355, -0.0792,  ..., -0.0054,  0.0002,  0.0141],
        [-0.0138, -0.0291, -0.0043,  ...,  0.0022, -0.0006, -0.0119]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0192, -0.0168,  0.0026,  ..., -0.0013,  0.0015,  0.0001],
        [-0.0168,  0.0303, -0.0139,  ...,  0.0002, -0.0006, -0.0007],
        [ 0.0026, -0.0139,  0.0155,  ...,  0.0010, -0.0002,  0.0002],
        ...,
        [-0.0013,  0.0002,  0.0010,  ...,  0.0127, -0.0074, -0.0005],
        [ 0.0015, -0.0006, -0.0002,  ..., -0.0074,  0.0172, -0.0064],
        [ 0.0001, -0.0007,  0.0002,  ..., -0.0005, -0.0064,  0.0122]],
       device='cuda:0') 

reserving basis 711/1152; cond: 384678.0, radio:0.007276501972228289
PARAMETER       :  Parameter containing:
tensor([[[[ 1.3869e-02,  2.9657e-02, -1.8975e-02],
          [-5.5800e-04, -1.5696e-02,  1.0776e-02],
          [ 1.8418e-02,  1.9228e-02, -3.0460e-03]],

         [[-1.9401e-02, -2.2744e-02, -2.5611e-02],
          [-2.2463e-02, -2.2522e-02,  6.9875e-03],
          [-1.5796e-02,  8.6899e-03, -2.8865e-02]],

         [[ 8.5675e-04, -1.3327e-02,  4.1520e-03],
          [ 7.8816e-03,  1.5194e-02, -1.5460e-02],
          [-1.5323e-02, -2.5531e-02,  2.7073e-02]],

         ...,

         [[-1.3096e-02,  1.7601e-03,  5.3964e-03],
          [-2.9615e-02, -1.9767e-02, -5.8312e-04],
          [ 7.5269e-03,  1.6091e-02,  2.8857e-02]],

         [[ 1.0077e-02, -3.3277e-02, -1.8025e-02],
          [ 2.3078e-02,  1.5780e-02,  7.8492e-04],
          [ 1.5520e-02, -1.5108e-02,  3.4699e-02]],

         [[ 1.8753e-02,  6.7956e-03,  1.5527e-02],
          [-2.2441e-02, -3.3308e-02, -2.5799e-02],
          [-2.8835e-02,  1.9718e-02, -1.5423e-02]]],


        [[[ 2.6132e-02,  1.6438e-03,  7.8198e-03],
          [-1.4481e-02, -1.1410e-02,  2.2383e-02],
          [-2.6076e-02, -2.8949e-02, -1.5116e-02]],

         [[ 8.8683e-03, -2.2628e-02, -2.1918e-02],
          [ 9.6735e-03, -1.1860e-02,  2.1407e-03],
          [-2.6893e-02,  6.5520e-03, -1.2485e-02]],

         [[ 1.2318e-03, -1.7182e-02, -1.8797e-02],
          [-3.6229e-03,  1.4626e-02, -2.8908e-02],
          [ 2.5988e-02,  1.8077e-02,  2.3227e-03]],

         ...,

         [[ 4.9854e-03,  1.3634e-02, -4.3822e-03],
          [-2.0735e-02,  2.5903e-02,  2.6874e-02],
          [ 1.5203e-02,  2.0355e-02,  2.1054e-02]],

         [[ 2.4915e-02,  2.0860e-03,  7.8608e-03],
          [-7.8844e-03, -2.1611e-02, -2.6417e-02],
          [ 4.2223e-02, -1.9255e-02, -2.9068e-02]],

         [[-1.3073e-02, -1.6927e-02, -3.1109e-02],
          [-2.1721e-02, -1.2813e-02, -7.4300e-03],
          [ 8.8256e-03, -1.0742e-02, -1.7388e-03]]],


        [[[-1.7123e-02,  1.7842e-02, -2.9130e-02],
          [-5.0433e-03,  1.7284e-02, -2.7040e-03],
          [-8.8900e-04,  2.7066e-02,  1.5976e-02]],

         [[ 4.7189e-03, -1.2114e-02,  2.4194e-02],
          [-4.7611e-03, -1.0043e-02,  4.6460e-03],
          [-2.0960e-02,  5.3695e-03,  9.5149e-03]],

         [[-1.7248e-02, -1.5031e-02, -1.2531e-02],
          [-2.6950e-02, -2.7906e-02, -3.1569e-02],
          [-1.3898e-02, -1.8543e-02,  1.6131e-02]],

         ...,

         [[ 2.4522e-02,  7.9511e-03, -1.7635e-02],
          [-3.1562e-02,  1.5036e-02,  4.0708e-03],
          [ 1.9801e-02, -1.6295e-02, -3.2801e-02]],

         [[-4.0579e-02,  1.6398e-03, -1.1365e-02],
          [-1.6879e-02,  1.7023e-02,  8.1694e-03],
          [-1.3643e-02,  8.0566e-03, -1.4288e-02]],

         [[-1.4026e-02, -1.8928e-02,  1.0901e-02],
          [-4.6765e-03, -9.9404e-03,  5.4034e-03],
          [ 1.0257e-02, -1.7905e-02,  3.0575e-02]]],


        ...,


        [[[ 2.7628e-02,  9.1873e-04,  3.2454e-02],
          [ 2.4044e-02,  1.6503e-02, -2.9653e-02],
          [ 2.7577e-03, -1.0789e-03,  2.7134e-02]],

         [[ 8.9894e-03, -7.8319e-03,  1.2305e-02],
          [ 4.6896e-03, -2.4603e-02,  4.7440e-03],
          [-1.5310e-02, -7.5696e-03,  1.5708e-02]],

         [[ 1.6867e-02,  1.2636e-02, -8.6710e-03],
          [-1.3296e-02,  2.8000e-02,  6.3727e-03],
          [ 5.6121e-03, -7.4027e-03, -1.3601e-02]],

         ...,

         [[-2.9515e-02,  1.3602e-02, -1.5505e-02],
          [ 7.1142e-03, -2.7928e-02, -1.6130e-02],
          [-1.9526e-02,  2.9375e-03,  7.9605e-03]],

         [[ 1.8937e-02,  1.7863e-02,  2.7868e-02],
          [-2.2328e-02,  1.3182e-05,  1.5837e-02],
          [-5.6201e-03,  9.8806e-03,  4.7361e-03]],

         [[-1.4534e-02, -2.4922e-03,  2.3999e-02],
          [ 1.5693e-02,  9.4001e-03,  3.1094e-02],
          [ 7.8452e-03,  1.0138e-02,  1.9912e-02]]],


        [[[-2.6036e-03, -1.0653e-02,  5.9060e-03],
          [-2.7782e-02, -7.9563e-03,  2.0105e-02],
          [ 1.0623e-02, -2.6704e-02, -2.1928e-02]],

         [[-1.3346e-02, -1.5189e-02,  2.4588e-02],
          [-1.9558e-02, -3.4259e-02, -4.3295e-03],
          [-2.5264e-02, -8.4863e-04, -2.2456e-02]],

         [[-1.6537e-02, -2.7184e-03, -2.1136e-02],
          [-1.1939e-02,  1.5145e-02,  2.3682e-02],
          [ 7.6167e-03, -2.0725e-02,  1.9745e-02]],

         ...,

         [[-1.4572e-02,  1.1501e-02,  5.2769e-03],
          [-3.0461e-02, -2.1591e-02, -1.4459e-02],
          [-2.4124e-02, -5.0698e-02, -1.7781e-02]],

         [[-2.3527e-03, -6.5050e-03, -1.2335e-02],
          [-3.0679e-02,  3.0758e-02,  2.9884e-02],
          [-3.6969e-02,  2.2984e-02, -7.7668e-03]],

         [[ 2.0407e-02, -2.4064e-02, -9.8263e-03],
          [ 2.8399e-02, -1.7091e-02,  3.1123e-02],
          [-5.4203e-03,  1.7492e-02,  8.2833e-04]]],


        [[[ 3.8239e-03, -2.7952e-02,  2.7846e-02],
          [-2.5260e-02,  3.4978e-03, -3.6701e-02],
          [-2.8908e-03, -2.8692e-02, -1.9070e-02]],

         [[ 1.6067e-02,  1.5646e-02, -1.4737e-02],
          [ 4.8871e-03,  1.4418e-02,  1.0022e-02],
          [-1.6133e-02, -2.6169e-02,  2.3889e-02]],

         [[-9.9385e-03, -9.0936e-03,  6.6086e-03],
          [-1.9596e-02,  2.7524e-04,  2.4765e-02],
          [-1.7500e-02,  2.4654e-02, -5.1054e-03]],

         ...,

         [[ 2.7535e-02,  1.0207e-02, -8.2203e-03],
          [-1.6342e-02,  3.0279e-02,  1.4497e-02],
          [ 2.8484e-02, -1.0830e-02, -1.6694e-03]],

         [[-1.5134e-02,  1.0126e-02, -4.2345e-03],
          [-2.2455e-02,  3.1722e-02,  2.7913e-02],
          [ 1.6524e-02,  1.2679e-02,  7.0143e-03]],

         [[ 1.0916e-02,  3.8492e-02, -1.7626e-02],
          [-1.4456e-02,  1.2128e-02,  3.2289e-02],
          [ 4.5795e-03, -2.5509e-02, -1.8122e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.6734e+07, 1.4413e+06, 1.3611e+06,  ..., 1.2849e+02, 1.1292e+02,
        9.5493e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 711]) 

NULL SPACE BASIS :  tensor([[ 0.0140, -0.0048, -0.0156,  ...,  0.0187, -0.0126,  0.0089],
        [-0.0139,  0.0032,  0.0076,  ..., -0.0046,  0.0143, -0.0029],
        [-0.0503, -0.0422, -0.0432,  ..., -0.0152, -0.0031, -0.0027],
        ...,
        [-0.0481, -0.0176,  0.0231,  ...,  0.0034,  0.0064, -0.0056],
        [ 0.0306, -0.0079,  0.0111,  ...,  0.0049, -0.0175, -0.0065],
        [-0.0151,  0.0314,  0.0015,  ..., -0.0203,  0.0142,  0.0098]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.9013e-02, -1.3155e-03, -6.7118e-04,  ..., -2.0949e-06,
          5.7285e-04,  6.4400e-05],
        [-1.3155e-03,  3.0577e-02, -9.7429e-04,  ...,  9.0007e-05,
          1.3739e-04,  5.6763e-04],
        [-6.7118e-04, -9.7429e-04,  3.0117e-02,  ..., -7.3103e-05,
          2.2582e-04,  2.3310e-04],
        ...,
        [-2.0949e-06,  9.0007e-05, -7.3103e-05,  ...,  2.2041e-02,
         -3.2769e-03, -1.5408e-03],
        [ 5.7285e-04,  1.3739e-04,  2.2582e-04,  ..., -3.2769e-03,
          2.3307e-02, -3.0208e-03],
        [ 6.4400e-05,  5.6763e-04,  2.3310e-04,  ..., -1.5408e-03,
         -3.0208e-03,  2.4372e-02]], device='cuda:0') 

reserving basis 44/64; cond: 7211.65673828125, radio:0.0178081002086401
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0147]],

         [[ 0.1011]],

         [[ 0.0604]],

         ...,

         [[-0.0860]],

         [[-0.0877]],

         [[-0.0354]]],


        [[[ 0.0244]],

         [[-0.0250]],

         [[-0.0949]],

         ...,

         [[-0.0384]],

         [[ 0.0149]],

         [[ 0.0298]]],


        [[[ 0.0247]],

         [[-0.0706]],

         [[-0.0149]],

         ...,

         [[-0.1410]],

         [[ 0.0925]],

         [[ 0.0242]]],


        ...,


        [[[ 0.0230]],

         [[-0.0877]],

         [[-0.1022]],

         ...,

         [[-0.0366]],

         [[ 0.0596]],

         [[-0.0096]]],


        [[[ 0.0288]],

         [[-0.0485]],

         [[ 0.1364]],

         ...,

         [[ 0.0010]],

         [[ 0.0294]],

         [[ 0.0642]]],


        [[[-0.0020]],

         [[-0.0080]],

         [[ 0.0871]],

         ...,

         [[ 0.0017]],

         [[ 0.0545]],

         [[ 0.0874]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.3703e+06, 1.3533e+05, 1.1393e+05, 5.2855e+04, 3.4404e+04, 2.7554e+04,
        1.9649e+04, 1.6270e+04, 1.0426e+04, 9.1347e+03, 8.3782e+03, 7.4549e+03,
        6.8651e+03, 5.9777e+03, 5.5442e+03, 5.2150e+03, 4.6025e+03, 4.5172e+03,
        3.9639e+03, 3.7610e+03, 3.1522e+03, 3.0188e+03, 2.5782e+03, 2.4547e+03,
        2.3360e+03, 2.2679e+03, 2.1641e+03, 1.8923e+03, 1.7931e+03, 1.6770e+03,
        1.5622e+03, 1.5003e+03, 1.4616e+03, 1.4056e+03, 1.2939e+03, 1.2519e+03,
        1.1891e+03, 1.1565e+03, 1.1113e+03, 1.0658e+03, 1.0266e+03, 9.9516e+02,
        9.3389e+02, 8.8061e+02, 8.1529e+02, 8.0331e+02, 7.4595e+02, 7.0977e+02,
        6.8695e+02, 6.7879e+02, 6.5486e+02, 6.2426e+02, 6.0442e+02, 5.8829e+02,
        5.5248e+02, 5.3264e+02, 5.2377e+02, 4.8493e+02, 4.6302e+02, 4.5079e+02,
        4.1554e+02, 3.9421e+02, 3.7570e+02, 3.2867e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 44]) 

NULL SPACE BASIS :  tensor([[-0.1626,  0.1540,  0.1793,  ..., -0.0101, -0.1521, -0.0953],
        [-0.0442,  0.0915, -0.0588,  ...,  0.0569,  0.0302, -0.0398],
        [-0.0147,  0.1374,  0.0162,  ..., -0.2384,  0.2803,  0.0318],
        ...,
        [-0.1374, -0.1114, -0.0260,  ..., -0.0161,  0.0118,  0.0153],
        [ 0.0179, -0.0893,  0.0796,  ..., -0.0291, -0.0179, -0.0360],
        [ 0.2572, -0.0301,  0.0129,  ..., -0.0110,  0.0088,  0.0351]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0948,  0.0043,  0.0197,  ...,  0.0002, -0.0026, -0.0126],
        [ 0.0043,  0.1237, -0.0103,  ...,  0.0099, -0.0090, -0.0044],
        [ 0.0197, -0.0103,  0.1073,  ..., -0.0072,  0.0024,  0.0062],
        ...,
        [ 0.0002,  0.0099, -0.0072,  ...,  0.0349, -0.0312, -0.0037],
        [-0.0026, -0.0090,  0.0024,  ..., -0.0312,  0.1161,  0.0020],
        [-0.0126, -0.0044,  0.0062,  ..., -0.0037,  0.0020,  0.0853]],
       device='cuda:0') 

reserving basis 775/1152; cond: 308118.28125, radio:0.009050561115145683
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0112, -0.0042,  0.0095],
          [-0.0035,  0.0162,  0.0116],
          [-0.0103, -0.0120, -0.0123]],

         [[ 0.0053, -0.0163, -0.0216],
          [ 0.0269, -0.0013,  0.0010],
          [ 0.0069,  0.0200, -0.0164]],

         [[ 0.0070,  0.0001,  0.0061],
          [ 0.0150,  0.0166,  0.0158],
          [-0.0244, -0.0143,  0.0125]],

         ...,

         [[ 0.0163,  0.0059, -0.0009],
          [-0.0112,  0.0271,  0.0059],
          [ 0.0144, -0.0020, -0.0071]],

         [[ 0.0086,  0.0122,  0.0069],
          [ 0.0027,  0.0232,  0.0212],
          [ 0.0237,  0.0275, -0.0063]],

         [[ 0.0003,  0.0269,  0.0472],
          [-0.0183,  0.0145,  0.0201],
          [ 0.0059, -0.0206,  0.0171]]],


        [[[-0.0103,  0.0069, -0.0222],
          [ 0.0134, -0.0083,  0.0042],
          [ 0.0030, -0.0240,  0.0200]],

         [[-0.0036,  0.0173,  0.0242],
          [ 0.0211,  0.0171, -0.0003],
          [-0.0097,  0.0168,  0.0184]],

         [[ 0.0179,  0.0162,  0.0084],
          [ 0.0257, -0.0066,  0.0202],
          [-0.0160,  0.0013,  0.0088]],

         ...,

         [[-0.0029,  0.0171, -0.0172],
          [-0.0276, -0.0228, -0.0317],
          [-0.0228, -0.0233, -0.0242]],

         [[-0.0224,  0.0013,  0.0303],
          [-0.0156, -0.0175, -0.0028],
          [ 0.0134, -0.0111, -0.0184]],

         [[ 0.0231, -0.0242,  0.0140],
          [-0.0080, -0.0138, -0.0273],
          [ 0.0021,  0.0160, -0.0317]]],


        [[[ 0.0187,  0.0200,  0.0141],
          [-0.0058, -0.0296,  0.0279],
          [-0.0120, -0.0018, -0.0250]],

         [[-0.0097,  0.0059, -0.0151],
          [ 0.0323,  0.0065, -0.0054],
          [-0.0070, -0.0190,  0.0180]],

         [[-0.0308, -0.0154, -0.0226],
          [-0.0156, -0.0141,  0.0021],
          [ 0.0244, -0.0128, -0.0079]],

         ...,

         [[-0.0353, -0.0072, -0.0218],
          [-0.0070, -0.0344, -0.0264],
          [-0.0032, -0.0269, -0.0003]],

         [[ 0.0249, -0.0133,  0.0061],
          [ 0.0193, -0.0180, -0.0198],
          [-0.0028, -0.0295,  0.0109]],

         [[ 0.0107,  0.0142, -0.0008],
          [ 0.0065,  0.0106, -0.0116],
          [ 0.0043,  0.0235, -0.0128]]],


        ...,


        [[[-0.0157, -0.0239, -0.0351],
          [-0.0100, -0.0152,  0.0017],
          [ 0.0153,  0.0066,  0.0225]],

         [[-0.0317, -0.0070, -0.0248],
          [-0.0130,  0.0158,  0.0116],
          [-0.0127,  0.0313,  0.0280]],

         [[ 0.0293,  0.0062,  0.0011],
          [-0.0168,  0.0021, -0.0283],
          [ 0.0320,  0.0238,  0.0092]],

         ...,

         [[-0.0225, -0.0272,  0.0072],
          [ 0.0162, -0.0141,  0.0164],
          [-0.0110,  0.0351, -0.0049]],

         [[ 0.0117,  0.0357,  0.0104],
          [-0.0105, -0.0189,  0.0245],
          [ 0.0167, -0.0017, -0.0049]],

         [[ 0.0367,  0.0076,  0.0173],
          [ 0.0211, -0.0150,  0.0163],
          [-0.0111, -0.0339, -0.0374]]],


        [[[-0.0283, -0.0014,  0.0142],
          [ 0.0203,  0.0147, -0.0203],
          [ 0.0297,  0.0254, -0.0017]],

         [[ 0.0077, -0.0037,  0.0021],
          [ 0.0208,  0.0390,  0.0192],
          [ 0.0228,  0.0379, -0.0110]],

         [[-0.0196, -0.0163, -0.0278],
          [-0.0173, -0.0011, -0.0086],
          [-0.0027,  0.0053, -0.0274]],

         ...,

         [[-0.0254,  0.0161, -0.0250],
          [ 0.0168, -0.0113, -0.0056],
          [-0.0103,  0.0277,  0.0133]],

         [[-0.0269,  0.0024,  0.0148],
          [-0.0025, -0.0047,  0.0039],
          [ 0.0211,  0.0239,  0.0031]],

         [[-0.0287, -0.0077,  0.0095],
          [-0.0259,  0.0126,  0.0275],
          [-0.0302,  0.0015,  0.0147]]],


        [[[-0.0151,  0.0155, -0.0029],
          [-0.0038,  0.0117,  0.0047],
          [-0.0052, -0.0013,  0.0194]],

         [[-0.0007, -0.0212, -0.0191],
          [-0.0245, -0.0133, -0.0391],
          [ 0.0380,  0.0153,  0.0028]],

         [[ 0.0294,  0.0006, -0.0096],
          [-0.0176, -0.0139, -0.0160],
          [ 0.0261, -0.0074, -0.0215]],

         ...,

         [[ 0.0096, -0.0064,  0.0089],
          [-0.0211, -0.0121,  0.0058],
          [ 0.0139, -0.0004, -0.0123]],

         [[-0.0153, -0.0104,  0.0019],
          [ 0.0170,  0.0101, -0.0317],
          [-0.0169, -0.0070, -0.0097]],

         [[-0.0027, -0.0304, -0.0161],
          [ 0.0035,  0.0231, -0.0265],
          [ 0.0259,  0.0113,  0.0215]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.5508e+07, 1.3992e+06, 1.3276e+06,  ..., 1.4180e+02, 1.3915e+02,
        1.1524e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 775]) 

NULL SPACE BASIS :  tensor([[ 0.0416,  0.0027, -0.0113,  ..., -0.0044, -0.0035,  0.0182],
        [ 0.0226, -0.0227, -0.0283,  ...,  0.0097,  0.0067, -0.0171],
        [-0.0188,  0.0006, -0.0264,  ...,  0.0022, -0.0081,  0.0076],
        ...,
        [ 0.0075, -0.0107, -0.0004,  ...,  0.0004, -0.0002, -0.0032],
        [ 0.0181, -0.0078, -0.0239,  ..., -0.0003,  0.0064,  0.0049],
        [ 0.0378, -0.0398, -0.0317,  ...,  0.0003, -0.0057, -0.0002]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2034e-02, -5.5337e-03, -2.6242e-03,  ..., -3.4287e-04,
         -4.6818e-04,  4.1465e-04],
        [-5.5337e-03,  2.2897e-02, -5.2199e-03,  ..., -1.8150e-06,
          1.7222e-04, -1.3501e-04],
        [-2.6242e-03, -5.2199e-03,  2.1635e-02,  ..., -4.7800e-05,
         -1.1156e-05,  2.0066e-05],
        ...,
        [-3.4287e-04, -1.8150e-06, -4.7800e-05,  ...,  1.9454e-02,
         -6.7003e-03, -2.7536e-03],
        [-4.6818e-04,  1.7222e-04, -1.1156e-05,  ..., -6.7003e-03,
          2.0178e-02, -6.7131e-03],
        [ 4.1465e-04, -1.3501e-04,  2.0066e-05,  ..., -2.7536e-03,
         -6.7131e-03,  1.9745e-02]], device='cuda:0') 

reserving basis 305/1152; cond: 872784.8125, radio:0.001787984394468367
PARAMETER       :  Parameter containing:
tensor([[[[ 2.2755e-03, -1.3035e-02, -7.2285e-03],
          [ 1.7282e-02,  1.7928e-02,  3.7333e-02],
          [-1.0911e-02, -5.1293e-03,  1.6254e-02]],

         [[-1.6407e-02,  1.9674e-02, -9.3406e-03],
          [-1.1489e-02, -2.3579e-02, -2.5344e-02],
          [-7.4273e-04, -1.7285e-02, -3.3714e-03]],

         [[-1.9652e-02, -1.3600e-02,  3.4019e-02],
          [ 3.6739e-02,  2.2994e-02,  9.2743e-04],
          [ 5.5516e-03,  6.2572e-03, -5.0400e-03]],

         ...,

         [[ 3.7740e-02,  1.2368e-02, -5.9801e-03],
          [ 4.0141e-02,  1.5671e-02,  1.5766e-02],
          [-1.5867e-02,  1.0601e-03, -1.5354e-02]],

         [[ 4.3749e-02,  5.1968e-03,  2.9448e-02],
          [ 3.0944e-02,  1.2599e-02,  1.9824e-02],
          [-2.2808e-02, -3.8429e-03,  1.9045e-02]],

         [[ 1.1011e-02, -1.3483e-02, -1.4012e-02],
          [ 2.0116e-02,  1.8004e-02,  3.7890e-02],
          [-1.3762e-02,  1.5499e-02,  1.3004e-03]]],


        [[[ 1.9963e-02,  3.6732e-02, -2.6128e-02],
          [ 2.4300e-02, -3.9554e-03, -2.7106e-02],
          [ 9.1777e-03, -2.5249e-02, -1.8752e-02]],

         [[-3.1198e-02, -3.4452e-02, -4.2412e-02],
          [-1.9394e-02, -2.3168e-02, -7.0356e-03],
          [ 1.7709e-02, -1.4059e-02,  5.6887e-03]],

         [[ 1.1349e-02,  1.8911e-02,  2.8188e-02],
          [-4.8238e-03,  1.3722e-02, -6.7362e-03],
          [ 1.3645e-02, -8.7342e-03, -2.9432e-02]],

         ...,

         [[-1.0869e-02,  3.0616e-02, -1.6859e-02],
          [ 3.1882e-02, -8.0909e-03,  1.1626e-02],
          [ 8.9995e-03,  1.8373e-02, -2.0820e-02]],

         [[-2.5573e-02,  1.8969e-02, -1.7611e-02],
          [-1.6507e-02, -3.1051e-03, -1.9725e-02],
          [ 2.8362e-02, -9.9671e-03,  2.1857e-02]],

         [[ 1.2953e-02, -2.5090e-02,  3.5421e-02],
          [-1.9214e-02,  2.2262e-02,  8.5938e-04],
          [ 1.6976e-02, -2.8255e-04, -7.6953e-03]]],


        [[[-1.6430e-03, -6.4117e-04,  9.5183e-03],
          [-2.0937e-02,  5.9439e-03, -1.7554e-02],
          [-1.6058e-02,  1.7282e-03, -5.3761e-03]],

         [[ 1.4008e-02, -1.6703e-02, -1.5290e-02],
          [ 2.5554e-02, -6.8970e-04,  3.0760e-02],
          [ 3.4090e-02,  1.9923e-02, -1.4044e-02]],

         [[-1.5631e-02,  9.8956e-03, -2.5735e-02],
          [ 6.4819e-03,  1.7903e-02,  2.1480e-03],
          [-6.1390e-05, -6.1159e-03,  3.0576e-02]],

         ...,

         [[ 1.8969e-02,  2.3486e-02,  1.6130e-02],
          [-1.6528e-02, -2.4937e-02,  4.4525e-03],
          [ 3.1051e-03,  1.7057e-02, -1.0329e-02]],

         [[ 1.5307e-02,  1.6657e-02,  8.8936e-03],
          [ 8.0038e-03, -9.2640e-03, -2.0549e-02],
          [ 4.2262e-03,  1.1127e-02,  3.1784e-02]],

         [[-1.3838e-02, -1.5678e-02, -5.1668e-03],
          [-2.2070e-02,  2.1555e-03,  1.0513e-03],
          [-2.7266e-03,  7.3104e-03,  4.1215e-02]]],


        ...,


        [[[-9.8661e-03, -2.4696e-02,  1.1401e-02],
          [ 1.9027e-02, -7.3164e-03,  1.6891e-02],
          [ 3.2095e-03, -1.5589e-02,  2.4341e-02]],

         [[-2.2053e-02, -2.3904e-02, -7.9930e-03],
          [-1.9038e-02, -1.8303e-02,  1.8513e-03],
          [-3.3355e-02,  2.2688e-02, -6.0662e-03]],

         [[-6.3927e-03,  2.9726e-03, -6.5677e-03],
          [-6.1826e-03,  1.9015e-02,  3.3278e-03],
          [ 3.2331e-02,  1.1701e-02,  1.9463e-02]],

         ...,

         [[ 5.8571e-04, -1.2201e-02,  3.9092e-02],
          [-1.9345e-03,  2.0628e-02,  2.0798e-02],
          [ 1.4346e-02,  1.0309e-02,  9.2035e-03]],

         [[-2.0341e-02,  1.9763e-03, -5.7173e-03],
          [-1.7594e-02,  1.3071e-02,  5.6422e-03],
          [ 1.1687e-02, -9.8849e-04,  2.0621e-02]],

         [[-4.6823e-03, -6.1779e-03, -9.7433e-03],
          [ 5.2673e-03,  2.3022e-02, -1.8068e-02],
          [-1.1290e-03, -1.2974e-02, -5.7642e-03]]],


        [[[-8.3640e-03,  3.8903e-02,  1.1208e-02],
          [-2.4665e-03, -1.1208e-03, -8.0619e-04],
          [-2.3607e-02,  2.3672e-02,  2.2638e-02]],

         [[ 1.6025e-02, -8.8413e-03,  5.0743e-06],
          [-3.0713e-03,  3.2320e-02,  1.1730e-02],
          [ 5.6447e-03, -7.6681e-03, -2.4572e-02]],

         [[-8.0326e-03, -1.5205e-02,  5.1037e-03],
          [-1.3760e-02, -3.2748e-02, -5.7635e-03],
          [ 1.4239e-02, -1.3775e-02,  2.6610e-02]],

         ...,

         [[-2.0289e-03,  1.2043e-02, -2.0334e-02],
          [ 1.5953e-02,  5.7478e-04,  1.7631e-02],
          [ 2.9928e-02,  1.2589e-02,  2.3884e-02]],

         [[ 8.7186e-03, -2.7519e-02,  3.8893e-03],
          [-1.7577e-02,  1.4315e-03, -2.0138e-02],
          [ 1.7979e-02,  2.0540e-02,  2.4648e-02]],

         [[-3.9604e-03, -2.9755e-02, -2.0676e-02],
          [ 1.3260e-02,  1.2081e-02,  3.4805e-03],
          [ 1.6496e-02,  5.8015e-03, -2.8407e-02]]],


        [[[ 2.2150e-02, -2.5110e-02,  1.0901e-03],
          [ 1.1334e-02,  6.9871e-03, -2.8709e-02],
          [ 2.1129e-02,  2.4302e-02,  1.1554e-02]],

         [[-4.1181e-02, -1.6759e-02, -3.1461e-02],
          [ 9.5286e-05,  1.2263e-02, -1.6809e-03],
          [ 4.7231e-03, -2.6251e-02,  1.0798e-02]],

         [[-1.3587e-02, -7.9040e-04, -9.3707e-03],
          [-2.5796e-02, -1.5021e-02, -2.0476e-02],
          [ 2.2252e-03, -9.3991e-03,  1.4121e-02]],

         ...,

         [[-1.4542e-02, -2.7876e-02, -2.8322e-02],
          [ 1.1151e-02,  1.6526e-03,  1.1284e-03],
          [ 1.8866e-02, -1.5505e-02, -1.5629e-02]],

         [[-3.1162e-03, -1.7242e-02, -6.3801e-03],
          [ 2.1616e-03,  3.8417e-03,  1.9130e-02],
          [ 1.8062e-03,  2.1031e-03,  2.4629e-03]],

         [[ 2.1426e-02,  1.3849e-02,  1.9211e-03],
          [ 2.0618e-02, -6.8556e-03,  1.9675e-02],
          [-1.4127e-02, -1.4798e-02,  1.4421e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.9391e+07, 1.4921e+06, 1.3247e+06,  ..., 7.6124e+01, 4.9848e+01,
        4.5133e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 305]) 

NULL SPACE BASIS :  tensor([[-2.4065e-02, -3.8615e-02,  3.3870e-02,  ...,  9.3857e-06,
         -5.1422e-04, -5.7320e-04],
        [-1.4976e-02,  2.7955e-03, -2.0718e-02,  ..., -8.8781e-04,
          1.8780e-03, -1.2576e-03],
        [-1.4588e-02,  1.6658e-02, -2.0597e-03,  ..., -1.4966e-03,
         -3.1568e-05,  2.4242e-03],
        ...,
        [-4.8072e-02,  2.5195e-02,  1.8469e-02,  ...,  5.9144e-03,
          2.3990e-03, -5.1213e-05],
        [-2.5913e-02, -1.7887e-02, -1.2205e-02,  ...,  1.2706e-03,
          1.0509e-03,  1.9161e-03],
        [-2.8435e-02, -5.5930e-02,  2.7435e-02,  ..., -3.7913e-03,
          4.9544e-04, -3.0972e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.0028e-03, -6.2852e-04, -2.3260e-04,  ...,  1.6720e-04,
         -4.5569e-04,  1.1916e-04],
        [-6.2852e-04,  2.2851e-03, -7.6643e-04,  ..., -9.9763e-05,
          3.7718e-04, -3.2737e-04],
        [-2.3260e-04, -7.6643e-04,  1.8429e-03,  ...,  6.1165e-05,
         -4.2182e-04, -4.0271e-05],
        ...,
        [ 1.6720e-04, -9.9763e-05,  6.1165e-05,  ...,  3.1702e-02,
         -1.0056e-02, -3.1972e-03],
        [-4.5569e-04,  3.7718e-04, -4.2182e-04,  ..., -1.0056e-02,
          3.3946e-02, -9.3077e-03],
        [ 1.1916e-04, -3.2737e-04, -4.0271e-05,  ..., -3.1972e-03,
         -9.3077e-03,  3.1938e-02]], device='cuda:0') 

reserving basis 742/1152; cond: 377892.46875, radio:0.007535938639193773
PARAMETER       :  Parameter containing:
tensor([[[[-2.0625e-02, -1.3991e-02,  4.1771e-02],
          [ 2.0784e-02,  8.9176e-03, -1.9446e-03],
          [ 1.5176e-02,  2.1104e-02, -2.7628e-02]],

         [[ 7.4816e-03,  1.7827e-03,  3.3726e-02],
          [-2.4127e-03,  1.7952e-02,  2.9511e-02],
          [ 2.7324e-02, -2.3527e-02, -7.3753e-03]],

         [[ 1.8252e-02,  5.2455e-03,  7.4530e-03],
          [-6.1079e-03, -8.6910e-03,  2.5856e-02],
          [ 9.8907e-03,  1.9497e-02,  2.6360e-02]],

         ...,

         [[ 6.9348e-03,  1.5526e-02,  2.6136e-02],
          [ 3.2434e-02, -2.0028e-02,  2.4004e-02],
          [-8.1061e-04, -1.3965e-02, -1.3327e-02]],

         [[-7.4602e-03, -6.8911e-03, -1.9108e-02],
          [-1.9877e-03,  4.0908e-03,  5.0756e-03],
          [ 2.0090e-03, -5.3398e-04, -7.7498e-03]],

         [[-3.7549e-02,  8.9069e-03, -1.6836e-02],
          [ 1.0788e-02,  2.7723e-03,  1.7789e-02],
          [ 2.5883e-02, -5.0676e-03,  2.3571e-02]]],


        [[[ 5.2320e-03, -3.6684e-03,  1.5269e-02],
          [ 2.0606e-02,  1.2682e-02, -1.1979e-03],
          [-2.2079e-03, -9.8706e-03, -3.0760e-03]],

         [[ 3.6716e-03,  1.1807e-02,  1.7426e-02],
          [-2.5691e-02, -2.8332e-03, -2.5314e-02],
          [-1.4083e-02,  1.8548e-02, -2.8197e-03]],

         [[-1.7591e-02, -5.9926e-03,  2.7124e-02],
          [ 4.1883e-03, -1.1128e-02, -1.1780e-02],
          [ 1.2201e-03,  1.3739e-02, -1.3231e-02]],

         ...,

         [[-2.1275e-02, -1.6807e-02, -2.2101e-02],
          [-2.6982e-02,  7.8413e-03, -2.0270e-02],
          [ 3.2520e-02,  1.8487e-03, -3.4953e-02]],

         [[ 1.7964e-02,  2.6360e-02,  3.4037e-02],
          [-1.4150e-02,  1.9073e-02, -1.0070e-02],
          [ 2.1753e-02,  1.9472e-02,  4.0538e-02]],

         [[ 2.5729e-04,  3.5945e-03,  3.7329e-06],
          [-3.8530e-03, -8.8248e-03,  2.1061e-02],
          [ 5.4161e-03,  4.6527e-02,  4.0871e-02]]],


        [[[-2.1551e-02, -2.1143e-02,  3.6791e-03],
          [-2.3698e-02,  4.2564e-03,  1.2637e-03],
          [ 6.0141e-03,  3.0475e-03,  4.2859e-03]],

         [[-3.2438e-03,  8.4826e-03,  9.4113e-03],
          [-1.1132e-03,  2.2861e-02,  6.5608e-03],
          [ 1.0773e-03,  3.4023e-02, -1.9645e-02]],

         [[-3.8856e-03, -1.8159e-02, -2.2361e-02],
          [-2.1908e-02, -2.9682e-02, -5.2729e-03],
          [-7.1395e-03, -2.8618e-02, -1.1330e-02]],

         ...,

         [[ 1.2829e-02,  1.0840e-02,  4.1842e-02],
          [-2.2968e-02,  7.7783e-03, -1.5946e-02],
          [-1.9411e-02,  2.7967e-02, -2.3730e-02]],

         [[-3.5959e-02, -1.0624e-02, -2.0956e-03],
          [ 6.1125e-03, -1.0845e-02,  7.8768e-03],
          [-9.3767e-03,  1.7698e-02, -2.5874e-02]],

         [[ 2.6134e-03, -7.9029e-03, -1.4951e-02],
          [-3.0972e-02,  3.6341e-03,  2.4441e-02],
          [-2.2086e-02,  1.0466e-02,  1.9249e-02]]],


        ...,


        [[[-1.4157e-03,  1.1091e-02, -1.9944e-02],
          [-1.9076e-02, -1.4068e-02, -2.3352e-02],
          [-4.8682e-03, -1.6373e-02, -2.3926e-02]],

         [[ 3.1333e-02,  3.4679e-03,  7.3943e-03],
          [-8.4407e-03,  6.3285e-03, -2.3810e-02],
          [-3.4970e-02, -3.7964e-02, -1.6179e-02]],

         [[ 8.8450e-03,  1.1198e-02, -2.5329e-03],
          [-5.2363e-03, -1.6755e-02,  3.6469e-03],
          [-3.6039e-03,  3.5757e-02,  2.3050e-02]],

         ...,

         [[ 2.0502e-02, -6.7029e-03,  8.7679e-03],
          [-1.2698e-02, -2.2797e-02,  2.0361e-02],
          [ 1.2357e-02, -2.5834e-02, -1.7338e-02]],

         [[-8.3939e-03, -1.2091e-02,  2.9086e-02],
          [-3.6171e-02,  2.5965e-02,  1.6330e-02],
          [-1.8299e-02,  4.9131e-03,  1.6134e-02]],

         [[-2.0655e-02, -2.0093e-02,  4.1453e-03],
          [-2.3837e-03, -1.8822e-02, -2.4265e-02],
          [-4.6532e-03,  1.3968e-02, -1.8619e-02]]],


        [[[-3.0720e-02,  8.4163e-03,  1.0258e-02],
          [-9.2193e-03, -1.5836e-02, -2.4808e-02],
          [-2.4673e-02, -9.7451e-03,  1.1750e-03]],

         [[ 9.5944e-03,  1.8762e-02,  1.2405e-02],
          [-1.8817e-02, -2.0742e-02, -2.3254e-02],
          [-9.2564e-03, -8.3125e-03, -1.8423e-02]],

         [[-2.7814e-02, -2.0423e-02, -2.0015e-02],
          [ 1.9155e-02,  2.2027e-02,  3.0281e-02],
          [ 1.6487e-02, -1.7290e-02, -1.8471e-02]],

         ...,

         [[-3.6389e-02,  1.4551e-02,  1.1340e-02],
          [-3.2819e-02,  7.6978e-04, -5.5133e-03],
          [-2.2609e-03,  1.3729e-03,  1.0603e-02]],

         [[ 1.8319e-03, -1.3083e-02, -8.1655e-04],
          [ 3.7990e-03, -3.0119e-02, -1.8296e-03],
          [ 4.9626e-03, -4.8168e-03,  9.7245e-03]],

         [[-4.3767e-03,  6.1401e-03,  3.7849e-03],
          [-1.1087e-02,  1.0376e-03, -2.1831e-02],
          [ 5.6476e-03,  9.8052e-03, -1.7810e-02]]],


        [[[ 6.5237e-03, -7.5041e-03,  1.7351e-02],
          [ 1.2601e-02, -1.9619e-02,  9.3466e-03],
          [ 7.0725e-03,  2.0934e-02,  4.5820e-03]],

         [[ 3.1790e-02,  6.2098e-03,  9.7882e-03],
          [-9.2600e-03,  3.1596e-02,  1.8223e-02],
          [ 8.4740e-03,  1.0172e-03,  1.4076e-02]],

         [[ 7.0571e-03, -1.2792e-02,  4.5945e-03],
          [-7.0002e-03, -3.5772e-02, -1.9344e-02],
          [-3.6806e-02, -1.2680e-02,  4.3906e-03]],

         ...,

         [[ 8.0346e-03, -3.2921e-02,  1.3556e-02],
          [-3.7125e-02,  1.6855e-02,  2.8100e-03],
          [-1.6662e-02,  4.5974e-03, -1.2401e-02]],

         [[ 1.6556e-02, -2.0794e-03,  1.3898e-03],
          [ 3.1546e-03,  2.7476e-02,  2.8157e-02],
          [ 9.1215e-03,  1.4081e-02,  2.3714e-02]],

         [[ 2.0631e-02,  3.0310e-02,  2.2560e-02],
          [-6.5696e-03,  6.8366e-03, -5.4728e-03],
          [-2.1694e-02,  8.6467e-03, -1.5635e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([9.5461e+06, 5.6861e+05, 5.4300e+05,  ..., 3.2890e+01, 3.0008e+01,
        2.5261e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 742]) 

NULL SPACE BASIS :  tensor([[ 0.0236,  0.0244, -0.0141,  ..., -0.0025,  0.0135, -0.0056],
        [ 0.0365,  0.0087,  0.0114,  ...,  0.0068, -0.0086,  0.0070],
        [ 0.0037, -0.0319,  0.0070,  ..., -0.0054,  0.0012, -0.0004],
        ...,
        [ 0.0095,  0.0491, -0.0474,  ..., -0.0129,  0.0102,  0.0042],
        [-0.0172, -0.0034, -0.0086,  ...,  0.0070, -0.0194,  0.0006],
        [-0.0090,  0.0260,  0.0083,  ..., -0.0036,  0.0030, -0.0054]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.3962e-02, -4.8826e-03, -1.7296e-03,  ..., -2.2702e-04,
         -1.1835e-04,  7.9957e-04],
        [-4.8826e-03,  2.3479e-02, -5.4767e-03,  ..., -3.9109e-04,
         -2.8798e-04,  4.2418e-05],
        [-1.7296e-03, -5.4767e-03,  2.3242e-02,  ..., -4.0432e-04,
          1.0206e-04, -2.3839e-04],
        ...,
        [-2.2702e-04, -3.9109e-04, -4.0432e-04,  ...,  2.3556e-02,
         -7.1504e-03, -2.9018e-03],
        [-1.1835e-04, -2.8798e-04,  1.0206e-04,  ..., -7.1504e-03,
          2.3137e-02, -6.9542e-03],
        [ 7.9957e-04,  4.2418e-05, -2.3839e-04,  ..., -2.9018e-03,
         -6.9542e-03,  2.1156e-02]], device='cuda:0') 

reserving basis 1384/2304; cond: 777444.5625, radio:0.00664993841201067
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0036, -0.0048, -0.0006],
          [-0.0245, -0.0077,  0.0135],
          [-0.0261, -0.0069, -0.0171]],

         [[ 0.0069,  0.0220,  0.0028],
          [-0.0061, -0.0186,  0.0205],
          [-0.0107, -0.0011,  0.0138]],

         [[-0.0182, -0.0171, -0.0133],
          [-0.0020, -0.0149, -0.0216],
          [ 0.0061,  0.0086, -0.0121]],

         ...,

         [[ 0.0155,  0.0127,  0.0170],
          [-0.0164, -0.0137,  0.0106],
          [ 0.0033, -0.0001,  0.0019]],

         [[ 0.0210,  0.0003,  0.0146],
          [-0.0138,  0.0003,  0.0231],
          [ 0.0214, -0.0021,  0.0090]],

         [[ 0.0077,  0.0080, -0.0213],
          [ 0.0057, -0.0040,  0.0025],
          [ 0.0235,  0.0053,  0.0112]]],


        [[[ 0.0089, -0.0053, -0.0026],
          [ 0.0048,  0.0075, -0.0090],
          [ 0.0236, -0.0093, -0.0135]],

         [[-0.0113,  0.0113, -0.0014],
          [-0.0194,  0.0220,  0.0125],
          [-0.0002, -0.0060, -0.0006]],

         [[-0.0151, -0.0018,  0.0129],
          [ 0.0251,  0.0050,  0.0071],
          [-0.0065,  0.0232, -0.0116]],

         ...,

         [[-0.0130,  0.0216,  0.0004],
          [ 0.0195, -0.0156,  0.0137],
          [ 0.0021, -0.0206, -0.0136]],

         [[-0.0149, -0.0261, -0.0058],
          [-0.0087,  0.0039, -0.0056],
          [-0.0161,  0.0161,  0.0177]],

         [[ 0.0078, -0.0057, -0.0204],
          [-0.0069, -0.0134,  0.0073],
          [-0.0186,  0.0104, -0.0058]]],


        [[[ 0.0163, -0.0222, -0.0128],
          [ 0.0168,  0.0188, -0.0023],
          [ 0.0177,  0.0215, -0.0245]],

         [[-0.0014,  0.0129,  0.0029],
          [-0.0111, -0.0135, -0.0199],
          [-0.0083,  0.0233,  0.0015]],

         [[-0.0059, -0.0124, -0.0017],
          [-0.0069, -0.0063,  0.0085],
          [-0.0008, -0.0136,  0.0089]],

         ...,

         [[-0.0151,  0.0226, -0.0072],
          [ 0.0170, -0.0160, -0.0035],
          [-0.0176, -0.0139, -0.0218]],

         [[-0.0030,  0.0187, -0.0115],
          [-0.0172, -0.0212, -0.0203],
          [ 0.0184, -0.0190, -0.0102]],

         [[ 0.0084,  0.0032, -0.0194],
          [ 0.0023, -0.0049,  0.0107],
          [-0.0100, -0.0005, -0.0092]]],


        ...,


        [[[ 0.0079,  0.0188,  0.0075],
          [ 0.0195, -0.0155, -0.0159],
          [-0.0050, -0.0132, -0.0112]],

         [[-0.0152, -0.0105,  0.0294],
          [ 0.0003,  0.0093, -0.0038],
          [ 0.0238, -0.0199,  0.0110]],

         [[-0.0194, -0.0018, -0.0142],
          [ 0.0032,  0.0264, -0.0239],
          [ 0.0091,  0.0320,  0.0149]],

         ...,

         [[-0.0060,  0.0122,  0.0029],
          [-0.0122, -0.0104,  0.0068],
          [ 0.0012,  0.0007, -0.0289]],

         [[-0.0118,  0.0026, -0.0109],
          [-0.0161, -0.0105, -0.0018],
          [-0.0084,  0.0037,  0.0201]],

         [[-0.0264, -0.0425, -0.0286],
          [ 0.0093, -0.0275, -0.0206],
          [ 0.0118, -0.0110,  0.0095]]],


        [[[-0.0085,  0.0169,  0.0014],
          [-0.0126, -0.0198,  0.0041],
          [ 0.0097, -0.0041, -0.0138]],

         [[-0.0030,  0.0060, -0.0043],
          [ 0.0057, -0.0152,  0.0145],
          [ 0.0092, -0.0136, -0.0157]],

         [[-0.0224, -0.0306, -0.0222],
          [ 0.0244,  0.0159, -0.0021],
          [-0.0037,  0.0086, -0.0056]],

         ...,

         [[ 0.0083, -0.0137, -0.0297],
          [ 0.0164,  0.0153, -0.0230],
          [-0.0139, -0.0228,  0.0132]],

         [[ 0.0051,  0.0012,  0.0109],
          [-0.0137, -0.0077, -0.0102],
          [-0.0081,  0.0088,  0.0052]],

         [[-0.0243,  0.0161,  0.0169],
          [-0.0265, -0.0097,  0.0258],
          [-0.0159,  0.0367,  0.0399]]],


        [[[-0.0020, -0.0311,  0.0090],
          [-0.0158, -0.0084,  0.0279],
          [-0.0204, -0.0089,  0.0095]],

         [[-0.0193, -0.0165,  0.0134],
          [-0.0095,  0.0079, -0.0063],
          [ 0.0050,  0.0119,  0.0011]],

         [[ 0.0094, -0.0055, -0.0044],
          [-0.0118,  0.0239, -0.0015],
          [-0.0213,  0.0008, -0.0043]],

         ...,

         [[-0.0041, -0.0296, -0.0028],
          [-0.0238, -0.0256,  0.0076],
          [ 0.0105, -0.0143,  0.0187]],

         [[ 0.0097,  0.0035,  0.0184],
          [ 0.0066, -0.0119, -0.0002],
          [-0.0218,  0.0151, -0.0265]],

         [[-0.0249,  0.0089, -0.0339],
          [ 0.0101, -0.0134, -0.0054],
          [ 0.0130, -0.0256, -0.0192]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.3896e+07, 8.4381e+05, 7.9208e+05,  ..., 1.9312e+01, 1.8206e+01,
        1.7874e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1384]) 

NULL SPACE BASIS :  tensor([[-0.0213, -0.0068,  0.0423,  ...,  0.0027, -0.0037, -0.0018],
        [-0.0189,  0.0132, -0.0268,  ...,  0.0013, -0.0020, -0.0035],
        [-0.0114,  0.0250,  0.0021,  ..., -0.0044,  0.0009,  0.0022],
        ...,
        [-0.0074,  0.0169,  0.0036,  ..., -0.0002,  0.0022,  0.0080],
        [ 0.0104,  0.0106,  0.0129,  ...,  0.0057, -0.0018, -0.0071],
        [-0.0083, -0.0089,  0.0051,  ..., -0.0015,  0.0011,  0.0065]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.5834e-02, -7.6631e-04, -4.2594e-04,  ..., -3.5745e-05,
         -1.2244e-04,  1.2241e-05],
        [-7.6631e-04,  1.5215e-02, -8.3354e-04,  ...,  2.7524e-05,
          1.3675e-06, -2.2762e-04],
        [-4.2594e-04, -8.3354e-04,  1.5666e-02,  ..., -1.1025e-04,
         -2.2273e-05, -9.0834e-05],
        ...,
        [-3.5745e-05,  2.7524e-05, -1.1025e-04,  ...,  2.0859e-02,
         -1.7548e-04, -6.9130e-05],
        [-1.2244e-04,  1.3675e-06, -2.2273e-05,  ..., -1.7548e-04,
          2.0336e-02, -3.3154e-04],
        [ 1.2241e-05, -2.2762e-04, -9.0834e-05,  ..., -6.9130e-05,
         -3.3154e-04,  2.1036e-02]], device='cuda:0') 

reserving basis 98/128; cond: 13541.33984375, radio:0.02106533758342266
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0248]],

         [[-0.0367]],

         [[-0.0282]],

         ...,

         [[-0.0409]],

         [[-0.0742]],

         [[ 0.0799]]],


        [[[-0.0492]],

         [[ 0.0642]],

         [[ 0.0739]],

         ...,

         [[-0.0458]],

         [[ 0.0005]],

         [[-0.0177]]],


        [[[ 0.0602]],

         [[-0.0195]],

         [[-0.0232]],

         ...,

         [[ 0.0529]],

         [[ 0.0083]],

         [[-0.0210]]],


        ...,


        [[[-0.0619]],

         [[ 0.0304]],

         [[ 0.0805]],

         ...,

         [[-0.0781]],

         [[-0.0493]],

         [[ 0.0300]]],


        [[[-0.0267]],

         [[-0.0713]],

         [[ 0.0110]],

         ...,

         [[ 0.0310]],

         [[ 0.0483]],

         [[ 0.0231]]],


        [[[ 0.0469]],

         [[-0.0396]],

         [[-0.0611]],

         ...,

         [[-0.0458]],

         [[ 0.0672]],

         [[-0.0329]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.3437e+06, 7.2756e+04, 5.9900e+04, 2.3649e+04, 1.5310e+04, 1.2624e+04,
        9.7729e+03, 6.9522e+03, 6.5944e+03, 5.7988e+03, 5.7458e+03, 4.3874e+03,
        4.0421e+03, 3.4540e+03, 2.8620e+03, 2.7697e+03, 2.3621e+03, 2.3416e+03,
        2.1195e+03, 1.9400e+03, 1.8383e+03, 1.7003e+03, 1.6298e+03, 1.5368e+03,
        1.3726e+03, 1.3622e+03, 1.2328e+03, 1.1833e+03, 1.0874e+03, 1.0586e+03,
        9.8155e+02, 9.6256e+02, 9.1251e+02, 8.6733e+02, 8.4219e+02, 8.1878e+02,
        7.7360e+02, 7.4501e+02, 7.0639e+02, 6.9441e+02, 6.7461e+02, 6.5357e+02,
        6.4145e+02, 6.1281e+02, 5.9579e+02, 5.6553e+02, 5.5310e+02, 5.4225e+02,
        5.3086e+02, 5.0956e+02, 5.0168e+02, 4.9289e+02, 4.7389e+02, 4.5378e+02,
        4.5214e+02, 4.4448e+02, 4.2415e+02, 4.1783e+02, 4.1535e+02, 4.0577e+02,
        3.9010e+02, 3.8480e+02, 3.7976e+02, 3.6984e+02, 3.6378e+02, 3.6037e+02,
        3.5526e+02, 3.4039e+02, 3.3480e+02, 3.2988e+02, 3.2377e+02, 3.1740e+02,
        3.1268e+02, 3.0695e+02, 3.0211e+02, 2.9187e+02, 2.9058e+02, 2.8249e+02,
        2.7844e+02, 2.7536e+02, 2.7001e+02, 2.6286e+02, 2.5817e+02, 2.5677e+02,
        2.5274e+02, 2.5157e+02, 2.4947e+02, 2.4260e+02, 2.4137e+02, 2.3586e+02,
        2.3084e+02, 2.2660e+02, 2.2587e+02, 2.2337e+02, 2.2205e+02, 2.1656e+02,
        2.1575e+02, 2.1191e+02, 2.0862e+02, 2.0660e+02, 2.0319e+02, 1.9939e+02,
        1.9607e+02, 1.9518e+02, 1.9163e+02, 1.9032e+02, 1.8707e+02, 1.8646e+02,
        1.8171e+02, 1.8014e+02, 1.7172e+02, 1.6839e+02, 1.6626e+02, 1.6408e+02,
        1.5904e+02, 1.5542e+02, 1.5283e+02, 1.5014e+02, 1.4867e+02, 1.4706e+02,
        1.4314e+02, 1.3919e+02, 1.3833e+02, 1.3562e+02, 1.3450e+02, 1.2941e+02,
        1.1662e+02, 9.9232e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 98]) 

NULL SPACE BASIS :  tensor([[-0.1413, -0.1495,  0.0500,  ..., -0.0663, -0.0565,  0.0502],
        [ 0.0131,  0.1681, -0.0133,  ...,  0.1498,  0.0229, -0.0232],
        [ 0.2121,  0.0343, -0.0141,  ...,  0.0856, -0.1718, -0.1155],
        ...,
        [ 0.0461, -0.0205, -0.0832,  ...,  0.0050,  0.0873, -0.0106],
        [-0.1135, -0.0142,  0.0046,  ..., -0.0752, -0.0290,  0.0455],
        [ 0.0697,  0.0273, -0.0235,  ..., -0.0652, -0.0352,  0.0746]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0790,  0.0011,  0.0004,  ..., -0.0041,  0.0025, -0.0086],
        [ 0.0011,  0.0656,  0.0014,  ..., -0.0019, -0.0049,  0.0086],
        [ 0.0004,  0.0014,  0.0870,  ...,  0.0006, -0.0010, -0.0036],
        ...,
        [-0.0041, -0.0019,  0.0006,  ...,  0.0766,  0.0006, -0.0071],
        [ 0.0025, -0.0049, -0.0010,  ...,  0.0006,  0.0790, -0.0017],
        [-0.0086,  0.0086, -0.0036,  ..., -0.0071, -0.0017,  0.0709]],
       device='cuda:0') 

reserving basis 1133/2304; cond: 914637.625, radio:0.005081913433969021
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0048,  0.0015, -0.0106],
          [ 0.0051, -0.0198,  0.0005],
          [-0.0063,  0.0216, -0.0053]],

         [[-0.0025,  0.0122,  0.0014],
          [ 0.0268, -0.0059, -0.0205],
          [-0.0023,  0.0069,  0.0130]],

         [[ 0.0156,  0.0278, -0.0138],
          [ 0.0145,  0.0115,  0.0124],
          [-0.0035,  0.0102,  0.0062]],

         ...,

         [[-0.0003,  0.0207,  0.0033],
          [-0.0270, -0.0058, -0.0304],
          [-0.0004, -0.0155, -0.0021]],

         [[-0.0250,  0.0131,  0.0089],
          [ 0.0040, -0.0259, -0.0190],
          [-0.0200, -0.0154, -0.0155]],

         [[ 0.0025, -0.0012, -0.0215],
          [ 0.0085,  0.0115,  0.0050],
          [ 0.0233,  0.0202, -0.0146]]],


        [[[-0.0045,  0.0228,  0.0308],
          [ 0.0020, -0.0188,  0.0158],
          [ 0.0066,  0.0179, -0.0253]],

         [[-0.0152, -0.0022, -0.0390],
          [-0.0016, -0.0062,  0.0082],
          [ 0.0009,  0.0178,  0.0242]],

         [[-0.0074, -0.0230,  0.0150],
          [-0.0106, -0.0002, -0.0151],
          [ 0.0064,  0.0066,  0.0188]],

         ...,

         [[-0.0118, -0.0267, -0.0107],
          [-0.0090,  0.0057,  0.0169],
          [ 0.0033,  0.0136, -0.0088]],

         [[ 0.0021, -0.0169, -0.0009],
          [ 0.0113,  0.0173,  0.0219],
          [-0.0165, -0.0197,  0.0189]],

         [[ 0.0057, -0.0128, -0.0296],
          [-0.0070,  0.0055,  0.0034],
          [-0.0133, -0.0255, -0.0129]]],


        [[[-0.0131,  0.0002,  0.0033],
          [ 0.0121, -0.0215, -0.0174],
          [-0.0093,  0.0213,  0.0036]],

         [[-0.0171,  0.0078,  0.0039],
          [-0.0111, -0.0045,  0.0185],
          [-0.0187,  0.0146,  0.0012]],

         [[ 0.0211, -0.0097,  0.0191],
          [-0.0152, -0.0040,  0.0256],
          [-0.0179, -0.0132,  0.0218]],

         ...,

         [[ 0.0167, -0.0059, -0.0171],
          [ 0.0118, -0.0288, -0.0031],
          [ 0.0095, -0.0290, -0.0332]],

         [[-0.0095,  0.0008, -0.0141],
          [ 0.0133,  0.0065,  0.0041],
          [ 0.0249, -0.0315, -0.0089]],

         [[-0.0037, -0.0134, -0.0128],
          [-0.0097, -0.0143,  0.0030],
          [ 0.0168,  0.0309,  0.0188]]],


        ...,


        [[[-0.0200,  0.0113, -0.0272],
          [ 0.0047, -0.0115, -0.0297],
          [ 0.0101,  0.0004, -0.0056]],

         [[-0.0097, -0.0238, -0.0019],
          [ 0.0134,  0.0161,  0.0070],
          [-0.0175, -0.0262, -0.0144]],

         [[ 0.0034, -0.0177,  0.0129],
          [-0.0194,  0.0003,  0.0215],
          [ 0.0163,  0.0048, -0.0174]],

         ...,

         [[ 0.0158, -0.0376,  0.0127],
          [-0.0124, -0.0019,  0.0065],
          [-0.0128, -0.0235,  0.0039]],

         [[ 0.0160, -0.0012,  0.0034],
          [-0.0155,  0.0115,  0.0014],
          [-0.0217,  0.0018,  0.0045]],

         [[-0.0200, -0.0123, -0.0233],
          [ 0.0080,  0.0118, -0.0056],
          [-0.0222,  0.0256,  0.0030]]],


        [[[ 0.0078, -0.0048,  0.0361],
          [ 0.0059,  0.0195,  0.0118],
          [ 0.0256, -0.0052,  0.0344]],

         [[ 0.0042,  0.0204, -0.0171],
          [-0.0138,  0.0030,  0.0217],
          [ 0.0039,  0.0096,  0.0269]],

         [[-0.0111,  0.0035,  0.0148],
          [-0.0132,  0.0088, -0.0222],
          [ 0.0025,  0.0180,  0.0104]],

         ...,

         [[ 0.0107,  0.0126, -0.0143],
          [ 0.0138,  0.0179, -0.0184],
          [-0.0002,  0.0176,  0.0308]],

         [[ 0.0046, -0.0095,  0.0190],
          [ 0.0102,  0.0163,  0.0205],
          [-0.0131,  0.0082,  0.0143]],

         [[-0.0134, -0.0100,  0.0047],
          [ 0.0064, -0.0048,  0.0024],
          [-0.0175, -0.0042,  0.0179]]],


        [[[ 0.0286,  0.0043, -0.0061],
          [ 0.0094, -0.0049,  0.0082],
          [-0.0026, -0.0133, -0.0077]],

         [[-0.0103, -0.0070,  0.0138],
          [ 0.0177, -0.0020,  0.0243],
          [-0.0031,  0.0248,  0.0097]],

         [[-0.0118, -0.0207, -0.0024],
          [-0.0133, -0.0265, -0.0129],
          [ 0.0017,  0.0013,  0.0047]],

         ...,

         [[ 0.0091,  0.0073,  0.0172],
          [ 0.0162,  0.0144, -0.0119],
          [ 0.0004, -0.0133, -0.0150]],

         [[ 0.0129, -0.0106, -0.0138],
          [-0.0006, -0.0231, -0.0150],
          [ 0.0001, -0.0216, -0.0201]],

         [[ 0.0053, -0.0030, -0.0113],
          [-0.0012, -0.0090, -0.0159],
          [ 0.0206,  0.0024,  0.0141]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.2020e+07, 8.1578e+05, 7.9159e+05,  ..., 1.7048e+01, 1.6204e+01,
        1.3142e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1133]) 

NULL SPACE BASIS :  tensor([[ 0.0020,  0.0043, -0.0186,  ..., -0.0059, -0.0022, -0.0018],
        [ 0.0070,  0.0159, -0.0205,  ...,  0.0019,  0.0045,  0.0041],
        [-0.0041,  0.0023,  0.0322,  ..., -0.0011, -0.0037, -0.0016],
        ...,
        [ 0.0103, -0.0138, -0.0064,  ...,  0.0251,  0.0124, -0.0012],
        [ 0.0014, -0.0032,  0.0074,  ..., -0.0258, -0.0065,  0.0003],
        [ 0.0142,  0.0274, -0.0274,  ...,  0.0033, -0.0022, -0.0014]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 9.4858e-03, -4.3848e-03, -2.8497e-04,  ...,  2.1474e-04,
         -7.1418e-05,  3.2931e-05],
        [-4.3848e-03,  1.0884e-02, -4.4081e-03,  ..., -2.6873e-04,
         -8.5623e-05, -2.6639e-04],
        [-2.8497e-04, -4.4081e-03,  9.0166e-03,  ...,  1.0279e-05,
         -6.9748e-05,  1.0847e-04],
        ...,
        [ 2.1474e-04, -2.6873e-04,  1.0279e-05,  ...,  2.2287e-02,
         -7.8952e-04, -5.9367e-04],
        [-7.1418e-05, -8.5623e-05, -6.9748e-05,  ..., -7.8952e-04,
          2.1327e-02, -6.5447e-04],
        [ 3.2931e-05, -2.6639e-04,  1.0847e-04,  ..., -5.9367e-04,
         -6.5447e-04,  2.1002e-02]], device='cuda:0') 

reserving basis 834/2304; cond: 1481120.25, radio:0.0021834077779203653
PARAMETER       :  Parameter containing:
tensor([[[[-1.4425e-02, -7.4473e-03, -1.5630e-02],
          [-4.6619e-03, -2.9709e-02,  6.7545e-03],
          [-1.5300e-02, -1.2684e-02, -4.0987e-03]],

         [[ 2.4038e-02,  2.7390e-02,  2.9179e-02],
          [-1.0269e-02,  1.2127e-02, -2.0463e-02],
          [-7.6010e-03,  2.4906e-02,  2.5302e-02]],

         [[-2.9525e-03, -9.5757e-03, -2.7047e-02],
          [-1.2538e-02, -1.5846e-02, -2.5859e-03],
          [ 1.7077e-02, -1.9891e-02, -1.4470e-02]],

         ...,

         [[-1.8117e-02,  2.2418e-02, -1.9365e-02],
          [ 5.0693e-03,  2.6944e-03, -2.2441e-02],
          [ 5.9905e-04,  1.7652e-03,  8.9800e-03]],

         [[ 2.0225e-02,  2.0999e-03, -2.0448e-02],
          [-1.2458e-02,  3.1298e-03, -8.8534e-03],
          [-2.6769e-02,  1.5316e-02,  9.7930e-03]],

         [[ 4.0682e-03,  2.5592e-02,  8.1546e-03],
          [-1.0435e-02,  1.4967e-02, -1.8031e-02],
          [ 8.9602e-03, -4.8232e-03,  1.6130e-02]]],


        [[[ 3.9948e-04,  2.0511e-02,  3.3045e-02],
          [-2.4428e-02,  1.9079e-02,  7.7796e-03],
          [ 3.0001e-03, -4.5773e-03,  7.8226e-03]],

         [[ 1.9445e-02,  1.5621e-02, -2.0317e-02],
          [ 1.0757e-02,  2.5916e-03, -3.3710e-03],
          [-2.8553e-02, -1.1298e-02, -1.6455e-02]],

         [[ 3.0725e-03, -2.8207e-03,  1.0566e-02],
          [ 4.7839e-03, -2.6919e-02, -1.6617e-02],
          [ 1.5498e-02, -9.8597e-03,  1.6386e-02]],

         ...,

         [[-1.8648e-02, -3.8691e-03, -1.4239e-02],
          [ 7.5974e-03,  1.2790e-02, -2.7086e-04],
          [-2.3977e-03,  1.3973e-02, -9.2853e-03]],

         [[ 2.6783e-03, -7.2874e-03,  2.0929e-02],
          [-7.2428e-04, -4.2503e-03,  2.0629e-02],
          [ 6.2257e-03,  9.8477e-03,  1.4425e-02]],

         [[-2.2699e-02, -1.6226e-02, -2.2574e-02],
          [-1.8798e-02, -1.5541e-02, -1.7484e-02],
          [-1.8794e-02, -1.6848e-02,  2.7636e-03]]],


        [[[-2.7211e-02, -1.9482e-02, -1.5415e-02],
          [ 1.3647e-03,  1.7205e-02,  9.3071e-03],
          [ 1.0500e-02, -9.6591e-03, -1.5829e-02]],

         [[-1.6107e-02,  1.5169e-02,  5.8695e-03],
          [-8.5113e-03,  4.5207e-03,  4.6225e-03],
          [-2.7605e-03, -1.2828e-02,  1.9595e-02]],

         [[-7.3078e-03,  7.9012e-03,  1.7780e-03],
          [-2.3252e-05, -1.5851e-02, -2.4154e-02],
          [ 9.5969e-03, -5.0997e-03,  1.7755e-03]],

         ...,

         [[-5.1069e-03,  2.7554e-02, -8.8889e-03],
          [-1.8027e-02,  1.8481e-03, -1.5785e-02],
          [-2.0952e-02, -2.3804e-02,  7.7190e-03]],

         [[-1.2060e-04,  2.3689e-03, -4.9437e-04],
          [-2.9288e-03,  1.2894e-02,  9.0898e-03],
          [-1.8345e-02, -1.6886e-02,  1.0046e-03]],

         [[-1.9962e-02, -1.0711e-02, -6.5927e-04],
          [ 1.0768e-02, -8.5785e-03,  1.9178e-02],
          [-6.2249e-03, -1.9425e-03, -1.2696e-02]]],


        ...,


        [[[-1.4385e-02, -1.6494e-02,  2.5022e-03],
          [-2.4592e-03,  1.0246e-02,  1.3018e-02],
          [-1.2742e-02, -1.1759e-02,  6.3553e-03]],

         [[ 1.9257e-04,  2.1723e-02, -1.4590e-03],
          [ 1.4451e-02, -3.0218e-03,  2.9378e-02],
          [-8.0414e-04, -1.1897e-02, -8.9475e-03]],

         [[ 9.6090e-04, -1.7093e-02,  7.9561e-03],
          [ 8.9394e-04,  2.0250e-02, -2.2456e-03],
          [-3.5631e-02, -5.9738e-03,  2.3837e-02]],

         ...,

         [[ 2.3783e-02,  4.3732e-03,  1.3466e-02],
          [ 8.9896e-03,  1.7366e-02,  1.6134e-02],
          [-2.1447e-03, -1.0575e-02,  6.8859e-03]],

         [[ 3.6626e-03, -1.4080e-02, -2.3889e-02],
          [-4.5977e-03,  1.8826e-02, -5.8731e-03],
          [ 1.1250e-02, -1.2756e-02,  1.6521e-02]],

         [[ 1.2073e-02,  1.3812e-02, -6.4381e-03],
          [-1.1981e-02, -2.4463e-03,  1.8150e-02],
          [ 1.3015e-02, -1.6404e-03,  6.6386e-03]]],


        [[[ 8.1259e-03, -1.7869e-02,  7.3379e-03],
          [ 2.6374e-02,  4.4512e-03, -9.1707e-03],
          [-3.7702e-03,  1.8373e-03, -1.7434e-02]],

         [[ 5.0281e-03, -7.5443e-03,  1.3966e-02],
          [ 2.0167e-02,  2.1343e-02,  3.5361e-02],
          [ 3.2062e-02,  2.3067e-02,  3.4584e-02]],

         [[-1.2644e-02,  1.6446e-02, -8.3550e-03],
          [ 2.8444e-02,  2.9851e-02,  1.1096e-02],
          [-1.7117e-02,  3.1788e-04,  2.4167e-02]],

         ...,

         [[-1.3923e-02,  6.3428e-03,  2.0335e-02],
          [ 1.7279e-02,  9.6179e-03,  1.1486e-02],
          [-1.4073e-02,  9.1095e-03, -1.5367e-02]],

         [[-2.1269e-02, -2.0529e-02,  8.6579e-03],
          [-1.0872e-02,  9.1744e-03, -4.0458e-04],
          [ 1.6513e-02, -1.4901e-02, -8.6031e-03]],

         [[-1.8618e-02,  2.1193e-03,  2.2725e-02],
          [ 1.3076e-02,  2.0661e-02, -1.1254e-02],
          [ 1.2757e-02,  1.2327e-02, -7.0882e-03]]],


        [[[ 1.5363e-02, -1.1696e-02, -2.7241e-03],
          [-6.6276e-03, -1.0816e-02, -3.0967e-03],
          [-2.2084e-02, -1.8454e-02, -1.1684e-03]],

         [[ 4.7901e-03, -5.7129e-03, -1.2411e-02],
          [-7.0851e-03, -2.7590e-03, -1.7546e-02],
          [ 1.1216e-03, -2.0329e-02, -9.5126e-03]],

         [[ 3.2079e-02,  2.2696e-03,  2.3358e-03],
          [ 2.5589e-02,  4.9329e-03,  8.5854e-03],
          [-2.6450e-03, -6.3341e-03, -1.4510e-02]],

         ...,

         [[ 1.4721e-02,  1.2909e-02,  1.9020e-03],
          [ 1.9761e-02, -7.1952e-03,  5.5701e-03],
          [-6.2512e-03, -3.6010e-03,  1.5699e-04]],

         [[-6.7607e-03,  3.9284e-03, -1.9932e-02],
          [ 2.1263e-02,  1.8654e-02,  1.2646e-02],
          [ 7.7338e-04,  3.8358e-02,  3.3225e-02]],

         [[ 2.0774e-02, -1.0513e-03, -1.1730e-02],
          [ 7.8697e-03,  7.8103e-03, -1.2461e-02],
          [-3.0920e-02, -4.2242e-03, -1.7553e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.0892e+07, 7.4952e+05, 7.1016e+05,  ..., 8.7798e+00, 8.4080e+00,
        7.3538e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 834]) 

NULL SPACE BASIS :  tensor([[ 2.9267e-02,  3.2971e-02,  4.9348e-02,  ..., -3.5986e-03,
          4.9617e-05,  5.2563e-03],
        [ 1.0614e-02, -3.6840e-02,  4.0592e-02,  ...,  5.2404e-03,
         -2.3590e-03, -8.7426e-04],
        [-1.9184e-02, -5.1532e-03,  2.0761e-02,  ...,  1.3550e-03,
         -2.9880e-04, -3.5713e-03],
        ...,
        [ 3.7208e-03,  4.1282e-02,  1.9988e-02,  ...,  1.5457e-03,
          4.6892e-03,  9.2996e-04],
        [-7.7108e-04, -2.5708e-02,  5.8275e-03,  ...,  2.2867e-03,
         -2.8498e-03,  1.7501e-04],
        [-2.2987e-03, -1.3505e-02, -3.3135e-04,  ..., -8.6180e-04,
          7.3417e-04, -4.7685e-04]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.5652e-02, -6.1374e-03, -7.6996e-04,  ...,  1.6029e-04,
         -2.0004e-04,  3.3482e-04],
        [-6.1374e-03,  1.5634e-02, -6.6950e-03,  ..., -2.1681e-04,
          1.0636e-04, -2.8390e-04],
        [-7.6996e-04, -6.6950e-03,  1.8101e-02,  ...,  1.4620e-04,
         -1.7224e-05,  1.1751e-04],
        ...,
        [ 1.6029e-04, -2.1681e-04,  1.4620e-04,  ...,  2.2350e-03,
         -9.1326e-04, -8.1636e-05],
        [-2.0004e-04,  1.0636e-04, -1.7224e-05,  ..., -9.1326e-04,
          2.4175e-03, -8.1916e-04],
        [ 3.3482e-04, -2.8390e-04,  1.1751e-04,  ..., -8.1636e-05,
         -8.1916e-04,  2.1267e-03]], device='cuda:0') 

reserving basis 1161/2304; cond: 1171413.875, radio:0.0037645373959094286
PARAMETER       :  Parameter containing:
tensor([[[[-0.0063, -0.0088, -0.0071],
          [-0.0078,  0.0155,  0.0133],
          [-0.0233, -0.0047, -0.0028]],

         [[ 0.0014,  0.0016, -0.0120],
          [-0.0071,  0.0109, -0.0055],
          [ 0.0200, -0.0252,  0.0054]],

         [[ 0.0130, -0.0052,  0.0077],
          [ 0.0117, -0.0105,  0.0013],
          [-0.0190, -0.0121, -0.0083]],

         ...,

         [[ 0.0152, -0.0157,  0.0020],
          [ 0.0107,  0.0106,  0.0136],
          [-0.0009, -0.0258,  0.0134]],

         [[ 0.0207,  0.0115, -0.0016],
          [-0.0103,  0.0022, -0.0107],
          [-0.0025,  0.0184, -0.0171]],

         [[-0.0062, -0.0044,  0.0082],
          [-0.0309,  0.0034,  0.0009],
          [ 0.0061, -0.0106,  0.0196]]],


        [[[-0.0118,  0.0070,  0.0075],
          [ 0.0101, -0.0070,  0.0150],
          [ 0.0156, -0.0069, -0.0010]],

         [[ 0.0185, -0.0017, -0.0192],
          [ 0.0032, -0.0030,  0.0094],
          [ 0.0128, -0.0218, -0.0155]],

         [[ 0.0173, -0.0242,  0.0042],
          [-0.0218,  0.0001,  0.0092],
          [-0.0103, -0.0184,  0.0041]],

         ...,

         [[-0.0218, -0.0179, -0.0083],
          [ 0.0003, -0.0128, -0.0264],
          [ 0.0041, -0.0051,  0.0002]],

         [[-0.0087,  0.0063, -0.0057],
          [ 0.0080,  0.0243, -0.0104],
          [-0.0150,  0.0117, -0.0092]],

         [[ 0.0106,  0.0215,  0.0046],
          [ 0.0006,  0.0067, -0.0092],
          [-0.0074,  0.0272, -0.0021]]],


        [[[ 0.0032, -0.0249, -0.0041],
          [-0.0028,  0.0108, -0.0052],
          [ 0.0067,  0.0105,  0.0035]],

         [[ 0.0225,  0.0177,  0.0210],
          [ 0.0202,  0.0094,  0.0048],
          [ 0.0091, -0.0047,  0.0085]],

         [[-0.0187, -0.0147,  0.0024],
          [ 0.0058, -0.0195, -0.0060],
          [-0.0143, -0.0091, -0.0314]],

         ...,

         [[ 0.0019, -0.0037, -0.0308],
          [-0.0056,  0.0029, -0.0084],
          [ 0.0184,  0.0010, -0.0032]],

         [[ 0.0127,  0.0050, -0.0052],
          [-0.0057, -0.0014,  0.0072],
          [-0.0213, -0.0219, -0.0149]],

         [[ 0.0017,  0.0058, -0.0068],
          [ 0.0018,  0.0171,  0.0003],
          [ 0.0116,  0.0111, -0.0095]]],


        ...,


        [[[ 0.0004,  0.0010, -0.0139],
          [-0.0142,  0.0025,  0.0006],
          [ 0.0079, -0.0110, -0.0081]],

         [[ 0.0179, -0.0038, -0.0114],
          [ 0.0221,  0.0155, -0.0156],
          [-0.0061,  0.0031, -0.0159]],

         [[-0.0210, -0.0023,  0.0084],
          [-0.0074, -0.0155,  0.0061],
          [ 0.0047,  0.0036, -0.0251]],

         ...,

         [[ 0.0092, -0.0130,  0.0077],
          [ 0.0013,  0.0043, -0.0059],
          [-0.0063, -0.0075, -0.0296]],

         [[-0.0160,  0.0063, -0.0033],
          [ 0.0077, -0.0234, -0.0213],
          [-0.0070, -0.0131, -0.0116]],

         [[ 0.0098,  0.0180,  0.0067],
          [-0.0105,  0.0119, -0.0101],
          [-0.0017,  0.0103, -0.0054]]],


        [[[-0.0188, -0.0038, -0.0009],
          [-0.0098,  0.0028,  0.0074],
          [ 0.0013,  0.0117,  0.0084]],

         [[-0.0110,  0.0216,  0.0117],
          [-0.0033,  0.0032,  0.0120],
          [ 0.0082, -0.0115,  0.0115]],

         [[-0.0094,  0.0034,  0.0173],
          [ 0.0133, -0.0048,  0.0155],
          [-0.0053, -0.0109, -0.0114]],

         ...,

         [[ 0.0114, -0.0194, -0.0013],
          [-0.0021, -0.0149,  0.0111],
          [-0.0156, -0.0129,  0.0038]],

         [[-0.0031,  0.0063, -0.0263],
          [-0.0156, -0.0060, -0.0337],
          [-0.0149, -0.0206, -0.0057]],

         [[ 0.0113,  0.0177,  0.0273],
          [ 0.0137,  0.0026,  0.0027],
          [ 0.0070, -0.0011,  0.0126]]],


        [[[ 0.0022,  0.0074,  0.0234],
          [-0.0040, -0.0030, -0.0170],
          [ 0.0064,  0.0046, -0.0082]],

         [[-0.0020, -0.0047, -0.0122],
          [-0.0100, -0.0094, -0.0170],
          [ 0.0032, -0.0268,  0.0008]],

         [[ 0.0021, -0.0175,  0.0087],
          [-0.0132,  0.0092,  0.0045],
          [-0.0204,  0.0005, -0.0097]],

         ...,

         [[-0.0147, -0.0162,  0.0198],
          [-0.0122, -0.0125, -0.0233],
          [ 0.0085, -0.0129,  0.0181]],

         [[ 0.0212,  0.0089, -0.0081],
          [ 0.0139,  0.0113,  0.0161],
          [-0.0098, -0.0224, -0.0011]],

         [[-0.0097, -0.0090,  0.0022],
          [-0.0177,  0.0281,  0.0152],
          [-0.0197, -0.0036, -0.0164]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([3.3769e+06, 3.7916e+05, 3.6761e+05,  ..., 3.5470e+00, 3.1207e+00,
        2.8828e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1161]) 

NULL SPACE BASIS :  tensor([[-0.0071,  0.0130, -0.0179,  ..., -0.0035,  0.0022,  0.0099],
        [ 0.0170,  0.0082, -0.0075,  ..., -0.0058, -0.0051, -0.0053],
        [-0.0032, -0.0258,  0.0066,  ...,  0.0046, -0.0006,  0.0043],
        ...,
        [ 0.0031,  0.0087, -0.0232,  ...,  0.0255, -0.0032, -0.0001],
        [ 0.0228,  0.0087,  0.0124,  ..., -0.0221,  0.0193,  0.0020],
        [ 0.0310, -0.0059, -0.0080,  ...,  0.0139, -0.0134, -0.0108]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 5.6166e-03, -2.7912e-03,  3.3691e-04,  ...,  1.5800e-04,
         -1.0651e-04, -1.2292e-04],
        [-2.7912e-03,  6.1197e-03, -2.5443e-03,  ..., -8.3594e-06,
          2.7105e-04,  1.3703e-04],
        [ 3.3691e-04, -2.5443e-03,  3.8418e-03,  ...,  7.5955e-05,
          1.2056e-04,  2.9994e-05],
        ...,
        [ 1.5800e-04, -8.3594e-06,  7.5955e-05,  ...,  2.3024e-02,
         -2.4588e-03, -1.0001e-03],
        [-1.0651e-04,  2.7105e-04,  1.2056e-04,  ..., -2.4588e-03,
          2.1623e-02, -3.1115e-03],
        [-1.2292e-04,  1.3703e-04,  2.9994e-05,  ..., -1.0001e-03,
         -3.1115e-03,  2.0705e-02]], device='cuda:0') 

reserving basis 1338/4608; cond: 5737974.5, radio:0.0008098860271275043
PARAMETER       :  Parameter containing:
tensor([[[[ 4.7382e-03,  4.2605e-03,  1.3203e-02],
          [-7.2028e-03, -8.1474e-03,  1.3275e-02],
          [-3.8173e-03, -4.6726e-03,  1.0308e-02]],

         [[ 4.2859e-03, -6.6390e-03,  1.4721e-03],
          [ 5.8203e-03,  9.8923e-03,  2.2740e-03],
          [ 1.7337e-02, -7.1983e-03,  8.6538e-03]],

         [[-2.7080e-03, -1.3986e-02, -2.2223e-04],
          [-5.1653e-03, -6.8265e-03, -2.3318e-02],
          [-1.1440e-02,  9.5330e-03, -2.1161e-02]],

         ...,

         [[-5.1445e-03,  2.2272e-03,  3.6516e-03],
          [-1.6256e-03, -1.2793e-03, -1.6999e-02],
          [-1.0275e-02, -1.0432e-03, -2.5407e-02]],

         [[-4.4947e-03,  1.1144e-02,  1.4216e-02],
          [ 2.8462e-03,  1.6194e-02,  3.2671e-03],
          [-8.5842e-04,  4.8621e-03,  4.7109e-03]],

         [[-1.4068e-02, -3.0947e-02, -2.7849e-02],
          [-6.0681e-03,  2.6194e-04, -1.9768e-02],
          [ 7.9770e-03,  1.4608e-03, -9.4429e-04]]],


        [[[ 8.7614e-03, -1.6805e-02,  8.4477e-03],
          [-3.1056e-03,  5.6907e-03,  6.5930e-03],
          [ 5.1283e-03, -1.2745e-02, -1.1192e-02]],

         [[-1.4809e-02,  7.2926e-03, -9.6820e-03],
          [-3.1435e-03, -2.5991e-03, -3.6799e-03],
          [ 7.1551e-03,  4.5038e-03, -1.0915e-02]],

         [[-1.6601e-03, -1.7081e-04,  1.0900e-02],
          [ 5.1629e-04,  1.3160e-02,  1.5253e-03],
          [ 1.0650e-02,  2.3050e-02,  1.8283e-02]],

         ...,

         [[-3.0778e-03,  3.1036e-03, -9.1219e-03],
          [ 3.6859e-04, -6.3968e-03, -2.2404e-03],
          [-1.9402e-03,  7.3247e-03,  1.3528e-02]],

         [[ 4.7173e-03, -3.8788e-03, -1.1723e-02],
          [-4.8560e-03, -3.6522e-03,  3.4126e-03],
          [-5.4879e-03,  1.8113e-06, -1.1948e-02]],

         [[ 8.7594e-04,  1.8670e-03, -4.3089e-03],
          [-1.4499e-02, -1.9177e-03,  4.4815e-04],
          [ 1.1307e-02, -6.3428e-03, -3.9570e-03]]],


        [[[ 2.5619e-03,  1.0397e-02,  2.8323e-03],
          [ 7.2507e-03,  1.4749e-02,  1.8563e-02],
          [-7.1619e-03, -2.5260e-03,  3.7122e-03]],

         [[ 7.9084e-04,  1.3404e-02,  9.0797e-03],
          [-1.1879e-02, -7.0709e-03,  3.9477e-03],
          [-9.8567e-03, -7.3844e-03, -5.8643e-03]],

         [[-3.9225e-03, -7.3626e-03,  6.1048e-03],
          [-6.0048e-03, -1.1491e-02,  9.7326e-03],
          [-1.1667e-03, -5.1832e-03, -1.0999e-03]],

         ...,

         [[ 8.0184e-03,  2.2611e-03, -9.9117e-03],
          [ 6.9207e-03, -2.0209e-03, -1.1736e-02],
          [ 8.6767e-03,  3.8462e-03,  1.6655e-03]],

         [[ 9.2015e-03,  4.6111e-03,  1.9729e-02],
          [-6.6118e-03, -6.6345e-03,  1.8034e-02],
          [-5.0959e-03,  2.6938e-03, -5.2564e-03]],

         [[-4.5738e-03,  4.2447e-03, -1.3193e-02],
          [-1.2846e-02,  2.8769e-03, -7.2415e-03],
          [-7.9157e-03,  1.0663e-02,  8.2830e-03]]],


        ...,


        [[[ 6.0951e-03, -4.1374e-03, -1.6320e-02],
          [-1.0608e-02,  5.7187e-03, -3.2117e-03],
          [ 1.7175e-03,  1.0937e-02, -1.3647e-02]],

         [[-6.4188e-03,  3.9004e-04,  3.5682e-03],
          [-8.1940e-03,  1.5511e-02, -1.8305e-04],
          [-7.2535e-03,  9.1283e-03, -7.9586e-03]],

         [[ 6.6544e-04, -9.4591e-03,  9.3826e-04],
          [-3.5057e-03, -2.6396e-02, -1.1645e-02],
          [ 1.1969e-02, -2.0136e-02, -1.7993e-02]],

         ...,

         [[-1.7929e-02,  6.1746e-03, -7.6381e-03],
          [-7.1343e-03, -4.8827e-03, -1.1625e-02],
          [-6.1720e-03,  1.1294e-02,  6.6028e-03]],

         [[-1.0444e-03, -3.6869e-03, -3.3342e-03],
          [-5.9793e-03,  5.5191e-03,  1.3810e-02],
          [ 8.7899e-03, -6.0577e-03, -9.8832e-03]],

         [[ 1.4620e-02,  1.4137e-02,  5.8537e-03],
          [-9.9163e-04,  4.8906e-03,  1.8600e-03],
          [-4.8347e-04,  3.6014e-04, -1.1856e-03]]],


        [[[-1.3630e-02, -4.0258e-03,  3.3903e-03],
          [-1.3378e-02,  8.5285e-03, -1.8592e-02],
          [-6.8117e-03, -2.5376e-03,  2.7529e-03]],

         [[ 1.3032e-03, -1.3850e-02, -2.2415e-02],
          [ 9.6098e-03,  4.1908e-03, -1.6338e-02],
          [ 8.4409e-03,  4.0492e-03,  4.8297e-03]],

         [[-6.6916e-03,  7.0506e-04,  1.0628e-02],
          [ 1.5503e-03, -1.0603e-03, -6.2600e-03],
          [-7.2523e-03, -4.9846e-03, -7.5617e-03]],

         ...,

         [[ 2.4415e-03,  1.5908e-03, -6.1507e-03],
          [-2.4467e-02, -1.7814e-02, -1.6896e-02],
          [-4.4671e-04, -1.7278e-03,  1.3926e-02]],

         [[ 4.6585e-03, -4.3886e-03,  3.8513e-03],
          [-2.6910e-03,  1.1481e-03,  1.6469e-04],
          [ 1.1286e-02,  1.0350e-02,  5.6310e-03]],

         [[ 8.6108e-03, -1.4043e-02, -1.3234e-02],
          [ 1.8656e-03, -3.3926e-03,  1.3703e-02],
          [-1.4470e-02,  2.7654e-03, -4.9472e-03]]],


        [[[ 8.9106e-03, -6.3581e-03, -3.6264e-03],
          [-1.4073e-02, -5.8041e-04,  4.7755e-03],
          [ 1.0899e-03, -9.9358e-03, -1.1998e-02]],

         [[-4.4278e-03,  1.4052e-02,  2.4252e-03],
          [ 5.6448e-03, -1.0118e-02,  7.0838e-04],
          [ 4.0501e-03,  6.8914e-03, -5.5230e-03]],

         [[-1.2826e-02, -3.5447e-03,  1.6890e-02],
          [-2.6902e-03,  4.5679e-03,  1.8457e-02],
          [-8.6239e-03, -1.7341e-03,  2.6849e-03]],

         ...,

         [[ 1.4347e-02,  5.9097e-03, -2.7661e-04],
          [ 1.1543e-02, -1.4032e-02,  5.9370e-03],
          [ 2.2464e-03, -1.4744e-02,  1.0453e-02]],

         [[ 2.0016e-02,  2.3508e-02,  2.6897e-03],
          [ 1.5825e-02,  1.3423e-03, -5.3329e-03],
          [-1.0087e-02,  1.1407e-02, -1.2984e-02]],

         [[-3.9717e-03, -1.3080e-02,  8.7967e-03],
          [-1.1643e-02, -5.9586e-03, -5.1354e-03],
          [ 1.1534e-02, -2.1598e-03,  2.7396e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.9428e+06, 6.2400e+05, 6.0899e+05,  ..., 8.0808e-01, 7.9458e-01,
        6.8714e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1338]) 

NULL SPACE BASIS :  tensor([[-1.5835e-02, -8.3038e-03, -8.4437e-03,  ...,  4.9178e-03,
          1.8453e-03,  2.3501e-03],
        [ 4.5770e-04, -1.1220e-02, -5.7832e-03,  ..., -6.6672e-04,
         -8.7188e-05,  1.4168e-04],
        [ 9.1248e-03, -1.5240e-02, -6.6529e-03,  ...,  7.4949e-04,
          5.1671e-03, -1.6028e-04],
        ...,
        [ 1.3858e-03, -1.1952e-02,  2.7599e-03,  ..., -3.6454e-03,
         -4.3605e-04,  1.1601e-03],
        [ 2.2724e-02, -1.7644e-02, -5.0998e-03,  ..., -4.3773e-03,
          3.0030e-03, -1.8910e-03],
        [ 3.1788e-02,  6.9488e-03, -1.6102e-02,  ...,  9.9000e-04,
          1.3549e-04, -5.1681e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.4738e-03, -4.2395e-04, -8.8370e-05,  ..., -1.1333e-04,
         -3.7549e-04, -1.3474e-04],
        [-4.2395e-04,  3.9160e-03, -8.4395e-05,  ..., -9.7550e-05,
          1.0646e-04, -1.1756e-04],
        [-8.8370e-05, -8.4395e-05,  5.4936e-03,  ...,  2.4972e-04,
         -8.9052e-05,  9.8044e-05],
        ...,
        [-1.1333e-04, -9.7550e-05,  2.4972e-04,  ...,  9.6652e-03,
         -6.0445e-04, -1.0569e-04],
        [-3.7549e-04,  1.0646e-04, -8.9052e-05,  ..., -6.0445e-04,
          5.7678e-03, -3.9559e-04],
        [-1.3474e-04, -1.1756e-04,  9.8044e-05,  ..., -1.0569e-04,
         -3.9559e-04,  7.0543e-03]], device='cuda:0') 

reserving basis 186/256; cond: 56086.1484375, radio:0.011057587340474129
PARAMETER       :  Parameter containing:
tensor([[[[-0.0138]],

         [[-0.0346]],

         [[-0.0331]],

         ...,

         [[ 0.0122]],

         [[-0.0477]],

         [[-0.0593]]],


        [[[ 0.0598]],

         [[-0.0561]],

         [[ 0.0136]],

         ...,

         [[ 0.0058]],

         [[ 0.0104]],

         [[ 0.0181]]],


        [[[ 0.0425]],

         [[ 0.0241]],

         [[-0.0111]],

         ...,

         [[ 0.0185]],

         [[ 0.0387]],

         [[-0.0271]]],


        ...,


        [[[-0.0259]],

         [[ 0.0114]],

         [[ 0.0397]],

         ...,

         [[ 0.0191]],

         [[-0.0182]],

         [[-0.0178]]],


        [[[ 0.0139]],

         [[ 0.0227]],

         [[-0.0302]],

         ...,

         [[ 0.0402]],

         [[ 0.0090]],

         [[ 0.0416]]],


        [[[-0.0327]],

         [[ 0.0272]],

         [[ 0.0385]],

         ...,

         [[ 0.0526]],

         [[-0.0604]],

         [[ 0.0446]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([6.6957e+05, 4.0328e+04, 3.4090e+04, 1.1792e+04, 9.3940e+03, 6.5745e+03,
        6.0940e+03, 3.2416e+03, 2.2560e+03, 1.9813e+03, 1.6824e+03, 1.3935e+03,
        1.3365e+03, 1.1728e+03, 1.1137e+03, 1.0622e+03, 1.0116e+03, 9.4603e+02,
        8.9123e+02, 8.2383e+02, 7.6379e+02, 7.5057e+02, 6.8067e+02, 6.2585e+02,
        5.7644e+02, 5.3600e+02, 5.1568e+02, 4.5123e+02, 4.2901e+02, 4.2578e+02,
        4.0874e+02, 3.7284e+02, 3.6923e+02, 3.6767e+02, 3.3116e+02, 3.1669e+02,
        3.0727e+02, 2.8467e+02, 2.7577e+02, 2.6778e+02, 2.5445e+02, 2.4534e+02,
        2.3864e+02, 2.3173e+02, 2.1933e+02, 2.1777e+02, 2.1258e+02, 2.0340e+02,
        1.9740e+02, 1.8851e+02, 1.8244e+02, 1.7771e+02, 1.6971e+02, 1.6670e+02,
        1.6449e+02, 1.6277e+02, 1.5974e+02, 1.5259e+02, 1.5231e+02, 1.4775e+02,
        1.4305e+02, 1.4276e+02, 1.3921e+02, 1.3403e+02, 1.3205e+02, 1.3000e+02,
        1.2444e+02, 1.2320e+02, 1.2203e+02, 1.2028e+02, 1.1818e+02, 1.1588e+02,
        1.1313e+02, 1.0978e+02, 1.0883e+02, 1.0774e+02, 1.0548e+02, 1.0447e+02,
        1.0166e+02, 9.9982e+01, 9.7904e+01, 9.6553e+01, 9.5406e+01, 9.4442e+01,
        9.4248e+01, 9.1345e+01, 9.0536e+01, 8.9928e+01, 8.7998e+01, 8.7396e+01,
        8.6377e+01, 8.4217e+01, 8.3744e+01, 8.3206e+01, 8.2035e+01, 8.1261e+01,
        8.0390e+01, 7.8827e+01, 7.7946e+01, 7.6898e+01, 7.6390e+01, 7.5883e+01,
        7.4124e+01, 7.3856e+01, 7.3515e+01, 7.1730e+01, 7.1100e+01, 7.0484e+01,
        6.9527e+01, 6.9213e+01, 6.8119e+01, 6.7801e+01, 6.7375e+01, 6.5277e+01,
        6.5073e+01, 6.4889e+01, 6.4174e+01, 6.3607e+01, 6.2476e+01, 6.1693e+01,
        6.1511e+01, 6.0721e+01, 5.9820e+01, 5.9739e+01, 5.9061e+01, 5.8483e+01,
        5.7959e+01, 5.7722e+01, 5.7346e+01, 5.7098e+01, 5.5740e+01, 5.5237e+01,
        5.5027e+01, 5.3986e+01, 5.3340e+01, 5.3115e+01, 5.2866e+01, 5.2575e+01,
        5.1984e+01, 5.1658e+01, 5.1261e+01, 5.1097e+01, 5.0175e+01, 4.9816e+01,
        4.9423e+01, 4.8980e+01, 4.8785e+01, 4.8110e+01, 4.7651e+01, 4.7305e+01,
        4.7093e+01, 4.6352e+01, 4.6110e+01, 4.5526e+01, 4.5423e+01, 4.4913e+01,
        4.4616e+01, 4.4518e+01, 4.4129e+01, 4.3946e+01, 4.3513e+01, 4.3357e+01,
        4.2805e+01, 4.2551e+01, 4.1875e+01, 4.1337e+01, 4.1165e+01, 4.0739e+01,
        4.0614e+01, 4.0411e+01, 4.0202e+01, 3.9797e+01, 3.9477e+01, 3.9034e+01,
        3.8797e+01, 3.8747e+01, 3.8074e+01, 3.7914e+01, 3.7689e+01, 3.7115e+01,
        3.6888e+01, 3.6631e+01, 3.6226e+01, 3.6037e+01, 3.5684e+01, 3.5574e+01,
        3.5228e+01, 3.5017e+01, 3.4456e+01, 3.3990e+01, 3.3706e+01, 3.3630e+01,
        3.3299e+01, 3.3019e+01, 3.2537e+01, 3.2353e+01, 3.2031e+01, 3.1819e+01,
        3.1752e+01, 3.1654e+01, 3.1186e+01, 3.1110e+01, 3.0785e+01, 3.0391e+01,
        3.0145e+01, 2.9763e+01, 2.9443e+01, 2.9326e+01, 2.9074e+01, 2.8951e+01,
        2.8837e+01, 2.8666e+01, 2.8244e+01, 2.7872e+01, 2.7657e+01, 2.7222e+01,
        2.7083e+01, 2.6750e+01, 2.6707e+01, 2.6272e+01, 2.6003e+01, 2.5826e+01,
        2.5593e+01, 2.5188e+01, 2.4903e+01, 2.4695e+01, 2.3884e+01, 2.3839e+01,
        2.3680e+01, 2.3514e+01, 2.3061e+01, 2.2909e+01, 2.2840e+01, 2.2638e+01,
        2.2260e+01, 2.1845e+01, 2.1438e+01, 2.1150e+01, 2.0691e+01, 2.0301e+01,
        2.0108e+01, 1.9657e+01, 1.9483e+01, 1.9093e+01, 1.8354e+01, 1.7672e+01,
        1.7477e+01, 1.6893e+01, 1.6473e+01, 1.6111e+01, 1.5710e+01, 1.5391e+01,
        1.4547e+01, 1.4246e+01, 1.3669e+01, 1.1938e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 186]) 

NULL SPACE BASIS :  tensor([[ 0.0315,  0.0640, -0.0742,  ..., -0.0035,  0.0097, -0.0040],
        [ 0.0424,  0.0333,  0.0448,  ...,  0.0164,  0.0223, -0.0394],
        [-0.0555,  0.0654,  0.0554,  ..., -0.0513,  0.0371, -0.0169],
        ...,
        [ 0.0221,  0.0714, -0.0027,  ...,  0.0311, -0.0155, -0.0083],
        [ 0.0750, -0.1324,  0.0321,  ..., -0.0139, -0.0022,  0.0029],
        [-0.0044,  0.0340, -0.0219,  ...,  0.0907, -0.3329, -0.0184]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0205, -0.0004, -0.0014,  ...,  0.0029, -0.0028,  0.0012],
        [-0.0004,  0.0574, -0.0025,  ..., -0.0045,  0.0031,  0.0006],
        [-0.0014, -0.0025,  0.0613,  ..., -0.0031,  0.0036, -0.0005],
        ...,
        [ 0.0029, -0.0045, -0.0031,  ...,  0.0549, -0.0022,  0.0018],
        [-0.0028,  0.0031,  0.0036,  ..., -0.0022,  0.0444, -0.0011],
        [ 0.0012,  0.0006, -0.0005,  ...,  0.0018, -0.0011,  0.0678]],
       device='cuda:0') 

reserving basis 1504/4608; cond: 6591264.0, radio:0.0007826955989003181
PARAMETER       :  Parameter containing:
tensor([[[[-3.9138e-03,  4.5274e-03, -1.2467e-02],
          [-8.5405e-03,  5.2665e-03, -1.5975e-02],
          [ 9.6308e-03, -2.9946e-03,  6.5684e-03]],

         [[-7.1935e-03, -2.3036e-03,  3.9237e-03],
          [ 9.1219e-03,  4.5973e-03, -4.3078e-03],
          [ 5.6020e-03, -5.2372e-03,  6.4900e-03]],

         [[ 9.0238e-04, -1.3231e-02, -1.4120e-02],
          [-4.6306e-03, -1.8099e-03, -1.2193e-02],
          [ 1.7781e-03,  1.4077e-02, -4.5200e-03]],

         ...,

         [[-3.4650e-03,  9.5211e-03, -1.3589e-03],
          [ 9.8854e-03,  3.7488e-04,  6.8932e-03],
          [ 5.3114e-03,  1.0872e-02, -5.1269e-03]],

         [[-1.1323e-02,  1.1804e-04, -1.0622e-02],
          [ 1.1563e-03, -6.9437e-03, -1.8594e-03],
          [-2.3520e-04, -1.0051e-02,  1.0565e-02]],

         [[ 2.9301e-04,  1.0381e-02,  8.2163e-03],
          [ 6.7362e-03,  8.8632e-03,  1.2756e-02],
          [-1.2885e-02,  2.8115e-03, -9.0984e-03]]],


        [[[ 5.8851e-03, -3.8819e-03,  7.8834e-03],
          [-1.3406e-02, -8.9604e-03,  5.7781e-03],
          [-2.2974e-02,  9.7952e-04,  6.9925e-03]],

         [[-7.4493e-03,  1.9161e-03,  2.6251e-03],
          [-6.7309e-03,  1.6680e-02, -1.7275e-03],
          [ 1.1134e-02,  7.7668e-03,  1.5415e-02]],

         [[ 5.4573e-03, -6.9563e-03,  6.0768e-03],
          [ 1.1999e-03, -1.1334e-02,  4.8947e-03],
          [ 1.1587e-02, -2.6170e-05,  7.2031e-03]],

         ...,

         [[ 9.0492e-03,  3.5876e-03,  7.8880e-03],
          [ 1.1496e-02, -3.5234e-03, -4.5208e-03],
          [-6.2101e-03, -1.0300e-03,  2.5848e-03]],

         [[ 2.1344e-03, -1.7232e-03, -9.5635e-03],
          [ 7.8265e-04, -4.7837e-03, -1.9377e-03],
          [-3.1548e-03,  1.2262e-02,  2.6875e-03]],

         [[ 1.2120e-02, -7.4189e-03, -9.5316e-03],
          [ 1.2790e-02, -5.1935e-03,  1.7627e-03],
          [-1.7838e-03,  6.0359e-03, -1.0431e-02]]],


        [[[-5.9852e-03, -1.8277e-02, -8.4798e-03],
          [-2.1605e-03,  8.0363e-03, -1.4867e-02],
          [ 1.5503e-02,  1.8055e-02,  5.8604e-03]],

         [[ 7.2421e-03, -1.0281e-02,  8.4741e-04],
          [-5.5456e-03, -2.8133e-03, -2.1392e-02],
          [-4.7070e-03, -4.3276e-03, -2.1278e-02]],

         [[-1.1892e-03, -2.0758e-03,  5.4891e-03],
          [ 1.5076e-03,  1.9699e-03, -6.3158e-04],
          [-5.7474e-03,  1.0115e-02, -2.2046e-03]],

         ...,

         [[ 7.4509e-03,  1.3964e-02,  8.2597e-03],
          [-5.5858e-03,  9.5142e-03, -8.5999e-03],
          [ 4.1481e-04, -1.2669e-02, -2.1743e-03]],

         [[ 5.3175e-03, -1.1187e-02, -5.8912e-03],
          [ 3.8201e-03, -6.5293e-03, -1.9798e-02],
          [-4.0037e-03,  1.4640e-03, -1.4333e-02]],

         [[-1.2820e-02, -1.9027e-02, -3.7647e-03],
          [ 7.5887e-03,  6.0499e-03, -1.1712e-02],
          [-4.9400e-03, -1.0812e-02,  6.0928e-03]]],


        ...,


        [[[-1.1620e-02,  8.3986e-03, -8.4106e-03],
          [ 8.8905e-03,  7.1002e-03,  1.0345e-02],
          [-1.2817e-02, -1.7499e-02,  9.8092e-04]],

         [[-1.6345e-02, -1.7432e-03, -1.2408e-03],
          [ 4.0682e-03,  2.5098e-03, -1.8649e-03],
          [-9.9678e-03,  4.4836e-03, -3.5141e-03]],

         [[-1.5846e-02,  8.2913e-04,  1.2233e-03],
          [-1.2921e-02, -5.5840e-03, -1.5767e-02],
          [-8.2834e-03, -1.4281e-02, -3.1431e-03]],

         ...,

         [[ 9.9329e-03, -1.4766e-02,  5.8558e-03],
          [-3.0843e-03,  4.2716e-03, -1.0603e-02],
          [-7.7392e-03, -1.2284e-03,  5.5635e-03]],

         [[-1.7784e-03, -2.7927e-03, -1.5613e-03],
          [-1.2492e-02, -8.2338e-03, -1.9391e-02],
          [-1.4428e-02, -4.1854e-03, -3.9757e-03]],

         [[-3.3412e-03, -4.0239e-03, -1.0727e-02],
          [ 8.8011e-03,  9.6145e-03, -1.3310e-03],
          [ 1.0773e-02,  5.9372e-03,  1.0564e-02]]],


        [[[-1.4001e-02, -2.5761e-03,  1.0020e-02],
          [ 6.9403e-03, -7.2069e-04,  2.2650e-02],
          [-6.1197e-03, -7.4656e-03,  2.0933e-02]],

         [[ 1.0548e-02, -7.7186e-03, -1.8466e-03],
          [-3.5380e-03, -3.7768e-03,  9.2378e-03],
          [-1.9077e-02, -7.0269e-03, -1.8982e-02]],

         [[-1.5673e-02, -1.4310e-02, -1.3170e-02],
          [ 1.3476e-02,  1.3404e-03, -1.5403e-02],
          [ 8.5898e-03, -1.2057e-02, -2.9356e-03]],

         ...,

         [[ 1.2690e-02,  7.2391e-03, -2.5221e-03],
          [ 1.2759e-02,  9.8006e-03,  1.5661e-02],
          [-6.0550e-03,  9.5960e-03, -1.5085e-02]],

         [[-8.0105e-03,  2.2652e-03, -4.3910e-03],
          [ 5.0755e-03, -4.7757e-03,  1.3440e-03],
          [ 1.5522e-02,  5.3733e-03,  6.3152e-03]],

         [[ 3.4484e-03,  1.1555e-02,  9.9768e-03],
          [-8.2073e-04,  4.4246e-03,  3.1599e-03],
          [-1.7733e-03,  1.6747e-02,  2.1320e-03]]],


        [[[ 1.9733e-02,  5.0307e-03, -1.6163e-03],
          [ 2.9666e-03, -5.5637e-03, -7.1969e-03],
          [-1.0204e-02,  2.4945e-03,  3.8921e-03]],

         [[ 1.5652e-02,  7.1297e-04, -2.9882e-03],
          [-9.7701e-04,  1.7466e-02, -2.7837e-03],
          [ 6.4393e-03,  2.4698e-03,  1.4069e-02]],

         [[ 1.8317e-02,  2.7351e-03,  5.8935e-03],
          [-5.6915e-03, -9.6881e-03, -9.5848e-03],
          [-1.0751e-02, -7.8916e-04, -9.5102e-03]],

         ...,

         [[ 1.0508e-02, -1.4457e-02,  1.9078e-03],
          [ 3.8445e-03,  9.3222e-03,  4.5603e-03],
          [ 1.9243e-02,  2.3762e-02,  2.6649e-02]],

         [[-8.3273e-04,  1.1385e-02, -8.0956e-03],
          [-1.3092e-02,  1.4552e-03, -1.7314e-02],
          [ 7.7951e-03,  7.1742e-04, -1.6236e-03]],

         [[ 5.3595e-05,  3.6991e-03, -1.3539e-02],
          [-1.0271e-02,  6.4307e-03, -1.0796e-02],
          [-5.2970e-03, -1.1732e-02,  1.0507e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.1909e+06, 1.0199e+06, 9.9123e+05,  ..., 8.6421e-01, 8.2823e-01,
        7.8755e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1504]) 

NULL SPACE BASIS :  tensor([[-1.5457e-03, -6.9485e-03,  2.7285e-02,  ...,  2.6614e-03,
         -2.1507e-05,  1.7564e-03],
        [ 2.2542e-02,  1.8341e-02, -7.5090e-03,  ...,  2.6946e-03,
          4.3211e-03, -5.4341e-03],
        [-8.4599e-03,  4.9022e-03,  1.6504e-02,  ..., -8.3518e-04,
         -3.9398e-03,  5.6680e-03],
        ...,
        [-2.9252e-03,  2.3953e-02,  5.8456e-03,  ...,  1.7741e-03,
          4.6117e-03,  3.4192e-03],
        [-8.9327e-03,  8.1484e-03, -7.2432e-04,  ..., -3.7790e-03,
         -4.5407e-03, -3.3522e-03],
        [-8.1293e-03, -1.2151e-02, -6.0709e-03,  ..., -4.1821e-03,
          2.6432e-03,  2.4358e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 8.7874e-03, -2.5990e-03, -7.8401e-05,  ...,  3.5213e-04,
         -1.3534e-04,  1.0902e-04],
        [-2.5990e-03,  6.8916e-03, -2.4309e-03,  ...,  3.0659e-05,
          1.5280e-04, -2.9616e-05],
        [-7.8401e-05, -2.4309e-03,  7.5459e-03,  ...,  8.5088e-05,
         -1.7494e-05,  1.4283e-04],
        ...,
        [ 3.5213e-04,  3.0659e-05,  8.5088e-05,  ...,  3.9449e-03,
         -3.1700e-04, -1.3902e-04],
        [-1.3534e-04,  1.5280e-04, -1.7494e-05,  ..., -3.1700e-04,
          2.7293e-03, -2.8317e-04],
        [ 1.0902e-04, -2.9616e-05,  1.4283e-04,  ..., -1.3902e-04,
         -2.8317e-04,  3.2357e-03]], device='cuda:0') 

reserving basis 548/4608; cond: 45746212.0, radio:3.361926428624429e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 6.5583e-03,  5.2331e-03, -4.6052e-03],
          [-3.1491e-03, -8.9888e-03, -2.6213e-03],
          [ 6.9918e-04, -8.4336e-03,  5.1354e-03]],

         [[-4.0708e-03, -1.1650e-02,  9.5204e-04],
          [-3.8376e-03, -1.0831e-03, -2.3997e-03],
          [ 2.9088e-03,  9.3236e-03, -1.6635e-03]],

         [[-1.0968e-02,  6.4642e-03,  1.2015e-02],
          [ 1.5328e-02,  1.3384e-02,  3.1639e-03],
          [-6.1107e-04,  1.7730e-02,  5.2166e-03]],

         ...,

         [[ 1.1918e-02,  3.5066e-03,  5.9155e-04],
          [ 2.5382e-03, -8.4067e-03,  1.3513e-03],
          [ 5.7986e-03, -5.9349e-03,  7.7013e-03]],

         [[ 7.6125e-03, -1.4945e-03,  2.9701e-03],
          [ 3.6975e-03,  1.2315e-03,  7.2820e-03],
          [ 7.7262e-03,  8.4688e-03, -3.0902e-04]],

         [[-7.8996e-03,  1.2973e-02, -3.0617e-04],
          [-9.9780e-03, -1.2952e-02,  1.8697e-03],
          [-1.2245e-02,  5.2763e-03, -1.4259e-02]]],


        [[[-2.2133e-03,  7.1984e-03, -2.1316e-03],
          [ 1.5488e-03, -2.9256e-03, -9.4925e-03],
          [-4.6170e-03, -1.5371e-02,  5.7732e-03]],

         [[ 3.9629e-03,  6.8551e-03,  3.0330e-03],
          [ 9.6011e-03,  1.4083e-02, -1.2691e-02],
          [ 1.6625e-02,  7.8832e-03, -1.0061e-02]],

         [[ 7.7987e-03,  5.6198e-03,  2.7609e-03],
          [ 3.8342e-03, -4.4409e-03,  6.8249e-03],
          [-2.7886e-03, -7.4673e-03,  4.8095e-04]],

         ...,

         [[-1.9404e-03,  9.4480e-03,  4.6050e-03],
          [-7.6089e-03, -8.5121e-03,  7.1894e-03],
          [-7.4491e-03,  6.5231e-03, -3.6496e-03]],

         [[ 4.5450e-03, -2.8077e-03,  9.4342e-03],
          [-2.9169e-03, -1.3972e-02, -4.5337e-03],
          [ 4.9378e-03, -2.1532e-03, -1.4456e-02]],

         [[ 1.6215e-02,  1.9205e-02, -2.0436e-03],
          [ 3.2391e-04,  1.9640e-05,  5.8822e-03],
          [ 4.4333e-03, -2.8292e-03, -1.0719e-03]]],


        [[[-3.0797e-03, -5.4902e-03, -7.1306e-03],
          [-5.1385e-03, -1.2004e-02, -7.9201e-03],
          [-7.6506e-03, -4.1720e-03, -3.0707e-03]],

         [[-3.2513e-03,  5.3147e-03, -1.2384e-02],
          [ 2.0854e-03, -1.0269e-02, -1.3797e-02],
          [-5.3935e-03, -4.1583e-03, -8.6159e-03]],

         [[-2.2163e-03, -1.2448e-02,  4.1943e-03],
          [-5.4940e-03, -7.5261e-03, -1.3401e-02],
          [-4.9002e-03,  1.3644e-02, -6.2514e-03]],

         ...,

         [[ 3.1352e-03, -1.9094e-02, -4.0653e-04],
          [ 1.5941e-02,  9.6550e-04, -4.8662e-03],
          [-4.9099e-03,  8.4169e-03, -2.5833e-03]],

         [[ 4.0062e-03,  1.8507e-02,  4.6057e-03],
          [-2.6580e-03, -5.0212e-03,  8.8250e-03],
          [ 3.0266e-04, -4.4293e-03,  6.4377e-04]],

         [[-1.4040e-02, -1.5123e-02, -1.6509e-02],
          [-1.2263e-02, -5.5723e-04,  1.2299e-03],
          [-1.2692e-02, -1.0963e-02, -1.7033e-02]]],


        ...,


        [[[-1.3687e-03,  1.6405e-03,  8.8017e-03],
          [ 8.5131e-04,  8.3529e-03, -6.9435e-03],
          [ 6.8273e-03, -8.9450e-04,  4.2507e-03]],

         [[-8.2621e-03, -3.3288e-03,  9.3218e-03],
          [ 9.4034e-03,  6.1259e-03, -7.5288e-03],
          [ 1.2529e-02, -3.7763e-03, -3.1678e-03]],

         [[-1.4610e-02, -1.5884e-03, -1.1331e-02],
          [-1.2464e-02, -1.1491e-02, -3.7291e-03],
          [ 1.1660e-02,  1.6712e-02, -7.1681e-03]],

         ...,

         [[-3.9700e-03,  6.4760e-03,  5.5353e-03],
          [ 9.5515e-03,  2.1810e-03,  5.5752e-03],
          [-4.9901e-03, -3.6233e-03, -2.8795e-03]],

         [[-8.8441e-03,  1.1862e-02, -9.0482e-03],
          [ 1.0090e-02, -1.2344e-02, -8.1297e-03],
          [ 5.4726e-04,  1.2615e-03, -1.6754e-02]],

         [[ 1.6924e-02,  2.0689e-02,  2.3840e-02],
          [ 1.0955e-02, -2.4028e-03,  4.0558e-03],
          [ 1.0360e-02,  1.6086e-02,  6.7178e-03]]],


        [[[ 3.9200e-03, -1.3874e-03,  9.7090e-04],
          [-1.4635e-03, -1.1161e-02, -8.1467e-03],
          [-1.0012e-02,  1.2890e-03, -1.7368e-03]],

         [[-4.2420e-03,  9.6133e-03,  2.7760e-03],
          [-1.2212e-03,  5.7203e-03,  1.3064e-02],
          [-6.4545e-03, -1.0019e-02,  9.4152e-04]],

         [[ 4.6923e-03,  7.2591e-04,  4.5522e-03],
          [-1.2359e-02, -8.5376e-03,  9.8491e-03],
          [ 3.1336e-03,  9.9144e-03, -5.8606e-03]],

         ...,

         [[ 4.3220e-03,  3.7519e-03, -3.9849e-03],
          [ 8.5131e-03,  3.6481e-03,  3.3741e-03],
          [ 2.8364e-03,  2.0387e-03, -1.4495e-02]],

         [[-9.0711e-03, -3.1301e-03, -5.0532e-03],
          [ 2.4597e-03,  1.6238e-03, -1.4585e-02],
          [-1.0688e-04, -8.0407e-03,  2.3978e-03]],

         [[-1.2992e-02,  6.4247e-04,  2.5096e-04],
          [ 3.0092e-03, -4.8289e-03,  4.8448e-03],
          [-1.1513e-02, -1.1640e-02,  1.2344e-03]]],


        [[[-6.3440e-03, -1.0222e-02, -8.7858e-03],
          [ 3.1915e-03, -6.6502e-03, -7.5892e-04],
          [ 2.4787e-03,  6.8728e-04, -6.0162e-03]],

         [[-6.0351e-03,  1.0217e-02,  1.3639e-03],
          [ 2.0714e-03,  8.9261e-03,  2.3505e-03],
          [ 2.9807e-03,  1.1698e-02,  2.3286e-03]],

         [[-1.6873e-03,  5.8834e-03,  6.1692e-03],
          [-7.0333e-03,  1.0197e-02, -1.2567e-02],
          [ 5.4227e-04,  1.9465e-03,  2.8121e-03]],

         ...,

         [[ 5.1346e-03, -8.7366e-04,  1.3538e-02],
          [ 1.0588e-02,  3.9354e-03,  1.4334e-02],
          [ 7.7878e-03,  1.1010e-02,  4.1836e-03]],

         [[-8.0671e-03, -1.9310e-02, -7.1417e-04],
          [-1.1403e-02, -1.2193e-02,  4.4558e-03],
          [ 5.6191e-03, -3.5816e-04,  4.8011e-03]],

         [[-6.8012e-03,  4.9532e-03, -1.5045e-02],
          [-4.8673e-03,  1.0219e-02, -1.2297e-02],
          [ 6.7075e-03, -3.3607e-03, -6.0020e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.1578e+06, 2.5288e+05, 2.2669e+05,  ..., 3.8969e-02, 3.3342e-02,
        2.5308e-02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 548]) 

NULL SPACE BASIS :  tensor([[ 0.0207, -0.0277, -0.0513,  ..., -0.0072,  0.0084, -0.0163],
        [-0.0280,  0.0218,  0.0409,  ...,  0.0120,  0.0101,  0.0142],
        [-0.0188, -0.0055,  0.0130,  ..., -0.0088, -0.0234,  0.0174],
        ...,
        [-0.0057,  0.0085,  0.0051,  ...,  0.0015,  0.0010, -0.0002],
        [ 0.0023, -0.0105, -0.0022,  ..., -0.0044,  0.0002, -0.0009],
        [ 0.0045,  0.0044,  0.0036,  ...,  0.0023,  0.0021,  0.0016]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.3970e-02, -4.8469e-03, -1.8797e-04,  ..., -1.0922e-04,
          1.9205e-04,  6.9709e-06],
        [-4.8469e-03,  1.9452e-02, -4.9096e-03,  ...,  1.6974e-04,
          2.3201e-05,  4.5757e-05],
        [-1.8797e-04, -4.9096e-03,  2.2352e-02,  ..., -6.3031e-05,
          2.5980e-04, -1.9267e-04],
        ...,
        [-1.0922e-04,  1.6974e-04, -6.3031e-05,  ...,  4.9610e-04,
         -3.0086e-04,  1.3411e-04],
        [ 1.9205e-04,  2.3201e-05,  2.5980e-04,  ..., -3.0086e-04,
          6.0591e-04, -3.2496e-04],
        [ 6.9709e-06,  4.5757e-05, -1.9267e-04,  ...,  1.3411e-04,
         -3.2496e-04,  5.1922e-04]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 84.100, Total time 0.56
 * Val loss 0.813, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 97.160, Total time 3.15
 * Val loss 0.087, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 69.500, Total time 0.56
 * Val loss 0.962, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 68.960, Total time 3.15
 * Val loss 0.905, Total time 0.00
**************************************************
validation split name: 3
 * Val Acc 66.000, Total time 0.57
 * Val loss 0.972, Total time 0.00
**************************************************
training split name: 3
 * Val Acc 70.840, Total time 3.17
 * Val loss 0.866, Total time 0.00
**************************************************
validation split name: 4
 * Val Acc 68.500, Total time 0.57
 * Val loss 0.898, Total time 0.00
**************************************************
training split name: 4
 * Val Acc 72.120, Total time 3.19
 * Val loss 0.812, Total time 0.00
**************************************************
validation split name: 5
 * Val Acc 73.000, Total time 0.57
 * Val loss 0.844, Total time 0.00
**************************************************
training split name: 5
 * Val Acc 74.540, Total time 3.21
 * Val loss 0.735, Total time 0.00
**************************************************
validation split name: 6
 * Val Acc 69.900, Total time 0.57
 * Val loss 0.867, Total time 0.00
**************************************************
training split name: 6
 * Val Acc 71.880, Total time 3.19
 * Val loss 0.787, Total time 0.00
**************************************************
validation split name: 7
 * Val Acc 71.900, Total time 0.57
 * Val loss 0.793, Total time 0.00
**************************************************
training split name: 7
 * Val Acc 73.840, Total time 3.19
 * Val loss 0.742, Total time 0.00
**************************************************
validation split name: 8
 * Val Acc 73.000, Total time 0.57
 * Val loss 0.788, Total time 0.00
**************************************************
training split name: 8
 * Val Acc 73.880, Total time 3.22
 * Val loss 0.780, Total time 0.00
**************************************************
====================== 9 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0083 (0.0083)	2.528 (2.528)	18.75 (18.75)
[10/157]	0.0977 (0.0915)	0.0561 (0.0539)	2.110 (2.238)	37.50 (24.15)
[20/157]	0.0951 (0.0931)	0.0581 (0.0557)	1.868 (2.108)	43.75 (30.51)
[30/157]	0.0956 (0.0937)	0.0579 (0.0567)	1.815 (2.013)	43.75 (34.27)
[40/157]	0.0944 (0.0940)	0.0576 (0.0571)	1.502 (1.919)	53.12 (38.72)
[50/157]	0.0954 (0.0942)	0.0577 (0.0573)	1.645 (1.857)	37.50 (40.93)
[60/157]	0.0940 (0.0943)	0.0571 (0.0574)	1.639 (1.806)	50.00 (42.52)
[70/157]	0.0966 (0.0945)	0.0587 (0.0576)	1.544 (1.767)	46.88 (43.49)
[80/157]	0.0939 (0.0945)	0.0568 (0.0576)	1.563 (1.736)	56.25 (44.41)
[90/157]	0.0947 (0.0946)	0.0580 (0.0576)	1.478 (1.709)	53.12 (45.19)
[100/157]	0.0950 (0.0946)	0.0575 (0.0577)	1.269 (1.681)	65.62 (46.10)
[110/157]	0.0952 (0.0946)	0.0581 (0.0577)	1.360 (1.659)	71.88 (46.54)
[120/157]	0.0951 (0.0946)	0.0578 (0.0578)	1.564 (1.641)	50.00 (47.34)
[130/157]	0.0970 (0.0947)	0.0589 (0.0578)	1.390 (1.624)	59.38 (48.07)
[140/157]	0.0947 (0.0947)	0.0575 (0.0578)	1.120 (1.604)	62.50 (48.85)
[150/157]	0.0960 (0.0947)	0.0589 (0.0579)	1.271 (1.589)	62.50 (49.05)
[156/157]	0.0769 (0.0946)	0.0521 (0.0578)	1.487 (1.582)	25.00 (49.14)
 * Train Acc 49.140
 * Val Acc 59.300, Total time 0.59
 * Val loss 1.185, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0087 (0.0087)	1.512 (1.512)	43.75 (43.75)
[10/157]	0.0967 (0.0915)	0.0583 (0.0541)	1.360 (1.390)	50.00 (50.57)
[20/157]	0.0949 (0.0931)	0.0576 (0.0561)	1.324 (1.374)	46.88 (52.53)
[30/157]	0.0962 (0.0937)	0.0581 (0.0569)	1.341 (1.375)	59.38 (54.94)
[40/157]	0.0954 (0.0941)	0.0583 (0.0572)	1.484 (1.379)	43.75 (54.73)
[50/157]	0.0967 (0.0942)	0.0591 (0.0574)	1.285 (1.364)	59.38 (55.21)
[60/157]	0.0952 (0.0944)	0.0573 (0.0576)	1.169 (1.364)	65.62 (55.12)
[70/157]	0.0954 (0.0945)	0.0577 (0.0577)	1.464 (1.380)	59.38 (54.75)
[80/157]	0.0963 (0.0947)	0.0576 (0.0578)	1.298 (1.377)	56.25 (54.90)
[90/157]	0.0949 (0.0947)	0.0567 (0.0578)	1.363 (1.369)	59.38 (55.49)
[100/157]	0.0968 (0.0948)	0.0579 (0.0578)	1.306 (1.354)	59.38 (55.94)
[110/157]	0.0956 (0.0948)	0.0578 (0.0578)	1.122 (1.339)	68.75 (56.39)
[120/157]	0.0944 (0.0949)	0.0567 (0.0578)	1.053 (1.338)	62.50 (56.40)
[130/157]	0.0935 (0.0949)	0.0574 (0.0578)	1.020 (1.330)	65.62 (56.85)
[140/157]	0.1201 (0.0953)	0.0800 (0.0581)	1.272 (1.326)	56.25 (57.07)
[150/157]	0.0957 (0.0956)	0.0582 (0.0585)	1.381 (1.328)	62.50 (57.06)
[156/157]	0.0770 (0.0955)	0.0521 (0.0584)	1.299 (1.325)	62.50 (57.26)
 * Train Acc 57.260
 * Val Acc 60.100, Total time 0.58
 * Val loss 1.127, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0087 (0.0087)	0.872 (0.872)	75.00 (75.00)
[10/157]	0.0938 (0.0921)	0.0569 (0.0545)	1.312 (1.246)	53.12 (59.66)
[20/157]	0.0949 (0.0937)	0.0574 (0.0564)	1.069 (1.264)	71.88 (59.52)
[30/157]	0.0973 (0.0942)	0.0587 (0.0568)	1.491 (1.272)	56.25 (59.68)
[40/157]	0.0959 (0.0944)	0.0579 (0.0570)	1.240 (1.278)	56.25 (58.99)
[50/157]	0.0969 (0.0945)	0.0585 (0.0572)	1.436 (1.268)	62.50 (58.95)
[60/157]	0.0960 (0.0947)	0.0576 (0.0574)	1.192 (1.281)	53.12 (58.09)
[70/157]	0.0960 (0.0963)	0.0573 (0.0589)	1.378 (1.274)	62.50 (58.45)
[80/157]	0.0956 (0.0962)	0.0564 (0.0587)	1.595 (1.275)	46.88 (58.45)
[90/157]	0.0948 (0.0961)	0.0571 (0.0586)	1.508 (1.282)	46.88 (57.90)
[100/157]	0.0943 (0.0960)	0.0558 (0.0586)	1.724 (1.286)	46.88 (57.70)
[110/157]	0.0947 (0.0959)	0.0570 (0.0585)	1.317 (1.294)	62.50 (57.52)
[120/157]	0.1068 (0.0961)	0.0666 (0.0587)	0.939 (1.287)	71.88 (57.88)
[130/157]	0.0967 (0.0965)	0.0587 (0.0590)	1.231 (1.275)	68.75 (58.54)
[140/157]	0.0946 (0.0964)	0.0573 (0.0589)	1.214 (1.275)	71.88 (58.75)
[150/157]	0.1204 (0.0969)	0.0799 (0.0594)	1.046 (1.275)	62.50 (58.59)
[156/157]	0.0870 (0.0972)	0.0607 (0.0598)	1.487 (1.274)	50.00 (58.62)
 * Train Acc 58.620
 * Val Acc 61.600, Total time 0.59
 * Val loss 1.090, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0413 (0.0413)	0.0085 (0.0085)	1.300 (1.300)	56.25 (56.25)
[10/157]	0.0967 (0.0916)	0.0586 (0.0540)	0.986 (1.164)	68.75 (64.49)
[20/157]	0.0944 (0.0933)	0.0561 (0.0560)	1.310 (1.204)	53.12 (62.20)
[30/157]	0.0965 (0.0940)	0.0573 (0.0567)	1.328 (1.218)	43.75 (60.99)
[40/157]	0.0958 (0.0945)	0.0568 (0.0568)	0.976 (1.209)	62.50 (61.20)
[50/157]	0.0970 (0.0948)	0.0571 (0.0569)	1.110 (1.246)	62.50 (59.80)
[60/157]	0.1016 (0.0964)	0.0599 (0.0583)	1.418 (1.237)	53.12 (60.14)
[70/157]	0.0994 (0.0970)	0.0601 (0.0587)	1.300 (1.235)	53.12 (60.26)
[80/157]	0.1004 (0.0973)	0.0607 (0.0588)	1.165 (1.238)	62.50 (60.11)
[90/157]	0.1003 (0.0976)	0.0615 (0.0591)	1.086 (1.231)	62.50 (60.13)
[100/157]	0.1001 (0.0978)	0.0614 (0.0592)	1.132 (1.224)	59.38 (60.40)
[110/157]	0.1013 (0.0979)	0.0623 (0.0594)	1.393 (1.224)	62.50 (60.28)
[120/157]	0.1001 (0.0981)	0.0610 (0.0596)	1.219 (1.222)	56.25 (60.30)
[130/157]	0.1001 (0.0983)	0.0621 (0.0597)	1.544 (1.226)	46.88 (60.38)
[140/157]	0.0934 (0.0981)	0.0565 (0.0597)	1.153 (1.227)	56.25 (60.24)
[150/157]	0.0962 (0.0979)	0.0586 (0.0596)	1.491 (1.232)	59.38 (60.16)
[156/157]	0.0792 (0.0977)	0.0546 (0.0595)	1.031 (1.237)	62.50 (59.90)
 * Train Acc 59.900
 * Val Acc 61.200, Total time 0.57
 * Val loss 1.085, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0085 (0.0085)	1.319 (1.319)	53.12 (53.12)
[10/157]	0.0959 (0.0922)	0.0573 (0.0533)	1.122 (1.361)	75.00 (55.68)
[20/157]	0.0950 (0.0936)	0.0570 (0.0555)	1.136 (1.278)	65.62 (57.89)
[30/157]	0.1114 (0.0983)	0.0730 (0.0600)	1.453 (1.276)	43.75 (57.56)
[40/157]	0.0943 (0.0977)	0.0562 (0.0597)	1.169 (1.276)	62.50 (58.08)
[50/157]	0.0991 (0.0991)	0.0600 (0.0609)	1.434 (1.264)	53.12 (58.64)
[60/157]	0.0989 (0.0990)	0.0607 (0.0608)	1.123 (1.244)	65.62 (59.53)
[70/157]	0.0992 (0.0990)	0.0598 (0.0607)	1.152 (1.226)	65.62 (60.52)
[80/157]	0.1006 (0.0990)	0.0604 (0.0607)	0.993 (1.234)	71.88 (60.38)
[90/157]	0.1000 (0.0990)	0.0608 (0.0606)	1.090 (1.230)	62.50 (60.37)
[100/157]	0.0994 (0.0989)	0.0590 (0.0606)	1.036 (1.218)	56.25 (60.77)
[110/157]	0.1001 (0.0989)	0.0611 (0.0605)	0.955 (1.214)	71.88 (60.95)
[120/157]	0.0995 (0.0990)	0.0604 (0.0606)	1.048 (1.213)	65.62 (60.85)
[130/157]	0.0986 (0.0990)	0.0589 (0.0605)	0.801 (1.207)	81.25 (61.21)
[140/157]	0.0976 (0.0990)	0.0593 (0.0605)	1.229 (1.206)	59.38 (61.24)
[150/157]	0.1000 (0.0989)	0.0615 (0.0605)	1.091 (1.204)	53.12 (61.13)
[156/157]	0.0808 (0.0988)	0.0553 (0.0604)	2.117 (1.207)	37.50 (61.20)
 * Train Acc 61.200
 * Val Acc 62.300, Total time 0.60
 * Val loss 1.035, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0087 (0.0087)	1.264 (1.264)	65.62 (65.62)
[10/157]	0.0985 (0.0933)	0.0594 (0.0551)	1.691 (1.189)	50.00 (61.36)
[20/157]	0.0993 (0.0957)	0.0598 (0.0578)	1.026 (1.206)	65.62 (61.16)
[30/157]	0.1003 (0.0968)	0.0617 (0.0587)	1.095 (1.191)	65.62 (61.79)
[40/157]	0.0990 (0.0974)	0.0599 (0.0593)	0.925 (1.181)	81.25 (61.66)
[50/157]	0.1012 (0.0976)	0.0607 (0.0594)	1.522 (1.191)	50.00 (61.70)
[60/157]	0.0978 (0.0978)	0.0591 (0.0596)	1.381 (1.211)	56.25 (61.53)
[70/157]	0.0994 (0.0980)	0.0602 (0.0596)	1.186 (1.205)	71.88 (61.66)
[80/157]	0.0977 (0.0981)	0.0580 (0.0597)	1.008 (1.190)	62.50 (62.31)
[90/157]	0.0990 (0.0981)	0.0598 (0.0598)	1.332 (1.196)	59.38 (62.12)
[100/157]	0.0997 (0.0981)	0.0605 (0.0599)	1.223 (1.199)	62.50 (62.41)
[110/157]	0.0995 (0.0982)	0.0592 (0.0599)	1.725 (1.199)	50.00 (62.19)
[120/157]	0.0999 (0.0983)	0.0613 (0.0599)	1.143 (1.194)	71.88 (62.45)
[130/157]	0.0953 (0.0983)	0.0575 (0.0599)	1.421 (1.198)	59.38 (62.29)
[140/157]	0.0956 (0.0981)	0.0570 (0.0598)	0.980 (1.198)	75.00 (62.32)
[150/157]	0.0940 (0.0979)	0.0567 (0.0597)	1.095 (1.196)	65.62 (62.21)
[156/157]	0.0787 (0.0976)	0.0538 (0.0596)	0.953 (1.190)	75.00 (62.60)
 * Train Acc 62.600
 * Val Acc 65.700, Total time 0.59
 * Val loss 1.005, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0085 (0.0085)	1.364 (1.364)	56.25 (56.25)
[10/157]	0.1011 (0.0945)	0.0621 (0.0557)	0.987 (1.122)	68.75 (63.35)
[20/157]	0.1020 (0.0973)	0.0633 (0.0588)	0.878 (1.108)	78.12 (65.48)
[30/157]	0.1002 (0.0986)	0.0611 (0.0600)	1.283 (1.129)	62.50 (64.11)
[40/157]	0.1019 (0.0990)	0.0625 (0.0604)	1.084 (1.129)	59.38 (64.79)
[50/157]	0.1034 (0.0994)	0.0621 (0.0608)	1.120 (1.143)	65.62 (63.73)
[60/157]	0.1012 (0.0996)	0.0616 (0.0609)	1.035 (1.159)	71.88 (63.47)
[70/157]	0.1006 (0.0997)	0.0618 (0.0610)	0.817 (1.160)	87.50 (63.42)
[80/157]	0.0971 (0.0997)	0.0589 (0.0609)	1.181 (1.172)	59.38 (62.89)
[90/157]	0.0943 (0.0992)	0.0563 (0.0606)	1.228 (1.174)	59.38 (62.98)
[100/157]	0.0970 (0.0988)	0.0584 (0.0604)	1.238 (1.171)	50.00 (62.72)
[110/157]	0.0964 (0.0985)	0.0578 (0.0601)	0.897 (1.164)	71.88 (62.87)
[120/157]	0.0943 (0.0982)	0.0570 (0.0600)	1.334 (1.167)	68.75 (62.78)
[130/157]	0.0958 (0.0980)	0.0579 (0.0599)	1.603 (1.169)	46.88 (63.10)
[140/157]	0.0966 (0.0978)	0.0584 (0.0597)	0.981 (1.160)	71.88 (63.30)
[150/157]	0.0924 (0.0976)	0.0545 (0.0596)	0.690 (1.155)	75.00 (63.33)
[156/157]	0.0780 (0.0974)	0.0533 (0.0595)	1.559 (1.158)	50.00 (63.14)
 * Train Acc 63.140
 * Val Acc 64.400, Total time 0.58
 * Val loss 1.015, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0086 (0.0086)	0.884 (0.884)	68.75 (68.75)
[10/157]	0.0948 (0.0924)	0.0570 (0.0544)	1.347 (1.145)	56.25 (65.34)
[20/157]	0.0958 (0.0937)	0.0585 (0.0563)	0.949 (1.123)	62.50 (62.65)
[30/157]	0.1080 (0.0953)	0.0679 (0.0579)	1.116 (1.127)	62.50 (63.51)
[40/157]	0.0956 (0.0979)	0.0582 (0.0602)	1.183 (1.134)	53.12 (62.96)
[50/157]	0.0965 (0.0974)	0.0593 (0.0599)	1.407 (1.128)	50.00 (63.48)
[60/157]	0.0968 (0.0971)	0.0589 (0.0596)	1.310 (1.128)	59.38 (63.52)
[70/157]	0.0966 (0.0969)	0.0583 (0.0594)	1.016 (1.133)	68.75 (63.51)
[80/157]	0.1209 (0.0976)	0.0803 (0.0601)	1.188 (1.132)	68.75 (63.70)
[90/157]	0.0947 (0.0977)	0.0574 (0.0602)	0.804 (1.126)	78.12 (64.32)
[100/157]	0.0946 (0.0985)	0.0560 (0.0608)	1.471 (1.132)	50.00 (63.89)
[110/157]	0.0987 (0.0989)	0.0588 (0.0611)	1.260 (1.136)	50.00 (63.68)
[120/157]	0.0971 (0.0988)	0.0586 (0.0609)	1.044 (1.134)	62.50 (63.53)
[130/157]	0.0976 (0.0987)	0.0600 (0.0608)	1.062 (1.139)	59.38 (63.07)
[140/157]	0.0974 (0.0986)	0.0599 (0.0607)	1.304 (1.147)	65.62 (62.92)
[150/157]	0.0980 (0.0986)	0.0594 (0.0607)	1.025 (1.143)	68.75 (63.00)
[156/157]	0.0796 (0.0984)	0.0537 (0.0606)	0.883 (1.141)	75.00 (63.04)
 * Train Acc 63.040
 * Val Acc 66.100, Total time 0.60
 * Val loss 0.985, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0089 (0.0089)	1.252 (1.252)	56.25 (56.25)
[10/157]	0.1000 (0.0929)	0.0602 (0.0546)	1.018 (1.168)	65.62 (59.38)
[20/157]	0.0989 (0.0952)	0.0604 (0.0570)	1.090 (1.135)	62.50 (63.54)
[30/157]	0.0972 (0.0960)	0.0590 (0.0578)	1.044 (1.136)	65.62 (63.51)
[40/157]	0.0981 (0.0963)	0.0602 (0.0584)	0.883 (1.112)	75.00 (64.48)
[50/157]	0.0981 (0.0966)	0.0599 (0.0587)	1.226 (1.118)	71.88 (63.48)
[60/157]	0.0973 (0.0968)	0.0589 (0.0589)	0.925 (1.120)	71.88 (64.19)
[70/157]	0.0984 (0.0970)	0.0603 (0.0590)	0.851 (1.122)	81.25 (64.35)
[80/157]	0.0992 (0.0971)	0.0600 (0.0592)	1.162 (1.122)	53.12 (64.04)
[90/157]	0.0979 (0.0971)	0.0598 (0.0593)	1.191 (1.113)	65.62 (64.01)
[100/157]	0.0995 (0.0972)	0.0593 (0.0593)	1.246 (1.120)	56.25 (63.92)
[110/157]	0.0985 (0.0973)	0.0601 (0.0594)	1.112 (1.118)	62.50 (63.85)
[120/157]	0.0983 (0.0973)	0.0596 (0.0594)	1.103 (1.128)	59.38 (63.38)
[130/157]	0.0971 (0.0973)	0.0586 (0.0594)	0.922 (1.127)	65.62 (63.36)
[140/157]	0.0985 (0.0973)	0.0601 (0.0594)	1.199 (1.121)	65.62 (63.56)
[150/157]	0.0979 (0.0973)	0.0599 (0.0594)	1.260 (1.122)	65.62 (63.64)
[156/157]	0.0793 (0.0972)	0.0542 (0.0594)	1.681 (1.124)	37.50 (63.64)
 * Train Acc 63.640
 * Val Acc 67.200, Total time 0.59
 * Val loss 0.958, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0416 (0.0416)	0.0082 (0.0082)	0.947 (0.947)	65.62 (65.62)
[10/157]	0.1057 (0.1034)	0.0658 (0.0649)	1.209 (1.204)	59.38 (60.23)
[20/157]	0.0951 (0.0995)	0.0586 (0.0617)	1.011 (1.156)	65.62 (61.76)
[30/157]	0.1089 (0.1017)	0.0680 (0.0636)	1.036 (1.117)	65.62 (63.91)
[40/157]	0.0957 (0.1005)	0.0575 (0.0624)	0.911 (1.092)	71.88 (64.79)
[50/157]	0.0956 (0.0995)	0.0578 (0.0616)	1.119 (1.097)	59.38 (64.58)
[60/157]	0.0955 (0.0989)	0.0576 (0.0611)	1.208 (1.107)	68.75 (64.91)
[70/157]	0.0967 (0.0984)	0.0581 (0.0607)	0.926 (1.115)	81.25 (64.48)
[80/157]	0.1035 (0.0997)	0.0628 (0.0618)	1.527 (1.127)	65.62 (64.47)
[90/157]	0.0951 (0.0993)	0.0564 (0.0614)	1.185 (1.122)	65.62 (64.49)
[100/157]	0.0942 (0.0989)	0.0554 (0.0610)	0.779 (1.118)	75.00 (64.33)
[110/157]	0.0987 (0.0994)	0.0605 (0.0614)	1.137 (1.114)	62.50 (64.30)
[120/157]	0.0998 (0.0994)	0.0611 (0.0614)	0.817 (1.106)	78.12 (64.51)
[130/157]	0.1003 (0.0994)	0.0611 (0.0614)	1.424 (1.109)	62.50 (64.62)
[140/157]	0.1001 (0.0994)	0.0599 (0.0614)	1.447 (1.115)	59.38 (64.43)
[150/157]	0.1004 (0.0995)	0.0612 (0.0614)	0.899 (1.118)	71.88 (64.69)
[156/157]	0.0833 (0.0993)	0.0566 (0.0613)	0.853 (1.116)	87.50 (64.76)
 * Train Acc 64.760
 * Val Acc 66.700, Total time 0.60
 * Val loss 0.961, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0084 (0.0084)	0.945 (0.945)	59.38 (59.38)
[10/157]	0.1001 (0.0938)	0.0612 (0.0557)	1.445 (1.048)	53.12 (67.05)
[20/157]	0.1017 (0.0966)	0.0600 (0.0581)	1.089 (1.074)	75.00 (66.67)
[30/157]	0.1008 (0.0976)	0.0606 (0.0589)	0.907 (1.092)	68.75 (66.63)
[40/157]	0.1010 (0.0981)	0.0612 (0.0593)	0.760 (1.071)	71.88 (67.68)
[50/157]	0.1001 (0.0986)	0.0597 (0.0595)	0.943 (1.081)	75.00 (67.10)
[60/157]	0.0989 (0.0988)	0.0600 (0.0595)	0.978 (1.071)	71.88 (67.21)
[70/157]	0.1004 (0.0989)	0.0615 (0.0597)	1.130 (1.079)	56.25 (66.64)
[80/157]	0.0991 (0.0990)	0.0609 (0.0599)	0.648 (1.068)	84.38 (67.28)
[90/157]	0.1001 (0.0990)	0.0615 (0.0601)	0.693 (1.077)	81.25 (66.59)
[100/157]	0.1000 (0.0991)	0.0609 (0.0601)	1.071 (1.069)	71.88 (67.14)
[110/157]	0.1007 (0.0991)	0.0610 (0.0602)	1.331 (1.082)	62.50 (66.55)
[120/157]	0.1001 (0.0992)	0.0613 (0.0603)	0.989 (1.088)	78.12 (66.35)
[130/157]	0.0990 (0.0992)	0.0608 (0.0603)	0.982 (1.092)	78.12 (66.22)
[140/157]	0.0997 (0.0992)	0.0612 (0.0604)	1.113 (1.095)	71.88 (66.13)
[150/157]	0.0995 (0.0992)	0.0605 (0.0604)	1.489 (1.099)	50.00 (66.08)
[156/157]	0.0849 (0.0991)	0.0561 (0.0604)	1.338 (1.104)	50.00 (65.94)
 * Train Acc 65.940
 * Val Acc 66.800, Total time 0.60
 * Val loss 0.972, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0090 (0.0090)	1.243 (1.243)	65.62 (65.62)
[10/157]	0.0982 (0.0940)	0.0589 (0.0554)	1.154 (1.084)	68.75 (66.48)
[20/157]	0.1023 (0.0967)	0.0628 (0.0582)	1.122 (1.065)	59.38 (67.56)
[30/157]	0.1004 (0.0977)	0.0612 (0.0591)	1.193 (1.090)	65.62 (66.33)
[40/157]	0.1009 (0.0984)	0.0611 (0.0594)	1.026 (1.095)	68.75 (65.55)
[50/157]	0.1002 (0.0987)	0.0602 (0.0597)	1.078 (1.082)	62.50 (66.12)
[60/157]	0.0990 (0.0988)	0.0606 (0.0598)	1.227 (1.092)	59.38 (65.52)
[70/157]	0.1014 (0.0990)	0.0619 (0.0601)	0.905 (1.094)	71.88 (65.45)
[80/157]	0.1006 (0.0990)	0.0618 (0.0603)	1.081 (1.094)	62.50 (65.47)
[90/157]	0.1002 (0.0992)	0.0614 (0.0604)	0.875 (1.087)	71.88 (65.56)
[100/157]	0.0999 (0.0992)	0.0610 (0.0605)	1.231 (1.083)	62.50 (65.62)
[110/157]	0.0996 (0.0992)	0.0609 (0.0605)	0.834 (1.080)	68.75 (65.57)
[120/157]	0.0996 (0.0992)	0.0612 (0.0606)	0.862 (1.076)	75.00 (65.93)
[130/157]	0.0993 (0.0992)	0.0606 (0.0606)	1.088 (1.081)	68.75 (66.05)
[140/157]	0.1003 (0.0992)	0.0604 (0.0607)	0.966 (1.082)	75.00 (66.18)
[150/157]	0.0991 (0.0992)	0.0597 (0.0606)	1.245 (1.085)	59.38 (66.10)
[156/157]	0.0865 (0.0991)	0.0568 (0.0606)	0.840 (1.087)	75.00 (66.06)
 * Train Acc 66.060
 * Val Acc 67.800, Total time 0.60
 * Val loss 0.928, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0088 (0.0088)	0.907 (0.907)	78.12 (78.12)
[10/157]	0.1006 (0.0946)	0.0613 (0.0559)	1.294 (1.083)	50.00 (65.62)
[20/157]	0.0998 (0.0970)	0.0601 (0.0587)	0.851 (1.058)	78.12 (65.18)
[30/157]	0.0997 (0.0977)	0.0612 (0.0594)	1.004 (1.091)	65.62 (64.72)
[40/157]	0.0997 (0.0980)	0.0612 (0.0598)	0.901 (1.091)	71.88 (64.63)
[50/157]	0.0998 (0.0984)	0.0611 (0.0600)	1.225 (1.089)	53.12 (64.46)
[60/157]	0.0987 (0.0986)	0.0605 (0.0603)	1.185 (1.084)	59.38 (64.75)
[70/157]	0.1001 (0.0987)	0.0604 (0.0604)	1.364 (1.082)	59.38 (64.79)
[80/157]	0.0996 (0.0987)	0.0616 (0.0605)	1.302 (1.085)	53.12 (64.54)
[90/157]	0.1006 (0.0988)	0.0619 (0.0605)	1.060 (1.088)	68.75 (64.49)
[100/157]	0.0992 (0.0989)	0.0604 (0.0606)	1.408 (1.087)	59.38 (64.73)
[110/157]	0.1009 (0.0989)	0.0617 (0.0606)	1.335 (1.086)	50.00 (64.70)
[120/157]	0.1001 (0.0989)	0.0616 (0.0607)	1.070 (1.087)	65.62 (64.77)
[130/157]	0.0998 (0.0990)	0.0601 (0.0606)	1.034 (1.097)	75.00 (64.65)
[140/157]	0.0991 (0.0990)	0.0603 (0.0607)	1.039 (1.091)	62.50 (64.85)
[150/157]	0.1004 (0.0990)	0.0610 (0.0607)	0.867 (1.084)	71.88 (65.17)
[156/157]	0.0838 (0.0990)	0.0563 (0.0607)	1.084 (1.081)	62.50 (65.44)
 * Train Acc 65.440
 * Val Acc 67.400, Total time 0.61
 * Val loss 0.935, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0087 (0.0087)	1.241 (1.241)	59.38 (59.38)
[10/157]	0.0986 (0.0939)	0.0595 (0.0557)	1.317 (1.097)	56.25 (65.62)
[20/157]	0.0962 (0.0964)	0.0574 (0.0579)	1.184 (1.078)	50.00 (66.22)
[30/157]	0.0998 (0.0972)	0.0612 (0.0589)	1.041 (1.064)	68.75 (66.33)
[40/157]	0.1004 (0.0977)	0.0605 (0.0593)	1.202 (1.053)	56.25 (67.15)
[50/157]	0.0987 (0.0980)	0.0602 (0.0595)	1.003 (1.048)	75.00 (67.71)
[60/157]	0.0997 (0.0982)	0.0610 (0.0598)	1.085 (1.053)	59.38 (66.96)
[70/157]	0.0988 (0.0983)	0.0602 (0.0599)	1.499 (1.060)	53.12 (66.90)
[80/157]	0.0996 (0.0984)	0.0607 (0.0600)	0.878 (1.065)	78.12 (66.78)
[90/157]	0.0995 (0.0985)	0.0602 (0.0602)	1.047 (1.067)	65.62 (66.86)
[100/157]	0.1001 (0.0986)	0.0608 (0.0602)	0.976 (1.057)	65.62 (67.17)
[110/157]	0.0981 (0.0987)	0.0589 (0.0602)	1.245 (1.064)	65.62 (67.06)
[120/157]	0.1005 (0.0987)	0.0605 (0.0603)	1.183 (1.067)	65.62 (66.86)
[130/157]	0.1009 (0.0988)	0.0607 (0.0603)	0.874 (1.063)	81.25 (67.08)
[140/157]	0.1003 (0.0989)	0.0604 (0.0603)	1.013 (1.063)	65.62 (66.89)
[150/157]	0.1003 (0.0989)	0.0603 (0.0603)	1.323 (1.064)	62.50 (66.80)
[156/157]	0.0838 (0.0988)	0.0556 (0.0603)	1.229 (1.064)	62.50 (66.90)
 * Train Acc 66.900
 * Val Acc 68.200, Total time 0.60
 * Val loss 0.930, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0444 (0.0444)	0.0088 (0.0088)	1.327 (1.327)	59.38 (59.38)
[10/157]	0.1005 (0.0938)	0.0614 (0.0553)	0.841 (1.033)	68.75 (66.76)
[20/157]	0.0996 (0.0962)	0.0612 (0.0578)	1.009 (1.051)	68.75 (68.15)
[30/157]	0.1000 (0.0973)	0.0617 (0.0590)	0.880 (1.045)	68.75 (68.65)
[40/157]	0.0975 (0.0973)	0.0587 (0.0593)	1.080 (1.068)	59.38 (67.30)
[50/157]	0.0948 (0.0969)	0.0574 (0.0591)	1.124 (1.068)	62.50 (67.46)
[60/157]	0.0942 (0.0966)	0.0569 (0.0590)	0.898 (1.065)	68.75 (67.26)
[70/157]	0.0979 (0.0965)	0.0583 (0.0589)	0.932 (1.072)	75.00 (66.77)
[80/157]	0.0962 (0.0964)	0.0570 (0.0588)	1.015 (1.065)	62.50 (67.09)
[90/157]	0.0965 (0.0963)	0.0575 (0.0587)	0.965 (1.062)	62.50 (66.90)
[100/157]	0.0948 (0.0962)	0.0568 (0.0587)	1.050 (1.056)	62.50 (66.92)
[110/157]	0.0960 (0.0961)	0.0579 (0.0586)	0.795 (1.059)	75.00 (66.98)
[120/157]	0.0978 (0.0962)	0.0589 (0.0586)	1.053 (1.061)	71.88 (66.86)
[130/157]	0.1008 (0.0965)	0.0612 (0.0588)	1.107 (1.051)	62.50 (67.03)
[140/157]	0.1001 (0.0968)	0.0608 (0.0590)	1.026 (1.053)	62.50 (66.71)
[150/157]	0.1001 (0.0970)	0.0608 (0.0591)	0.990 (1.052)	65.62 (66.89)
[156/157]	0.0835 (0.0970)	0.0565 (0.0592)	1.348 (1.049)	50.00 (66.98)
 * Train Acc 66.980
 * Val Acc 68.500, Total time 0.59
 * Val loss 0.914, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0086 (0.0086)	0.882 (0.882)	71.88 (71.88)
[10/157]	0.1026 (0.0970)	0.0637 (0.0584)	1.026 (0.951)	59.38 (67.33)
[20/157]	0.0948 (0.0985)	0.0572 (0.0602)	1.228 (1.009)	56.25 (65.62)
[30/157]	0.0953 (0.0976)	0.0578 (0.0594)	0.923 (1.015)	75.00 (66.23)
[40/157]	0.0962 (0.0970)	0.0579 (0.0592)	1.094 (1.019)	56.25 (67.07)
[50/157]	0.0947 (0.0967)	0.0568 (0.0590)	0.763 (1.021)	68.75 (67.89)
[60/157]	0.1026 (0.0981)	0.0634 (0.0603)	1.223 (1.032)	53.12 (67.42)
[70/157]	0.1053 (0.0990)	0.0649 (0.0610)	1.281 (1.044)	62.50 (67.25)
[80/157]	0.1043 (0.0996)	0.0648 (0.0615)	0.865 (1.050)	65.62 (66.94)
[90/157]	0.1061 (0.1001)	0.0639 (0.0619)	0.920 (1.042)	81.25 (67.41)
[100/157]	0.0944 (0.1003)	0.0568 (0.0620)	1.186 (1.040)	59.38 (67.79)
[110/157]	0.0945 (0.0999)	0.0559 (0.0616)	1.129 (1.039)	62.50 (68.13)
[120/157]	0.0960 (0.0995)	0.0567 (0.0613)	0.852 (1.037)	71.88 (68.10)
[130/157]	0.0956 (0.0992)	0.0577 (0.0610)	1.012 (1.041)	65.62 (67.84)
[140/157]	0.1016 (0.0993)	0.0616 (0.0610)	1.402 (1.039)	59.38 (68.06)
[150/157]	0.0991 (0.0993)	0.0612 (0.0611)	1.025 (1.043)	71.88 (67.78)
[156/157]	0.0865 (0.0993)	0.0580 (0.0611)	1.385 (1.042)	75.00 (67.80)
 * Train Acc 67.800
 * Val Acc 68.500, Total time 0.60
 * Val loss 0.902, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0084 (0.0084)	1.243 (1.243)	59.38 (59.38)
[10/157]	0.1007 (0.0951)	0.0613 (0.0571)	1.038 (1.011)	68.75 (68.75)
[20/157]	0.1004 (0.0978)	0.0611 (0.0595)	1.051 (1.006)	56.25 (68.30)
[30/157]	0.1004 (0.0985)	0.0617 (0.0602)	0.978 (1.046)	71.88 (66.43)
[40/157]	0.0960 (0.0979)	0.0584 (0.0599)	0.855 (1.033)	68.75 (67.38)
[50/157]	0.0966 (0.0975)	0.0588 (0.0595)	1.239 (1.030)	56.25 (66.97)
[60/157]	0.0944 (0.0971)	0.0574 (0.0594)	0.903 (1.033)	75.00 (67.21)
[70/157]	0.1064 (0.0970)	0.0658 (0.0593)	0.828 (1.038)	78.12 (67.34)
[80/157]	0.0949 (0.0981)	0.0573 (0.0603)	0.963 (1.034)	68.75 (67.52)
[90/157]	0.0965 (0.0980)	0.0588 (0.0601)	0.855 (1.029)	75.00 (67.62)
[100/157]	0.0967 (0.0979)	0.0584 (0.0600)	1.179 (1.022)	56.25 (67.91)
[110/157]	0.0985 (0.0978)	0.0595 (0.0599)	0.827 (1.022)	81.25 (68.07)
[120/157]	0.0948 (0.0977)	0.0563 (0.0598)	1.162 (1.024)	62.50 (68.16)
[130/157]	0.1089 (0.0980)	0.0674 (0.0600)	0.917 (1.028)	68.75 (68.08)
[140/157]	0.0982 (0.0984)	0.0597 (0.0604)	1.224 (1.032)	59.38 (67.95)
[150/157]	0.0984 (0.0982)	0.0589 (0.0602)	0.983 (1.036)	65.62 (67.78)
[156/157]	0.0773 (0.0980)	0.0526 (0.0601)	1.553 (1.041)	62.50 (67.66)
 * Train Acc 67.660
 * Val Acc 69.500, Total time 0.58
 * Val loss 0.882, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0086 (0.0086)	1.421 (1.421)	56.25 (56.25)
[10/157]	0.0969 (0.0924)	0.0587 (0.0545)	0.758 (0.999)	78.12 (69.60)
[20/157]	0.1004 (0.0985)	0.0607 (0.0604)	1.048 (0.993)	68.75 (69.79)
[30/157]	0.1006 (0.0990)	0.0615 (0.0608)	1.014 (1.004)	68.75 (70.46)
[40/157]	0.1010 (0.0992)	0.0623 (0.0608)	1.116 (1.012)	56.25 (69.97)
[50/157]	0.1008 (0.0994)	0.0622 (0.0610)	1.159 (1.019)	65.62 (69.85)
[60/157]	0.1006 (0.0994)	0.0622 (0.0611)	0.788 (1.010)	71.88 (69.83)
[70/157]	0.1015 (0.0995)	0.0624 (0.0612)	0.586 (1.006)	87.50 (69.45)
[80/157]	0.1002 (0.0995)	0.0615 (0.0613)	0.973 (1.006)	65.62 (69.71)
[90/157]	0.1006 (0.0996)	0.0617 (0.0613)	1.079 (1.001)	65.62 (69.81)
[100/157]	0.0999 (0.0995)	0.0608 (0.0613)	1.102 (1.013)	56.25 (68.87)
[110/157]	0.0999 (0.0996)	0.0605 (0.0612)	1.281 (1.020)	62.50 (68.78)
[120/157]	0.0986 (0.0995)	0.0602 (0.0612)	1.197 (1.023)	56.25 (68.49)
[130/157]	0.0993 (0.0995)	0.0610 (0.0612)	0.932 (1.029)	68.75 (68.30)
[140/157]	0.1000 (0.0996)	0.0615 (0.0613)	0.965 (1.027)	75.00 (68.35)
[150/157]	0.0997 (0.0995)	0.0601 (0.0612)	1.126 (1.029)	62.50 (68.07)
[156/157]	0.0824 (0.0994)	0.0560 (0.0612)	1.205 (1.029)	50.00 (68.02)
 * Train Acc 68.020
 * Val Acc 69.000, Total time 0.59
 * Val loss 0.878, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0090 (0.0090)	0.934 (0.934)	65.62 (65.62)
[10/157]	0.1001 (0.0938)	0.0606 (0.0556)	1.228 (0.976)	59.38 (67.33)
[20/157]	0.1006 (0.0965)	0.0619 (0.0581)	0.976 (1.001)	71.88 (66.37)
[30/157]	0.1000 (0.0974)	0.0617 (0.0592)	0.637 (0.998)	84.38 (67.54)
[40/157]	0.0995 (0.0979)	0.0603 (0.0596)	0.827 (0.992)	75.00 (67.84)
[50/157]	0.1004 (0.0982)	0.0603 (0.0598)	0.946 (1.000)	75.00 (68.01)
[60/157]	0.1003 (0.0984)	0.0613 (0.0600)	1.042 (0.984)	62.50 (68.24)
[70/157]	0.1002 (0.0987)	0.0603 (0.0600)	0.948 (0.997)	65.62 (67.96)
[80/157]	0.1007 (0.0988)	0.0615 (0.0601)	1.187 (1.009)	62.50 (67.52)
[90/157]	0.1002 (0.0989)	0.0608 (0.0602)	1.226 (1.016)	59.38 (67.48)
[100/157]	0.1027 (0.0990)	0.0637 (0.0604)	1.227 (1.025)	56.25 (67.11)
[110/157]	0.0988 (0.0990)	0.0594 (0.0603)	1.220 (1.022)	56.25 (67.09)
[120/157]	0.1002 (0.0990)	0.0610 (0.0604)	0.843 (1.029)	71.88 (67.15)
[130/157]	0.0997 (0.0991)	0.0617 (0.0604)	1.218 (1.024)	56.25 (67.25)
[140/157]	0.0991 (0.0991)	0.0599 (0.0605)	0.758 (1.021)	81.25 (67.53)
[150/157]	0.0991 (0.0991)	0.0606 (0.0605)	1.108 (1.025)	62.50 (67.34)
[156/157]	0.0819 (0.0990)	0.0556 (0.0605)	1.191 (1.029)	62.50 (67.34)
 * Train Acc 67.340
 * Val Acc 70.500, Total time 0.59
 * Val loss 0.881, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0085 (0.0085)	0.966 (0.966)	78.12 (78.12)
[10/157]	0.1011 (0.0939)	0.0626 (0.0559)	1.115 (1.042)	68.75 (69.03)
[20/157]	0.1006 (0.0967)	0.0618 (0.0588)	1.007 (1.018)	68.75 (68.75)
[30/157]	0.1010 (0.0978)	0.0627 (0.0598)	1.152 (1.010)	65.62 (68.75)
[40/157]	0.1005 (0.0983)	0.0614 (0.0603)	1.117 (1.023)	62.50 (67.38)
[50/157]	0.1019 (0.0987)	0.0620 (0.0605)	0.942 (1.029)	65.62 (66.97)
[60/157]	0.0997 (0.0989)	0.0612 (0.0605)	0.807 (1.023)	78.12 (67.37)
[70/157]	0.0995 (0.0990)	0.0606 (0.0607)	1.004 (1.021)	68.75 (67.21)
[80/157]	0.0995 (0.0990)	0.0596 (0.0607)	1.118 (1.018)	62.50 (67.32)
[90/157]	0.1013 (0.0991)	0.0616 (0.0606)	1.234 (1.018)	65.62 (67.58)
[100/157]	0.0990 (0.0991)	0.0601 (0.0606)	1.043 (1.016)	65.62 (67.42)
[110/157]	0.1001 (0.0991)	0.0610 (0.0607)	0.831 (1.018)	75.00 (67.37)
[120/157]	0.1003 (0.0992)	0.0604 (0.0608)	0.924 (1.020)	68.75 (67.30)
[130/157]	0.0997 (0.0992)	0.0603 (0.0607)	0.815 (1.013)	75.00 (67.60)
[140/157]	0.1004 (0.0992)	0.0605 (0.0607)	0.815 (1.014)	65.62 (67.64)
[150/157]	0.0989 (0.0992)	0.0599 (0.0607)	1.224 (1.013)	59.38 (67.90)
[156/157]	0.0840 (0.0991)	0.0558 (0.0608)	0.562 (1.016)	87.50 (67.82)
 * Train Acc 67.820
 * Val Acc 70.000, Total time 0.60
 * Val loss 0.873, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0086 (0.0086)	0.757 (0.757)	84.38 (84.38)
[10/157]	0.0961 (0.0927)	0.0582 (0.0541)	1.160 (1.008)	65.62 (71.31)
[20/157]	0.0950 (0.0941)	0.0573 (0.0562)	1.019 (1.058)	65.62 (68.90)
[30/157]	0.1054 (0.0949)	0.0658 (0.0572)	1.382 (1.078)	59.38 (67.24)
[40/157]	0.1063 (0.0971)	0.0663 (0.0589)	1.138 (1.060)	65.62 (67.15)
[50/157]	0.1039 (0.0983)	0.0644 (0.0598)	0.889 (1.043)	75.00 (67.83)
[60/157]	0.1019 (0.0991)	0.0632 (0.0606)	1.029 (1.031)	65.62 (68.03)
[70/157]	0.0975 (0.0986)	0.0589 (0.0601)	0.828 (1.028)	75.00 (68.05)
[80/157]	0.0971 (0.0982)	0.0584 (0.0598)	0.916 (1.025)	78.12 (68.09)
[90/157]	0.0957 (0.0979)	0.0572 (0.0596)	1.121 (1.028)	59.38 (67.89)
[100/157]	0.1177 (0.0986)	0.0779 (0.0603)	0.854 (1.023)	68.75 (68.04)
[110/157]	0.0959 (0.0985)	0.0582 (0.0602)	0.773 (1.010)	84.38 (68.38)
[120/157]	0.0945 (0.0983)	0.0571 (0.0600)	0.973 (1.006)	65.62 (68.34)
[130/157]	0.1136 (0.0982)	0.0735 (0.0601)	0.951 (1.006)	68.75 (68.51)
[140/157]	0.0964 (0.0989)	0.0586 (0.0607)	0.750 (1.008)	84.38 (68.42)
[150/157]	0.0952 (0.0986)	0.0576 (0.0606)	0.855 (1.004)	75.00 (68.65)
[156/157]	0.0770 (0.0984)	0.0520 (0.0604)	1.769 (1.005)	50.00 (68.68)
 * Train Acc 68.680
 * Val Acc 68.600, Total time 0.59
 * Val loss 0.889, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0084 (0.0084)	1.084 (1.084)	62.50 (62.50)
[10/157]	0.1048 (0.0966)	0.0648 (0.0580)	1.177 (1.011)	78.12 (69.32)
[20/157]	0.1039 (0.0992)	0.0638 (0.0608)	0.695 (1.002)	87.50 (69.94)
[30/157]	0.1068 (0.1003)	0.0635 (0.0617)	0.899 (1.028)	81.25 (68.55)
[40/157]	0.1012 (0.1007)	0.0619 (0.0618)	0.989 (1.022)	75.00 (68.90)
[50/157]	0.1009 (0.1010)	0.0617 (0.0620)	0.928 (1.016)	71.88 (68.63)
[60/157]	0.0955 (0.1009)	0.0572 (0.0620)	0.696 (1.013)	81.25 (68.44)
[70/157]	0.0961 (0.1002)	0.0581 (0.0615)	1.279 (1.009)	65.62 (68.53)
[80/157]	0.0976 (0.0996)	0.0586 (0.0610)	1.171 (1.006)	56.25 (69.02)
[90/157]	0.0969 (0.0991)	0.0583 (0.0607)	0.896 (1.003)	78.12 (68.85)
[100/157]	0.0949 (0.0988)	0.0568 (0.0605)	0.894 (0.999)	75.00 (69.03)
[110/157]	0.0958 (0.0984)	0.0576 (0.0603)	1.060 (1.000)	68.75 (69.17)
[120/157]	0.1026 (0.0986)	0.0621 (0.0603)	0.977 (1.005)	71.88 (69.06)
[130/157]	0.1016 (0.0988)	0.0614 (0.0605)	0.735 (0.998)	81.25 (69.16)
[140/157]	0.1015 (0.0991)	0.0618 (0.0607)	1.066 (1.008)	62.50 (68.82)
[150/157]	0.1005 (0.0992)	0.0612 (0.0608)	1.012 (1.008)	68.75 (68.71)
[156/157]	0.0863 (0.0992)	0.0583 (0.0608)	0.705 (1.007)	75.00 (68.64)
 * Train Acc 68.640
 * Val Acc 69.000, Total time 0.58
 * Val loss 0.881, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0087 (0.0087)	0.824 (0.824)	68.75 (68.75)
[10/157]	0.0951 (0.0923)	0.0568 (0.0542)	1.226 (0.979)	46.88 (69.03)
[20/157]	0.0950 (0.0938)	0.0576 (0.0562)	1.113 (0.970)	59.38 (67.71)
[30/157]	0.0957 (0.0944)	0.0576 (0.0571)	1.084 (0.981)	65.62 (67.84)
[40/157]	0.0945 (0.0946)	0.0565 (0.0574)	0.904 (0.974)	81.25 (68.75)
[50/157]	0.0946 (0.0948)	0.0564 (0.0576)	0.930 (0.986)	78.12 (68.38)
[60/157]	0.0955 (0.0965)	0.0577 (0.0592)	0.855 (0.990)	62.50 (68.08)
[70/157]	0.1011 (0.0966)	0.0614 (0.0593)	0.813 (0.998)	81.25 (68.44)
[80/157]	0.0999 (0.0971)	0.0609 (0.0596)	0.943 (1.007)	68.75 (68.21)
[90/157]	0.0997 (0.0974)	0.0580 (0.0598)	0.861 (1.000)	71.88 (68.41)
[100/157]	0.0999 (0.0977)	0.0611 (0.0599)	0.991 (1.000)	71.88 (68.41)
[110/157]	0.0987 (0.0979)	0.0598 (0.0600)	0.954 (0.992)	71.88 (68.64)
[120/157]	0.1002 (0.0981)	0.0611 (0.0601)	0.978 (0.991)	68.75 (68.60)
[130/157]	0.0994 (0.0982)	0.0604 (0.0603)	1.102 (0.991)	65.62 (68.68)
[140/157]	0.1004 (0.0983)	0.0609 (0.0603)	0.888 (0.992)	75.00 (68.71)
[150/157]	0.0999 (0.0984)	0.0609 (0.0604)	1.041 (0.992)	71.88 (68.69)
[156/157]	0.0849 (0.0984)	0.0580 (0.0604)	0.968 (0.995)	75.00 (68.60)
 * Train Acc 68.600
 * Val Acc 71.000, Total time 0.60
 * Val loss 0.859, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0083 (0.0083)	0.904 (0.904)	59.38 (59.38)
[10/157]	0.1006 (0.0945)	0.0612 (0.0560)	0.920 (0.948)	65.62 (68.47)
[20/157]	0.1005 (0.0971)	0.0615 (0.0587)	0.905 (0.934)	71.88 (69.35)
[30/157]	0.0995 (0.0980)	0.0605 (0.0595)	1.061 (0.958)	65.62 (68.75)
[40/157]	0.0958 (0.0981)	0.0577 (0.0598)	0.780 (0.963)	78.12 (68.90)
[50/157]	0.0972 (0.0976)	0.0588 (0.0595)	1.102 (0.956)	62.50 (69.42)
[60/157]	0.0935 (0.0972)	0.0562 (0.0592)	1.015 (0.978)	62.50 (68.95)
[70/157]	0.0950 (0.0969)	0.0574 (0.0591)	1.172 (0.983)	65.62 (69.01)
[80/157]	0.0977 (0.0968)	0.0590 (0.0590)	0.708 (0.989)	81.25 (69.25)
[90/157]	0.0958 (0.0966)	0.0578 (0.0589)	1.239 (1.003)	59.38 (68.82)
[100/157]	0.0941 (0.0965)	0.0571 (0.0588)	0.712 (0.990)	81.25 (69.25)
[110/157]	0.1073 (0.0974)	0.0671 (0.0596)	1.096 (0.993)	62.50 (69.20)
[120/157]	0.0955 (0.0973)	0.0577 (0.0596)	1.093 (0.996)	71.88 (69.01)
[130/157]	0.0995 (0.0978)	0.0608 (0.0601)	1.210 (0.994)	68.75 (69.11)
[140/157]	0.0990 (0.0979)	0.0597 (0.0601)	1.146 (0.993)	59.38 (69.15)
[150/157]	0.0991 (0.0980)	0.0608 (0.0602)	0.943 (0.994)	71.88 (68.98)
[156/157]	0.0820 (0.0979)	0.0557 (0.0602)	0.911 (0.989)	62.50 (69.10)
 * Train Acc 69.100
 * Val Acc 70.900, Total time 0.60
 * Val loss 0.856, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0084 (0.0084)	0.591 (0.591)	87.50 (87.50)
[10/157]	0.0940 (0.0994)	0.0569 (0.0615)	1.051 (0.914)	62.50 (72.16)
[20/157]	0.1120 (0.1027)	0.0716 (0.0646)	1.315 (1.023)	62.50 (67.56)
[30/157]	0.0974 (0.1010)	0.0598 (0.0628)	1.109 (1.000)	68.75 (68.15)
[40/157]	0.0973 (0.0999)	0.0594 (0.0620)	1.124 (1.020)	62.50 (67.23)
[50/157]	0.0981 (0.0993)	0.0594 (0.0615)	1.093 (1.025)	68.75 (67.77)
[60/157]	0.0977 (0.0988)	0.0597 (0.0611)	0.930 (1.014)	75.00 (68.49)
[70/157]	0.0962 (0.0985)	0.0579 (0.0608)	1.159 (1.006)	59.38 (68.57)
[80/157]	0.0972 (0.0983)	0.0592 (0.0606)	1.278 (1.020)	71.88 (68.29)
[90/157]	0.0962 (0.0981)	0.0581 (0.0604)	0.990 (1.010)	75.00 (68.82)
[100/157]	0.0974 (0.0980)	0.0584 (0.0603)	0.887 (1.003)	81.25 (69.31)
[110/157]	0.0972 (0.0979)	0.0588 (0.0602)	0.924 (0.989)	68.75 (69.65)
[120/157]	0.0967 (0.0978)	0.0573 (0.0601)	0.574 (0.988)	87.50 (69.68)
[130/157]	0.0973 (0.0977)	0.0595 (0.0600)	0.842 (0.995)	68.75 (69.49)
[140/157]	0.0957 (0.0977)	0.0579 (0.0599)	0.966 (0.995)	65.62 (69.59)
[150/157]	0.1204 (0.0983)	0.0799 (0.0605)	0.979 (0.994)	68.75 (69.47)
[156/157]	0.0798 (0.0983)	0.0536 (0.0605)	1.171 (0.991)	62.50 (69.68)
 * Train Acc 69.680
 * Val Acc 70.600, Total time 0.58
 * Val loss 0.868, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0090 (0.0090)	0.699 (0.699)	84.38 (84.38)
[10/157]	0.0965 (0.0928)	0.0581 (0.0545)	0.847 (0.893)	81.25 (75.85)
[20/157]	0.1201 (0.0975)	0.0799 (0.0595)	1.189 (0.963)	53.12 (71.58)
[30/157]	0.0981 (0.0988)	0.0590 (0.0606)	0.844 (0.931)	68.75 (72.18)
[40/157]	0.0970 (0.0983)	0.0591 (0.0602)	1.164 (0.954)	62.50 (71.49)
[50/157]	0.0973 (0.0981)	0.0586 (0.0600)	0.989 (0.964)	59.38 (70.65)
[60/157]	0.0971 (0.0979)	0.0596 (0.0599)	1.103 (0.980)	62.50 (70.03)
[70/157]	0.0980 (0.0978)	0.0591 (0.0598)	1.151 (0.998)	68.75 (69.63)
[80/157]	0.1044 (0.0983)	0.0647 (0.0602)	0.713 (0.987)	81.25 (70.25)
[90/157]	0.1046 (0.0989)	0.0642 (0.0606)	0.910 (0.993)	75.00 (69.51)
[100/157]	0.1026 (0.0993)	0.0622 (0.0610)	1.052 (0.990)	59.38 (69.31)
[110/157]	0.0963 (0.0990)	0.0565 (0.0607)	0.917 (0.987)	68.75 (69.43)
[120/157]	0.0961 (0.0987)	0.0581 (0.0605)	1.029 (0.989)	71.88 (69.42)
[130/157]	0.1072 (0.0996)	0.0683 (0.0611)	1.160 (0.990)	71.88 (69.68)
[140/157]	0.1059 (0.0999)	0.0646 (0.0613)	1.079 (0.993)	50.00 (69.46)
[150/157]	0.1010 (0.1000)	0.0619 (0.0615)	1.588 (0.992)	46.88 (69.41)
[156/157]	0.0871 (0.1000)	0.0600 (0.0615)	0.849 (0.985)	87.50 (69.76)
 * Train Acc 69.760
 * Val Acc 70.900, Total time 0.59
 * Val loss 0.852, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0089 (0.0089)	1.096 (1.096)	59.38 (59.38)
[10/157]	0.0965 (0.0926)	0.0582 (0.0542)	1.079 (1.056)	59.38 (64.20)
[20/157]	0.0971 (0.0941)	0.0570 (0.0562)	1.252 (1.023)	59.38 (65.92)
[30/157]	0.0936 (0.0945)	0.0565 (0.0566)	0.724 (1.015)	81.25 (66.83)
[40/157]	0.1038 (0.0950)	0.0633 (0.0570)	0.898 (1.001)	71.88 (68.14)
[50/157]	0.0955 (0.0969)	0.0571 (0.0588)	0.883 (0.989)	81.25 (68.57)
[60/157]	0.0976 (0.0968)	0.0583 (0.0587)	0.959 (0.988)	84.38 (68.60)
[70/157]	0.0994 (0.0967)	0.0587 (0.0587)	1.007 (0.987)	59.38 (68.05)
[80/157]	0.0973 (0.0980)	0.0581 (0.0599)	0.926 (0.979)	65.62 (68.48)
[90/157]	0.0974 (0.0978)	0.0583 (0.0597)	0.967 (0.984)	68.75 (68.44)
[100/157]	0.0957 (0.0977)	0.0576 (0.0596)	0.903 (0.983)	68.75 (68.41)
[110/157]	0.1203 (0.0982)	0.0798 (0.0601)	1.284 (0.987)	53.12 (68.27)
[120/157]	0.0955 (0.0984)	0.0580 (0.0603)	0.747 (0.982)	75.00 (68.52)
[130/157]	0.0952 (0.0982)	0.0560 (0.0601)	1.050 (0.981)	65.62 (68.58)
[140/157]	0.0955 (0.0981)	0.0575 (0.0599)	0.967 (0.976)	81.25 (68.91)
[150/157]	0.1049 (0.0985)	0.0656 (0.0603)	0.657 (0.973)	78.12 (69.10)
[156/157]	0.0877 (0.0986)	0.0603 (0.0604)	0.966 (0.974)	62.50 (69.20)
 * Train Acc 69.200
 * Val Acc 69.900, Total time 0.62
 * Val loss 0.850, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0083 (0.0083)	0.832 (0.832)	78.12 (78.12)
[10/157]	0.1067 (0.0980)	0.0656 (0.0593)	0.999 (0.927)	62.50 (71.02)
[20/157]	0.0954 (0.0985)	0.0577 (0.0603)	0.925 (0.895)	68.75 (72.17)
[30/157]	0.0968 (0.0975)	0.0585 (0.0597)	0.808 (0.909)	71.88 (71.37)
[40/157]	0.0950 (0.0970)	0.0575 (0.0594)	0.966 (0.929)	68.75 (70.43)
[50/157]	0.1050 (0.0968)	0.0640 (0.0593)	0.898 (0.946)	75.00 (70.04)
[60/157]	0.0973 (0.0985)	0.0584 (0.0607)	0.890 (0.951)	68.75 (69.77)
[70/157]	0.0964 (0.0980)	0.0585 (0.0604)	1.172 (0.952)	65.62 (69.32)
[80/157]	0.0973 (0.0978)	0.0567 (0.0601)	0.958 (0.958)	71.88 (69.48)
[90/157]	0.0966 (0.0975)	0.0590 (0.0599)	1.211 (0.951)	59.38 (69.85)
[100/157]	0.1202 (0.0981)	0.0793 (0.0604)	1.126 (0.956)	65.62 (69.77)
[110/157]	0.0950 (0.0982)	0.0575 (0.0606)	0.994 (0.953)	75.00 (69.90)
[120/157]	0.0965 (0.0981)	0.0584 (0.0603)	1.018 (0.955)	68.75 (69.91)
[130/157]	0.0977 (0.0980)	0.0593 (0.0602)	1.206 (0.962)	62.50 (69.68)
[140/157]	0.1065 (0.0984)	0.0652 (0.0605)	1.015 (0.963)	62.50 (69.68)
[150/157]	0.0962 (0.0987)	0.0583 (0.0608)	1.235 (0.964)	59.38 (69.97)
[156/157]	0.0793 (0.0984)	0.0536 (0.0606)	1.375 (0.967)	37.50 (69.82)
 * Train Acc 69.820
 * Val Acc 70.500, Total time 0.58
 * Val loss 0.836, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0089 (0.0089)	1.001 (1.001)	65.62 (65.62)
[10/157]	0.0953 (0.0920)	0.0574 (0.0537)	1.328 (0.902)	59.38 (72.44)
[20/157]	0.0968 (0.0938)	0.0584 (0.0560)	0.859 (0.922)	78.12 (71.88)
[30/157]	0.0994 (0.0960)	0.0598 (0.0578)	0.816 (0.940)	78.12 (70.67)
[40/157]	0.1024 (0.0974)	0.0618 (0.0588)	0.628 (0.935)	81.25 (71.27)
[50/157]	0.1044 (0.0981)	0.0642 (0.0596)	0.926 (0.942)	68.75 (71.08)
[60/157]	0.1001 (0.0985)	0.0614 (0.0600)	0.782 (0.949)	71.88 (70.54)
[70/157]	0.1016 (0.0988)	0.0628 (0.0604)	1.055 (0.963)	81.25 (69.76)
[80/157]	0.1017 (0.0992)	0.0624 (0.0606)	1.170 (0.953)	65.62 (70.25)
[90/157]	0.1014 (0.0994)	0.0617 (0.0608)	1.134 (0.961)	65.62 (70.12)
[100/157]	0.1018 (0.0996)	0.0626 (0.0610)	0.978 (0.958)	75.00 (70.20)
[110/157]	0.1027 (0.0998)	0.0615 (0.0611)	0.747 (0.957)	81.25 (69.93)
[120/157]	0.1005 (0.0999)	0.0616 (0.0611)	1.139 (0.950)	59.38 (70.09)
[130/157]	0.0963 (0.0999)	0.0584 (0.0612)	0.889 (0.946)	68.75 (70.40)
[140/157]	0.0963 (0.0995)	0.0586 (0.0610)	0.716 (0.948)	87.50 (70.50)
[150/157]	0.0968 (0.0992)	0.0585 (0.0608)	1.370 (0.950)	65.62 (70.34)
[156/157]	0.0979 (0.0995)	0.0714 (0.0611)	1.525 (0.957)	50.00 (69.98)
 * Train Acc 69.980
 * Val Acc 70.700, Total time 0.61
 * Val loss 0.849, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0086 (0.0086)	0.792 (0.792)	71.88 (71.88)
[10/157]	0.1003 (0.0947)	0.0614 (0.0560)	0.990 (0.887)	62.50 (73.86)
[20/157]	0.1003 (0.0973)	0.0616 (0.0588)	0.795 (0.924)	78.12 (73.36)
[30/157]	0.1013 (0.0984)	0.0629 (0.0600)	0.663 (0.919)	84.38 (73.49)
[40/157]	0.0935 (0.0984)	0.0568 (0.0602)	0.849 (0.938)	68.75 (71.95)
[50/157]	0.0928 (0.0978)	0.0559 (0.0597)	1.148 (0.929)	62.50 (72.37)
[60/157]	0.0960 (0.0974)	0.0580 (0.0594)	0.923 (0.942)	75.00 (71.72)
[70/157]	0.0953 (0.0971)	0.0577 (0.0592)	0.659 (0.948)	87.50 (71.57)
[80/157]	0.0974 (0.0983)	0.0586 (0.0604)	0.968 (0.950)	65.62 (71.41)
[90/157]	0.0965 (0.0980)	0.0589 (0.0601)	1.101 (0.952)	68.75 (71.33)
[100/157]	0.0970 (0.0978)	0.0586 (0.0600)	0.971 (0.950)	62.50 (71.07)
[110/157]	0.1009 (0.0978)	0.0619 (0.0601)	1.174 (0.950)	59.38 (70.95)
[120/157]	0.1030 (0.0982)	0.0639 (0.0603)	1.119 (0.948)	71.88 (70.79)
[130/157]	0.1029 (0.0984)	0.0631 (0.0606)	0.748 (0.950)	68.75 (70.68)
[140/157]	0.1042 (0.0987)	0.0640 (0.0607)	1.589 (0.955)	37.50 (70.26)
[150/157]	0.1022 (0.0989)	0.0630 (0.0609)	0.894 (0.957)	65.62 (70.18)
[156/157]	0.0862 (0.0988)	0.0588 (0.0609)	0.739 (0.957)	87.50 (70.20)
 * Train Acc 70.200
 * Val Acc 71.100, Total time 0.61
 * Val loss 0.835, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0086 (0.0086)	1.308 (1.308)	53.12 (53.12)
[10/157]	0.1011 (0.0963)	0.0614 (0.0575)	1.120 (0.992)	65.62 (69.32)
[20/157]	0.1020 (0.0991)	0.0625 (0.0604)	0.805 (0.931)	78.12 (70.83)
[30/157]	0.1022 (0.1000)	0.0628 (0.0613)	1.234 (0.956)	65.62 (70.67)
[40/157]	0.1013 (0.1004)	0.0621 (0.0617)	1.143 (0.968)	62.50 (69.89)
[50/157]	0.1011 (0.1007)	0.0618 (0.0619)	1.063 (0.960)	56.25 (70.10)
[60/157]	0.1022 (0.1009)	0.0625 (0.0621)	1.090 (0.970)	65.62 (70.08)
[70/157]	0.1015 (0.1011)	0.0623 (0.0623)	1.025 (0.976)	62.50 (70.33)
[80/157]	0.1020 (0.1012)	0.0625 (0.0624)	1.040 (0.967)	59.38 (70.64)
[90/157]	0.0955 (0.1011)	0.0575 (0.0624)	0.777 (0.960)	78.12 (70.81)
[100/157]	0.0925 (0.1005)	0.0551 (0.0619)	0.876 (0.952)	71.88 (71.26)
[110/157]	0.0952 (0.1000)	0.0571 (0.0616)	0.797 (0.949)	78.12 (71.23)
[120/157]	0.0958 (0.0996)	0.0586 (0.0613)	0.860 (0.939)	71.88 (71.54)
[130/157]	0.0985 (0.0993)	0.0586 (0.0611)	1.121 (0.937)	68.75 (71.76)
[140/157]	0.0973 (0.0991)	0.0588 (0.0609)	0.872 (0.936)	71.88 (71.90)
[150/157]	0.0948 (0.0988)	0.0578 (0.0607)	0.824 (0.930)	78.12 (72.19)
[156/157]	0.0783 (0.0986)	0.0532 (0.0606)	0.838 (0.932)	75.00 (72.12)
 * Train Acc 72.120
 * Val Acc 70.900, Total time 0.59
 * Val loss 0.832, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0455 (0.0455)	0.0086 (0.0086)	0.671 (0.671)	78.12 (78.12)
[10/157]	0.1091 (0.1017)	0.0699 (0.0616)	1.159 (0.948)	71.88 (73.30)
[20/157]	0.0943 (0.1002)	0.0572 (0.0614)	0.714 (0.920)	75.00 (72.77)
[30/157]	0.0954 (0.0986)	0.0580 (0.0604)	1.051 (0.939)	62.50 (71.57)
[40/157]	0.0968 (0.0978)	0.0579 (0.0599)	1.081 (0.942)	71.88 (71.72)
[50/157]	0.0953 (0.0973)	0.0576 (0.0596)	1.040 (0.917)	56.25 (72.12)
[60/157]	0.0970 (0.0969)	0.0590 (0.0593)	0.892 (0.936)	71.88 (71.67)
[70/157]	0.0990 (0.0987)	0.0611 (0.0610)	1.210 (0.941)	71.88 (71.48)
[80/157]	0.0948 (0.0983)	0.0559 (0.0606)	1.007 (0.945)	65.62 (71.10)
[90/157]	0.0957 (0.0980)	0.0584 (0.0603)	0.919 (0.941)	78.12 (71.15)
[100/157]	0.0963 (0.0977)	0.0584 (0.0601)	0.746 (0.940)	78.12 (71.07)
[110/157]	0.0964 (0.0975)	0.0585 (0.0600)	0.899 (0.938)	75.00 (71.45)
[120/157]	0.0994 (0.0984)	0.0604 (0.0608)	1.184 (0.942)	65.62 (71.07)
[130/157]	0.1006 (0.0985)	0.0609 (0.0608)	1.138 (0.944)	65.62 (70.87)
[140/157]	0.1009 (0.0986)	0.0619 (0.0609)	0.826 (0.946)	81.25 (71.01)
[150/157]	0.1010 (0.0987)	0.0616 (0.0608)	0.958 (0.944)	68.75 (70.94)
[156/157]	0.0828 (0.0986)	0.0559 (0.0608)	1.355 (0.943)	87.50 (71.00)
 * Train Acc 71.000
 * Val Acc 71.500, Total time 0.60
 * Val loss 0.837, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0088 (0.0088)	0.744 (0.744)	75.00 (75.00)
[10/157]	0.0988 (0.0937)	0.0606 (0.0552)	1.260 (1.010)	62.50 (69.03)
[20/157]	0.0995 (0.0963)	0.0598 (0.0579)	0.851 (0.969)	71.88 (71.43)
[30/157]	0.1001 (0.0973)	0.0604 (0.0587)	0.778 (0.965)	81.25 (70.97)
[40/157]	0.1035 (0.0978)	0.0607 (0.0593)	1.014 (0.957)	75.00 (71.65)
[50/157]	0.1007 (0.0980)	0.0614 (0.0594)	0.895 (0.961)	78.12 (71.57)
[60/157]	0.1004 (0.0983)	0.0605 (0.0596)	0.888 (0.953)	75.00 (71.72)
[70/157]	0.1000 (0.0984)	0.0618 (0.0598)	0.814 (0.947)	68.75 (71.65)
[80/157]	0.0992 (0.0985)	0.0606 (0.0600)	1.292 (0.950)	59.38 (71.37)
[90/157]	0.0991 (0.0986)	0.0601 (0.0601)	0.664 (0.948)	75.00 (71.26)
[100/157]	0.0993 (0.0987)	0.0603 (0.0601)	1.067 (0.946)	68.75 (71.16)
[110/157]	0.1015 (0.0988)	0.0622 (0.0602)	1.039 (0.953)	71.88 (70.78)
[120/157]	0.1007 (0.0989)	0.0618 (0.0604)	0.891 (0.947)	78.12 (70.97)
[130/157]	0.1001 (0.0990)	0.0610 (0.0604)	1.153 (0.946)	65.62 (71.04)
[140/157]	0.1003 (0.0990)	0.0615 (0.0605)	0.989 (0.951)	65.62 (70.86)
[150/157]	0.0993 (0.0991)	0.0604 (0.0605)	0.681 (0.946)	84.38 (70.99)
[156/157]	0.0834 (0.0990)	0.0562 (0.0605)	0.624 (0.941)	75.00 (71.04)
 * Train Acc 71.040
 * Val Acc 71.800, Total time 0.60
 * Val loss 0.812, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0439 (0.0439)	0.0084 (0.0084)	0.923 (0.923)	71.88 (71.88)
[10/157]	0.0998 (0.0941)	0.0606 (0.0550)	0.942 (0.947)	65.62 (70.45)
[20/157]	0.0994 (0.0966)	0.0612 (0.0580)	0.813 (0.944)	84.38 (71.58)
[30/157]	0.1003 (0.0974)	0.0614 (0.0591)	1.109 (0.947)	71.88 (72.08)
[40/157]	0.1004 (0.0979)	0.0610 (0.0595)	0.883 (0.941)	68.75 (71.95)
[50/157]	0.1000 (0.0981)	0.0611 (0.0598)	1.244 (0.953)	53.12 (71.32)
[60/157]	0.0991 (0.0983)	0.0603 (0.0601)	0.744 (0.945)	84.38 (71.41)
[70/157]	0.0997 (0.0985)	0.0601 (0.0601)	1.036 (0.938)	56.25 (71.21)
[80/157]	0.1003 (0.0987)	0.0602 (0.0602)	0.998 (0.944)	75.00 (71.53)
[90/157]	0.1026 (0.0988)	0.0600 (0.0603)	0.873 (0.941)	68.75 (71.57)
[100/157]	0.1014 (0.0988)	0.0618 (0.0603)	0.843 (0.941)	71.88 (71.38)
[110/157]	0.0984 (0.0989)	0.0594 (0.0604)	0.998 (0.937)	75.00 (71.45)
[120/157]	0.1002 (0.0989)	0.0598 (0.0604)	1.227 (0.937)	56.25 (71.31)
[130/157]	0.0996 (0.0989)	0.0606 (0.0604)	0.861 (0.931)	62.50 (71.30)
[140/157]	0.1013 (0.0989)	0.0618 (0.0604)	0.916 (0.934)	75.00 (71.17)
[150/157]	0.0999 (0.0990)	0.0599 (0.0604)	1.296 (0.940)	56.25 (71.11)
[156/157]	0.0817 (0.0989)	0.0546 (0.0604)	1.295 (0.939)	50.00 (71.16)
 * Train Acc 71.160
 * Val Acc 70.900, Total time 0.59
 * Val loss 0.827, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0087 (0.0087)	0.793 (0.793)	81.25 (81.25)
[10/157]	0.1000 (0.0940)	0.0601 (0.0553)	0.790 (0.864)	78.12 (76.14)
[20/157]	0.0996 (0.0966)	0.0605 (0.0579)	0.734 (0.840)	75.00 (75.60)
[30/157]	0.0999 (0.0976)	0.0606 (0.0590)	1.064 (0.887)	78.12 (73.79)
[40/157]	0.0990 (0.0980)	0.0603 (0.0595)	1.216 (0.897)	65.62 (73.25)
[50/157]	0.0999 (0.0983)	0.0610 (0.0598)	1.207 (0.897)	59.38 (72.98)
[60/157]	0.0996 (0.0984)	0.0614 (0.0600)	1.047 (0.917)	68.75 (72.64)
[70/157]	0.0995 (0.0986)	0.0614 (0.0602)	1.176 (0.928)	71.88 (72.36)
[80/157]	0.0988 (0.0987)	0.0604 (0.0603)	0.904 (0.936)	68.75 (71.95)
[90/157]	0.1002 (0.0988)	0.0608 (0.0604)	1.040 (0.939)	71.88 (71.63)
[100/157]	0.0965 (0.0985)	0.0580 (0.0602)	0.858 (0.941)	84.38 (71.66)
[110/157]	0.0953 (0.0982)	0.0582 (0.0600)	0.862 (0.933)	68.75 (71.99)
[120/157]	0.0966 (0.0980)	0.0586 (0.0599)	0.778 (0.929)	81.25 (71.93)
[130/157]	0.0957 (0.0978)	0.0572 (0.0598)	1.008 (0.928)	59.38 (71.73)
[140/157]	0.0950 (0.0975)	0.0579 (0.0597)	1.308 (0.929)	53.12 (71.76)
[150/157]	0.1002 (0.0977)	0.0611 (0.0598)	0.825 (0.935)	78.12 (71.21)
[156/157]	0.0860 (0.0977)	0.0586 (0.0598)	1.201 (0.935)	75.00 (71.32)
 * Train Acc 71.320
 * Val Acc 72.000, Total time 0.60
 * Val loss 0.821, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0088 (0.0088)	1.060 (1.060)	68.75 (68.75)
[10/157]	0.1023 (0.0954)	0.0636 (0.0568)	1.043 (0.946)	62.50 (69.89)
[20/157]	0.1005 (0.0979)	0.0617 (0.0594)	1.062 (0.958)	75.00 (71.13)
[30/157]	0.1000 (0.0987)	0.0618 (0.0603)	1.127 (0.955)	62.50 (71.77)
[40/157]	0.1007 (0.0994)	0.0611 (0.0610)	1.052 (0.934)	68.75 (72.10)
[50/157]	0.1033 (0.0999)	0.0623 (0.0614)	0.860 (0.928)	78.12 (72.55)
[60/157]	0.0948 (0.0993)	0.0575 (0.0609)	0.806 (0.921)	78.12 (72.95)
[70/157]	0.0964 (0.0988)	0.0580 (0.0605)	1.190 (0.905)	53.12 (73.24)
[80/157]	0.0944 (0.0984)	0.0568 (0.0602)	1.041 (0.902)	71.88 (73.46)
[90/157]	0.0953 (0.0980)	0.0575 (0.0600)	1.171 (0.916)	65.62 (72.91)
[100/157]	0.0953 (0.0978)	0.0561 (0.0598)	0.955 (0.920)	65.62 (72.59)
[110/157]	0.1201 (0.0980)	0.0799 (0.0600)	1.031 (0.920)	65.62 (72.47)
[120/157]	0.0961 (0.0984)	0.0575 (0.0604)	0.731 (0.918)	81.25 (72.42)
[130/157]	0.0972 (0.0982)	0.0584 (0.0603)	1.089 (0.922)	53.12 (72.23)
[140/157]	0.0977 (0.0981)	0.0597 (0.0602)	0.763 (0.916)	75.00 (72.45)
[150/157]	0.0983 (0.0980)	0.0593 (0.0602)	0.893 (0.921)	81.25 (72.43)
[156/157]	0.0794 (0.0978)	0.0542 (0.0601)	0.840 (0.922)	75.00 (72.22)
 * Train Acc 72.220
 * Val Acc 72.400, Total time 0.59
 * Val loss 0.824, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0087 (0.0087)	0.720 (0.720)	81.25 (81.25)
[10/157]	0.0975 (0.0917)	0.0598 (0.0538)	0.848 (0.926)	68.75 (69.32)
[20/157]	0.1187 (0.1002)	0.0783 (0.0618)	1.125 (0.891)	62.50 (70.83)
[30/157]	0.0980 (0.0996)	0.0588 (0.0613)	1.099 (0.933)	65.62 (69.66)
[40/157]	0.0965 (0.0991)	0.0587 (0.0608)	0.857 (0.946)	87.50 (69.36)
[50/157]	0.0986 (0.0986)	0.0595 (0.0604)	1.117 (0.963)	71.88 (69.18)
[60/157]	0.0969 (0.0983)	0.0587 (0.0602)	0.985 (0.956)	59.38 (69.31)
[70/157]	0.0967 (0.0981)	0.0580 (0.0600)	0.896 (0.942)	75.00 (70.38)
[80/157]	0.0979 (0.0980)	0.0596 (0.0598)	0.679 (0.937)	81.25 (70.87)
[90/157]	0.0965 (0.0978)	0.0584 (0.0597)	0.822 (0.936)	78.12 (71.36)
[100/157]	0.0964 (0.0977)	0.0579 (0.0596)	1.156 (0.933)	68.75 (71.44)
[110/157]	0.0972 (0.0976)	0.0583 (0.0595)	0.830 (0.935)	75.00 (71.51)
[120/157]	0.0963 (0.0975)	0.0579 (0.0595)	0.998 (0.927)	62.50 (71.75)
[130/157]	0.0979 (0.0975)	0.0590 (0.0594)	0.848 (0.932)	78.12 (71.52)
[140/157]	0.0977 (0.0974)	0.0597 (0.0594)	0.865 (0.929)	71.88 (71.39)
[150/157]	0.0970 (0.0974)	0.0581 (0.0594)	0.925 (0.928)	75.00 (71.44)
[156/157]	0.0809 (0.0972)	0.0561 (0.0593)	0.872 (0.930)	62.50 (71.22)
 * Train Acc 71.220
 * Val Acc 72.200, Total time 0.59
 * Val loss 0.825, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0085 (0.0085)	0.868 (0.868)	75.00 (75.00)
[10/157]	0.0970 (0.0913)	0.0599 (0.0540)	0.824 (0.923)	84.38 (71.59)
[20/157]	0.1071 (0.0943)	0.0660 (0.0568)	1.110 (0.956)	56.25 (69.49)
[30/157]	0.1083 (0.0987)	0.0688 (0.0605)	0.872 (0.953)	65.62 (70.06)
[40/157]	0.0967 (0.0981)	0.0582 (0.0601)	0.651 (0.952)	90.62 (70.58)
[50/157]	0.0972 (0.0976)	0.0589 (0.0597)	0.745 (0.939)	81.25 (71.26)
[60/157]	0.1219 (0.0988)	0.0808 (0.0608)	0.940 (0.928)	75.00 (71.72)
[70/157]	0.1030 (0.0996)	0.0628 (0.0615)	0.998 (0.928)	71.88 (71.79)
[80/157]	0.1030 (0.1000)	0.0632 (0.0616)	1.122 (0.922)	59.38 (71.99)
[90/157]	0.1031 (0.1002)	0.0635 (0.0618)	1.033 (0.926)	62.50 (71.60)
[100/157]	0.1037 (0.1003)	0.0635 (0.0619)	0.884 (0.915)	71.88 (71.94)
[110/157]	0.1047 (0.1005)	0.0643 (0.0620)	0.827 (0.915)	75.00 (72.02)
[120/157]	0.0932 (0.1004)	0.0558 (0.0620)	0.906 (0.913)	75.00 (72.11)
[130/157]	0.0941 (0.1000)	0.0571 (0.0617)	0.838 (0.916)	75.00 (72.07)
[140/157]	0.1195 (0.0999)	0.0796 (0.0616)	0.769 (0.912)	75.00 (72.05)
[150/157]	0.0943 (0.0999)	0.0568 (0.0617)	0.909 (0.919)	75.00 (71.85)
[156/157]	0.0791 (0.0996)	0.0541 (0.0615)	1.125 (0.923)	50.00 (71.72)
 * Train Acc 71.720
 * Val Acc 71.600, Total time 0.59
 * Val loss 0.816, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0089 (0.0089)	0.778 (0.778)	71.88 (71.88)
[10/157]	0.0947 (0.0993)	0.0573 (0.0595)	0.842 (0.949)	68.75 (69.89)
[20/157]	0.0966 (0.0976)	0.0581 (0.0589)	0.709 (0.907)	78.12 (72.02)
[30/157]	0.1098 (0.0977)	0.0693 (0.0592)	0.846 (0.908)	78.12 (71.88)
[40/157]	0.0944 (0.0983)	0.0576 (0.0600)	0.681 (0.896)	84.38 (72.64)
[50/157]	0.0965 (0.0979)	0.0584 (0.0596)	1.130 (0.908)	62.50 (72.24)
[60/157]	0.0955 (0.0975)	0.0583 (0.0595)	1.171 (0.906)	62.50 (72.28)
[70/157]	0.0950 (0.0973)	0.0573 (0.0593)	1.073 (0.913)	59.38 (71.57)
[80/157]	0.1065 (0.0990)	0.0666 (0.0610)	0.660 (0.911)	84.38 (71.76)
[90/157]	0.1085 (0.0997)	0.0672 (0.0616)	1.045 (0.910)	75.00 (71.63)
[100/157]	0.0948 (0.0995)	0.0573 (0.0614)	1.037 (0.921)	71.88 (71.32)
[110/157]	0.0936 (0.0991)	0.0562 (0.0612)	0.873 (0.917)	78.12 (71.71)
[120/157]	0.0962 (0.0988)	0.0581 (0.0610)	0.926 (0.919)	68.75 (71.62)
[130/157]	0.0951 (0.0985)	0.0576 (0.0608)	0.876 (0.927)	75.00 (71.16)
[140/157]	0.1045 (0.0984)	0.0652 (0.0606)	0.784 (0.928)	65.62 (71.01)
[150/157]	0.1025 (0.0987)	0.0631 (0.0608)	0.777 (0.924)	65.62 (71.07)
[156/157]	0.0866 (0.0987)	0.0581 (0.0609)	0.836 (0.925)	75.00 (71.04)
 * Train Acc 71.040
 * Val Acc 71.500, Total time 0.60
 * Val loss 0.816, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0457 (0.0457)	0.0092 (0.0092)	0.649 (0.649)	78.12 (78.12)
[10/157]	0.1058 (0.0975)	0.0656 (0.0585)	0.992 (0.927)	78.12 (71.88)
[20/157]	0.1018 (0.0999)	0.0624 (0.0607)	0.945 (0.982)	65.62 (70.24)
[30/157]	0.1041 (0.1008)	0.0641 (0.0616)	0.975 (0.975)	71.88 (70.77)
[40/157]	0.1041 (0.1012)	0.0639 (0.0621)	1.118 (0.971)	56.25 (70.50)
[50/157]	0.0950 (0.1007)	0.0573 (0.0617)	0.956 (0.956)	75.00 (71.02)
[60/157]	0.0959 (0.0998)	0.0577 (0.0612)	0.967 (0.958)	68.75 (70.54)
[70/157]	0.0956 (0.0991)	0.0576 (0.0607)	0.630 (0.950)	84.38 (70.55)
[80/157]	0.0959 (0.0987)	0.0580 (0.0603)	0.815 (0.939)	75.00 (71.26)
[90/157]	0.0966 (0.0983)	0.0584 (0.0601)	0.851 (0.937)	71.88 (71.22)
[100/157]	0.0959 (0.0981)	0.0578 (0.0599)	1.240 (0.936)	65.62 (71.26)
[110/157]	0.0955 (0.0978)	0.0578 (0.0597)	0.815 (0.933)	75.00 (71.20)
[120/157]	0.0961 (0.0976)	0.0584 (0.0596)	0.889 (0.929)	68.75 (71.23)
[130/157]	0.0983 (0.0975)	0.0592 (0.0595)	0.827 (0.931)	71.88 (71.18)
[140/157]	0.0942 (0.0973)	0.0555 (0.0593)	1.245 (0.936)	68.75 (71.19)
[150/157]	0.0947 (0.0972)	0.0569 (0.0592)	0.939 (0.933)	75.00 (71.15)
[156/157]	0.0786 (0.0970)	0.0522 (0.0591)	0.571 (0.933)	100.00 (71.00)
 * Train Acc 71.000
 * Val Acc 71.300, Total time 0.58
 * Val loss 0.818, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0089 (0.0089)	0.900 (0.900)	75.00 (75.00)
[10/157]	0.0943 (0.0922)	0.0565 (0.0542)	1.020 (0.999)	65.62 (69.03)
[20/157]	0.0960 (0.0935)	0.0581 (0.0560)	1.113 (0.982)	53.12 (67.56)
[30/157]	0.1213 (0.0957)	0.0805 (0.0581)	1.440 (0.978)	50.00 (67.84)
[40/157]	0.1088 (0.0994)	0.0680 (0.0613)	0.783 (0.960)	75.00 (68.90)
[50/157]	0.0945 (0.0995)	0.0567 (0.0614)	0.820 (0.945)	68.75 (69.42)
[60/157]	0.0958 (0.0989)	0.0571 (0.0607)	0.777 (0.941)	81.25 (69.72)
[70/157]	0.0944 (0.0984)	0.0567 (0.0604)	0.731 (0.926)	81.25 (70.60)
[80/157]	0.0971 (0.0981)	0.0584 (0.0601)	0.999 (0.925)	68.75 (70.91)
[90/157]	0.0971 (0.0978)	0.0589 (0.0600)	1.105 (0.937)	68.75 (70.71)
[100/157]	0.0965 (0.0976)	0.0588 (0.0598)	0.784 (0.937)	78.12 (70.82)
[110/157]	0.0953 (0.0974)	0.0575 (0.0597)	0.652 (0.937)	84.38 (70.72)
[120/157]	0.0947 (0.0973)	0.0576 (0.0596)	0.916 (0.928)	68.75 (71.18)
[130/157]	0.0948 (0.0971)	0.0570 (0.0595)	0.830 (0.926)	78.12 (71.28)
[140/157]	0.0996 (0.0971)	0.0597 (0.0595)	0.822 (0.922)	75.00 (71.41)
[150/157]	0.1055 (0.0974)	0.0654 (0.0597)	1.119 (0.927)	68.75 (71.13)
[156/157]	0.0887 (0.0975)	0.0610 (0.0599)	0.955 (0.930)	50.00 (71.10)
 * Train Acc 71.100
 * Val Acc 72.200, Total time 0.58
 * Val loss 0.815, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0089 (0.0089)	1.134 (1.134)	68.75 (68.75)
[10/157]	0.1021 (0.0966)	0.0624 (0.0576)	1.031 (0.971)	65.62 (67.05)
[20/157]	0.0962 (0.0989)	0.0580 (0.0599)	0.730 (0.948)	84.38 (69.05)
[30/157]	0.0959 (0.0980)	0.0574 (0.0593)	0.911 (0.933)	75.00 (70.06)
[40/157]	0.0943 (0.0974)	0.0555 (0.0589)	0.887 (0.922)	75.00 (70.35)
[50/157]	0.0952 (0.0970)	0.0574 (0.0588)	0.794 (0.916)	84.38 (71.26)
[60/157]	0.0980 (0.0968)	0.0600 (0.0587)	1.125 (0.911)	53.12 (71.26)
[70/157]	0.0968 (0.0966)	0.0588 (0.0586)	0.998 (0.912)	68.75 (71.26)
[80/157]	0.1037 (0.0973)	0.0648 (0.0592)	0.932 (0.913)	71.88 (71.03)
[90/157]	0.1025 (0.0980)	0.0634 (0.0599)	0.628 (0.921)	81.25 (71.02)
[100/157]	0.0953 (0.0984)	0.0574 (0.0603)	1.145 (0.925)	62.50 (70.98)
[110/157]	0.1053 (0.0987)	0.0660 (0.0606)	0.967 (0.921)	59.38 (71.06)
[120/157]	0.0968 (0.0988)	0.0585 (0.0607)	0.897 (0.917)	68.75 (71.26)
[130/157]	0.0936 (0.0985)	0.0566 (0.0605)	0.999 (0.919)	68.75 (71.42)
[140/157]	0.1010 (0.0989)	0.0623 (0.0609)	0.848 (0.917)	68.75 (71.52)
[150/157]	0.0993 (0.0989)	0.0611 (0.0609)	1.273 (0.922)	65.62 (71.36)
[156/157]	0.0819 (0.0988)	0.0561 (0.0609)	0.736 (0.923)	87.50 (71.24)
 * Train Acc 71.240
 * Val Acc 71.800, Total time 0.58
 * Val loss 0.809, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0415 (0.0415)	0.0084 (0.0084)	0.692 (0.692)	84.38 (84.38)
[10/157]	0.0979 (0.0921)	0.0595 (0.0543)	0.817 (0.829)	71.88 (74.72)
[20/157]	0.0968 (0.0938)	0.0577 (0.0559)	0.635 (0.858)	90.62 (74.26)
[30/157]	0.1016 (0.0979)	0.0619 (0.0596)	0.891 (0.873)	81.25 (73.49)
[40/157]	0.1027 (0.0988)	0.0627 (0.0604)	0.737 (0.872)	78.12 (73.55)
[50/157]	0.1018 (0.0992)	0.0625 (0.0607)	0.790 (0.911)	68.75 (72.06)
[60/157]	0.1013 (0.0995)	0.0619 (0.0610)	1.088 (0.913)	65.62 (71.98)
[70/157]	0.1012 (0.0998)	0.0619 (0.0612)	0.874 (0.919)	62.50 (71.70)
[80/157]	0.1025 (0.0999)	0.0630 (0.0614)	0.887 (0.926)	75.00 (71.41)
[90/157]	0.0996 (0.1001)	0.0594 (0.0614)	0.831 (0.928)	71.88 (71.15)
[100/157]	0.0937 (0.0999)	0.0564 (0.0613)	0.935 (0.928)	65.62 (70.88)
[110/157]	0.0952 (0.0996)	0.0574 (0.0611)	1.410 (0.933)	50.00 (70.83)
[120/157]	0.0978 (0.0992)	0.0589 (0.0609)	0.953 (0.932)	71.88 (70.89)
[130/157]	0.0962 (0.0989)	0.0581 (0.0606)	0.982 (0.926)	65.62 (71.06)
[140/157]	0.0951 (0.0987)	0.0575 (0.0605)	0.977 (0.928)	71.88 (71.08)
[150/157]	0.0966 (0.0985)	0.0589 (0.0604)	0.690 (0.926)	78.12 (71.03)
[156/157]	0.0854 (0.0984)	0.0582 (0.0604)	1.058 (0.929)	62.50 (70.98)
 * Train Acc 70.980
 * Val Acc 71.300, Total time 0.61
 * Val loss 0.810, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0083 (0.0083)	1.028 (1.028)	71.88 (71.88)
[10/157]	0.1006 (0.0959)	0.0620 (0.0576)	0.914 (0.838)	75.00 (74.72)
[20/157]	0.1010 (0.0988)	0.0622 (0.0603)	0.828 (0.808)	75.00 (76.34)
[30/157]	0.1001 (0.0998)	0.0610 (0.0612)	1.058 (0.875)	59.38 (73.79)
[40/157]	0.0997 (0.1003)	0.0609 (0.0617)	0.843 (0.891)	81.25 (72.79)
[50/157]	0.1011 (0.1006)	0.0627 (0.0621)	1.038 (0.907)	68.75 (72.30)
[60/157]	0.0920 (0.1003)	0.0551 (0.0618)	0.926 (0.909)	71.88 (72.54)
[70/157]	0.0963 (0.0996)	0.0582 (0.0612)	1.194 (0.923)	53.12 (71.79)
[80/157]	0.0971 (0.0991)	0.0576 (0.0609)	1.196 (0.931)	59.38 (71.72)
[90/157]	0.0958 (0.0987)	0.0568 (0.0605)	0.772 (0.925)	78.12 (71.88)
[100/157]	0.0948 (0.0984)	0.0567 (0.0602)	1.000 (0.929)	59.38 (71.50)
[110/157]	0.1109 (0.0991)	0.0700 (0.0608)	1.171 (0.932)	68.75 (71.45)
[120/157]	0.0965 (0.0990)	0.0581 (0.0607)	0.982 (0.935)	71.88 (71.46)
[130/157]	0.0951 (0.0988)	0.0579 (0.0606)	0.919 (0.931)	62.50 (71.68)
[140/157]	0.0962 (0.0987)	0.0581 (0.0604)	0.899 (0.933)	71.88 (71.52)
[150/157]	0.0983 (0.0986)	0.0589 (0.0603)	0.809 (0.927)	84.38 (71.77)
[156/157]	0.0801 (0.0984)	0.0544 (0.0603)	1.181 (0.925)	62.50 (71.86)
 * Train Acc 71.860
 * Val Acc 72.700, Total time 0.59
 * Val loss 0.806, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0092 (0.0092)	1.042 (1.042)	68.75 (68.75)
[10/157]	0.0958 (0.0916)	0.0580 (0.0540)	0.962 (0.919)	75.00 (72.73)
[20/157]	0.0947 (0.0941)	0.0569 (0.0564)	0.940 (0.936)	65.62 (72.02)
[30/157]	0.0980 (0.0949)	0.0591 (0.0573)	0.584 (0.912)	84.38 (72.98)
[40/157]	0.0952 (0.0953)	0.0567 (0.0576)	1.255 (0.918)	65.62 (72.71)
[50/157]	0.0962 (0.0955)	0.0590 (0.0579)	0.737 (0.922)	84.38 (72.43)
[60/157]	0.0993 (0.0957)	0.0594 (0.0581)	0.834 (0.911)	62.50 (72.34)
[70/157]	0.0956 (0.0959)	0.0579 (0.0581)	1.224 (0.910)	62.50 (72.18)
[80/157]	0.0980 (0.0960)	0.0591 (0.0583)	1.261 (0.902)	62.50 (72.38)
[90/157]	0.0971 (0.0961)	0.0594 (0.0584)	1.051 (0.901)	62.50 (72.56)
[100/157]	0.0954 (0.0962)	0.0581 (0.0584)	0.858 (0.907)	71.88 (72.22)
[110/157]	0.1008 (0.0972)	0.0628 (0.0594)	0.762 (0.914)	81.25 (72.10)
[120/157]	0.0962 (0.0971)	0.0582 (0.0593)	1.231 (0.916)	56.25 (71.82)
[130/157]	0.0949 (0.0970)	0.0572 (0.0592)	0.719 (0.913)	81.25 (71.85)
[140/157]	0.1202 (0.0973)	0.0802 (0.0595)	0.926 (0.913)	65.62 (71.59)
[150/157]	0.0950 (0.0977)	0.0564 (0.0598)	0.989 (0.920)	68.75 (71.34)
[156/157]	0.0792 (0.0975)	0.0546 (0.0597)	0.916 (0.920)	62.50 (71.40)
 * Train Acc 71.400
 * Val Acc 72.400, Total time 0.58
 * Val loss 0.810, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0461 (0.0461)	0.0085 (0.0085)	0.653 (0.653)	87.50 (87.50)
[10/157]	0.0989 (0.0948)	0.0604 (0.0569)	0.807 (0.874)	75.00 (73.01)
[20/157]	0.1110 (0.0994)	0.0705 (0.0611)	1.091 (0.900)	62.50 (72.32)
[30/157]	0.0944 (0.0985)	0.0574 (0.0606)	0.823 (0.889)	78.12 (72.58)
[40/157]	0.0958 (0.0978)	0.0579 (0.0601)	0.935 (0.924)	71.88 (71.27)
[50/157]	0.0968 (0.0974)	0.0580 (0.0597)	0.875 (0.936)	84.38 (70.96)
[60/157]	0.1067 (0.0976)	0.0658 (0.0598)	0.876 (0.915)	71.88 (72.13)
[70/157]	0.0937 (0.0987)	0.0558 (0.0608)	1.049 (0.925)	71.88 (71.88)
[80/157]	0.0949 (0.0983)	0.0572 (0.0605)	0.865 (0.912)	68.75 (72.15)
[90/157]	0.0989 (0.0981)	0.0599 (0.0602)	0.602 (0.908)	84.38 (72.08)
[100/157]	0.0955 (0.0979)	0.0577 (0.0600)	0.717 (0.913)	75.00 (71.81)
[110/157]	0.0979 (0.0977)	0.0600 (0.0599)	1.291 (0.916)	50.00 (71.62)
[120/157]	0.0973 (0.0977)	0.0592 (0.0599)	0.835 (0.908)	71.88 (71.67)
[130/157]	0.0981 (0.0977)	0.0592 (0.0598)	0.828 (0.915)	78.12 (71.49)
[140/157]	0.0991 (0.0977)	0.0599 (0.0598)	1.076 (0.919)	65.62 (71.43)
[150/157]	0.0990 (0.0977)	0.0596 (0.0598)	1.019 (0.925)	71.88 (71.15)
[156/157]	0.0812 (0.0976)	0.0553 (0.0597)	0.538 (0.924)	100.00 (71.10)
 * Train Acc 71.100
 * Val Acc 72.000, Total time 0.59
 * Val loss 0.826, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0080 (0.0080)	1.039 (1.039)	59.38 (59.38)
[10/157]	0.0987 (0.0997)	0.0588 (0.0605)	0.827 (0.828)	75.00 (73.86)
[20/157]	0.0969 (0.0985)	0.0591 (0.0600)	1.143 (0.899)	78.12 (72.47)
[30/157]	0.0992 (0.0980)	0.0601 (0.0599)	1.070 (0.891)	59.38 (72.38)
[40/157]	0.0972 (0.0978)	0.0587 (0.0598)	0.683 (0.873)	75.00 (73.32)
[50/157]	0.0980 (0.0978)	0.0599 (0.0597)	1.299 (0.896)	65.62 (72.18)
[60/157]	0.0966 (0.0977)	0.0582 (0.0596)	0.749 (0.896)	78.12 (71.98)
[70/157]	0.0986 (0.0977)	0.0602 (0.0596)	0.843 (0.895)	78.12 (71.92)
[80/157]	0.0971 (0.0976)	0.0582 (0.0595)	0.853 (0.902)	75.00 (71.49)
[90/157]	0.0980 (0.0976)	0.0595 (0.0596)	1.309 (0.910)	62.50 (70.91)
[100/157]	0.0982 (0.0976)	0.0593 (0.0595)	1.166 (0.911)	68.75 (70.92)
[110/157]	0.0950 (0.0975)	0.0580 (0.0595)	0.896 (0.913)	78.12 (71.20)
[120/157]	0.0988 (0.0975)	0.0599 (0.0595)	1.021 (0.911)	75.00 (71.33)
[130/157]	0.0985 (0.0975)	0.0593 (0.0595)	0.863 (0.916)	75.00 (71.21)
[140/157]	0.1088 (0.0975)	0.0683 (0.0596)	1.104 (0.915)	59.38 (71.23)
[150/157]	0.1008 (0.0978)	0.0596 (0.0598)	0.742 (0.913)	81.25 (71.30)
[156/157]	0.0800 (0.0977)	0.0547 (0.0597)	1.266 (0.916)	50.00 (71.22)
 * Train Acc 71.220
 * Val Acc 72.700, Total time 0.59
 * Val loss 0.803, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0440 (0.0440)	0.0088 (0.0088)	0.721 (0.721)	75.00 (75.00)
[10/157]	0.1084 (0.0977)	0.0680 (0.0584)	0.877 (0.949)	71.88 (69.60)
[20/157]	0.1196 (0.0992)	0.0797 (0.0603)	1.301 (0.940)	62.50 (69.94)
[30/157]	0.0985 (0.1011)	0.0592 (0.0625)	0.811 (0.928)	71.88 (70.46)
[40/157]	0.0976 (0.0999)	0.0594 (0.0613)	0.920 (0.936)	62.50 (70.27)
[50/157]	0.0971 (0.0991)	0.0581 (0.0608)	0.741 (0.930)	78.12 (70.34)
[60/157]	0.0973 (0.0986)	0.0588 (0.0604)	0.799 (0.918)	71.88 (71.31)
[70/157]	0.1033 (0.0986)	0.0639 (0.0604)	1.065 (0.917)	68.75 (70.95)
[80/157]	0.1051 (0.0992)	0.0651 (0.0609)	0.762 (0.918)	68.75 (71.06)
[90/157]	0.1027 (0.0995)	0.0633 (0.0612)	1.155 (0.914)	68.75 (71.39)
[100/157]	0.1052 (0.0999)	0.0652 (0.0616)	0.651 (0.916)	81.25 (71.07)
[110/157]	0.1037 (0.1002)	0.0644 (0.0618)	1.057 (0.917)	68.75 (71.14)
[120/157]	0.0965 (0.0999)	0.0583 (0.0616)	0.897 (0.917)	75.00 (71.31)
[130/157]	0.0955 (0.0996)	0.0575 (0.0614)	0.706 (0.910)	81.25 (71.90)
[140/157]	0.0951 (0.0993)	0.0572 (0.0611)	1.038 (0.912)	65.62 (71.68)
[150/157]	0.0961 (0.0990)	0.0577 (0.0609)	0.759 (0.911)	75.00 (71.71)
[156/157]	0.0778 (0.0988)	0.0524 (0.0608)	1.175 (0.910)	62.50 (71.64)
 * Train Acc 71.640
 * Val Acc 72.400, Total time 0.58
 * Val loss 0.805, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0087 (0.0087)	0.615 (0.615)	78.12 (78.12)
[10/157]	0.0963 (0.1018)	0.0586 (0.0624)	0.638 (0.872)	78.12 (72.44)
[20/157]	0.1085 (0.1040)	0.0682 (0.0649)	1.103 (0.932)	59.38 (68.60)
[30/157]	0.0969 (0.1015)	0.0586 (0.0630)	0.798 (0.923)	71.88 (69.56)
[40/157]	0.0971 (0.1001)	0.0583 (0.0619)	0.658 (0.912)	75.00 (70.73)
[50/157]	0.0986 (0.0994)	0.0584 (0.0612)	1.000 (0.915)	68.75 (70.65)
[60/157]	0.0983 (0.0989)	0.0586 (0.0607)	0.979 (0.908)	65.62 (70.54)
[70/157]	0.1218 (0.1001)	0.0792 (0.0618)	0.770 (0.897)	71.88 (71.13)
[80/157]	0.1070 (0.1008)	0.0645 (0.0622)	1.038 (0.897)	68.75 (71.06)
[90/157]	0.1047 (0.1013)	0.0635 (0.0623)	1.120 (0.904)	62.50 (71.12)
[100/157]	0.0954 (0.1010)	0.0589 (0.0622)	1.096 (0.908)	62.50 (71.23)
[110/157]	0.0956 (0.1006)	0.0577 (0.0618)	0.859 (0.913)	75.00 (71.06)
[120/157]	0.1056 (0.1005)	0.0670 (0.0618)	0.977 (0.922)	68.75 (70.58)
[130/157]	0.0948 (0.1008)	0.0584 (0.0621)	0.844 (0.917)	68.75 (70.83)
[140/157]	0.0976 (0.1005)	0.0571 (0.0617)	1.069 (0.912)	62.50 (71.01)
[150/157]	0.1118 (0.1004)	0.0613 (0.0616)	0.808 (0.912)	84.38 (71.27)
[156/157]	0.0874 (0.1004)	0.0584 (0.0616)	0.860 (0.913)	87.50 (71.34)
 * Train Acc 71.340
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.807, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0083 (0.0083)	0.789 (0.789)	75.00 (75.00)
[10/157]	0.1020 (0.0973)	0.0641 (0.0573)	0.943 (0.870)	75.00 (73.01)
[20/157]	0.1030 (0.1000)	0.0642 (0.0605)	0.982 (0.893)	65.62 (71.88)
[30/157]	0.1053 (0.1009)	0.0639 (0.0617)	0.702 (0.895)	84.38 (72.18)
[40/157]	0.1024 (0.1012)	0.0648 (0.0621)	1.171 (0.884)	65.62 (72.41)
[50/157]	0.1035 (0.1016)	0.0649 (0.0625)	1.067 (0.891)	65.62 (72.67)
[60/157]	0.1008 (0.1017)	0.0637 (0.0626)	1.147 (0.892)	68.75 (72.95)
[70/157]	0.1021 (0.1020)	0.0636 (0.0629)	0.800 (0.885)	75.00 (73.20)
[80/157]	0.1013 (0.1020)	0.0643 (0.0630)	0.842 (0.873)	68.75 (73.42)
[90/157]	0.0962 (0.1015)	0.0579 (0.0626)	0.706 (0.875)	84.38 (73.49)
[100/157]	0.0935 (0.1009)	0.0577 (0.0621)	1.313 (0.888)	59.38 (72.80)
[110/157]	0.1201 (0.1013)	0.0812 (0.0626)	0.905 (0.892)	68.75 (72.66)
[120/157]	0.1013 (0.1016)	0.0619 (0.0628)	0.847 (0.895)	71.88 (72.34)
[130/157]	0.1034 (0.1016)	0.0647 (0.0629)	0.964 (0.909)	75.00 (71.73)
[140/157]	0.0935 (0.1015)	0.0574 (0.0628)	1.134 (0.913)	65.62 (71.59)
[150/157]	0.0955 (0.1011)	0.0589 (0.0625)	0.775 (0.909)	71.88 (71.75)
[156/157]	0.0771 (0.1007)	0.0515 (0.0623)	0.812 (0.912)	75.00 (71.68)
 * Train Acc 71.680
 * Val Acc 71.900, Total time 0.59
 * Val loss 0.808, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0419 (0.0419)	0.0083 (0.0083)	0.963 (0.963)	62.50 (62.50)
[10/157]	0.0941 (0.0912)	0.0582 (0.0542)	0.980 (0.954)	78.12 (70.74)
[20/157]	0.0945 (0.0929)	0.0591 (0.0561)	0.848 (0.920)	71.88 (72.47)
[30/157]	0.0941 (0.0936)	0.0570 (0.0569)	0.788 (0.902)	81.25 (73.08)
[40/157]	0.0949 (0.0940)	0.0578 (0.0572)	0.812 (0.920)	71.88 (71.57)
[50/157]	0.0961 (0.0943)	0.0592 (0.0574)	1.003 (0.919)	62.50 (71.51)
[60/157]	0.0928 (0.0943)	0.0568 (0.0575)	0.760 (0.911)	78.12 (71.62)
[70/157]	0.0967 (0.0945)	0.0590 (0.0576)	0.926 (0.903)	65.62 (72.01)
[80/157]	0.0950 (0.0945)	0.0585 (0.0577)	1.023 (0.901)	65.62 (72.26)
[90/157]	0.0966 (0.0946)	0.0596 (0.0577)	0.821 (0.893)	78.12 (72.60)
[100/157]	0.0941 (0.0946)	0.0578 (0.0577)	1.134 (0.908)	53.12 (72.03)
[110/157]	0.0958 (0.0946)	0.0577 (0.0577)	0.892 (0.903)	65.62 (72.30)
[120/157]	0.0958 (0.0946)	0.0595 (0.0577)	0.909 (0.907)	71.88 (72.13)
[130/157]	0.0946 (0.0947)	0.0580 (0.0578)	0.915 (0.906)	75.00 (71.90)
[140/157]	0.0955 (0.0947)	0.0584 (0.0578)	0.580 (0.908)	84.38 (71.99)
[150/157]	0.0935 (0.0947)	0.0574 (0.0578)	0.820 (0.904)	71.88 (72.14)
[156/157]	0.0791 (0.0946)	0.0537 (0.0578)	0.784 (0.900)	87.50 (72.18)
 * Train Acc 72.180
 * Val Acc 73.100, Total time 0.58
 * Val loss 0.793, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0449 (0.0449)	0.0088 (0.0088)	0.723 (0.723)	78.12 (78.12)
[10/157]	0.0981 (0.1023)	0.0602 (0.0636)	1.005 (0.920)	59.38 (70.45)
[20/157]	0.0958 (0.0997)	0.0597 (0.0615)	0.723 (0.889)	75.00 (71.88)
[30/157]	0.0977 (0.0989)	0.0604 (0.0610)	0.860 (0.887)	78.12 (71.98)
[40/157]	0.0976 (0.0983)	0.0603 (0.0607)	0.927 (0.902)	75.00 (71.04)
[50/157]	0.0978 (0.0981)	0.0606 (0.0605)	0.933 (0.914)	65.62 (70.53)
[60/157]	0.0985 (0.0978)	0.0613 (0.0604)	0.936 (0.900)	65.62 (71.26)
[70/157]	0.0967 (0.0976)	0.0600 (0.0603)	0.654 (0.889)	90.62 (72.01)
[80/157]	0.0946 (0.0975)	0.0586 (0.0603)	0.732 (0.888)	71.88 (71.91)
[90/157]	0.0977 (0.0974)	0.0601 (0.0602)	0.851 (0.885)	78.12 (72.32)
[100/157]	0.0950 (0.0973)	0.0594 (0.0602)	0.788 (0.886)	68.75 (72.28)
[110/157]	0.0970 (0.0973)	0.0597 (0.0602)	0.817 (0.890)	68.75 (72.10)
[120/157]	0.0962 (0.0972)	0.0598 (0.0601)	0.682 (0.889)	84.38 (72.39)
[130/157]	0.0975 (0.0971)	0.0611 (0.0601)	0.902 (0.893)	78.12 (72.35)
[140/157]	0.0955 (0.0971)	0.0590 (0.0601)	0.866 (0.896)	71.88 (72.34)
[150/157]	0.0963 (0.0971)	0.0596 (0.0601)	0.908 (0.895)	75.00 (72.37)
[156/157]	0.0806 (0.0970)	0.0556 (0.0600)	1.109 (0.900)	50.00 (72.16)
 * Train Acc 72.160
 * Val Acc 71.600, Total time 0.59
 * Val loss 0.808, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0413 (0.0413)	0.0084 (0.0084)	0.722 (0.722)	78.12 (78.12)
[10/157]	0.0960 (0.0913)	0.0597 (0.0546)	0.678 (0.841)	87.50 (73.58)
[20/157]	0.0950 (0.0937)	0.0595 (0.0571)	0.993 (0.858)	62.50 (72.02)
[30/157]	0.0983 (0.0947)	0.0602 (0.0580)	0.891 (0.855)	71.88 (73.19)
[40/157]	0.0973 (0.0952)	0.0598 (0.0584)	0.695 (0.864)	75.00 (72.33)
[50/157]	0.0960 (0.0955)	0.0596 (0.0587)	0.847 (0.851)	78.12 (73.41)
[60/157]	0.0977 (0.0957)	0.0603 (0.0588)	1.310 (0.862)	62.50 (73.41)
[70/157]	0.0977 (0.0957)	0.0602 (0.0590)	1.128 (0.872)	59.38 (72.93)
[80/157]	0.0960 (0.0959)	0.0596 (0.0591)	0.896 (0.878)	71.88 (72.76)
[90/157]	0.0959 (0.0960)	0.0595 (0.0592)	0.900 (0.884)	65.62 (72.70)
[100/157]	0.0974 (0.0960)	0.0600 (0.0592)	0.985 (0.893)	75.00 (72.28)
[110/157]	0.0967 (0.0960)	0.0604 (0.0593)	0.849 (0.896)	75.00 (72.18)
[120/157]	0.0962 (0.0960)	0.0598 (0.0593)	0.814 (0.899)	78.12 (72.18)
[130/157]	0.0965 (0.0961)	0.0583 (0.0593)	0.887 (0.900)	78.12 (71.97)
[140/157]	0.0977 (0.0962)	0.0599 (0.0593)	1.030 (0.903)	62.50 (71.74)
[150/157]	0.0972 (0.0962)	0.0590 (0.0593)	0.738 (0.899)	84.38 (71.98)
[156/157]	0.0795 (0.0961)	0.0535 (0.0593)	0.610 (0.905)	87.50 (71.92)
 * Train Acc 71.920
 * Val Acc 71.800, Total time 0.56
 * Val loss 0.797, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0090 (0.0090)	0.683 (0.683)	71.88 (71.88)
[10/157]	0.0929 (0.0917)	0.0572 (0.0539)	1.012 (0.846)	68.75 (73.58)
[20/157]	0.0965 (0.0933)	0.0593 (0.0562)	0.860 (0.864)	78.12 (74.26)
[30/157]	0.0943 (0.0936)	0.0593 (0.0568)	0.752 (0.880)	75.00 (73.08)
[40/157]	0.0954 (0.0939)	0.0591 (0.0573)	0.745 (0.882)	75.00 (73.09)
[50/157]	0.0963 (0.0941)	0.0597 (0.0576)	1.153 (0.893)	65.62 (72.92)
[60/157]	0.0953 (0.0941)	0.0596 (0.0577)	0.654 (0.889)	78.12 (72.80)
[70/157]	0.0946 (0.0943)	0.0587 (0.0578)	0.782 (0.892)	75.00 (72.54)
[80/157]	0.0959 (0.0943)	0.0596 (0.0579)	0.790 (0.897)	75.00 (72.30)
[90/157]	0.0931 (0.0943)	0.0567 (0.0579)	0.826 (0.898)	75.00 (72.39)
[100/157]	0.0965 (0.0944)	0.0590 (0.0580)	0.783 (0.898)	75.00 (72.25)
[110/157]	0.0945 (0.0944)	0.0581 (0.0580)	0.903 (0.898)	71.88 (72.33)
[120/157]	0.0956 (0.0945)	0.0584 (0.0580)	0.877 (0.892)	78.12 (72.86)
[130/157]	0.0935 (0.0945)	0.0577 (0.0580)	0.813 (0.891)	71.88 (72.73)
[140/157]	0.0959 (0.0946)	0.0591 (0.0580)	1.065 (0.892)	78.12 (72.65)
[150/157]	0.0962 (0.0945)	0.0597 (0.0581)	0.851 (0.892)	68.75 (72.68)
[156/157]	0.0763 (0.0944)	0.0515 (0.0580)	1.855 (0.897)	50.00 (72.60)
 * Train Acc 72.600
 * Val Acc 71.500, Total time 0.59
 * Val loss 0.799, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0085 (0.0085)	0.746 (0.746)	78.12 (78.12)
[10/157]	0.1032 (0.0927)	0.0641 (0.0549)	0.907 (0.807)	75.00 (77.27)
[20/157]	0.1031 (0.0970)	0.0646 (0.0592)	0.859 (0.821)	71.88 (75.74)
[30/157]	0.1037 (0.0987)	0.0651 (0.0608)	0.885 (0.844)	78.12 (74.80)
[40/157]	0.0940 (0.0985)	0.0572 (0.0608)	0.766 (0.851)	75.00 (74.54)
[50/157]	0.0976 (0.0978)	0.0605 (0.0604)	1.002 (0.859)	71.88 (74.26)
[60/157]	0.0934 (0.0972)	0.0570 (0.0600)	1.008 (0.867)	78.12 (74.03)
[70/157]	0.0963 (0.0969)	0.0589 (0.0598)	0.935 (0.872)	75.00 (73.86)
[80/157]	0.0928 (0.0966)	0.0581 (0.0595)	0.724 (0.877)	71.88 (73.65)
[90/157]	0.0944 (0.0964)	0.0577 (0.0594)	0.990 (0.878)	68.75 (73.59)
[100/157]	0.0957 (0.0963)	0.0593 (0.0593)	0.959 (0.880)	68.75 (73.45)
[110/157]	0.0945 (0.0961)	0.0579 (0.0592)	0.764 (0.884)	68.75 (73.14)
[120/157]	0.0955 (0.0960)	0.0594 (0.0592)	0.975 (0.889)	71.88 (72.83)
[130/157]	0.0955 (0.0959)	0.0594 (0.0591)	0.835 (0.891)	75.00 (72.85)
[140/157]	0.0947 (0.0958)	0.0581 (0.0591)	1.107 (0.897)	65.62 (72.67)
[150/157]	0.0968 (0.0964)	0.0594 (0.0596)	0.584 (0.895)	78.12 (72.70)
[156/157]	0.0800 (0.0963)	0.0553 (0.0596)	1.202 (0.901)	50.00 (72.54)
 * Train Acc 72.540
 * Val Acc 71.300, Total time 0.59
 * Val loss 0.797, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0085 (0.0085)	0.691 (0.691)	90.62 (90.62)
[10/157]	0.0975 (0.0911)	0.0602 (0.0545)	0.977 (0.903)	65.62 (72.44)
[20/157]	0.0969 (0.0933)	0.0601 (0.0569)	1.006 (0.893)	75.00 (72.77)
[30/157]	0.0975 (0.0942)	0.0609 (0.0577)	0.735 (0.895)	75.00 (72.68)
[40/157]	0.0971 (0.0946)	0.0604 (0.0581)	0.921 (0.890)	68.75 (72.79)
[50/157]	0.0966 (0.0949)	0.0603 (0.0584)	0.949 (0.911)	56.25 (71.81)
[60/157]	0.0971 (0.0951)	0.0597 (0.0586)	0.706 (0.907)	81.25 (71.98)
[70/157]	0.0961 (0.0952)	0.0596 (0.0586)	1.255 (0.918)	50.00 (71.39)
[80/157]	0.0961 (0.0953)	0.0595 (0.0587)	0.860 (0.913)	65.62 (71.53)
[90/157]	0.0951 (0.0954)	0.0595 (0.0588)	0.645 (0.902)	78.12 (71.74)
[100/157]	0.0947 (0.0955)	0.0582 (0.0589)	1.139 (0.900)	68.75 (72.12)
[110/157]	0.0962 (0.0955)	0.0604 (0.0589)	0.902 (0.904)	65.62 (71.73)
[120/157]	0.0949 (0.0956)	0.0591 (0.0589)	0.673 (0.904)	87.50 (71.69)
[130/157]	0.0957 (0.0957)	0.0596 (0.0590)	1.300 (0.908)	53.12 (71.68)
[140/157]	0.0972 (0.0957)	0.0597 (0.0591)	0.750 (0.903)	81.25 (72.03)
[150/157]	0.0963 (0.0957)	0.0592 (0.0591)	0.573 (0.901)	84.38 (72.12)
[156/157]	0.0805 (0.0956)	0.0540 (0.0590)	0.989 (0.901)	87.50 (72.16)
 * Train Acc 72.160
 * Val Acc 73.000, Total time 0.58
 * Val loss 0.793, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0083 (0.0083)	0.947 (0.947)	62.50 (62.50)
[10/157]	0.0960 (0.0973)	0.0599 (0.0600)	0.788 (0.890)	65.62 (70.74)
[20/157]	0.0933 (0.0962)	0.0572 (0.0595)	1.015 (0.881)	68.75 (71.73)
[30/157]	0.0963 (0.0959)	0.0605 (0.0594)	0.912 (0.888)	71.88 (71.37)
[40/157]	0.0979 (0.0958)	0.0605 (0.0593)	0.981 (0.900)	78.12 (71.57)
[50/157]	0.0962 (0.0956)	0.0599 (0.0592)	0.884 (0.880)	71.88 (72.79)
[60/157]	0.0973 (0.0955)	0.0608 (0.0592)	1.174 (0.891)	62.50 (72.64)
[70/157]	0.0935 (0.0954)	0.0581 (0.0592)	0.753 (0.901)	81.25 (72.58)
[80/157]	0.0966 (0.0954)	0.0598 (0.0591)	1.279 (0.907)	62.50 (72.69)
[90/157]	0.0929 (0.0954)	0.0574 (0.0591)	1.009 (0.901)	68.75 (72.70)
[100/157]	0.0951 (0.0953)	0.0588 (0.0590)	0.815 (0.895)	78.12 (73.02)
[110/157]	0.0945 (0.0952)	0.0585 (0.0590)	0.704 (0.888)	78.12 (72.86)
[120/157]	0.0940 (0.0952)	0.0583 (0.0589)	1.150 (0.889)	75.00 (72.96)
[130/157]	0.0977 (0.0952)	0.0612 (0.0589)	0.767 (0.891)	68.75 (72.73)
[140/157]	0.1036 (0.0954)	0.0643 (0.0590)	0.736 (0.887)	78.12 (72.89)
[150/157]	0.1019 (0.0959)	0.0650 (0.0594)	0.534 (0.885)	93.75 (73.03)
[156/157]	0.0909 (0.0961)	0.0625 (0.0596)	0.762 (0.887)	75.00 (73.02)
 * Train Acc 73.020
 * Val Acc 72.900, Total time 0.58
 * Val loss 0.790, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0091 (0.0091)	0.612 (0.612)	78.12 (78.12)
[10/157]	0.0942 (0.0917)	0.0585 (0.0541)	0.788 (0.836)	84.38 (73.58)
[20/157]	0.0962 (0.0933)	0.0592 (0.0564)	0.797 (0.844)	78.12 (74.26)
[30/157]	0.0974 (0.0961)	0.0608 (0.0592)	0.889 (0.845)	62.50 (74.40)
[40/157]	0.0985 (0.0964)	0.0604 (0.0594)	0.635 (0.837)	93.75 (74.85)
[50/157]	0.0982 (0.0967)	0.0594 (0.0594)	1.276 (0.858)	59.38 (74.08)
[60/157]	0.0974 (0.0969)	0.0587 (0.0594)	0.843 (0.864)	78.12 (73.72)
[70/157]	0.0971 (0.0969)	0.0609 (0.0594)	0.877 (0.868)	78.12 (73.24)
[80/157]	0.0980 (0.0969)	0.0607 (0.0595)	0.967 (0.863)	71.88 (73.53)
[90/157]	0.0967 (0.0970)	0.0601 (0.0596)	1.271 (0.867)	62.50 (73.49)
[100/157]	0.0967 (0.0970)	0.0599 (0.0596)	1.063 (0.879)	68.75 (73.05)
[110/157]	0.0938 (0.0970)	0.0580 (0.0596)	0.741 (0.880)	81.25 (72.89)
[120/157]	0.0976 (0.0970)	0.0610 (0.0596)	0.768 (0.884)	78.12 (72.75)
[130/157]	0.0961 (0.0969)	0.0598 (0.0596)	0.835 (0.882)	68.75 (72.76)
[140/157]	0.0983 (0.0969)	0.0606 (0.0597)	1.121 (0.887)	62.50 (72.65)
[150/157]	0.0968 (0.0969)	0.0602 (0.0597)	0.835 (0.879)	68.75 (73.03)
[156/157]	0.0793 (0.0968)	0.0541 (0.0597)	1.340 (0.877)	37.50 (73.12)
 * Train Acc 73.120
 * Val Acc 72.600, Total time 0.59
 * Val loss 0.785, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0086 (0.0086)	0.845 (0.845)	71.88 (71.88)
[10/157]	0.0974 (0.0916)	0.0607 (0.0539)	0.785 (0.837)	78.12 (76.14)
[20/157]	0.0958 (0.0941)	0.0600 (0.0569)	0.864 (0.872)	71.88 (73.07)
[30/157]	0.0970 (0.0950)	0.0601 (0.0579)	0.982 (0.892)	68.75 (72.18)
[40/157]	0.0955 (0.0954)	0.0590 (0.0583)	1.075 (0.897)	65.62 (72.03)
[50/157]	0.0976 (0.0957)	0.0600 (0.0587)	1.091 (0.896)	75.00 (72.37)
[60/157]	0.0977 (0.0959)	0.0602 (0.0589)	1.061 (0.903)	62.50 (71.82)
[70/157]	0.0962 (0.0960)	0.0588 (0.0590)	0.930 (0.885)	68.75 (72.58)
[80/157]	0.0978 (0.0962)	0.0606 (0.0590)	0.886 (0.882)	71.88 (72.57)
[90/157]	0.0959 (0.0962)	0.0578 (0.0591)	0.658 (0.877)	78.12 (72.80)
[100/157]	0.0966 (0.0963)	0.0594 (0.0591)	0.852 (0.886)	78.12 (72.37)
[110/157]	0.0949 (0.0963)	0.0582 (0.0591)	0.944 (0.884)	71.88 (72.78)
[120/157]	0.0985 (0.0964)	0.0604 (0.0592)	0.977 (0.886)	71.88 (72.57)
[130/157]	0.0955 (0.0964)	0.0592 (0.0592)	0.644 (0.885)	87.50 (72.50)
[140/157]	0.0974 (0.0964)	0.0599 (0.0593)	0.951 (0.887)	71.88 (72.54)
[150/157]	0.0971 (0.0965)	0.0604 (0.0593)	0.743 (0.883)	68.75 (72.54)
[156/157]	0.0820 (0.0964)	0.0542 (0.0593)	0.614 (0.883)	87.50 (72.56)
 * Train Acc 72.560
 * Val Acc 72.800, Total time 0.58
 * Val loss 0.799, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0088 (0.0088)	1.107 (1.107)	71.88 (71.88)
[10/157]	0.0932 (0.0914)	0.0573 (0.0540)	0.681 (0.839)	84.38 (75.00)
[20/157]	0.0957 (0.0931)	0.0585 (0.0560)	0.764 (0.866)	84.38 (72.92)
[30/157]	0.0935 (0.0936)	0.0573 (0.0567)	1.035 (0.918)	56.25 (71.07)
[40/157]	0.0967 (0.0939)	0.0597 (0.0571)	0.888 (0.923)	68.75 (70.96)
[50/157]	0.0929 (0.0941)	0.0574 (0.0573)	0.934 (0.929)	65.62 (70.53)
[60/157]	0.0952 (0.0942)	0.0586 (0.0576)	0.925 (0.934)	81.25 (70.85)
[70/157]	0.0930 (0.0943)	0.0572 (0.0577)	0.878 (0.937)	71.88 (70.25)
[80/157]	0.0952 (0.0944)	0.0580 (0.0578)	1.088 (0.939)	62.50 (70.29)
[90/157]	0.0958 (0.0944)	0.0585 (0.0578)	0.727 (0.926)	78.12 (70.84)
[100/157]	0.0937 (0.0944)	0.0581 (0.0578)	1.016 (0.918)	75.00 (71.35)
[110/157]	0.0956 (0.0945)	0.0592 (0.0579)	0.942 (0.917)	71.88 (71.40)
[120/157]	0.0938 (0.0945)	0.0563 (0.0579)	0.798 (0.906)	78.12 (71.85)
[130/157]	0.0954 (0.0946)	0.0583 (0.0580)	0.997 (0.907)	71.88 (71.88)
[140/157]	0.0953 (0.0946)	0.0580 (0.0580)	0.756 (0.906)	68.75 (71.79)
[150/157]	0.0946 (0.0946)	0.0576 (0.0579)	0.996 (0.904)	71.88 (71.83)
[156/157]	0.0787 (0.0945)	0.0534 (0.0579)	1.243 (0.907)	62.50 (71.74)
 * Train Acc 71.740
 * Val Acc 73.100, Total time 0.58
 * Val loss 0.793, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0089 (0.0089)	0.791 (0.791)	78.12 (78.12)
[10/157]	0.0943 (0.0918)	0.0567 (0.0543)	0.841 (0.977)	71.88 (67.61)
[20/157]	0.0954 (0.0933)	0.0587 (0.0563)	0.795 (0.945)	71.88 (70.24)
[30/157]	0.0966 (0.0970)	0.0599 (0.0598)	0.815 (0.909)	68.75 (71.27)
[40/157]	0.0973 (0.0971)	0.0592 (0.0597)	1.100 (0.898)	65.62 (71.49)
[50/157]	0.0972 (0.0969)	0.0596 (0.0596)	1.034 (0.900)	59.38 (71.26)
[60/157]	0.0976 (0.0968)	0.0606 (0.0595)	0.717 (0.892)	81.25 (72.03)
[70/157]	0.0941 (0.0968)	0.0570 (0.0594)	0.824 (0.880)	78.12 (72.67)
[80/157]	0.0957 (0.0968)	0.0579 (0.0593)	0.708 (0.887)	81.25 (72.38)
[90/157]	0.0970 (0.0967)	0.0593 (0.0592)	1.098 (0.894)	65.62 (71.94)
[100/157]	0.0971 (0.0967)	0.0590 (0.0592)	1.125 (0.891)	65.62 (72.12)
[110/157]	0.0947 (0.0966)	0.0597 (0.0591)	0.754 (0.889)	81.25 (72.13)
[120/157]	0.0952 (0.0965)	0.0604 (0.0592)	0.831 (0.882)	75.00 (72.42)
[130/157]	0.0971 (0.0966)	0.0598 (0.0592)	0.545 (0.881)	84.38 (72.38)
[140/157]	0.0947 (0.0965)	0.0592 (0.0593)	0.733 (0.881)	75.00 (72.36)
[150/157]	0.0947 (0.0964)	0.0585 (0.0592)	1.071 (0.881)	65.62 (72.48)
[156/157]	0.0794 (0.0963)	0.0547 (0.0592)	0.825 (0.880)	87.50 (72.54)
 * Train Acc 72.540
 * Val Acc 72.500, Total time 0.58
 * Val loss 0.789, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0083 (0.0083)	0.815 (0.815)	59.38 (59.38)
[10/157]	0.0948 (0.0906)	0.0582 (0.0537)	1.041 (0.898)	65.62 (71.59)
[20/157]	0.0954 (0.0932)	0.0590 (0.0564)	1.078 (0.872)	62.50 (71.88)
[30/157]	0.0932 (0.0940)	0.0585 (0.0574)	0.687 (0.861)	78.12 (72.78)
[40/157]	0.0947 (0.0945)	0.0584 (0.0579)	1.116 (0.853)	62.50 (73.02)
[50/157]	0.0959 (0.0949)	0.0588 (0.0582)	0.890 (0.861)	59.38 (73.22)
[60/157]	0.0935 (0.0950)	0.0575 (0.0584)	0.965 (0.869)	75.00 (72.75)
[70/157]	0.0955 (0.0951)	0.0588 (0.0585)	0.810 (0.878)	75.00 (72.10)
[80/157]	0.0973 (0.0953)	0.0602 (0.0586)	0.864 (0.865)	78.12 (72.92)
[90/157]	0.0973 (0.0954)	0.0607 (0.0587)	0.733 (0.867)	78.12 (72.56)
[100/157]	0.0968 (0.0955)	0.0602 (0.0588)	0.687 (0.866)	78.12 (72.65)
[110/157]	0.0952 (0.0955)	0.0584 (0.0589)	0.954 (0.869)	71.88 (72.64)
[120/157]	0.0946 (0.0956)	0.0591 (0.0590)	0.827 (0.873)	68.75 (72.62)
[130/157]	0.0955 (0.0957)	0.0596 (0.0590)	1.103 (0.884)	71.88 (72.54)
[140/157]	0.0973 (0.0957)	0.0598 (0.0590)	0.851 (0.883)	75.00 (72.67)
[150/157]	0.0973 (0.0957)	0.0596 (0.0591)	0.757 (0.878)	81.25 (72.99)
[156/157]	0.0781 (0.0956)	0.0529 (0.0590)	1.414 (0.880)	62.50 (72.88)
 * Train Acc 72.880
 * Val Acc 72.600, Total time 0.59
 * Val loss 0.790, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0092 (0.0092)	0.720 (0.720)	75.00 (75.00)
[10/157]	0.0973 (0.0914)	0.0607 (0.0542)	1.128 (0.876)	56.25 (71.02)
[20/157]	0.0957 (0.0938)	0.0595 (0.0567)	0.824 (0.903)	75.00 (71.58)
[30/157]	0.0942 (0.0946)	0.0585 (0.0577)	0.995 (0.908)	65.62 (70.97)
[40/157]	0.0978 (0.0951)	0.0604 (0.0582)	0.682 (0.893)	81.25 (72.03)
[50/157]	0.0974 (0.0954)	0.0609 (0.0585)	0.925 (0.900)	75.00 (71.57)
[60/157]	0.0952 (0.0956)	0.0588 (0.0585)	0.917 (0.911)	71.88 (71.47)
[70/157]	0.0967 (0.0957)	0.0597 (0.0587)	0.974 (0.907)	62.50 (71.57)
[80/157]	0.0979 (0.0958)	0.0604 (0.0589)	0.967 (0.915)	68.75 (71.33)
[90/157]	0.0969 (0.0958)	0.0600 (0.0589)	0.712 (0.905)	68.75 (71.63)
[100/157]	0.0981 (0.0958)	0.0607 (0.0590)	0.732 (0.895)	75.00 (71.97)
[110/157]	0.0962 (0.0959)	0.0595 (0.0590)	0.844 (0.893)	81.25 (72.16)
[120/157]	0.0960 (0.0959)	0.0593 (0.0590)	0.779 (0.885)	68.75 (72.47)
[130/157]	0.0953 (0.0959)	0.0595 (0.0591)	0.991 (0.887)	68.75 (72.54)
[140/157]	0.0953 (0.0960)	0.0589 (0.0591)	0.934 (0.883)	71.88 (72.63)
[150/157]	0.0967 (0.0960)	0.0604 (0.0592)	1.082 (0.887)	59.38 (72.50)
[156/157]	0.0780 (0.0959)	0.0529 (0.0591)	1.535 (0.884)	25.00 (72.60)
 * Train Acc 72.600
 * Val Acc 72.900, Total time 0.59
 * Val loss 0.795, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0087 (0.0087)	0.832 (0.832)	65.62 (65.62)
[10/157]	0.0940 (0.0910)	0.0579 (0.0537)	0.918 (0.836)	65.62 (73.86)
[20/157]	0.0974 (0.0937)	0.0596 (0.0565)	0.695 (0.876)	71.88 (71.73)
[30/157]	0.0967 (0.0945)	0.0606 (0.0575)	0.744 (0.849)	81.25 (74.29)
[40/157]	0.0955 (0.0951)	0.0591 (0.0579)	0.772 (0.857)	78.12 (74.24)
[50/157]	0.0973 (0.0955)	0.0607 (0.0583)	0.854 (0.859)	65.62 (74.08)
[60/157]	0.0954 (0.0956)	0.0596 (0.0586)	0.800 (0.855)	71.88 (73.92)
[70/157]	0.0953 (0.0957)	0.0595 (0.0588)	0.898 (0.871)	68.75 (73.15)
[80/157]	0.0981 (0.0959)	0.0605 (0.0588)	0.657 (0.870)	81.25 (72.96)
[90/157]	0.0969 (0.0959)	0.0592 (0.0589)	0.843 (0.885)	68.75 (72.32)
[100/157]	0.0978 (0.0959)	0.0594 (0.0589)	1.009 (0.877)	68.75 (72.68)
[110/157]	0.0980 (0.0959)	0.0599 (0.0589)	0.723 (0.886)	81.25 (72.44)
[120/157]	0.0949 (0.0960)	0.0581 (0.0589)	1.069 (0.882)	71.88 (72.62)
[130/157]	0.0963 (0.0960)	0.0592 (0.0590)	0.941 (0.878)	68.75 (72.69)
[140/157]	0.0970 (0.0960)	0.0601 (0.0590)	0.736 (0.880)	71.88 (72.63)
[150/157]	0.0994 (0.0961)	0.0613 (0.0590)	1.035 (0.878)	71.88 (72.79)
[156/157]	0.0822 (0.0960)	0.0535 (0.0590)	1.864 (0.886)	25.00 (72.58)
 * Train Acc 72.580
 * Val Acc 72.800, Total time 0.61
 * Val loss 0.785, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0442 (0.0442)	0.0090 (0.0090)	0.901 (0.901)	78.12 (78.12)
[10/157]	0.0978 (0.0916)	0.0604 (0.0542)	0.827 (0.900)	78.12 (71.88)
[20/157]	0.0970 (0.0937)	0.0604 (0.0567)	0.835 (0.886)	75.00 (74.26)
[30/157]	0.0963 (0.0949)	0.0583 (0.0574)	0.925 (0.880)	62.50 (73.99)
[40/157]	0.0949 (0.0955)	0.0569 (0.0578)	0.691 (0.871)	81.25 (74.85)
[50/157]	0.0968 (0.0958)	0.0594 (0.0580)	1.222 (0.881)	65.62 (74.33)
[60/157]	0.0948 (0.0961)	0.0580 (0.0582)	0.722 (0.878)	81.25 (74.44)
[70/157]	0.0982 (0.0962)	0.0600 (0.0584)	0.950 (0.881)	65.62 (74.25)
[80/157]	0.0949 (0.0963)	0.0578 (0.0585)	0.685 (0.879)	78.12 (74.19)
[90/157]	0.0998 (0.0964)	0.0605 (0.0586)	0.663 (0.871)	84.38 (74.31)
[100/157]	0.0936 (0.0964)	0.0578 (0.0586)	0.891 (0.869)	81.25 (74.44)
[110/157]	0.0951 (0.0964)	0.0578 (0.0585)	0.740 (0.855)	81.25 (74.83)
[120/157]	0.1006 (0.0964)	0.0626 (0.0585)	0.793 (0.854)	71.88 (74.56)
[130/157]	0.1008 (0.0969)	0.0637 (0.0590)	0.817 (0.859)	68.75 (74.26)
[140/157]	0.1032 (0.0973)	0.0655 (0.0594)	0.874 (0.858)	68.75 (74.14)
[150/157]	0.1007 (0.0976)	0.0640 (0.0597)	0.810 (0.862)	78.12 (73.92)
[156/157]	0.0881 (0.0977)	0.0603 (0.0599)	0.898 (0.866)	75.00 (73.86)
 * Train Acc 73.860
 * Val Acc 73.100, Total time 0.61
 * Val loss 0.784, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0087 (0.0087)	0.849 (0.849)	75.00 (75.00)
[10/157]	0.1017 (0.0967)	0.0627 (0.0583)	0.801 (0.821)	87.50 (76.42)
[20/157]	0.1049 (0.0996)	0.0639 (0.0610)	0.883 (0.862)	75.00 (74.11)
[30/157]	0.1013 (0.1006)	0.0625 (0.0620)	0.944 (0.872)	75.00 (74.09)
[40/157]	0.1015 (0.1010)	0.0636 (0.0624)	0.928 (0.868)	71.88 (74.31)
[50/157]	0.0955 (0.1004)	0.0587 (0.0619)	0.700 (0.872)	78.12 (74.33)
[60/157]	0.0924 (0.0995)	0.0571 (0.0613)	0.894 (0.870)	81.25 (74.54)
[70/157]	0.0971 (0.0991)	0.0584 (0.0609)	0.894 (0.865)	71.88 (74.56)
[80/157]	0.0954 (0.0986)	0.0572 (0.0606)	0.798 (0.874)	65.62 (73.46)
[90/157]	0.0974 (0.0982)	0.0609 (0.0603)	0.626 (0.875)	84.38 (73.39)
[100/157]	0.0945 (0.0979)	0.0568 (0.0601)	0.722 (0.873)	78.12 (73.33)
[110/157]	0.1040 (0.0982)	0.0660 (0.0604)	1.047 (0.874)	68.75 (73.51)
[120/157]	0.1011 (0.0985)	0.0627 (0.0607)	0.835 (0.881)	75.00 (73.14)
[130/157]	0.1017 (0.0987)	0.0638 (0.0609)	0.654 (0.878)	84.38 (73.19)
[140/157]	0.1032 (0.0990)	0.0651 (0.0611)	0.643 (0.882)	84.38 (73.12)
[150/157]	0.1019 (0.0991)	0.0642 (0.0613)	0.910 (0.880)	71.88 (73.30)
[156/157]	0.0848 (0.0991)	0.0578 (0.0613)	0.575 (0.875)	87.50 (73.58)
 * Train Acc 73.580
 * Val Acc 73.200, Total time 0.60
 * Val loss 0.776, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0413 (0.0413)	0.0083 (0.0083)	0.934 (0.934)	71.88 (71.88)
[10/157]	0.0937 (0.0907)	0.0580 (0.0540)	1.169 (0.949)	68.75 (68.18)
[20/157]	0.0987 (0.0927)	0.0613 (0.0562)	0.914 (0.934)	65.62 (69.20)
[30/157]	0.0956 (0.0932)	0.0592 (0.0568)	0.770 (0.920)	75.00 (69.86)
[40/157]	0.0939 (0.0937)	0.0583 (0.0572)	0.941 (0.885)	65.62 (71.88)
[50/157]	0.1194 (0.0944)	0.0806 (0.0579)	0.832 (0.887)	71.88 (71.94)
[60/157]	0.0925 (0.0959)	0.0564 (0.0593)	0.921 (0.898)	75.00 (71.93)
[70/157]	0.0936 (0.0957)	0.0572 (0.0591)	0.929 (0.891)	71.88 (72.18)
[80/157]	0.0964 (0.0956)	0.0594 (0.0590)	0.801 (0.896)	68.75 (72.15)
[90/157]	0.0934 (0.0955)	0.0564 (0.0589)	0.929 (0.899)	75.00 (72.25)
[100/157]	0.0961 (0.0955)	0.0597 (0.0589)	1.075 (0.895)	71.88 (72.43)
[110/157]	0.0956 (0.0965)	0.0586 (0.0598)	0.821 (0.893)	78.12 (72.58)
[120/157]	0.0955 (0.0964)	0.0589 (0.0597)	1.237 (0.894)	62.50 (72.57)
[130/157]	0.0939 (0.0963)	0.0579 (0.0596)	1.016 (0.885)	65.62 (73.07)
[140/157]	0.0954 (0.0962)	0.0585 (0.0595)	0.830 (0.883)	78.12 (73.07)
[150/157]	0.0926 (0.0962)	0.0578 (0.0595)	0.803 (0.883)	78.12 (73.03)
[156/157]	0.0771 (0.0961)	0.0523 (0.0594)	0.968 (0.879)	50.00 (73.16)
 * Train Acc 73.160
 * Val Acc 72.700, Total time 0.58
 * Val loss 0.781, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0086 (0.0086)	0.673 (0.673)	75.00 (75.00)
[10/157]	0.0938 (0.0908)	0.0573 (0.0535)	1.170 (0.853)	68.75 (73.86)
[20/157]	0.0973 (0.0930)	0.0597 (0.0561)	0.786 (0.825)	71.88 (73.81)
[30/157]	0.0951 (0.0936)	0.0589 (0.0569)	0.859 (0.808)	71.88 (76.01)
[40/157]	0.0945 (0.0940)	0.0584 (0.0574)	1.126 (0.855)	59.38 (74.85)
[50/157]	0.0967 (0.0943)	0.0601 (0.0577)	1.147 (0.867)	68.75 (74.14)
[60/157]	0.0954 (0.0945)	0.0591 (0.0579)	0.863 (0.878)	71.88 (73.92)
[70/157]	0.0961 (0.0947)	0.0588 (0.0581)	0.821 (0.881)	78.12 (73.86)
[80/157]	0.0944 (0.0947)	0.0580 (0.0581)	0.738 (0.887)	78.12 (73.26)
[90/157]	0.0957 (0.0948)	0.0590 (0.0582)	0.926 (0.895)	75.00 (72.91)
[100/157]	0.0944 (0.0948)	0.0589 (0.0583)	0.763 (0.899)	81.25 (72.87)
[110/157]	0.0960 (0.0948)	0.0595 (0.0584)	0.689 (0.893)	81.25 (73.11)
[120/157]	0.1006 (0.0958)	0.0649 (0.0593)	1.050 (0.895)	65.62 (73.14)
[130/157]	0.0956 (0.0958)	0.0597 (0.0592)	0.702 (0.893)	78.12 (73.12)
[140/157]	0.0973 (0.0958)	0.0598 (0.0592)	0.644 (0.890)	81.25 (73.12)
[150/157]	0.0947 (0.0957)	0.0586 (0.0592)	0.856 (0.892)	75.00 (73.12)
[156/157]	0.0772 (0.0956)	0.0524 (0.0591)	0.670 (0.890)	75.00 (73.16)
 * Train Acc 73.160
 * Val Acc 72.300, Total time 0.58
 * Val loss 0.789, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0417 (0.0417)	0.0086 (0.0086)	0.764 (0.764)	81.25 (81.25)
[10/157]	0.0968 (0.0910)	0.0601 (0.0544)	0.720 (0.842)	87.50 (76.70)
[20/157]	0.0971 (0.0933)	0.0597 (0.0568)	1.129 (0.859)	62.50 (76.19)
[30/157]	0.0953 (0.0940)	0.0591 (0.0576)	1.302 (0.886)	65.62 (74.09)
[40/157]	0.0978 (0.0946)	0.0604 (0.0581)	0.824 (0.877)	71.88 (73.78)
[50/157]	0.0973 (0.0949)	0.0596 (0.0583)	1.188 (0.875)	59.38 (73.22)
[60/157]	0.0968 (0.0950)	0.0598 (0.0584)	0.960 (0.898)	56.25 (71.93)
[70/157]	0.0959 (0.0952)	0.0580 (0.0585)	0.895 (0.896)	68.75 (72.10)
[80/157]	0.0952 (0.0951)	0.0583 (0.0584)	0.673 (0.895)	81.25 (72.15)
[90/157]	0.0966 (0.0951)	0.0600 (0.0585)	0.555 (0.895)	87.50 (72.12)
[100/157]	0.0939 (0.0951)	0.0576 (0.0585)	1.110 (0.887)	62.50 (72.37)
[110/157]	0.0948 (0.0961)	0.0585 (0.0594)	0.809 (0.890)	75.00 (72.47)
[120/157]	0.0945 (0.0961)	0.0576 (0.0594)	0.800 (0.888)	71.88 (72.39)
[130/157]	0.0957 (0.0960)	0.0578 (0.0593)	0.834 (0.884)	71.88 (72.71)
[140/157]	0.0938 (0.0959)	0.0573 (0.0592)	0.840 (0.887)	81.25 (72.47)
[150/157]	0.0947 (0.0959)	0.0581 (0.0592)	0.772 (0.884)	81.25 (72.45)
[156/157]	0.0785 (0.0958)	0.0546 (0.0592)	0.894 (0.881)	62.50 (72.64)
 * Train Acc 72.640
 * Val Acc 73.100, Total time 0.58
 * Val loss 0.786, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0410 (0.0410)	0.0083 (0.0083)	0.686 (0.686)	75.00 (75.00)
[10/157]	0.0968 (0.0916)	0.0600 (0.0544)	0.880 (0.862)	75.00 (76.42)
[20/157]	0.0948 (0.0933)	0.0587 (0.0566)	0.846 (0.853)	78.12 (75.45)
[30/157]	0.1016 (0.0952)	0.0639 (0.0584)	0.650 (0.863)	81.25 (75.40)
[40/157]	0.1039 (0.0971)	0.0661 (0.0600)	0.856 (0.870)	68.75 (74.24)
[50/157]	0.1013 (0.0982)	0.0641 (0.0608)	0.818 (0.866)	78.12 (74.88)
[60/157]	0.1011 (0.0989)	0.0634 (0.0614)	0.712 (0.865)	78.12 (75.10)
[70/157]	0.1010 (0.0993)	0.0642 (0.0618)	0.898 (0.858)	68.75 (74.96)
[80/157]	0.0998 (0.0996)	0.0640 (0.0622)	0.887 (0.849)	75.00 (75.31)
[90/157]	0.1013 (0.0999)	0.0640 (0.0625)	0.979 (0.849)	71.88 (75.48)
[100/157]	0.1008 (0.1001)	0.0637 (0.0626)	0.869 (0.852)	78.12 (75.15)
[110/157]	0.0942 (0.1000)	0.0588 (0.0626)	0.765 (0.849)	81.25 (75.08)
[120/157]	0.0952 (0.0996)	0.0589 (0.0622)	0.731 (0.855)	78.12 (74.82)
[130/157]	0.0956 (0.0992)	0.0591 (0.0620)	0.731 (0.855)	84.38 (74.76)
[140/157]	0.0962 (0.0989)	0.0595 (0.0617)	0.924 (0.857)	75.00 (74.78)
[150/157]	0.0928 (0.0986)	0.0565 (0.0615)	0.684 (0.860)	78.12 (74.77)
[156/157]	0.0792 (0.0984)	0.0539 (0.0613)	1.262 (0.863)	50.00 (74.60)
 * Train Acc 74.600
 * Val Acc 72.900, Total time 0.58
 * Val loss 0.779, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0083 (0.0083)	0.992 (0.992)	71.88 (71.88)
[10/157]	0.0943 (0.0919)	0.0581 (0.0544)	0.936 (0.871)	75.00 (73.01)
[20/157]	0.0975 (0.0944)	0.0601 (0.0572)	0.950 (0.857)	71.88 (73.51)
[30/157]	0.0969 (0.0955)	0.0597 (0.0583)	1.064 (0.842)	53.12 (73.29)
[40/157]	0.0966 (0.0960)	0.0600 (0.0588)	0.918 (0.855)	71.88 (72.10)
[50/157]	0.0975 (0.0964)	0.0603 (0.0591)	0.649 (0.861)	84.38 (72.49)
[60/157]	0.0974 (0.0967)	0.0604 (0.0594)	0.796 (0.867)	75.00 (72.13)
[70/157]	0.0971 (0.0969)	0.0604 (0.0596)	0.769 (0.869)	81.25 (72.40)
[80/157]	0.0975 (0.0970)	0.0607 (0.0597)	1.253 (0.860)	68.75 (73.23)
[90/157]	0.0966 (0.0971)	0.0603 (0.0598)	0.701 (0.858)	75.00 (73.45)
[100/157]	0.0971 (0.0971)	0.0603 (0.0599)	0.928 (0.853)	71.88 (73.79)
[110/157]	0.0981 (0.0972)	0.0609 (0.0599)	1.055 (0.853)	81.25 (73.90)
[120/157]	0.0990 (0.0972)	0.0608 (0.0599)	0.939 (0.862)	71.88 (73.66)
[130/157]	0.0983 (0.0973)	0.0604 (0.0600)	0.998 (0.862)	68.75 (73.69)
[140/157]	0.0982 (0.0973)	0.0610 (0.0600)	0.698 (0.864)	78.12 (73.54)
[150/157]	0.0979 (0.0973)	0.0608 (0.0600)	0.847 (0.864)	71.88 (73.59)
[156/157]	0.0805 (0.0972)	0.0553 (0.0600)	1.221 (0.863)	62.50 (73.60)
 * Train Acc 73.600
 * Val Acc 72.800, Total time 0.60
 * Val loss 0.782, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0414 (0.0414)	0.0084 (0.0084)	1.100 (1.100)	75.00 (75.00)
[10/157]	0.0976 (0.0925)	0.0609 (0.0552)	0.925 (0.914)	68.75 (71.59)
[20/157]	0.0965 (0.0951)	0.0604 (0.0580)	0.863 (0.926)	75.00 (72.17)
[30/157]	0.0968 (0.0959)	0.0613 (0.0589)	0.879 (0.901)	78.12 (72.48)
[40/157]	0.0969 (0.0964)	0.0604 (0.0593)	0.819 (0.875)	81.25 (73.55)
[50/157]	0.0974 (0.0968)	0.0606 (0.0596)	1.212 (0.860)	62.50 (74.39)
[60/157]	0.0971 (0.0970)	0.0602 (0.0598)	0.802 (0.873)	75.00 (73.57)
[70/157]	0.0993 (0.0971)	0.0623 (0.0599)	0.656 (0.862)	84.38 (74.12)
[80/157]	0.0984 (0.0972)	0.0614 (0.0601)	0.917 (0.859)	75.00 (74.27)
[90/157]	0.0980 (0.0972)	0.0608 (0.0601)	1.105 (0.861)	59.38 (74.11)
[100/157]	0.0985 (0.0973)	0.0617 (0.0602)	0.946 (0.864)	75.00 (74.20)
[110/157]	0.0967 (0.0974)	0.0601 (0.0603)	0.823 (0.868)	75.00 (74.07)
[120/157]	0.0961 (0.0975)	0.0587 (0.0603)	0.793 (0.862)	68.75 (74.04)
[130/157]	0.0981 (0.0976)	0.0593 (0.0603)	0.650 (0.858)	87.50 (74.21)
[140/157]	0.0986 (0.0976)	0.0598 (0.0603)	0.747 (0.858)	81.25 (74.42)
[150/157]	0.0968 (0.0976)	0.0593 (0.0603)	0.552 (0.863)	84.38 (74.21)
[156/157]	0.0810 (0.0975)	0.0553 (0.0603)	0.946 (0.868)	62.50 (74.02)
 * Train Acc 74.020
 * Val Acc 73.000, Total time 0.57
 * Val loss 0.774, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0088 (0.0088)	0.974 (0.974)	65.62 (65.62)
[10/157]	0.0961 (0.0919)	0.0586 (0.0545)	0.912 (0.841)	68.75 (72.73)
[20/157]	0.0935 (0.0933)	0.0573 (0.0564)	0.960 (0.841)	62.50 (72.77)
[30/157]	0.0956 (0.0939)	0.0588 (0.0570)	0.703 (0.846)	78.12 (72.88)
[40/157]	0.0944 (0.0941)	0.0584 (0.0572)	1.005 (0.843)	68.75 (73.09)
[50/157]	0.0970 (0.0943)	0.0590 (0.0576)	0.627 (0.834)	87.50 (73.65)
[60/157]	0.0946 (0.0944)	0.0574 (0.0576)	0.783 (0.842)	81.25 (73.87)
[70/157]	0.0959 (0.0945)	0.0589 (0.0577)	0.880 (0.846)	71.88 (74.03)
[80/157]	0.0926 (0.0945)	0.0571 (0.0578)	0.742 (0.850)	81.25 (73.77)
[90/157]	0.0948 (0.0946)	0.0583 (0.0579)	0.762 (0.861)	75.00 (73.42)
[100/157]	0.0959 (0.0946)	0.0596 (0.0580)	1.244 (0.869)	62.50 (73.30)
[110/157]	0.1191 (0.0951)	0.0806 (0.0584)	0.978 (0.869)	68.75 (73.56)
[120/157]	0.0939 (0.0955)	0.0580 (0.0588)	0.675 (0.871)	71.88 (73.40)
[130/157]	0.0942 (0.0954)	0.0585 (0.0588)	0.758 (0.865)	71.88 (73.54)
[140/157]	0.0997 (0.0959)	0.0619 (0.0592)	0.607 (0.868)	75.00 (73.38)
[150/157]	0.0982 (0.0961)	0.0611 (0.0593)	0.760 (0.866)	75.00 (73.39)
[156/157]	0.0836 (0.0961)	0.0567 (0.0594)	0.754 (0.866)	75.00 (73.48)
 * Train Acc 73.480
 * Val Acc 73.100, Total time 0.58
 * Val loss 0.782, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0079 (0.0079)	0.800 (0.800)	75.00 (75.00)
[10/157]	0.0969 (0.0932)	0.0603 (0.0554)	0.996 (0.931)	62.50 (69.89)
[20/157]	0.0974 (0.0956)	0.0603 (0.0581)	0.891 (0.898)	68.75 (71.88)
[30/157]	0.0985 (0.0965)	0.0618 (0.0591)	0.650 (0.902)	78.12 (71.47)
[40/157]	0.0976 (0.0971)	0.0613 (0.0595)	0.895 (0.884)	71.88 (72.64)
[50/157]	0.0983 (0.0973)	0.0605 (0.0598)	1.292 (0.900)	59.38 (72.00)
[60/157]	0.0988 (0.0975)	0.0613 (0.0599)	0.940 (0.881)	65.62 (72.80)
[70/157]	0.0984 (0.0977)	0.0614 (0.0601)	0.743 (0.882)	71.88 (72.40)
[80/157]	0.0980 (0.0978)	0.0601 (0.0602)	0.820 (0.882)	78.12 (72.61)
[90/157]	0.0991 (0.0978)	0.0613 (0.0603)	0.875 (0.885)	71.88 (72.66)
[100/157]	0.0985 (0.0979)	0.0615 (0.0603)	1.062 (0.881)	53.12 (72.77)
[110/157]	0.0988 (0.0979)	0.0616 (0.0603)	0.758 (0.886)	78.12 (72.69)
[120/157]	0.0949 (0.0979)	0.0586 (0.0603)	0.575 (0.881)	87.50 (73.11)
[130/157]	0.0931 (0.0977)	0.0568 (0.0602)	1.093 (0.876)	62.50 (73.14)
[140/157]	0.0960 (0.0975)	0.0593 (0.0600)	0.657 (0.880)	81.25 (73.12)
[150/157]	0.0953 (0.0973)	0.0590 (0.0599)	0.909 (0.876)	68.75 (73.22)
[156/157]	0.0781 (0.0971)	0.0530 (0.0598)	0.715 (0.872)	62.50 (73.32)
 * Train Acc 73.320
 * Val Acc 73.400, Total time 0.57
 * Val loss 0.782, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0079 (0.0079)	1.020 (1.020)	71.88 (71.88)
[10/157]	0.0939 (0.0918)	0.0575 (0.0540)	1.102 (0.887)	68.75 (74.15)
[20/157]	0.0942 (0.0932)	0.0583 (0.0561)	0.797 (0.887)	78.12 (72.77)
[30/157]	0.0963 (0.0940)	0.0591 (0.0568)	1.013 (0.881)	62.50 (72.28)
[40/157]	0.0937 (0.0941)	0.0582 (0.0573)	0.739 (0.884)	78.12 (71.80)
[50/157]	0.0930 (0.0942)	0.0569 (0.0576)	0.914 (0.888)	68.75 (71.51)
[60/157]	0.0963 (0.0943)	0.0594 (0.0577)	0.804 (0.872)	75.00 (72.34)
[70/157]	0.0949 (0.0943)	0.0582 (0.0578)	1.144 (0.868)	68.75 (72.71)
[80/157]	0.0928 (0.0944)	0.0569 (0.0579)	1.004 (0.872)	65.62 (72.57)
[90/157]	0.0968 (0.0944)	0.0596 (0.0580)	0.778 (0.870)	68.75 (72.87)
[100/157]	0.0921 (0.0945)	0.0566 (0.0580)	0.708 (0.868)	78.12 (72.77)
[110/157]	0.0965 (0.0945)	0.0592 (0.0581)	0.833 (0.870)	68.75 (72.75)
[120/157]	0.0939 (0.0953)	0.0580 (0.0589)	0.393 (0.871)	96.88 (72.93)
[130/157]	0.0934 (0.0953)	0.0573 (0.0588)	0.630 (0.874)	87.50 (72.81)
[140/157]	0.1102 (0.0955)	0.0710 (0.0590)	0.836 (0.874)	75.00 (73.09)
[150/157]	0.0938 (0.0959)	0.0582 (0.0593)	0.663 (0.870)	81.25 (73.22)
[156/157]	0.0850 (0.0958)	0.0536 (0.0593)	1.620 (0.869)	37.50 (73.26)
 * Train Acc 73.260
 * Val Acc 72.100, Total time 0.62
 * Val loss 0.791, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0469 (0.0469)	0.0096 (0.0096)	1.090 (1.090)	68.75 (68.75)
[10/157]	0.1059 (0.0952)	0.0608 (0.0543)	0.791 (0.972)	75.00 (69.60)
[20/157]	0.0963 (0.0983)	0.0556 (0.0561)	0.765 (0.900)	71.88 (72.02)
[30/157]	0.1044 (0.0987)	0.0644 (0.0571)	0.783 (0.874)	84.38 (74.09)
[40/157]	0.1095 (0.1009)	0.0681 (0.0597)	0.846 (0.872)	71.88 (73.86)
[50/157]	0.1068 (0.1022)	0.0674 (0.0611)	1.029 (0.885)	68.75 (73.59)
[60/157]	0.1062 (0.1030)	0.0680 (0.0621)	0.983 (0.893)	75.00 (73.21)
[70/157]	0.1044 (0.1036)	0.0648 (0.0627)	0.773 (0.888)	81.25 (73.06)
[80/157]	0.1071 (0.1041)	0.0675 (0.0634)	0.623 (0.869)	81.25 (73.69)
[90/157]	0.1084 (0.1043)	0.0702 (0.0638)	0.947 (0.874)	71.88 (73.73)
[100/157]	0.1081 (0.1046)	0.0685 (0.0642)	0.599 (0.866)	90.62 (74.29)
[110/157]	0.1086 (0.1048)	0.0681 (0.0645)	1.165 (0.869)	62.50 (73.93)
[120/157]	0.1065 (0.1050)	0.0685 (0.0648)	0.727 (0.869)	75.00 (73.79)
[130/157]	0.0955 (0.1050)	0.0584 (0.0649)	0.839 (0.866)	75.00 (73.83)
[140/157]	0.1066 (0.1047)	0.0690 (0.0647)	0.908 (0.866)	68.75 (73.87)
[150/157]	0.0934 (0.1041)	0.0566 (0.0644)	0.986 (0.865)	65.62 (73.61)
[156/157]	0.0810 (0.1037)	0.0546 (0.0641)	0.895 (0.863)	62.50 (73.68)
 * Train Acc 73.680
 * Val Acc 72.900, Total time 0.61
 * Val loss 0.782, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0450 (0.0450)	0.0089 (0.0089)	0.523 (0.523)	93.75 (93.75)
[10/157]	0.1095 (0.1032)	0.0699 (0.0632)	0.969 (0.829)	68.75 (75.85)
[20/157]	0.0935 (0.0997)	0.0585 (0.0614)	0.798 (0.840)	75.00 (75.30)
[30/157]	0.0936 (0.0985)	0.0578 (0.0606)	1.022 (0.825)	68.75 (76.31)
[40/157]	0.0980 (0.0977)	0.0598 (0.0601)	0.931 (0.828)	75.00 (75.76)
[50/157]	0.1195 (0.0985)	0.0803 (0.0609)	0.888 (0.837)	71.88 (74.88)
[60/157]	0.1062 (0.0999)	0.0666 (0.0621)	1.262 (0.846)	62.50 (74.28)
[70/157]	0.1031 (0.1007)	0.0663 (0.0628)	1.026 (0.855)	62.50 (73.90)
[80/157]	0.0966 (0.1000)	0.0596 (0.0621)	0.857 (0.850)	68.75 (73.88)
[90/157]	0.0964 (0.0995)	0.0592 (0.0617)	0.741 (0.859)	78.12 (73.45)
[100/157]	0.0940 (0.0991)	0.0572 (0.0614)	1.006 (0.866)	65.62 (73.39)
[110/157]	0.0960 (0.0987)	0.0577 (0.0611)	0.689 (0.868)	87.50 (73.42)
[120/157]	0.0947 (0.0984)	0.0575 (0.0608)	0.968 (0.868)	71.88 (73.30)
[130/157]	0.0937 (0.0982)	0.0566 (0.0606)	0.742 (0.866)	75.00 (73.35)
[140/157]	0.0962 (0.0980)	0.0583 (0.0604)	0.778 (0.862)	75.00 (73.49)
[150/157]	0.1189 (0.0983)	0.0800 (0.0607)	0.947 (0.863)	68.75 (73.65)
[156/157]	0.0810 (0.0983)	0.0541 (0.0608)	0.980 (0.862)	75.00 (73.72)
 * Train Acc 73.720
 * Val Acc 72.900, Total time 0.58
 * Val loss 0.786, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0086 (0.0086)	0.662 (0.662)	81.25 (81.25)
[10/157]	0.0958 (0.0909)	0.0591 (0.0537)	1.068 (0.982)	59.38 (70.17)
[20/157]	0.0951 (0.0982)	0.0577 (0.0608)	1.085 (0.896)	62.50 (73.07)
[30/157]	0.0970 (0.0974)	0.0589 (0.0599)	0.952 (0.888)	65.62 (72.98)
[40/157]	0.0954 (0.0970)	0.0577 (0.0594)	0.696 (0.881)	84.38 (73.86)
[50/157]	0.0944 (0.0966)	0.0575 (0.0591)	0.759 (0.883)	71.88 (73.22)
[60/157]	0.0959 (0.0964)	0.0592 (0.0590)	0.503 (0.863)	90.62 (74.03)
[70/157]	0.0941 (0.0962)	0.0579 (0.0589)	0.699 (0.869)	84.38 (74.03)
[80/157]	0.0941 (0.0961)	0.0582 (0.0588)	0.714 (0.865)	81.25 (73.96)
[90/157]	0.0929 (0.0959)	0.0572 (0.0588)	0.850 (0.872)	68.75 (73.59)
[100/157]	0.0939 (0.0959)	0.0573 (0.0588)	0.995 (0.869)	68.75 (73.92)
[110/157]	0.0953 (0.0966)	0.0588 (0.0595)	0.856 (0.866)	75.00 (74.18)
[120/157]	0.0960 (0.0965)	0.0585 (0.0594)	0.835 (0.862)	81.25 (74.56)
[130/157]	0.0945 (0.0963)	0.0581 (0.0593)	1.000 (0.870)	68.75 (74.38)
[140/157]	0.0956 (0.0963)	0.0582 (0.0592)	0.991 (0.871)	62.50 (74.07)
[150/157]	0.0909 (0.0962)	0.0559 (0.0592)	0.900 (0.867)	62.50 (74.07)
[156/157]	0.0786 (0.0960)	0.0532 (0.0591)	1.233 (0.866)	50.00 (74.10)
 * Train Acc 74.100
 * Val Acc 72.600, Total time 0.57
 * Val loss 0.776, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0403 (0.0403)	0.0075 (0.0075)	0.928 (0.928)	75.00 (75.00)
[10/157]	0.1005 (0.0932)	0.0617 (0.0552)	0.825 (0.798)	71.88 (76.42)
[20/157]	0.0989 (0.0957)	0.0599 (0.0578)	0.831 (0.853)	71.88 (74.55)
[30/157]	0.0984 (0.0966)	0.0603 (0.0587)	0.739 (0.851)	78.12 (74.70)
[40/157]	0.0979 (0.0971)	0.0613 (0.0589)	0.951 (0.843)	65.62 (75.15)
[50/157]	0.0993 (0.0975)	0.0606 (0.0592)	1.185 (0.852)	56.25 (74.75)
[60/157]	0.0982 (0.0977)	0.0610 (0.0594)	0.864 (0.858)	81.25 (74.18)
[70/157]	0.0981 (0.0978)	0.0609 (0.0596)	1.173 (0.854)	65.62 (74.16)
[80/157]	0.0975 (0.0978)	0.0604 (0.0597)	0.993 (0.851)	71.88 (74.54)
[90/157]	0.0992 (0.0979)	0.0610 (0.0598)	0.778 (0.850)	71.88 (74.35)
[100/157]	0.0991 (0.0980)	0.0602 (0.0598)	1.402 (0.855)	59.38 (74.23)
[110/157]	0.0999 (0.0980)	0.0619 (0.0599)	0.847 (0.865)	68.75 (73.99)
[120/157]	0.0997 (0.0981)	0.0609 (0.0599)	0.928 (0.874)	68.75 (73.71)
[130/157]	0.0981 (0.0981)	0.0607 (0.0600)	0.772 (0.875)	81.25 (73.64)
[140/157]	0.0981 (0.0981)	0.0603 (0.0600)	1.267 (0.874)	62.50 (73.54)
[150/157]	0.0984 (0.0981)	0.0611 (0.0601)	0.826 (0.877)	68.75 (73.39)
[156/157]	0.0796 (0.0980)	0.0548 (0.0600)	0.557 (0.878)	87.50 (73.46)
 * Train Acc 73.460
 * Val Acc 73.300, Total time 0.58
 * Val loss 0.783, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0087 (0.0087)	0.884 (0.884)	75.00 (75.00)
[10/157]	0.0970 (0.0916)	0.0592 (0.0542)	0.728 (0.860)	81.25 (73.30)
[20/157]	0.0949 (0.0982)	0.0584 (0.0604)	0.873 (0.848)	81.25 (75.00)
[30/157]	0.0953 (0.0975)	0.0578 (0.0599)	0.985 (0.856)	65.62 (74.50)
[40/157]	0.0941 (0.0971)	0.0575 (0.0596)	0.890 (0.860)	75.00 (74.16)
[50/157]	0.0937 (0.0968)	0.0570 (0.0594)	0.870 (0.858)	78.12 (74.08)
[60/157]	0.0931 (0.0966)	0.0571 (0.0593)	0.813 (0.872)	75.00 (73.21)
[70/157]	0.0974 (0.0965)	0.0594 (0.0593)	0.865 (0.880)	81.25 (73.15)
[80/157]	0.0961 (0.0964)	0.0586 (0.0592)	0.916 (0.878)	56.25 (73.03)
[90/157]	0.1176 (0.0970)	0.0797 (0.0599)	0.797 (0.879)	78.12 (72.80)
[100/157]	0.0942 (0.0971)	0.0583 (0.0601)	0.749 (0.876)	71.88 (72.87)
[110/157]	0.0971 (0.0970)	0.0601 (0.0600)	0.714 (0.869)	81.25 (73.25)
[120/157]	0.0945 (0.0969)	0.0586 (0.0599)	0.860 (0.870)	65.62 (72.93)
[130/157]	0.0962 (0.0967)	0.0597 (0.0598)	0.935 (0.876)	65.62 (72.73)
[140/157]	0.0929 (0.0966)	0.0572 (0.0597)	0.909 (0.869)	68.75 (73.01)
[150/157]	0.1025 (0.0967)	0.0641 (0.0598)	0.630 (0.864)	81.25 (73.24)
[156/157]	0.0834 (0.0967)	0.0566 (0.0599)	1.065 (0.865)	62.50 (73.20)
 * Train Acc 73.200
 * Val Acc 73.100, Total time 0.60
 * Val loss 0.781, Total time 0.00
Classifier Optimizer is reset!
svd: True
svd: False
svd: False
reserving basis 6/27; cond: 428680.96875, radio:5.301838609739207e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0040,  0.1072, -0.1578],
          [-0.1378, -0.0744,  0.0496],
          [ 0.0050,  0.1571, -0.0147]],

         [[ 0.0549, -0.0551, -0.0377],
          [-0.1813, -0.1279, -0.0787],
          [ 0.0138,  0.0788,  0.1179]],

         [[-0.1299, -0.0832,  0.0676],
          [ 0.1594, -0.0410,  0.1448],
          [-0.0280,  0.0217,  0.1756]]],


        [[[-0.1681, -0.1048, -0.0262],
          [-0.0663,  0.1731, -0.1106],
          [-0.0793, -0.1348, -0.1715]],

         [[-0.1060,  0.1788,  0.1033],
          [ 0.0968,  0.0177, -0.0889],
          [ 0.0344, -0.1828, -0.1369]],

         [[-0.0980,  0.1280,  0.1212],
          [-0.0830, -0.0010,  0.1281],
          [ 0.1915,  0.0729,  0.0269]]],


        [[[ 0.1332, -0.1135,  0.0363],
          [-0.1476, -0.1350, -0.0988],
          [ 0.0869,  0.0755, -0.1129]],

         [[ 0.0592,  0.1009, -0.0278],
          [ 0.0059,  0.0401,  0.1155],
          [ 0.1801, -0.1532, -0.0734]],

         [[ 0.0774,  0.1568,  0.1639],
          [ 0.1684,  0.0345, -0.1718],
          [ 0.0119, -0.1275, -0.1838]]],


        ...,


        [[[ 0.0685, -0.0768, -0.1205],
          [ 0.0761,  0.0336, -0.1410],
          [-0.1324,  0.0948,  0.1443]],

         [[ 0.0183, -0.0977,  0.1004],
          [ 0.0361, -0.1679, -0.1244],
          [ 0.0341,  0.1231, -0.0630]],

         [[ 0.0803, -0.1300,  0.1818],
          [ 0.1276,  0.0266,  0.0759],
          [ 0.0456, -0.1186, -0.0127]]],


        [[[ 0.0627, -0.0407,  0.1924],
          [-0.0632, -0.1656, -0.0865],
          [-0.1705,  0.1686, -0.0842]],

         [[-0.1291, -0.0212, -0.1254],
          [-0.0925,  0.1034, -0.1427],
          [ 0.0977,  0.1511,  0.0008]],

         [[ 0.0848, -0.1123,  0.1496],
          [ 0.1869,  0.0453,  0.1723],
          [-0.1953, -0.0194,  0.0318]]],


        [[[-0.1667,  0.0943, -0.0725],
          [-0.0710, -0.1546,  0.0823],
          [-0.1407, -0.0455,  0.0850]],

         [[ 0.1450,  0.0468, -0.0711],
          [-0.0832,  0.1101,  0.0388],
          [ 0.1594,  0.0672,  0.1518]],

         [[-0.1559,  0.1529, -0.0939],
          [-0.0556, -0.0039, -0.0432],
          [ 0.0337,  0.1604, -0.1260]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([5.1214e+06, 4.5144e+05, 4.4624e+05, 2.0671e+05, 1.6605e+05, 1.4034e+05,
        2.6470e+04, 1.6964e+04, 5.6278e+03, 5.3149e+03, 5.2269e+03, 5.0274e+03,
        1.8667e+03, 1.1642e+03, 1.1566e+03, 9.5029e+02, 8.7932e+02, 5.4534e+02,
        2.3637e+02, 2.1178e+02, 1.8198e+02, 1.1870e+02, 1.0698e+02, 4.3589e+01,
        3.9391e+01, 2.9553e+01, 1.1947e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([27, 6]) 

NULL SPACE BASIS :  tensor([[-2.0409e-01,  2.0587e-01, -1.3668e-01, -1.1941e-01, -1.2260e-01,
          8.6434e-02],
        [ 3.8741e-01, -2.3564e-03,  2.3242e-01, -1.7037e-02,  2.2866e-01,
         -1.5475e-01],
        [-2.0526e-01, -2.0340e-01, -1.1942e-01,  1.3776e-01, -1.2126e-01,
          8.4972e-02],
        [-3.5264e-03, -3.8704e-01,  1.1392e-02,  2.3144e-01,  2.2755e-01,
         -1.5541e-01],
        [-4.8802e-03,  4.5751e-04,  8.4199e-03,  4.4612e-03, -4.2027e-01,
          2.7709e-01],
        [ 2.5777e-03,  3.8660e-01, -1.9886e-02, -2.3561e-01,  2.2395e-01,
         -1.5339e-01],
        [ 2.0876e-01,  2.0796e-01,  1.2355e-01, -1.3552e-01, -1.2405e-01,
          8.6660e-02],
        [-3.8409e-01,  2.3618e-03, -2.4083e-01,  1.2236e-02,  2.2856e-01,
         -1.5601e-01],
        [ 2.0294e-01, -2.1031e-01,  1.4109e-01,  1.2170e-01, -1.2161e-01,
          8.5794e-02],
        [ 1.1849e-02, -1.4880e-02,  2.5996e-01,  2.2732e-01,  7.0354e-03,
         -1.5409e-01],
        [-2.1883e-02, -5.4668e-04, -4.4057e-01,  3.0462e-02, -1.0573e-02,
          2.7442e-01],
        [ 1.0279e-02,  1.5515e-02,  2.2717e-01, -2.6013e-01,  4.6726e-03,
         -1.5011e-01],
        [-1.9540e-03,  2.4860e-02, -2.9117e-02, -4.3951e-01, -5.6993e-03,
          2.7514e-01],
        [ 4.8323e-03, -5.1409e-04, -9.2080e-04, -3.3327e-03,  3.5740e-03,
         -4.8892e-01],
        [-4.1816e-03, -2.4125e-02,  2.8682e-02,  4.4198e-01,  9.2545e-04,
          2.7035e-01],
        [-1.1711e-02, -8.6095e-03, -2.2747e-01,  2.5904e-01, -8.8293e-04,
         -1.5280e-01],
        [ 2.0149e-02,  1.3644e-05,  4.4146e-01, -2.7568e-02,  6.1661e-03,
          2.7464e-01],
        [-7.0907e-03,  8.0430e-03, -2.5918e-01, -2.2815e-01, -5.4610e-03,
         -1.5120e-01],
        [ 2.1978e-01, -2.1625e-01, -1.4599e-01, -1.2773e-01,  1.3150e-01,
          7.9931e-02],
        [-4.1714e-01,  3.2894e-03,  2.4679e-01, -1.6007e-02, -2.4769e-01,
         -1.4165e-01],
        [ 2.2334e-01,  2.1275e-01, -1.2801e-01,  1.4500e-01,  1.3284e-01,
          7.7203e-02],
        [ 5.3558e-04,  4.0789e-01,  2.0901e-02,  2.4611e-01, -2.5082e-01,
         -1.4165e-01],
        [ 9.8932e-03,  8.0228e-05, -8.8350e-03, -1.0758e-03,  4.7059e-01,
          2.5108e-01],
        [-4.1510e-03, -4.0832e-01, -1.0061e-02, -2.4442e-01, -2.5489e-01,
         -1.3871e-01],
        [-2.1966e-01, -2.2470e-01,  1.2311e-01, -1.4579e-01,  1.4114e-01,
          7.8226e-02],
        [ 4.0516e-01, -2.7795e-03, -2.3789e-01,  1.7864e-02, -2.6538e-01,
         -1.4055e-01],
        [-2.1849e-01,  2.2819e-01,  1.3978e-01,  1.2595e-01,  1.4403e-01,
          7.7506e-02]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0569, -0.0615,  0.0090, -0.0610,  0.0306,  0.0282,  0.0091,  0.0281,
         -0.0393, -0.0336,  0.0351, -0.0051,  0.0353, -0.0177, -0.0153, -0.0050,
         -0.0156,  0.0218, -0.0259,  0.0294, -0.0046,  0.0286, -0.0150, -0.0136,
         -0.0046, -0.0133,  0.0190],
        [-0.0615,  0.1146, -0.0612,  0.0303, -0.0567,  0.0304,  0.0284, -0.0525,
          0.0281,  0.0354, -0.0638,  0.0349, -0.0180,  0.0319, -0.0180, -0.0157,
          0.0285, -0.0151,  0.0292, -0.0566,  0.0295, -0.0145,  0.0288, -0.0145,
         -0.0136,  0.0255, -0.0138],
        [ 0.0090, -0.0612,  0.0566,  0.0282,  0.0306, -0.0610, -0.0393,  0.0277,
          0.0094, -0.0053,  0.0351, -0.0333, -0.0154, -0.0176,  0.0351,  0.0221,
         -0.0155, -0.0052, -0.0043,  0.0291, -0.0259, -0.0136, -0.0151,  0.0288,
          0.0186, -0.0129, -0.0048],
        [-0.0610,  0.0303,  0.0282,  0.1141, -0.0562, -0.0529, -0.0624,  0.0313,
          0.0284,  0.0355, -0.0174, -0.0160, -0.0636,  0.0311,  0.0286,  0.0344,
         -0.0174, -0.0149,  0.0282, -0.0143, -0.0134, -0.0554,  0.0276,  0.0265,
          0.0308, -0.0153, -0.0147],
        [ 0.0306, -0.0567,  0.0306, -0.0562,  0.1035, -0.0562,  0.0309, -0.0569,
          0.0308, -0.0174,  0.0314, -0.0175,  0.0312, -0.0559,  0.0313, -0.0174,
          0.0314, -0.0175, -0.0147,  0.0281, -0.0146,  0.0276, -0.0524,  0.0275,
         -0.0148,  0.0280, -0.0148],
        [ 0.0282,  0.0304, -0.0610, -0.0529, -0.0562,  0.1139,  0.0283,  0.0314,
         -0.0623, -0.0160, -0.0176,  0.0355,  0.0287,  0.0312, -0.0634, -0.0150,
         -0.0175,  0.0343, -0.0134, -0.0142,  0.0282,  0.0265,  0.0275, -0.0555,
         -0.0147, -0.0153,  0.0308],
        [ 0.0091,  0.0284, -0.0393, -0.0624,  0.0309,  0.0283,  0.0585, -0.0625,
          0.0090, -0.0055, -0.0156,  0.0225,  0.0348, -0.0170, -0.0159, -0.0329,
          0.0349, -0.0054, -0.0038, -0.0144,  0.0186,  0.0298, -0.0145, -0.0140,
         -0.0279,  0.0298, -0.0037],
        [ 0.0281, -0.0525,  0.0277,  0.0313, -0.0569,  0.0314, -0.0625,  0.1152,
         -0.0621, -0.0158,  0.0284, -0.0152, -0.0171,  0.0308, -0.0171,  0.0351,
         -0.0636,  0.0346, -0.0138,  0.0270, -0.0140, -0.0149,  0.0272, -0.0149,
          0.0296, -0.0559,  0.0299],
        [-0.0393,  0.0281,  0.0094,  0.0284,  0.0308, -0.0623,  0.0090, -0.0621,
          0.0581,  0.0228, -0.0155, -0.0058, -0.0159, -0.0171,  0.0348, -0.0058,
          0.0350, -0.0326,  0.0183, -0.0141, -0.0038, -0.0141, -0.0143,  0.0298,
         -0.0033,  0.0292, -0.0278],
        [-0.0336,  0.0354, -0.0053,  0.0355, -0.0174, -0.0160, -0.0055, -0.0158,
          0.0228,  0.0585, -0.0613,  0.0094, -0.0614,  0.0304,  0.0272,  0.0095,
          0.0271, -0.0393, -0.0296,  0.0309, -0.0048,  0.0308, -0.0154, -0.0133,
         -0.0047, -0.0135,  0.0196],
        [ 0.0351, -0.0638,  0.0351, -0.0174,  0.0314, -0.0176, -0.0156,  0.0284,
         -0.0155, -0.0613,  0.1106, -0.0610,  0.0306, -0.0547,  0.0307,  0.0271,
         -0.0492,  0.0269,  0.0311, -0.0557,  0.0309, -0.0156,  0.0276, -0.0155,
         -0.0138,  0.0248, -0.0136],
        [-0.0051,  0.0349, -0.0333, -0.0160, -0.0175,  0.0355,  0.0225, -0.0152,
         -0.0058,  0.0094, -0.0610,  0.0580,  0.0273,  0.0303, -0.0610, -0.0393,
          0.0271,  0.0095, -0.0051,  0.0311, -0.0295, -0.0134, -0.0152,  0.0304,
          0.0200, -0.0142, -0.0044],
        [ 0.0353, -0.0180, -0.0154, -0.0636,  0.0312,  0.0287,  0.0348, -0.0171,
         -0.0159, -0.0614,  0.0306,  0.0273,  0.1104, -0.0543, -0.0495, -0.0610,
          0.0305,  0.0271,  0.0310, -0.0150, -0.0142, -0.0556,  0.0274,  0.0248,
          0.0310, -0.0159, -0.0134],
        [-0.0177,  0.0319, -0.0176,  0.0311, -0.0559,  0.0312, -0.0170,  0.0308,
         -0.0171,  0.0304, -0.0547,  0.0303, -0.0543,  0.0976, -0.0546,  0.0302,
         -0.0549,  0.0306, -0.0151,  0.0270, -0.0150,  0.0275, -0.0494,  0.0277,
         -0.0156,  0.0285, -0.0160],
        [-0.0153, -0.0180,  0.0351,  0.0286,  0.0313, -0.0634, -0.0159, -0.0171,
          0.0348,  0.0272,  0.0307, -0.0610, -0.0495, -0.0546,  0.1102,  0.0273,
          0.0305, -0.0610, -0.0141, -0.0150,  0.0308,  0.0249,  0.0276, -0.0556,
         -0.0136, -0.0158,  0.0311],
        [-0.0050, -0.0157,  0.0221,  0.0344, -0.0174, -0.0150, -0.0329,  0.0351,
         -0.0058,  0.0095,  0.0271, -0.0393, -0.0610,  0.0302,  0.0273,  0.0581,
         -0.0611,  0.0094, -0.0053, -0.0137,  0.0205,  0.0316, -0.0152, -0.0147,
         -0.0299,  0.0309, -0.0043],
        [-0.0156,  0.0285, -0.0155, -0.0174,  0.0314, -0.0175,  0.0349, -0.0636,
          0.0350,  0.0271, -0.0492,  0.0271,  0.0305, -0.0549,  0.0305, -0.0611,
          0.1108, -0.0612, -0.0138,  0.0247, -0.0139, -0.0155,  0.0278, -0.0153,
          0.0311, -0.0562,  0.0310],
        [ 0.0218, -0.0151, -0.0052, -0.0149, -0.0175,  0.0343, -0.0054,  0.0346,
         -0.0326, -0.0393,  0.0269,  0.0095,  0.0271,  0.0306, -0.0610,  0.0094,
         -0.0612,  0.0581,  0.0208, -0.0141, -0.0050, -0.0145, -0.0155,  0.0316,
         -0.0047,  0.0316, -0.0302],
        [-0.0259,  0.0292, -0.0043,  0.0282, -0.0147, -0.0134, -0.0038, -0.0138,
          0.0183, -0.0296,  0.0311, -0.0051,  0.0310, -0.0151, -0.0141, -0.0053,
         -0.0138,  0.0208,  0.0638, -0.0695,  0.0110, -0.0681,  0.0349,  0.0308,
          0.0105,  0.0310, -0.0444],
        [ 0.0294, -0.0566,  0.0291, -0.0143,  0.0281, -0.0142, -0.0144,  0.0270,
         -0.0141,  0.0309, -0.0557,  0.0311, -0.0150,  0.0270, -0.0150, -0.0137,
          0.0247, -0.0141, -0.0695,  0.1292, -0.0695,  0.0345, -0.0647,  0.0345,
          0.0317, -0.0581,  0.0317],
        [-0.0046,  0.0295, -0.0259, -0.0134, -0.0146,  0.0282,  0.0186, -0.0140,
         -0.0038, -0.0048,  0.0309, -0.0295, -0.0142, -0.0150,  0.0308,  0.0205,
         -0.0139, -0.0050,  0.0110, -0.0695,  0.0638,  0.0309,  0.0347, -0.0680,
         -0.0445,  0.0314,  0.0103],
        [ 0.0286, -0.0145, -0.0136, -0.0554,  0.0276,  0.0265,  0.0298, -0.0149,
         -0.0141,  0.0308, -0.0156, -0.0134, -0.0556,  0.0275,  0.0249,  0.0316,
         -0.0155, -0.0145, -0.0681,  0.0345,  0.0309,  0.1267, -0.0629, -0.0585,
         -0.0700,  0.0347,  0.0326],
        [-0.0150,  0.0288, -0.0151,  0.0276, -0.0524,  0.0275, -0.0145,  0.0272,
         -0.0143, -0.0154,  0.0276, -0.0152,  0.0274, -0.0494,  0.0276, -0.0152,
          0.0278, -0.0155,  0.0349, -0.0647,  0.0347, -0.0629,  0.1162, -0.0631,
          0.0339, -0.0629,  0.0342],
        [-0.0136, -0.0145,  0.0288,  0.0265,  0.0275, -0.0555, -0.0140, -0.0149,
          0.0298, -0.0133, -0.0155,  0.0304,  0.0248,  0.0277, -0.0556, -0.0147,
         -0.0153,  0.0316,  0.0308,  0.0345, -0.0680, -0.0585, -0.0631,  0.1269,
          0.0328,  0.0345, -0.0702],
        [-0.0046, -0.0136,  0.0186,  0.0308, -0.0148, -0.0147, -0.0279,  0.0296,
         -0.0033, -0.0047, -0.0138,  0.0200,  0.0310, -0.0156, -0.0136, -0.0299,
          0.0311, -0.0047,  0.0105,  0.0317, -0.0445, -0.0700,  0.0339,  0.0328,
          0.0658, -0.0689,  0.0090],
        [-0.0133,  0.0255, -0.0129, -0.0153,  0.0280, -0.0153,  0.0298, -0.0559,
          0.0292, -0.0135,  0.0248, -0.0142, -0.0159,  0.0285, -0.0158,  0.0309,
         -0.0562,  0.0316,  0.0310, -0.0581,  0.0314,  0.0347, -0.0629,  0.0345,
         -0.0689,  0.1271, -0.0691],
        [ 0.0190, -0.0138, -0.0048, -0.0147, -0.0148,  0.0308, -0.0037,  0.0299,
         -0.0278,  0.0196, -0.0136, -0.0044, -0.0134, -0.0160,  0.0311, -0.0043,
          0.0310, -0.0302, -0.0444,  0.0317,  0.0103,  0.0326,  0.0342, -0.0702,
          0.0090, -0.0691,  0.0661]], device='cuda:0') 

reserving basis 78/576; cond: 9523107.0, radio:3.7480287573998794e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1478e-02, -3.9626e-02, -7.5703e-03],
          [ 3.8381e-02,  2.6427e-02,  3.1945e-02],
          [ 2.2120e-02, -1.7747e-03,  8.6815e-03]],

         [[-1.4068e-02,  3.7957e-02,  1.8420e-02],
          [-2.9847e-02,  3.6714e-02, -3.1757e-03],
          [-7.1222e-03, -1.6984e-02, -2.0396e-02]],

         [[-3.1195e-03, -2.0660e-02, -3.6698e-02],
          [-2.9240e-02, -1.0481e-02,  3.3366e-02],
          [ 2.0619e-02, -3.1909e-02, -9.5582e-03]],

         ...,

         [[-2.5550e-02,  1.1644e-02,  7.1144e-05],
          [ 1.4968e-02, -6.4900e-03, -9.5216e-03],
          [-2.0417e-03,  8.6283e-03, -3.1054e-02]],

         [[-4.1496e-02,  1.3749e-02, -3.4967e-02],
          [-6.6998e-03,  1.2047e-02,  1.1468e-03],
          [ 2.0794e-02,  1.1195e-04,  4.1035e-03]],

         [[-1.5939e-02,  1.8658e-03,  1.1380e-02],
          [ 2.3063e-02, -3.9621e-02, -1.6110e-02],
          [-4.1500e-02, -1.1368e-02,  2.9618e-02]]],


        [[[-4.2307e-02, -4.7051e-02,  1.3872e-02],
          [-2.3488e-02,  1.9855e-02,  2.9255e-03],
          [ 8.0582e-03,  2.3297e-02, -4.2262e-02]],

         [[ 3.2907e-02,  7.8576e-03, -3.0211e-02],
          [ 1.2032e-02, -1.1382e-02, -1.7408e-02],
          [ 3.2953e-03, -1.1635e-02,  4.0715e-02]],

         [[ 2.6486e-02, -1.6443e-02, -4.4862e-02],
          [-1.7295e-02, -1.0943e-02,  1.0135e-02],
          [-1.2324e-02, -2.3387e-03, -6.3972e-03]],

         ...,

         [[-5.4740e-03, -3.8261e-03,  8.7343e-03],
          [ 1.5775e-02, -3.4131e-02,  1.7199e-02],
          [ 3.3129e-02,  7.4752e-03,  3.0898e-02]],

         [[-2.1715e-02,  6.6974e-03, -4.1892e-04],
          [-1.1075e-02,  2.3859e-02, -4.6510e-02],
          [-1.1308e-03, -1.4410e-02, -2.0038e-02]],

         [[-1.4867e-02, -3.1992e-02,  4.2557e-03],
          [-6.7044e-03, -3.1422e-02, -8.2258e-03],
          [-8.7896e-03,  1.5866e-02, -4.1544e-02]]],


        [[[-3.0001e-03,  1.8535e-02,  2.0529e-02],
          [ 7.9520e-03,  1.2926e-02,  4.1855e-02],
          [-1.3447e-02,  2.5652e-02,  3.2361e-02]],

         [[-3.1171e-02,  2.0100e-02,  2.9023e-02],
          [-2.5257e-03,  3.2220e-02,  6.2395e-03],
          [ 3.5511e-02,  6.6425e-03, -2.0958e-02]],

         [[ 3.8707e-02,  1.5350e-02,  3.4328e-02],
          [ 1.8270e-02,  2.5583e-02,  2.0326e-02],
          [ 1.6806e-02, -5.8884e-03,  9.2947e-03]],

         ...,

         [[ 2.1010e-02, -1.6843e-02,  4.4107e-02],
          [-3.1192e-03,  3.8418e-02, -3.4598e-03],
          [ 2.4363e-02,  3.3221e-02, -3.6187e-02]],

         [[-2.0645e-02, -3.6394e-02,  3.6817e-02],
          [ 4.5179e-02, -1.3439e-02, -1.8949e-02],
          [ 1.2030e-02,  3.0934e-02,  1.3912e-02]],

         [[-1.4874e-02, -2.3223e-02,  3.0055e-02],
          [-2.9699e-02, -4.1886e-02, -2.1310e-02],
          [ 3.5903e-03,  6.0724e-03,  6.1259e-03]]],


        ...,


        [[[-4.4238e-02, -1.8334e-02,  9.5084e-03],
          [ 2.4280e-02, -1.7121e-02, -8.6920e-03],
          [-4.2907e-03,  2.1055e-02, -3.4667e-02]],

         [[-4.1755e-02,  2.7582e-02,  5.9301e-03],
          [-1.3567e-02, -1.1761e-02,  8.6549e-03],
          [-1.9317e-02,  2.8316e-02,  1.2304e-02]],

         [[ 2.3941e-02,  3.2055e-02,  1.0420e-03],
          [ 4.7889e-02, -7.7874e-03, -3.4090e-02],
          [ 3.8565e-02, -1.4970e-02, -2.8041e-03]],

         ...,

         [[-2.5756e-02, -5.3316e-02,  2.3097e-02],
          [-2.0456e-02,  2.2694e-02, -4.7259e-02],
          [ 1.6362e-02,  4.0694e-02,  2.7395e-02]],

         [[-4.6433e-02, -5.5604e-02,  1.6270e-02],
          [-1.3051e-02, -1.4817e-02, -6.5085e-03],
          [-2.0938e-02,  9.7830e-03, -9.2964e-03]],

         [[ 3.6260e-02, -2.2398e-02,  4.1227e-02],
          [ 9.6355e-03, -5.8134e-03,  4.7873e-02],
          [ 3.1162e-02,  2.8020e-02,  4.8039e-02]]],


        [[[ 2.5495e-02,  1.3930e-02, -1.3702e-02],
          [-1.3340e-02, -3.1993e-02, -2.0704e-02],
          [ 1.5734e-02, -5.8626e-02, -4.1863e-02]],

         [[-8.2105e-03,  2.0923e-02,  8.7461e-03],
          [ 1.0004e-02,  9.2930e-04, -4.0481e-02],
          [-9.4206e-03,  3.7816e-02,  1.2336e-02]],

         [[-9.4382e-03,  8.3890e-03,  4.0546e-02],
          [ 5.2304e-06,  5.6404e-03,  3.0430e-02],
          [-3.0961e-02,  2.5610e-02,  3.2366e-02]],

         ...,

         [[ 4.0165e-02,  1.9614e-02,  3.8792e-02],
          [-3.5678e-02,  9.9644e-03,  6.9449e-04],
          [-1.2962e-03, -4.1758e-02, -3.9106e-02]],

         [[ 1.6774e-02,  2.3290e-02, -9.1765e-03],
          [ 2.6007e-02,  8.2222e-03,  1.8144e-02],
          [-4.8408e-02, -2.4962e-02, -5.2256e-03]],

         [[ 2.7814e-02,  2.5223e-02, -3.1936e-02],
          [-3.3072e-02, -2.8829e-02, -1.2044e-02],
          [-8.0394e-03, -3.9882e-02,  2.2332e-02]]],


        [[[ 2.7630e-02,  1.0355e-02, -1.7005e-02],
          [-6.2120e-03,  6.2805e-03, -1.2300e-02],
          [-1.6420e-03, -3.5108e-02, -7.8948e-03]],

         [[-4.3892e-02, -2.8520e-02, -2.9446e-02],
          [ 1.0975e-02, -5.6520e-02, -1.3570e-03],
          [ 2.6672e-02, -3.5981e-02, -6.7813e-03]],

         [[-1.7001e-02,  1.3473e-02,  2.2516e-02],
          [ 1.5426e-02, -1.3409e-02,  1.0054e-02],
          [-2.2153e-02,  3.6047e-02,  3.8246e-03]],

         ...,

         [[-2.1336e-02, -9.1136e-03,  9.9979e-03],
          [ 4.0090e-03,  3.7311e-02, -4.2844e-02],
          [-2.2989e-02,  1.5768e-02,  2.4683e-02]],

         [[-3.1381e-02, -3.8665e-02,  5.5467e-03],
          [-1.9645e-02, -1.0614e-02, -4.8776e-02],
          [-2.8307e-02, -5.1280e-02, -3.9125e-02]],

         [[-4.0026e-03, -1.0512e-02,  3.1195e-02],
          [ 3.3807e-02,  4.7354e-03,  3.5742e-02],
          [-2.8731e-02,  4.5811e-02, -3.2644e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.0159e+08, 4.5290e+06, 3.6131e+06, 3.4051e+06, 2.5864e+06, 1.8445e+06,
        1.7606e+06, 1.6353e+06, 1.3953e+06, 1.1543e+06, 9.6498e+05, 8.5033e+05,
        7.1551e+05, 5.0106e+05, 3.4755e+05, 2.7055e+05, 2.6222e+05, 2.3208e+05,
        1.9142e+05, 1.8220e+05, 1.6147e+05, 1.4167e+05, 1.1961e+05, 1.0581e+05,
        9.7916e+04, 9.4432e+04, 8.9210e+04, 8.5146e+04, 8.1546e+04, 7.6958e+04,
        7.1771e+04, 6.4412e+04, 6.1177e+04, 5.7233e+04, 5.4180e+04, 5.1577e+04,
        4.8333e+04, 4.6427e+04, 4.2048e+04, 3.7635e+04, 3.7392e+04, 3.4410e+04,
        3.3983e+04, 3.2723e+04, 3.1096e+04, 3.0036e+04, 2.6693e+04, 2.6291e+04,
        2.4700e+04, 2.4250e+04, 2.3090e+04, 2.2451e+04, 2.1666e+04, 2.1369e+04,
        2.0487e+04, 1.8698e+04, 1.7939e+04, 1.7514e+04, 1.7023e+04, 1.6658e+04,
        1.6273e+04, 1.5486e+04, 1.5337e+04, 1.4759e+04, 1.4541e+04, 1.4353e+04,
        1.3826e+04, 1.3651e+04, 1.3588e+04, 1.2878e+04, 1.2503e+04, 1.2195e+04,
        1.1739e+04, 1.1708e+04, 1.1488e+04, 1.0875e+04, 1.0552e+04, 1.0283e+04,
        1.0207e+04, 9.8454e+03, 9.5871e+03, 9.2730e+03, 9.2463e+03, 9.1010e+03,
        8.8044e+03, 8.6911e+03, 8.2152e+03, 7.9483e+03, 7.8828e+03, 7.8473e+03,
        7.6218e+03, 7.4792e+03, 7.2571e+03, 6.9617e+03, 6.7915e+03, 6.4811e+03,
        6.4579e+03, 6.1874e+03, 6.0868e+03, 6.0144e+03, 5.8116e+03, 5.5958e+03,
        5.5213e+03, 5.3169e+03, 5.2928e+03, 5.1654e+03, 5.0726e+03, 5.0255e+03,
        4.9301e+03, 4.8260e+03, 4.7617e+03, 4.6794e+03, 4.4792e+03, 4.4227e+03,
        4.3903e+03, 4.3025e+03, 4.2453e+03, 4.1298e+03, 4.0726e+03, 4.0238e+03,
        3.9513e+03, 3.9252e+03, 3.8229e+03, 3.7546e+03, 3.6832e+03, 3.6261e+03,
        3.5931e+03, 3.5473e+03, 3.4621e+03, 3.3507e+03, 3.3418e+03, 3.3105e+03,
        3.2406e+03, 3.1881e+03, 3.1519e+03, 3.0872e+03, 3.0212e+03, 3.0015e+03,
        2.9685e+03, 2.9116e+03, 2.8372e+03, 2.8133e+03, 2.7800e+03, 2.7555e+03,
        2.7328e+03, 2.6850e+03, 2.6507e+03, 2.6057e+03, 2.5511e+03, 2.4979e+03,
        2.4744e+03, 2.4523e+03, 2.4472e+03, 2.4164e+03, 2.3733e+03, 2.3467e+03,
        2.3255e+03, 2.2937e+03, 2.2274e+03, 2.2091e+03, 2.1899e+03, 2.1604e+03,
        2.1262e+03, 2.1092e+03, 2.0548e+03, 2.0052e+03, 1.9880e+03, 1.9620e+03,
        1.9540e+03, 1.9185e+03, 1.9061e+03, 1.8896e+03, 1.8419e+03, 1.8072e+03,
        1.7818e+03, 1.7683e+03, 1.7598e+03, 1.7428e+03, 1.7196e+03, 1.7108e+03,
        1.6932e+03, 1.6482e+03, 1.6307e+03, 1.5999e+03, 1.5713e+03, 1.5403e+03,
        1.5332e+03, 1.5212e+03, 1.5001e+03, 1.4946e+03, 1.4777e+03, 1.4614e+03,
        1.4346e+03, 1.4280e+03, 1.4175e+03, 1.4026e+03, 1.3740e+03, 1.3652e+03,
        1.3515e+03, 1.3454e+03, 1.3245e+03, 1.3021e+03, 1.2983e+03, 1.2848e+03,
        1.2717e+03, 1.2540e+03, 1.2390e+03, 1.2316e+03, 1.2199e+03, 1.1949e+03,
        1.1902e+03, 1.1726e+03, 1.1616e+03, 1.1451e+03, 1.1407e+03, 1.1269e+03,
        1.1149e+03, 1.1122e+03, 1.0905e+03, 1.0739e+03, 1.0640e+03, 1.0491e+03,
        1.0315e+03, 1.0202e+03, 1.0197e+03, 1.0077e+03, 9.9369e+02, 9.8822e+02,
        9.7915e+02, 9.6486e+02, 9.6133e+02, 9.5272e+02, 9.3721e+02, 9.2830e+02,
        9.1825e+02, 9.0605e+02, 8.9557e+02, 8.8471e+02, 8.7461e+02, 8.7327e+02,
        8.6571e+02, 8.5101e+02, 8.4477e+02, 8.3222e+02, 8.2757e+02, 8.2347e+02,
        8.0814e+02, 8.0544e+02, 7.9574e+02, 7.9064e+02, 7.8645e+02, 7.6446e+02,
        7.6091e+02, 7.5644e+02, 7.5388e+02, 7.4702e+02, 7.4264e+02, 7.3971e+02,
        7.3063e+02, 7.1883e+02, 7.1517e+02, 7.0692e+02, 7.0119e+02, 6.9710e+02,
        6.9486e+02, 6.8071e+02, 6.7482e+02, 6.6875e+02, 6.5576e+02, 6.5056e+02,
        6.4936e+02, 6.4548e+02, 6.4114e+02, 6.3156e+02, 6.3072e+02, 6.2233e+02,
        6.2166e+02, 6.1421e+02, 6.1004e+02, 6.0395e+02, 5.9667e+02, 5.8813e+02,
        5.8670e+02, 5.8050e+02, 5.7452e+02, 5.6958e+02, 5.6771e+02, 5.6312e+02,
        5.5742e+02, 5.5386e+02, 5.4851e+02, 5.4365e+02, 5.4112e+02, 5.3542e+02,
        5.2993e+02, 5.2721e+02, 5.2374e+02, 5.1458e+02, 5.1322e+02, 5.0893e+02,
        5.0481e+02, 4.9922e+02, 4.9278e+02, 4.9039e+02, 4.8160e+02, 4.7856e+02,
        4.7591e+02, 4.7287e+02, 4.6987e+02, 4.6801e+02, 4.6432e+02, 4.6175e+02,
        4.5562e+02, 4.5390e+02, 4.4990e+02, 4.4497e+02, 4.4153e+02, 4.3708e+02,
        4.3215e+02, 4.2881e+02, 4.2459e+02, 4.2173e+02, 4.1784e+02, 4.1483e+02,
        4.1236e+02, 4.0679e+02, 4.0563e+02, 4.0294e+02, 4.0089e+02, 3.9714e+02,
        3.9189e+02, 3.9069e+02, 3.8889e+02, 3.8155e+02, 3.8080e+02, 3.7784e+02,
        3.7391e+02, 3.7193e+02, 3.6728e+02, 3.6547e+02, 3.6127e+02, 3.6004e+02,
        3.5784e+02, 3.5281e+02, 3.5040e+02, 3.4905e+02, 3.4797e+02, 3.4411e+02,
        3.4337e+02, 3.4018e+02, 3.3552e+02, 3.3204e+02, 3.3024e+02, 3.2892e+02,
        3.2662e+02, 3.2379e+02, 3.2137e+02, 3.1969e+02, 3.1721e+02, 3.1601e+02,
        3.1314e+02, 3.1093e+02, 3.0714e+02, 3.0423e+02, 3.0250e+02, 2.9855e+02,
        2.9721e+02, 2.9635e+02, 2.9345e+02, 2.9283e+02, 2.9173e+02, 2.8901e+02,
        2.8552e+02, 2.8544e+02, 2.8259e+02, 2.7802e+02, 2.7652e+02, 2.7464e+02,
        2.7155e+02, 2.7082e+02, 2.6958e+02, 2.6715e+02, 2.6500e+02, 2.6261e+02,
        2.6094e+02, 2.6024e+02, 2.5828e+02, 2.5781e+02, 2.5535e+02, 2.5267e+02,
        2.5119e+02, 2.4875e+02, 2.4797e+02, 2.4577e+02, 2.4473e+02, 2.4343e+02,
        2.4152e+02, 2.3893e+02, 2.3799e+02, 2.3487e+02, 2.3299e+02, 2.3124e+02,
        2.2894e+02, 2.2776e+02, 2.2596e+02, 2.2546e+02, 2.2394e+02, 2.2132e+02,
        2.2048e+02, 2.1929e+02, 2.1829e+02, 2.1675e+02, 2.1489e+02, 2.1351e+02,
        2.1308e+02, 2.0987e+02, 2.0903e+02, 2.0701e+02, 2.0485e+02, 2.0401e+02,
        2.0348e+02, 2.0296e+02, 2.0178e+02, 2.0034e+02, 1.9769e+02, 1.9701e+02,
        1.9518e+02, 1.9247e+02, 1.9045e+02, 1.8951e+02, 1.8869e+02, 1.8575e+02,
        1.8431e+02, 1.8280e+02, 1.8268e+02, 1.8124e+02, 1.7977e+02, 1.7779e+02,
        1.7630e+02, 1.7510e+02, 1.7424e+02, 1.7307e+02, 1.7282e+02, 1.7073e+02,
        1.6886e+02, 1.6827e+02, 1.6759e+02, 1.6486e+02, 1.6321e+02, 1.6194e+02,
        1.6069e+02, 1.5884e+02, 1.5839e+02, 1.5650e+02, 1.5542e+02, 1.5399e+02,
        1.5294e+02, 1.5132e+02, 1.4968e+02, 1.4857e+02, 1.4746e+02, 1.4639e+02,
        1.4499e+02, 1.4373e+02, 1.4268e+02, 1.4137e+02, 1.4074e+02, 1.3867e+02,
        1.3825e+02, 1.3754e+02, 1.3675e+02, 1.3405e+02, 1.3335e+02, 1.3282e+02,
        1.3128e+02, 1.2986e+02, 1.2908e+02, 1.2788e+02, 1.2684e+02, 1.2495e+02,
        1.2391e+02, 1.2210e+02, 1.2188e+02, 1.2092e+02, 1.1880e+02, 1.1820e+02,
        1.1730e+02, 1.1662e+02, 1.1542e+02, 1.1381e+02, 1.1283e+02, 1.1228e+02,
        1.1067e+02, 1.1002e+02, 1.0946e+02, 1.0919e+02, 1.0750e+02, 1.0684e+02,
        1.0501e+02, 1.0424e+02, 1.0369e+02, 1.0182e+02, 1.0087e+02, 1.0036e+02,
        9.9356e+01, 9.8970e+01, 9.7222e+01, 9.6027e+01, 9.5401e+01, 9.3926e+01,
        9.3644e+01, 9.2897e+01, 9.2200e+01, 9.0673e+01, 8.9525e+01, 8.7660e+01,
        8.7386e+01, 8.5912e+01, 8.5437e+01, 8.3849e+01, 8.3443e+01, 8.1702e+01,
        8.1190e+01, 7.9302e+01, 7.8918e+01, 7.8807e+01, 7.7783e+01, 7.6199e+01,
        7.4180e+01, 7.3579e+01, 7.3174e+01, 7.0207e+01, 6.9612e+01, 6.8930e+01,
        6.7088e+01, 6.6487e+01, 6.5111e+01, 6.4904e+01, 6.2668e+01, 6.1948e+01,
        6.0514e+01, 6.0088e+01, 5.9687e+01, 5.7255e+01, 5.6654e+01, 5.6159e+01,
        5.4951e+01, 5.3977e+01, 5.2399e+01, 5.0471e+01, 4.8655e+01, 4.7775e+01,
        4.6252e+01, 4.4183e+01, 4.2837e+01, 4.1399e+01, 3.9538e+01, 3.7952e+01,
        3.6557e+01, 3.5198e+01, 3.4536e+01, 3.3416e+01, 3.2275e+01, 3.1028e+01,
        3.0604e+01, 2.9917e+01, 2.7480e+01, 2.6756e+01, 2.4145e+01, 2.2304e+01,
        2.1386e+01, 1.7080e+01, 1.5305e+01, 1.4571e+01, 1.2813e+01, 1.0668e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 78]) 

NULL SPACE BASIS :  tensor([[-5.2564e-02,  6.2679e-02, -3.9055e-03,  ...,  1.3816e-02,
         -3.7943e-03, -6.0702e-03],
        [ 5.5368e-03, -6.0951e-03,  5.4986e-04,  ..., -5.2544e-03,
          3.2098e-03,  5.6284e-03],
        [ 1.7566e-02, -5.6925e-02,  2.7906e-02,  ..., -3.7739e-03,
         -3.0950e-03, -1.7444e-03],
        ...,
        [ 6.3694e-03, -1.1408e-02, -3.7727e-02,  ...,  3.5986e-03,
          2.4537e-05,  2.3984e-03],
        [ 2.3120e-02, -5.5281e-02,  1.7580e-02,  ..., -4.9637e-04,
          3.8793e-06, -1.3074e-03],
        [-2.0051e-02,  2.9989e-02, -1.4452e-02,  ..., -3.8259e-03,
          2.8865e-05, -2.5004e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0065, -0.0062,  0.0006,  ..., -0.0006,  0.0005, -0.0002],
        [-0.0062,  0.0132, -0.0076,  ...,  0.0014, -0.0004, -0.0004],
        [ 0.0006, -0.0076,  0.0083,  ..., -0.0011,  0.0004,  0.0005],
        ...,
        [-0.0006,  0.0014, -0.0011,  ...,  0.0022, -0.0015, -0.0001],
        [ 0.0005, -0.0004,  0.0004,  ..., -0.0015,  0.0032, -0.0013],
        [-0.0002, -0.0004,  0.0005,  ..., -0.0001, -0.0013,  0.0016]],
       device='cuda:0') 

reserving basis 134/576; cond: 1362203.875, radio:0.00044516311027109623
PARAMETER       :  Parameter containing:
tensor([[[[-0.0136,  0.0040,  0.0143],
          [ 0.0240,  0.0071,  0.0076],
          [-0.0527, -0.0492, -0.0111]],

         [[ 0.0033, -0.0117, -0.0064],
          [ 0.0268, -0.0289,  0.0260],
          [-0.0624, -0.0079, -0.0567]],

         [[-0.0002,  0.0194,  0.0255],
          [-0.0141, -0.0341,  0.0466],
          [-0.0168, -0.0082,  0.0516]],

         ...,

         [[-0.0396,  0.0275,  0.0214],
          [-0.0113,  0.0077, -0.0068],
          [-0.0349,  0.0124,  0.0238]],

         [[-0.0176,  0.0124,  0.0018],
          [-0.0331,  0.0194, -0.0042],
          [ 0.0176, -0.0394,  0.0345]],

         [[ 0.0211, -0.0005,  0.0241],
          [ 0.0100,  0.0106,  0.0397],
          [-0.0290,  0.0205, -0.0056]]],


        [[[ 0.0325,  0.0108, -0.0223],
          [-0.0080, -0.0005,  0.0103],
          [ 0.0039, -0.0470,  0.0093]],

         [[ 0.0346,  0.0246,  0.0222],
          [-0.0205, -0.0089, -0.0238],
          [ 0.0084, -0.0294,  0.0153]],

         [[-0.0306, -0.0146, -0.0278],
          [-0.0042,  0.0135,  0.0102],
          [ 0.0352,  0.0012, -0.0430]],

         ...,

         [[ 0.0171,  0.0325,  0.0062],
          [ 0.0362,  0.0391, -0.0080],
          [-0.0084, -0.0294,  0.0360]],

         [[-0.0043,  0.0125, -0.0185],
          [-0.0055, -0.0212,  0.0154],
          [-0.0400, -0.0238, -0.0339]],

         [[ 0.0344, -0.0174,  0.0089],
          [ 0.0106,  0.0157,  0.0047],
          [ 0.0218, -0.0134,  0.0240]]],


        [[[ 0.0309, -0.0052, -0.0459],
          [ 0.0311,  0.0377,  0.0303],
          [-0.0149, -0.0067,  0.0386]],

         [[ 0.0138,  0.0036,  0.0093],
          [ 0.0194, -0.0290, -0.0030],
          [ 0.0297,  0.0281, -0.0157]],

         [[ 0.0420,  0.0021,  0.0094],
          [ 0.0141, -0.0185,  0.0390],
          [-0.0338, -0.0382,  0.0081]],

         ...,

         [[-0.0005,  0.0308, -0.0126],
          [-0.0161, -0.0448,  0.0062],
          [-0.0102, -0.0253, -0.0253]],

         [[-0.0085,  0.0245, -0.0455],
          [ 0.0323,  0.0237, -0.0226],
          [-0.0271, -0.0238,  0.0110]],

         [[ 0.0375, -0.0032, -0.0082],
          [-0.0254, -0.0258,  0.0087],
          [ 0.0028,  0.0421,  0.0339]]],


        ...,


        [[[ 0.0374, -0.0313, -0.0345],
          [ 0.0363, -0.0352, -0.0470],
          [-0.0145, -0.0223,  0.0142]],

         [[ 0.0381,  0.0170,  0.0280],
          [-0.0417, -0.0429,  0.0035],
          [-0.0473,  0.0241, -0.0389]],

         [[ 0.0123,  0.0451,  0.0053],
          [-0.0446, -0.0422,  0.0209],
          [-0.0213,  0.0177,  0.0277]],

         ...,

         [[-0.0251, -0.0243,  0.0186],
          [ 0.0024,  0.0294,  0.0366],
          [ 0.0344, -0.0484,  0.0116]],

         [[ 0.0011, -0.0232,  0.0332],
          [-0.0387, -0.0057, -0.0025],
          [-0.0085, -0.0444,  0.0067]],

         [[-0.0041, -0.0207, -0.0072],
          [ 0.0309,  0.0491, -0.0028],
          [-0.0025,  0.0312,  0.0079]]],


        [[[-0.0001, -0.0415,  0.0146],
          [-0.0410,  0.0207,  0.0335],
          [ 0.0134,  0.0277, -0.0116]],

         [[ 0.0129, -0.0205, -0.0191],
          [ 0.0074, -0.0448,  0.0017],
          [-0.0217, -0.0205, -0.0052]],

         [[ 0.0262,  0.0251,  0.0152],
          [-0.0043,  0.0059, -0.0202],
          [ 0.0330, -0.0349, -0.0012]],

         ...,

         [[-0.0216,  0.0427,  0.0052],
          [ 0.0252,  0.0236, -0.0309],
          [-0.0042, -0.0389,  0.0144]],

         [[-0.0240, -0.0140, -0.0354],
          [-0.0034,  0.0210,  0.0029],
          [ 0.0101, -0.0320,  0.0333]],

         [[ 0.0118,  0.0423,  0.0106],
          [-0.0019,  0.0231,  0.0177],
          [-0.0139,  0.0051,  0.0233]]],


        [[[ 0.0090, -0.0093, -0.0080],
          [-0.0163,  0.0162, -0.0075],
          [-0.0396, -0.0432,  0.0428]],

         [[-0.0250,  0.0045, -0.0184],
          [-0.0320, -0.0285,  0.0007],
          [-0.0332,  0.0122,  0.0106]],

         [[ 0.0006,  0.0139, -0.0149],
          [ 0.0063, -0.0372, -0.0080],
          [-0.0229,  0.0276, -0.0476]],

         ...,

         [[ 0.0401, -0.0361, -0.0337],
          [-0.0108,  0.0119,  0.0401],
          [-0.0143,  0.0304, -0.0368]],

         [[-0.0123, -0.0312, -0.0118],
          [ 0.0170,  0.0220, -0.0297],
          [ 0.0300, -0.0310,  0.0296]],

         [[-0.0027, -0.0318, -0.0281],
          [-0.0211,  0.0320,  0.0006],
          [-0.0148,  0.0395,  0.0058]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([9.0614e+07, 3.4619e+06, 2.9044e+06, 2.7454e+06, 2.6334e+06, 1.5750e+06,
        1.3477e+06, 1.2231e+06, 1.1307e+06, 7.8166e+05, 7.6985e+05, 7.1260e+05,
        6.2600e+05, 5.8203e+05, 4.9377e+05, 3.6007e+05, 3.2209e+05, 3.0308e+05,
        2.7678e+05, 2.4564e+05, 2.3030e+05, 1.8463e+05, 1.7138e+05, 1.6157e+05,
        1.5746e+05, 1.3653e+05, 1.2888e+05, 1.2322e+05, 1.1790e+05, 1.0503e+05,
        9.9403e+04, 9.4306e+04, 8.7827e+04, 8.7472e+04, 7.6624e+04, 7.2713e+04,
        7.1212e+04, 6.7655e+04, 5.8780e+04, 5.6427e+04, 5.4145e+04, 4.8846e+04,
        4.7512e+04, 4.6187e+04, 4.3949e+04, 4.1804e+04, 4.0738e+04, 3.8308e+04,
        3.6795e+04, 3.5357e+04, 3.4460e+04, 3.3699e+04, 3.3055e+04, 3.2450e+04,
        3.1823e+04, 3.0127e+04, 2.8830e+04, 2.8339e+04, 2.8178e+04, 2.7090e+04,
        2.6126e+04, 2.5040e+04, 2.4078e+04, 2.3969e+04, 2.3641e+04, 2.2374e+04,
        2.1903e+04, 2.1510e+04, 2.0771e+04, 2.0255e+04, 2.0060e+04, 1.9261e+04,
        1.9201e+04, 1.8851e+04, 1.8697e+04, 1.7955e+04, 1.7675e+04, 1.7101e+04,
        1.6853e+04, 1.6690e+04, 1.6204e+04, 1.6044e+04, 1.5867e+04, 1.5263e+04,
        1.5027e+04, 1.4890e+04, 1.4671e+04, 1.4410e+04, 1.4086e+04, 1.4015e+04,
        1.3594e+04, 1.3452e+04, 1.3290e+04, 1.2923e+04, 1.2719e+04, 1.2411e+04,
        1.2371e+04, 1.2067e+04, 1.1914e+04, 1.1795e+04, 1.1587e+04, 1.1435e+04,
        1.1235e+04, 1.1047e+04, 1.0892e+04, 1.0807e+04, 1.0496e+04, 1.0452e+04,
        1.0200e+04, 1.0083e+04, 9.9545e+03, 9.9375e+03, 9.6464e+03, 9.4965e+03,
        9.3700e+03, 9.1712e+03, 9.0664e+03, 8.9496e+03, 8.8594e+03, 8.6573e+03,
        8.5999e+03, 8.4752e+03, 8.4276e+03, 8.3268e+03, 8.1986e+03, 8.1390e+03,
        8.0660e+03, 7.8870e+03, 7.7214e+03, 7.6320e+03, 7.6036e+03, 7.5302e+03,
        7.4920e+03, 7.4048e+03, 7.3130e+03, 7.2654e+03, 7.0102e+03, 6.9829e+03,
        6.8632e+03, 6.8047e+03, 6.6471e+03, 6.6103e+03, 6.4575e+03, 6.4143e+03,
        6.2641e+03, 6.2475e+03, 6.2102e+03, 6.1506e+03, 6.0413e+03, 6.0231e+03,
        5.9498e+03, 5.8685e+03, 5.8265e+03, 5.6854e+03, 5.6492e+03, 5.5874e+03,
        5.5547e+03, 5.4717e+03, 5.4188e+03, 5.2973e+03, 5.2753e+03, 5.2319e+03,
        5.2117e+03, 5.1688e+03, 5.0489e+03, 5.0353e+03, 4.9788e+03, 4.9297e+03,
        4.8836e+03, 4.8398e+03, 4.8212e+03, 4.7538e+03, 4.7172e+03, 4.7133e+03,
        4.6861e+03, 4.5972e+03, 4.5750e+03, 4.5085e+03, 4.4275e+03, 4.3896e+03,
        4.3414e+03, 4.3245e+03, 4.2656e+03, 4.2148e+03, 4.1809e+03, 4.1633e+03,
        4.1044e+03, 4.0669e+03, 4.0372e+03, 4.0214e+03, 3.9819e+03, 3.9337e+03,
        3.8935e+03, 3.8582e+03, 3.8333e+03, 3.7892e+03, 3.7505e+03, 3.7121e+03,
        3.6957e+03, 3.6511e+03, 3.6282e+03, 3.5994e+03, 3.5751e+03, 3.5350e+03,
        3.4980e+03, 3.4663e+03, 3.4202e+03, 3.3962e+03, 3.3594e+03, 3.3580e+03,
        3.3238e+03, 3.3045e+03, 3.2864e+03, 3.2488e+03, 3.2142e+03, 3.1734e+03,
        3.1564e+03, 3.1242e+03, 3.0941e+03, 3.0845e+03, 3.0267e+03, 2.9917e+03,
        2.9793e+03, 2.9616e+03, 2.9605e+03, 2.9396e+03, 2.9066e+03, 2.8652e+03,
        2.8444e+03, 2.7991e+03, 2.7917e+03, 2.7711e+03, 2.7568e+03, 2.7363e+03,
        2.7011e+03, 2.6657e+03, 2.6588e+03, 2.6413e+03, 2.6269e+03, 2.6192e+03,
        2.5829e+03, 2.5710e+03, 2.5449e+03, 2.5236e+03, 2.5142e+03, 2.5041e+03,
        2.4801e+03, 2.4682e+03, 2.4414e+03, 2.3963e+03, 2.3783e+03, 2.3723e+03,
        2.3526e+03, 2.3469e+03, 2.3144e+03, 2.3013e+03, 2.2781e+03, 2.2731e+03,
        2.2526e+03, 2.2479e+03, 2.2300e+03, 2.2190e+03, 2.1840e+03, 2.1772e+03,
        2.1640e+03, 2.1443e+03, 2.1272e+03, 2.1099e+03, 2.0925e+03, 2.0684e+03,
        2.0618e+03, 2.0426e+03, 2.0307e+03, 2.0045e+03, 1.9897e+03, 1.9864e+03,
        1.9689e+03, 1.9521e+03, 1.9458e+03, 1.9353e+03, 1.9155e+03, 1.9015e+03,
        1.8932e+03, 1.8684e+03, 1.8647e+03, 1.8577e+03, 1.8444e+03, 1.8314e+03,
        1.8162e+03, 1.7965e+03, 1.7852e+03, 1.7668e+03, 1.7523e+03, 1.7425e+03,
        1.7358e+03, 1.7196e+03, 1.7132e+03, 1.6933e+03, 1.6804e+03, 1.6787e+03,
        1.6696e+03, 1.6541e+03, 1.6491e+03, 1.6277e+03, 1.6134e+03, 1.6076e+03,
        1.6029e+03, 1.5952e+03, 1.5866e+03, 1.5723e+03, 1.5602e+03, 1.5445e+03,
        1.5435e+03, 1.5375e+03, 1.5269e+03, 1.5205e+03, 1.4921e+03, 1.4782e+03,
        1.4720e+03, 1.4697e+03, 1.4537e+03, 1.4494e+03, 1.4428e+03, 1.4308e+03,
        1.4277e+03, 1.4136e+03, 1.4015e+03, 1.3992e+03, 1.3848e+03, 1.3809e+03,
        1.3761e+03, 1.3702e+03, 1.3592e+03, 1.3507e+03, 1.3378e+03, 1.3181e+03,
        1.3144e+03, 1.3057e+03, 1.3001e+03, 1.2865e+03, 1.2844e+03, 1.2804e+03,
        1.2622e+03, 1.2615e+03, 1.2580e+03, 1.2319e+03, 1.2258e+03, 1.2213e+03,
        1.2159e+03, 1.2090e+03, 1.2007e+03, 1.1944e+03, 1.1865e+03, 1.1796e+03,
        1.1726e+03, 1.1632e+03, 1.1505e+03, 1.1433e+03, 1.1382e+03, 1.1311e+03,
        1.1267e+03, 1.1205e+03, 1.1090e+03, 1.1027e+03, 1.0895e+03, 1.0878e+03,
        1.0830e+03, 1.0789e+03, 1.0700e+03, 1.0684e+03, 1.0657e+03, 1.0647e+03,
        1.0495e+03, 1.0445e+03, 1.0343e+03, 1.0326e+03, 1.0212e+03, 1.0169e+03,
        1.0130e+03, 1.0024e+03, 1.0006e+03, 9.9046e+02, 9.8351e+02, 9.7394e+02,
        9.6709e+02, 9.6165e+02, 9.5664e+02, 9.5210e+02, 9.4929e+02, 9.3513e+02,
        9.2988e+02, 9.2607e+02, 9.1768e+02, 9.1157e+02, 9.0884e+02, 8.9949e+02,
        8.9640e+02, 8.8905e+02, 8.8503e+02, 8.7537e+02, 8.6867e+02, 8.6626e+02,
        8.5777e+02, 8.5351e+02, 8.4819e+02, 8.4185e+02, 8.3814e+02, 8.3610e+02,
        8.2884e+02, 8.2288e+02, 8.2167e+02, 8.1613e+02, 8.0847e+02, 8.0296e+02,
        7.9507e+02, 7.9026e+02, 7.8819e+02, 7.8332e+02, 7.8159e+02, 7.7028e+02,
        7.6912e+02, 7.6520e+02, 7.5975e+02, 7.5650e+02, 7.5072e+02, 7.4376e+02,
        7.3958e+02, 7.3339e+02, 7.3002e+02, 7.2677e+02, 7.2292e+02, 7.2138e+02,
        7.1376e+02, 7.0439e+02, 7.0312e+02, 6.9643e+02, 6.8812e+02, 6.8195e+02,
        6.7939e+02, 6.7411e+02, 6.7170e+02, 6.6622e+02, 6.6239e+02, 6.6047e+02,
        6.5575e+02, 6.5339e+02, 6.4088e+02, 6.3954e+02, 6.3743e+02, 6.3237e+02,
        6.3061e+02, 6.2462e+02, 6.2185e+02, 6.1725e+02, 6.1257e+02, 6.0907e+02,
        6.0640e+02, 6.0402e+02, 6.0059e+02, 5.9325e+02, 5.8547e+02, 5.8124e+02,
        5.7583e+02, 5.7368e+02, 5.6724e+02, 5.6394e+02, 5.5784e+02, 5.5561e+02,
        5.5399e+02, 5.5066e+02, 5.4866e+02, 5.4248e+02, 5.4072e+02, 5.3622e+02,
        5.3014e+02, 5.2464e+02, 5.2121e+02, 5.1819e+02, 5.1485e+02, 5.1154e+02,
        5.1068e+02, 5.0149e+02, 5.0114e+02, 4.9697e+02, 4.9411e+02, 4.9171e+02,
        4.8419e+02, 4.7985e+02, 4.7874e+02, 4.7543e+02, 4.6906e+02, 4.6595e+02,
        4.6137e+02, 4.6003e+02, 4.5275e+02, 4.4921e+02, 4.4441e+02, 4.4132e+02,
        4.3639e+02, 4.3443e+02, 4.2639e+02, 4.1955e+02, 4.1615e+02, 4.1142e+02,
        4.0811e+02, 4.0459e+02, 3.9892e+02, 3.9649e+02, 3.9345e+02, 3.8812e+02,
        3.8431e+02, 3.8346e+02, 3.8094e+02, 3.7960e+02, 3.7565e+02, 3.7107e+02,
        3.6567e+02, 3.6188e+02, 3.5795e+02, 3.5357e+02, 3.4581e+02, 3.4082e+02,
        3.3975e+02, 3.3661e+02, 3.3420e+02, 3.3333e+02, 3.2664e+02, 3.2193e+02,
        3.1706e+02, 3.1294e+02, 3.1060e+02, 3.0507e+02, 2.9703e+02, 2.9459e+02,
        2.8986e+02, 2.8795e+02, 2.8369e+02, 2.8348e+02, 2.7946e+02, 2.7534e+02,
        2.7124e+02, 2.6550e+02, 2.6141e+02, 2.5900e+02, 2.5811e+02, 2.5556e+02,
        2.4843e+02, 2.4373e+02, 2.3769e+02, 2.3486e+02, 2.2738e+02, 2.2329e+02,
        2.2062e+02, 2.1646e+02, 2.1287e+02, 2.1109e+02, 2.0147e+02, 1.9796e+02,
        1.9701e+02, 1.9291e+02, 1.8243e+02, 1.8022e+02, 1.7849e+02, 1.6554e+02,
        1.5674e+02, 1.5297e+02, 1.4330e+02, 1.4105e+02, 1.3359e+02, 1.2786e+02,
        1.2545e+02, 1.2248e+02, 1.1121e+02, 1.0855e+02, 8.8734e+01, 6.6520e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 134]) 

NULL SPACE BASIS :  tensor([[-0.0348, -0.0056,  0.0191,  ..., -0.0168,  0.0028, -0.0122],
        [-0.0213,  0.0081, -0.0032,  ...,  0.0308,  0.0044,  0.0240],
        [ 0.0163, -0.0113,  0.0215,  ..., -0.0168,  0.0023, -0.0149],
        ...,
        [ 0.0486,  0.0357,  0.0186,  ...,  0.0248, -0.2043, -0.0410],
        [ 0.0152, -0.0162,  0.0142,  ..., -0.0548,  0.3358,  0.0442],
        [ 0.0059,  0.0049, -0.0046,  ...,  0.0411, -0.1589, -0.0156]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 3.6414e-02, -2.2703e-02, -6.9494e-03,  ...,  8.3342e-05,
          5.8204e-05, -2.4311e-05],
        [-2.2703e-02,  4.2900e-02, -2.1205e-02,  ..., -6.6128e-04,
          3.7335e-05, -1.8086e-04],
        [-6.9494e-03, -2.1205e-02,  3.3905e-02,  ..., -1.5529e-03,
          1.1360e-05,  7.1473e-04],
        ...,
        [ 8.3342e-05, -6.6128e-04, -1.5529e-03,  ...,  4.9699e-02,
         -2.4374e-02, -1.1081e-02],
        [ 5.8204e-05,  3.7335e-05,  1.1360e-05,  ..., -2.4374e-02,
          5.5619e-02, -2.2458e-02],
        [-2.4311e-05, -1.8086e-04,  7.1473e-04,  ..., -1.1081e-02,
         -2.2458e-02,  4.8605e-02]], device='cuda:0') 

reserving basis 218/576; cond: 1113485.25, radio:0.0007906839600764215
PARAMETER       :  Parameter containing:
tensor([[[[ 1.8204e-02,  4.0167e-02,  3.2052e-02],
          [ 4.1142e-03, -3.0951e-03, -8.2525e-03],
          [-3.4903e-02,  2.8221e-02,  1.6443e-02]],

         [[-4.1797e-02, -1.9731e-02, -2.2414e-02],
          [-4.7121e-02, -4.6686e-02, -2.7402e-02],
          [-2.4047e-02,  5.6938e-03, -6.2830e-03]],

         [[ 1.7971e-02,  2.3749e-02, -5.0354e-02],
          [-1.4764e-02, -3.0256e-02, -1.8710e-02],
          [ 1.2040e-02,  1.0892e-02,  1.9278e-02]],

         ...,

         [[-5.5624e-02,  3.2041e-02,  4.4575e-02],
          [-2.9908e-02, -6.4072e-03, -1.5419e-02],
          [ 2.3709e-02, -2.3241e-02, -3.3492e-02]],

         [[ 1.1054e-02, -2.8462e-02,  3.4856e-02],
          [ 4.0282e-02, -2.6094e-02, -3.4934e-03],
          [-1.8863e-02, -3.2879e-02,  1.9779e-02]],

         [[-2.6825e-02, -2.0594e-02,  5.8167e-03],
          [ 1.7001e-02,  1.4846e-02, -2.3440e-02],
          [-2.6079e-02,  3.6523e-03,  3.7032e-03]]],


        [[[-2.9988e-02, -2.0143e-02, -1.1794e-02],
          [ 3.5872e-02,  2.1001e-02,  2.9154e-02],
          [-1.3648e-02,  4.6249e-02,  1.2235e-03]],

         [[-3.9170e-02, -3.3423e-02, -3.1259e-02],
          [ 4.8142e-04, -4.0193e-02, -2.3814e-02],
          [-1.5099e-02,  1.9793e-02,  2.2294e-02]],

         [[-9.3542e-03,  1.6665e-02, -2.2553e-02],
          [-3.5791e-02, -2.9811e-02,  1.4106e-02],
          [ 2.9134e-02,  1.1654e-02, -1.9283e-02]],

         ...,

         [[ 2.4501e-02, -4.6111e-02,  1.7558e-02],
          [-1.2350e-02,  1.2023e-02, -2.0406e-02],
          [ 2.7143e-02,  2.5223e-02, -7.0369e-03]],

         [[ 1.1845e-02, -4.0151e-02, -7.7294e-03],
          [-1.3181e-02, -1.9000e-02, -4.1263e-02],
          [-4.1537e-03,  5.6608e-03, -6.0811e-04]],

         [[-1.3223e-03, -2.6941e-02, -1.0840e-02],
          [-3.1195e-02, -2.7548e-02, -1.6212e-02],
          [ 3.4457e-02,  2.1280e-02, -1.7813e-02]]],


        [[[-4.0774e-02, -8.6298e-03, -1.8120e-03],
          [-5.7509e-02, -4.2264e-02, -3.5120e-02],
          [-1.3438e-03,  3.4986e-02,  3.7986e-02]],

         [[-1.8735e-02, -9.3458e-03, -4.1195e-03],
          [-2.3249e-02, -4.0552e-02, -3.8863e-02],
          [-1.7037e-02, -1.8184e-02, -9.9045e-03]],

         [[ 7.2838e-03, -1.4329e-02,  4.5736e-03],
          [-3.5544e-02,  5.0087e-02, -9.1836e-03],
          [-5.1024e-03,  2.6671e-02,  3.8468e-02]],

         ...,

         [[-4.5055e-02,  2.2682e-02,  3.6461e-02],
          [-1.8550e-02, -4.0526e-02,  3.3809e-02],
          [-1.1696e-02, -2.9384e-02,  1.3086e-02]],

         [[ 2.6897e-02,  1.8394e-02,  4.0269e-02],
          [ 2.3243e-02, -3.0014e-02, -2.2161e-02],
          [-3.4351e-02, -4.9897e-02,  1.9589e-02]],

         [[-2.8606e-02,  4.9624e-02,  3.2637e-02],
          [ 2.0265e-02,  8.0628e-03, -1.2213e-02],
          [-1.5509e-02, -6.1417e-05, -3.7476e-02]]],


        ...,


        [[[ 3.4578e-02,  5.3325e-03, -6.0864e-03],
          [ 3.9865e-02,  4.4607e-02,  1.2294e-03],
          [-1.5854e-02,  1.2698e-02, -3.7841e-02]],

         [[-3.0660e-02,  1.6711e-03, -2.8158e-02],
          [-1.0955e-02,  3.4031e-02,  1.4205e-02],
          [-4.2890e-03, -1.7558e-02,  2.0718e-02]],

         [[-4.1308e-02, -4.0872e-02, -3.6269e-02],
          [-2.7143e-02,  3.5492e-02,  4.3265e-02],
          [-1.2336e-02, -3.5267e-02, -2.7094e-02]],

         ...,

         [[ 3.3817e-02, -6.2898e-03,  3.3843e-02],
          [-1.1372e-02,  4.3069e-02, -1.2426e-02],
          [-3.9315e-02,  2.5849e-02, -2.3602e-02]],

         [[-1.8281e-02, -4.2537e-02, -1.4247e-02],
          [-1.1688e-02,  1.8876e-02, -3.5002e-02],
          [ 2.4508e-02,  2.2648e-02,  1.3200e-02]],

         [[ 2.1684e-02, -1.5489e-02,  1.0785e-02],
          [ 3.2742e-02, -4.1537e-03, -2.6674e-02],
          [-4.0158e-02,  1.4437e-02, -1.2051e-02]]],


        [[[-3.7613e-02,  2.3628e-02, -4.7467e-02],
          [ 3.1100e-03, -4.1940e-02,  2.0768e-02],
          [-1.6674e-02, -7.1638e-03,  3.6933e-02]],

         [[ 1.8607e-02, -3.0266e-02,  7.2120e-03],
          [-7.9561e-03,  2.7251e-02,  5.0459e-03],
          [ 2.4256e-02, -1.1320e-02,  1.6258e-02]],

         [[-3.5232e-02, -4.2389e-02, -2.4885e-02],
          [-2.3681e-02, -1.7770e-02,  1.7925e-02],
          [ 9.2804e-03, -3.3078e-02, -3.6561e-02]],

         ...,

         [[-9.1401e-03, -1.6669e-02, -3.0739e-02],
          [-2.0500e-02,  2.8582e-02, -2.3438e-02],
          [-4.0857e-02,  3.4286e-02,  3.2041e-02]],

         [[-2.9957e-02, -1.0679e-02, -2.9316e-02],
          [ 3.6509e-02,  2.3749e-03, -1.6159e-02],
          [ 3.0161e-02,  3.2395e-02, -2.0585e-02]],

         [[ 1.5434e-02, -2.6208e-02, -7.0757e-03],
          [-4.0467e-02, -3.4443e-02,  3.8477e-02],
          [ 3.8074e-02,  2.2250e-02,  3.9193e-02]]],


        [[[-3.9122e-02,  3.2640e-02, -4.4596e-03],
          [-3.3739e-02,  3.5579e-02, -8.1840e-03],
          [-3.8295e-02, -2.2536e-02,  2.9646e-02]],

         [[-3.4288e-03, -2.6283e-02,  9.1654e-03],
          [ 1.8713e-02,  2.0398e-03, -9.5780e-03],
          [-1.5247e-02, -2.4181e-03,  3.1217e-02]],

         [[-2.1995e-02, -3.6969e-02, -6.5725e-03],
          [-1.1568e-03, -7.9873e-04,  2.6240e-03],
          [ 3.5187e-02, -2.6421e-02,  1.8605e-03]],

         ...,

         [[ 1.2217e-02, -1.6443e-02, -1.9589e-02],
          [-1.9934e-02,  3.0855e-02, -4.4562e-02],
          [ 3.4396e-04,  1.5433e-02, -3.0008e-02]],

         [[ 3.6076e-02, -6.6739e-03,  3.1839e-02],
          [ 5.0921e-03, -2.1222e-02, -3.8463e-02],
          [-2.0885e-02,  8.5689e-03, -3.6245e-02]],

         [[ 1.0183e-02,  4.1478e-02, -1.7911e-02],
          [-3.1723e-02,  4.2123e-02,  3.6227e-02],
          [ 2.6731e-02,  1.5936e-02,  7.3319e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([9.1883e+07, 3.5130e+06, 3.0718e+06, 2.5536e+06, 2.1310e+06, 1.5130e+06,
        1.3848e+06, 1.1002e+06, 1.0474e+06, 8.0976e+05, 7.3598e+05, 6.9812e+05,
        5.4968e+05, 4.2087e+05, 2.2880e+05, 2.0549e+05, 1.8870e+05, 1.7742e+05,
        1.6718e+05, 1.5005e+05, 1.4407e+05, 1.2658e+05, 1.1820e+05, 1.1016e+05,
        1.0163e+05, 9.1628e+04, 8.8893e+04, 8.5939e+04, 8.1185e+04, 7.9949e+04,
        7.7159e+04, 6.9228e+04, 6.7730e+04, 6.7072e+04, 6.0156e+04, 5.7192e+04,
        5.5172e+04, 5.3334e+04, 5.0910e+04, 4.9512e+04, 4.7992e+04, 4.5609e+04,
        4.5136e+04, 4.3338e+04, 4.1819e+04, 4.0262e+04, 3.8623e+04, 3.6710e+04,
        3.5848e+04, 3.5073e+04, 3.3026e+04, 3.2500e+04, 3.0048e+04, 2.8734e+04,
        2.8219e+04, 2.6133e+04, 2.5544e+04, 2.5104e+04, 2.4505e+04, 2.3843e+04,
        2.2301e+04, 2.1868e+04, 2.1139e+04, 2.0902e+04, 2.0331e+04, 1.9882e+04,
        1.9521e+04, 1.9178e+04, 1.8498e+04, 1.8310e+04, 1.8208e+04, 1.6945e+04,
        1.6511e+04, 1.6243e+04, 1.5603e+04, 1.5317e+04, 1.5011e+04, 1.4855e+04,
        1.4751e+04, 1.4411e+04, 1.4048e+04, 1.3906e+04, 1.3430e+04, 1.3173e+04,
        1.2949e+04, 1.2677e+04, 1.2520e+04, 1.2253e+04, 1.2012e+04, 1.1850e+04,
        1.1748e+04, 1.1582e+04, 1.1473e+04, 1.1235e+04, 1.0722e+04, 1.0548e+04,
        1.0082e+04, 9.9308e+03, 9.9091e+03, 9.7656e+03, 9.5384e+03, 9.4310e+03,
        9.2262e+03, 9.0277e+03, 8.9512e+03, 8.8626e+03, 8.6508e+03, 8.5307e+03,
        8.3187e+03, 8.2336e+03, 8.0736e+03, 8.0318e+03, 7.7701e+03, 7.6213e+03,
        7.5310e+03, 7.3867e+03, 7.3145e+03, 7.1711e+03, 7.0655e+03, 6.9856e+03,
        6.8560e+03, 6.7029e+03, 6.6559e+03, 6.5203e+03, 6.4396e+03, 6.3889e+03,
        6.2531e+03, 6.2129e+03, 6.1804e+03, 6.0181e+03, 5.9327e+03, 5.8564e+03,
        5.7514e+03, 5.6729e+03, 5.6346e+03, 5.5276e+03, 5.4200e+03, 5.2830e+03,
        5.2599e+03, 5.1122e+03, 5.0921e+03, 5.0847e+03, 4.9980e+03, 4.9742e+03,
        4.9439e+03, 4.8358e+03, 4.7734e+03, 4.6823e+03, 4.6715e+03, 4.6005e+03,
        4.5715e+03, 4.5229e+03, 4.4088e+03, 4.3390e+03, 4.3232e+03, 4.2512e+03,
        4.2164e+03, 4.1337e+03, 4.1166e+03, 4.0623e+03, 4.0115e+03, 3.9845e+03,
        3.9622e+03, 3.9430e+03, 3.9052e+03, 3.8939e+03, 3.8508e+03, 3.8115e+03,
        3.7301e+03, 3.6975e+03, 3.6482e+03, 3.6105e+03, 3.5965e+03, 3.5762e+03,
        3.5428e+03, 3.5262e+03, 3.4955e+03, 3.4478e+03, 3.3899e+03, 3.3258e+03,
        3.3162e+03, 3.2811e+03, 3.2582e+03, 3.2450e+03, 3.1857e+03, 3.1461e+03,
        3.1284e+03, 3.1156e+03, 3.0829e+03, 3.0641e+03, 3.0211e+03, 2.9900e+03,
        2.9638e+03, 2.9522e+03, 2.8702e+03, 2.8394e+03, 2.8053e+03, 2.7692e+03,
        2.7347e+03, 2.7198e+03, 2.6797e+03, 2.6562e+03, 2.6482e+03, 2.6198e+03,
        2.6156e+03, 2.5937e+03, 2.5612e+03, 2.5285e+03, 2.5032e+03, 2.4786e+03,
        2.4530e+03, 2.4367e+03, 2.4165e+03, 2.4020e+03, 2.4008e+03, 2.3871e+03,
        2.3708e+03, 2.3407e+03, 2.3228e+03, 2.3109e+03, 2.2795e+03, 2.2780e+03,
        2.2448e+03, 2.2330e+03, 2.2065e+03, 2.1776e+03, 2.1677e+03, 2.1427e+03,
        2.1090e+03, 2.0891e+03, 2.0739e+03, 2.0655e+03, 2.0522e+03, 2.0344e+03,
        2.0186e+03, 2.0083e+03, 1.9893e+03, 1.9691e+03, 1.9533e+03, 1.9461e+03,
        1.9326e+03, 1.9258e+03, 1.9048e+03, 1.8726e+03, 1.8680e+03, 1.8453e+03,
        1.8324e+03, 1.8223e+03, 1.8040e+03, 1.7897e+03, 1.7831e+03, 1.7469e+03,
        1.7427e+03, 1.7270e+03, 1.7152e+03, 1.7047e+03, 1.6885e+03, 1.6759e+03,
        1.6659e+03, 1.6526e+03, 1.6397e+03, 1.6308e+03, 1.6146e+03, 1.5908e+03,
        1.5834e+03, 1.5754e+03, 1.5655e+03, 1.5579e+03, 1.5455e+03, 1.5261e+03,
        1.5136e+03, 1.5074e+03, 1.5005e+03, 1.4925e+03, 1.4780e+03, 1.4720e+03,
        1.4577e+03, 1.4465e+03, 1.4302e+03, 1.4240e+03, 1.4126e+03, 1.4108e+03,
        1.3994e+03, 1.3834e+03, 1.3619e+03, 1.3588e+03, 1.3452e+03, 1.3418e+03,
        1.3279e+03, 1.3196e+03, 1.3183e+03, 1.3062e+03, 1.3009e+03, 1.2932e+03,
        1.2870e+03, 1.2768e+03, 1.2738e+03, 1.2613e+03, 1.2472e+03, 1.2357e+03,
        1.2258e+03, 1.2241e+03, 1.2117e+03, 1.2054e+03, 1.1928e+03, 1.1841e+03,
        1.1808e+03, 1.1733e+03, 1.1633e+03, 1.1521e+03, 1.1415e+03, 1.1355e+03,
        1.1245e+03, 1.1118e+03, 1.1060e+03, 1.1048e+03, 1.0986e+03, 1.0964e+03,
        1.0881e+03, 1.0734e+03, 1.0602e+03, 1.0554e+03, 1.0491e+03, 1.0378e+03,
        1.0269e+03, 1.0222e+03, 1.0152e+03, 1.0045e+03, 9.9818e+02, 9.8957e+02,
        9.8301e+02, 9.7550e+02, 9.7124e+02, 9.6420e+02, 9.5717e+02, 9.5567e+02,
        9.4455e+02, 9.4040e+02, 9.3796e+02, 9.3142e+02, 9.2623e+02, 9.1637e+02,
        9.1175e+02, 9.0812e+02, 9.0220e+02, 8.9634e+02, 8.8693e+02, 8.8102e+02,
        8.6513e+02, 8.6452e+02, 8.5884e+02, 8.5593e+02, 8.5080e+02, 8.4803e+02,
        8.4386e+02, 8.4169e+02, 8.3486e+02, 8.3282e+02, 8.1927e+02, 8.1655e+02,
        8.1379e+02, 8.0822e+02, 8.0521e+02, 7.8891e+02, 7.8723e+02, 7.8113e+02,
        7.7299e+02, 7.6827e+02, 7.6375e+02, 7.6001e+02, 7.5427e+02, 7.4774e+02,
        7.4025e+02, 7.3594e+02, 7.3307e+02, 7.2772e+02, 7.2533e+02, 7.1943e+02,
        7.1720e+02, 7.1019e+02, 7.0915e+02, 7.0047e+02, 6.9937e+02, 6.9429e+02,
        6.9079e+02, 6.8374e+02, 6.8278e+02, 6.7399e+02, 6.7026e+02, 6.6759e+02,
        6.6161e+02, 6.5678e+02, 6.5340e+02, 6.4694e+02, 6.4414e+02, 6.4217e+02,
        6.3862e+02, 6.3631e+02, 6.3213e+02, 6.2504e+02, 6.2231e+02, 6.1923e+02,
        6.1188e+02, 6.0924e+02, 6.0675e+02, 6.0222e+02, 5.9744e+02, 5.9529e+02,
        5.9237e+02, 5.8881e+02, 5.8741e+02, 5.8030e+02, 5.7662e+02, 5.7241e+02,
        5.7083e+02, 5.6672e+02, 5.6181e+02, 5.5922e+02, 5.5720e+02, 5.5102e+02,
        5.4540e+02, 5.4142e+02, 5.3492e+02, 5.3194e+02, 5.2842e+02, 5.2489e+02,
        5.2353e+02, 5.2072e+02, 5.1680e+02, 5.1481e+02, 5.1125e+02, 5.1014e+02,
        5.0589e+02, 5.0347e+02, 4.9977e+02, 4.9666e+02, 4.9395e+02, 4.8938e+02,
        4.8371e+02, 4.8118e+02, 4.7821e+02, 4.7620e+02, 4.7446e+02, 4.6880e+02,
        4.6653e+02, 4.6532e+02, 4.6318e+02, 4.5678e+02, 4.5454e+02, 4.5270e+02,
        4.5027e+02, 4.4737e+02, 4.4593e+02, 4.4180e+02, 4.3980e+02, 4.3590e+02,
        4.3309e+02, 4.2703e+02, 4.2652e+02, 4.2160e+02, 4.1982e+02, 4.1708e+02,
        4.1252e+02, 4.1024e+02, 4.0677e+02, 4.0296e+02, 4.0065e+02, 3.9574e+02,
        3.9380e+02, 3.9016e+02, 3.8782e+02, 3.8631e+02, 3.8251e+02, 3.7888e+02,
        3.7831e+02, 3.7546e+02, 3.7417e+02, 3.7056e+02, 3.6740e+02, 3.6609e+02,
        3.5996e+02, 3.5711e+02, 3.5510e+02, 3.5305e+02, 3.5007e+02, 3.4842e+02,
        3.4669e+02, 3.4375e+02, 3.4129e+02, 3.3728e+02, 3.3523e+02, 3.3292e+02,
        3.2777e+02, 3.2667e+02, 3.2352e+02, 3.2085e+02, 3.2016e+02, 3.1745e+02,
        3.1508e+02, 3.1300e+02, 3.0812e+02, 3.0625e+02, 3.0297e+02, 3.0114e+02,
        3.0006e+02, 2.9418e+02, 2.9197e+02, 2.8892e+02, 2.8631e+02, 2.8402e+02,
        2.8135e+02, 2.7941e+02, 2.7795e+02, 2.7625e+02, 2.7297e+02, 2.6945e+02,
        2.6711e+02, 2.6321e+02, 2.6151e+02, 2.6049e+02, 2.5650e+02, 2.5303e+02,
        2.5103e+02, 2.4850e+02, 2.4588e+02, 2.4443e+02, 2.4423e+02, 2.4228e+02,
        2.4113e+02, 2.3851e+02, 2.3446e+02, 2.3299e+02, 2.3121e+02, 2.3034e+02,
        2.2515e+02, 2.2290e+02, 2.1998e+02, 2.1714e+02, 2.1491e+02, 2.1417e+02,
        2.1178e+02, 2.1042e+02, 2.0628e+02, 2.0530e+02, 2.0188e+02, 1.9976e+02,
        1.9867e+02, 1.9799e+02, 1.9683e+02, 1.9210e+02, 1.9091e+02, 1.8947e+02,
        1.8397e+02, 1.8287e+02, 1.8154e+02, 1.8065e+02, 1.7539e+02, 1.7220e+02,
        1.6968e+02, 1.6312e+02, 1.6099e+02, 1.5845e+02, 1.5434e+02, 1.5274e+02,
        1.4762e+02, 1.4645e+02, 1.3958e+02, 1.3565e+02, 1.3179e+02, 1.2737e+02,
        1.2221e+02, 1.1883e+02, 1.1679e+02, 1.1189e+02, 8.8732e+01, 8.2518e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 218]) 

NULL SPACE BASIS :  tensor([[ 0.0314, -0.0109, -0.0016,  ...,  0.0003, -0.0204,  0.0016],
        [-0.0335,  0.0054, -0.0486,  ..., -0.0037,  0.0282, -0.0022],
        [ 0.0032, -0.0110, -0.0054,  ...,  0.0017, -0.0094, -0.0005],
        ...,
        [ 0.0447,  0.0078, -0.0751,  ...,  0.0048, -0.0020, -0.0135],
        [ 0.0050, -0.0308,  0.0520,  ..., -0.0254,  0.0030,  0.0169],
        [-0.0680, -0.0450, -0.0260,  ...,  0.0185,  0.0033, -0.0135]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0204, -0.0162,  0.0006,  ...,  0.0002,  0.0004,  0.0007],
        [-0.0162,  0.0317, -0.0140,  ..., -0.0015,  0.0011, -0.0012],
        [ 0.0006, -0.0140,  0.0162,  ...,  0.0015, -0.0017, -0.0002],
        ...,
        [ 0.0002, -0.0015,  0.0015,  ...,  0.0195, -0.0108, -0.0023],
        [ 0.0004,  0.0011, -0.0017,  ..., -0.0108,  0.0257, -0.0114],
        [ 0.0007, -0.0012, -0.0002,  ..., -0.0023, -0.0114,  0.0215]],
       device='cuda:0') 

reserving basis 293/576; cond: 314985.46875, radio:0.003619791939854622
PARAMETER       :  Parameter containing:
tensor([[[[-2.6663e-02, -4.2480e-03,  1.8998e-02],
          [-2.4164e-02, -2.8755e-02, -1.3151e-02],
          [ 1.1578e-02,  1.1765e-02,  8.3444e-03]],

         [[ 9.3626e-04,  1.5631e-02,  6.1254e-03],
          [-5.2599e-03,  4.5889e-04,  1.7773e-02],
          [-1.9365e-02, -1.6764e-02, -4.8124e-02]],

         [[ 1.6924e-03,  4.3542e-02,  4.3371e-02],
          [-4.1766e-03, -3.7450e-02, -4.0047e-03],
          [-3.4945e-02, -4.9064e-03, -2.4717e-02]],

         ...,

         [[-1.8897e-02,  4.3043e-02,  3.6251e-03],
          [-2.6562e-02, -9.1453e-03,  4.3014e-02],
          [-1.4209e-02, -4.0493e-02, -1.8071e-02]],

         [[ 2.9094e-02,  1.6007e-02, -4.4892e-02],
          [ 2.8528e-02,  6.5584e-03,  4.5279e-02],
          [-3.3865e-02,  2.2280e-02, -2.0743e-02]],

         [[ 3.5053e-02,  2.1547e-02,  2.1509e-02],
          [-3.6440e-02, -9.7863e-03,  2.6229e-02],
          [ 3.0615e-02, -1.5219e-02, -2.6668e-02]]],


        [[[-2.6677e-02, -2.3758e-02, -4.7401e-02],
          [ 9.6686e-03, -9.4486e-03,  7.9523e-03],
          [-5.3293e-02,  7.3088e-03,  2.2184e-03]],

         [[-3.3744e-02,  1.9376e-02,  5.3146e-02],
          [ 5.7015e-04,  2.2239e-02, -1.2185e-02],
          [-1.3264e-02,  2.9838e-02,  4.5779e-02]],

         [[-1.9835e-03, -1.6632e-02,  3.0776e-02],
          [-1.2059e-03, -3.2384e-02,  3.5524e-02],
          [ 3.5365e-02,  3.4339e-02, -4.1468e-03]],

         ...,

         [[-6.8698e-03,  2.1437e-02, -2.2643e-02],
          [-2.7535e-02,  2.2881e-02,  1.4059e-02],
          [-3.8578e-02,  2.8131e-02, -4.3152e-02]],

         [[-9.9084e-03,  2.1576e-02, -2.0614e-02],
          [-1.9489e-02, -2.9661e-02, -3.5693e-02],
          [-4.3344e-02, -1.0060e-02, -4.0215e-02]],

         [[ 2.2746e-02, -3.7696e-02, -3.4516e-02],
          [-2.0717e-02, -3.3911e-02, -1.6417e-02],
          [-2.2603e-02,  1.5848e-03,  9.1615e-03]]],


        [[[-6.0632e-03, -5.2995e-03, -1.7099e-02],
          [-7.8052e-05, -4.4602e-02, -3.8815e-02],
          [-4.0693e-02, -2.2902e-03, -1.4324e-02]],

         [[-2.5346e-02, -2.2028e-02, -1.9820e-02],
          [-1.6993e-04, -2.7432e-02, -1.2700e-02],
          [ 1.6804e-02, -2.5837e-02,  1.7159e-03]],

         [[ 4.7090e-02,  4.2283e-02,  2.7369e-03],
          [ 2.9801e-02,  5.0422e-03, -3.9559e-02],
          [ 1.1454e-02,  7.8487e-03, -8.5791e-04]],

         ...,

         [[ 1.3280e-02, -1.3577e-02, -1.4891e-03],
          [ 1.5068e-02,  1.7500e-02, -4.0334e-02],
          [-3.3695e-02,  1.0075e-02, -1.3789e-02]],

         [[ 1.1181e-02,  5.6576e-03, -2.4925e-02],
          [-2.8070e-02,  9.1837e-03,  2.5740e-02],
          [ 5.8625e-03, -2.1297e-02, -2.0178e-04]],

         [[ 1.8311e-02, -1.2045e-02, -3.1232e-02],
          [ 1.1258e-02, -3.3990e-02, -3.2852e-02],
          [ 3.1123e-02, -1.5516e-02,  7.4488e-04]]],


        ...,


        [[[ 1.1314e-02,  3.2001e-02,  1.4778e-02],
          [-1.1161e-03, -1.8322e-02, -1.8069e-02],
          [ 4.8204e-02,  2.3851e-02, -5.5216e-02]],

         [[ 4.2507e-03, -2.5778e-02,  2.7024e-03],
          [-3.1592e-02,  2.6982e-02, -2.1856e-02],
          [ 1.5669e-02, -2.6071e-02,  4.4021e-02]],

         [[ 2.5380e-02, -2.1624e-02,  4.2292e-02],
          [ 3.3627e-02, -3.9424e-02, -4.0554e-02],
          [ 1.7087e-02, -9.8749e-03, -4.1292e-03]],

         ...,

         [[-1.4398e-02, -1.3468e-02,  4.8009e-03],
          [ 1.4595e-02,  8.7459e-03,  1.3774e-02],
          [ 2.3431e-03, -1.0019e-02,  2.0021e-03]],

         [[ 1.3820e-02,  2.0846e-02, -4.7354e-02],
          [-3.0853e-02, -1.9596e-02, -3.1743e-03],
          [ 1.1154e-02,  7.4722e-03,  1.3750e-02]],

         [[ 3.1600e-02,  5.9152e-03,  2.9695e-02],
          [ 1.8190e-02,  4.3844e-02,  3.0683e-03],
          [ 3.9995e-02, -9.7618e-03,  2.1461e-02]]],


        [[[-1.3385e-02,  2.0853e-02,  3.3834e-02],
          [ 1.5702e-02,  2.5506e-02,  7.1623e-03],
          [-4.6080e-03,  2.8105e-02, -5.1938e-02]],

         [[ 3.5952e-02, -1.4597e-02, -2.7063e-02],
          [-3.7530e-02,  5.9605e-03, -3.5590e-02],
          [-4.0028e-02, -3.6297e-03,  3.9292e-02]],

         [[ 2.0713e-02, -4.2808e-02, -2.3919e-02],
          [-3.4242e-02,  2.7631e-02, -1.6099e-02],
          [-3.4525e-02,  1.6085e-02, -3.1940e-03]],

         ...,

         [[-1.6356e-02, -2.8526e-02,  1.4575e-02],
          [ 1.4027e-02,  3.7950e-02,  8.7702e-03],
          [-2.0591e-02, -3.4147e-02, -7.2927e-03]],

         [[-2.0521e-02,  3.7361e-02, -2.4427e-02],
          [ 6.3915e-03, -1.5099e-02,  3.8455e-02],
          [ 4.6339e-02, -2.8446e-03, -1.0675e-02]],

         [[-2.3385e-02,  6.2142e-03,  2.8347e-02],
          [-3.3431e-02, -2.1801e-02,  9.7944e-03],
          [ 1.1058e-02,  1.1802e-02, -2.3718e-03]]],


        [[[ 3.7081e-02,  4.3015e-02,  6.1881e-04],
          [ 2.0771e-02, -1.1597e-02,  2.4973e-02],
          [ 8.0243e-03, -3.4445e-02, -4.9725e-02]],

         [[ 4.6699e-02, -5.1594e-03,  3.2010e-02],
          [ 2.1035e-02,  3.4718e-02,  9.3136e-03],
          [ 1.1232e-04, -9.9208e-03,  3.6994e-02]],

         [[-2.2219e-02,  9.8532e-05, -4.3544e-02],
          [-1.9761e-03, -9.7059e-03,  3.9625e-02],
          [ 2.1316e-02, -1.9501e-02,  2.7568e-02]],

         ...,

         [[-8.1034e-03,  3.7680e-02, -3.4769e-02],
          [-9.7120e-03, -3.7785e-02,  2.7319e-02],
          [-3.7376e-02,  7.8579e-03,  1.0579e-03]],

         [[ 3.7770e-03, -3.4812e-02, -1.6358e-02],
          [-3.6854e-02,  2.6433e-02, -2.1775e-02],
          [-2.4354e-02, -3.9965e-02,  7.0642e-03]],

         [[ 4.0734e-02, -4.5701e-03,  3.5113e-02],
          [ 3.5267e-02, -1.4052e-02,  2.3796e-02],
          [ 1.5641e-04,  2.4839e-02,  2.3773e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.1051e+08, 2.9767e+06, 2.5815e+06, 2.2796e+06, 2.0006e+06, 1.3956e+06,
        1.3541e+06, 1.1650e+06, 1.0101e+06, 8.5788e+05, 8.2907e+05, 6.4991e+05,
        6.0372e+05, 5.0403e+05, 4.0469e+05, 3.4975e+05, 3.1403e+05, 2.5451e+05,
        2.3786e+05, 2.2218e+05, 2.1090e+05, 2.0162e+05, 1.9861e+05, 1.7591e+05,
        1.6550e+05, 1.5699e+05, 1.4821e+05, 1.4227e+05, 1.3202e+05, 1.2063e+05,
        1.1603e+05, 1.1201e+05, 1.0382e+05, 1.0128e+05, 8.8636e+04, 8.5747e+04,
        8.3207e+04, 7.8233e+04, 7.5154e+04, 7.3711e+04, 7.3271e+04, 6.9365e+04,
        6.6180e+04, 6.1631e+04, 5.8759e+04, 5.6629e+04, 5.4417e+04, 5.4087e+04,
        5.2972e+04, 5.1755e+04, 5.0676e+04, 4.7561e+04, 4.5657e+04, 4.4732e+04,
        4.3183e+04, 4.2538e+04, 4.1430e+04, 4.0969e+04, 4.0055e+04, 3.9434e+04,
        3.8536e+04, 3.7280e+04, 3.6178e+04, 3.3567e+04, 3.3412e+04, 3.2618e+04,
        3.1855e+04, 3.1833e+04, 3.1377e+04, 2.9487e+04, 2.9434e+04, 2.9061e+04,
        2.8679e+04, 2.8233e+04, 2.7429e+04, 2.7235e+04, 2.6496e+04, 2.6018e+04,
        2.5600e+04, 2.4573e+04, 2.4474e+04, 2.4262e+04, 2.3424e+04, 2.2999e+04,
        2.2744e+04, 2.2364e+04, 2.2081e+04, 2.1461e+04, 2.0886e+04, 2.0769e+04,
        2.0568e+04, 2.0051e+04, 1.9862e+04, 1.9265e+04, 1.8807e+04, 1.8607e+04,
        1.8206e+04, 1.8113e+04, 1.7532e+04, 1.7422e+04, 1.7313e+04, 1.7008e+04,
        1.6722e+04, 1.6627e+04, 1.6438e+04, 1.6149e+04, 1.5831e+04, 1.5515e+04,
        1.5371e+04, 1.5228e+04, 1.4873e+04, 1.4755e+04, 1.4539e+04, 1.4197e+04,
        1.4066e+04, 1.4003e+04, 1.3712e+04, 1.3697e+04, 1.3556e+04, 1.3495e+04,
        1.3127e+04, 1.3040e+04, 1.2934e+04, 1.2773e+04, 1.2611e+04, 1.2434e+04,
        1.2377e+04, 1.2157e+04, 1.1984e+04, 1.1868e+04, 1.1613e+04, 1.1567e+04,
        1.1507e+04, 1.1371e+04, 1.1303e+04, 1.1135e+04, 1.0963e+04, 1.0892e+04,
        1.0730e+04, 1.0684e+04, 1.0577e+04, 1.0474e+04, 1.0368e+04, 1.0230e+04,
        1.0121e+04, 9.9433e+03, 9.8777e+03, 9.8120e+03, 9.6991e+03, 9.6504e+03,
        9.5371e+03, 9.4142e+03, 9.2952e+03, 9.1358e+03, 9.1237e+03, 9.0307e+03,
        8.8342e+03, 8.7842e+03, 8.7140e+03, 8.5632e+03, 8.5349e+03, 8.4761e+03,
        8.4014e+03, 8.3522e+03, 8.3274e+03, 8.2913e+03, 8.1470e+03, 8.1242e+03,
        7.9729e+03, 7.9248e+03, 7.8833e+03, 7.8182e+03, 7.7339e+03, 7.7065e+03,
        7.6225e+03, 7.5979e+03, 7.5496e+03, 7.4940e+03, 7.4365e+03, 7.3868e+03,
        7.3333e+03, 7.2603e+03, 7.2485e+03, 7.1832e+03, 7.1591e+03, 7.0815e+03,
        7.0404e+03, 6.8890e+03, 6.8253e+03, 6.7735e+03, 6.7535e+03, 6.7090e+03,
        6.6033e+03, 6.5518e+03, 6.5309e+03, 6.4953e+03, 6.4160e+03, 6.3383e+03,
        6.2752e+03, 6.2541e+03, 6.2310e+03, 6.1807e+03, 6.1250e+03, 6.0947e+03,
        6.0059e+03, 5.9829e+03, 5.9552e+03, 5.8914e+03, 5.8709e+03, 5.8506e+03,
        5.7978e+03, 5.7104e+03, 5.6681e+03, 5.6015e+03, 5.5947e+03, 5.5355e+03,
        5.5213e+03, 5.4668e+03, 5.4289e+03, 5.4110e+03, 5.3172e+03, 5.3024e+03,
        5.2778e+03, 5.1965e+03, 5.1409e+03, 5.0956e+03, 5.0880e+03, 5.0340e+03,
        4.9908e+03, 4.9744e+03, 4.9497e+03, 4.9465e+03, 4.8887e+03, 4.8458e+03,
        4.8185e+03, 4.7977e+03, 4.7130e+03, 4.6959e+03, 4.6866e+03, 4.6462e+03,
        4.6252e+03, 4.6149e+03, 4.5798e+03, 4.5753e+03, 4.5374e+03, 4.4636e+03,
        4.4340e+03, 4.4069e+03, 4.3877e+03, 4.3645e+03, 4.3284e+03, 4.2681e+03,
        4.2518e+03, 4.2430e+03, 4.2235e+03, 4.1837e+03, 4.1645e+03, 4.1133e+03,
        4.0785e+03, 4.0725e+03, 4.0442e+03, 4.0178e+03, 3.9904e+03, 3.9879e+03,
        3.9455e+03, 3.9301e+03, 3.8794e+03, 3.8603e+03, 3.8400e+03, 3.8084e+03,
        3.7885e+03, 3.7773e+03, 3.7653e+03, 3.7365e+03, 3.6848e+03, 3.6799e+03,
        3.6568e+03, 3.6320e+03, 3.6174e+03, 3.5797e+03, 3.5544e+03, 3.5391e+03,
        3.5170e+03, 3.4975e+03, 3.4737e+03, 3.4516e+03, 3.4416e+03, 3.4001e+03,
        3.3915e+03, 3.3858e+03, 3.3538e+03, 3.3359e+03, 3.3280e+03, 3.3067e+03,
        3.3046e+03, 3.2600e+03, 3.2349e+03, 3.2221e+03, 3.2016e+03, 3.1780e+03,
        3.1597e+03, 3.1474e+03, 3.1125e+03, 3.1052e+03, 3.0790e+03, 3.0757e+03,
        3.0562e+03, 3.0346e+03, 3.0229e+03, 3.0135e+03, 2.9919e+03, 2.9810e+03,
        2.9588e+03, 2.9445e+03, 2.9204e+03, 2.8883e+03, 2.8855e+03, 2.8576e+03,
        2.8525e+03, 2.8325e+03, 2.8188e+03, 2.8119e+03, 2.7857e+03, 2.7613e+03,
        2.7577e+03, 2.7306e+03, 2.7259e+03, 2.7165e+03, 2.6984e+03, 2.6846e+03,
        2.6709e+03, 2.6377e+03, 2.6360e+03, 2.6237e+03, 2.6179e+03, 2.5936e+03,
        2.5873e+03, 2.5709e+03, 2.5671e+03, 2.5391e+03, 2.5307e+03, 2.5210e+03,
        2.4963e+03, 2.4858e+03, 2.4710e+03, 2.4583e+03, 2.4489e+03, 2.4332e+03,
        2.4161e+03, 2.4023e+03, 2.3950e+03, 2.3842e+03, 2.3733e+03, 2.3567e+03,
        2.3356e+03, 2.3333e+03, 2.3257e+03, 2.3065e+03, 2.2918e+03, 2.2767e+03,
        2.2706e+03, 2.2472e+03, 2.2426e+03, 2.2346e+03, 2.2272e+03, 2.2097e+03,
        2.2030e+03, 2.1829e+03, 2.1664e+03, 2.1613e+03, 2.1507e+03, 2.1326e+03,
        2.1242e+03, 2.1198e+03, 2.1075e+03, 2.0959e+03, 2.0935e+03, 2.0745e+03,
        2.0609e+03, 2.0559e+03, 2.0464e+03, 2.0269e+03, 2.0090e+03, 2.0028e+03,
        1.9896e+03, 1.9874e+03, 1.9777e+03, 1.9644e+03, 1.9521e+03, 1.9443e+03,
        1.9293e+03, 1.9215e+03, 1.9008e+03, 1.8933e+03, 1.8911e+03, 1.8838e+03,
        1.8727e+03, 1.8663e+03, 1.8512e+03, 1.8497e+03, 1.8382e+03, 1.8268e+03,
        1.8179e+03, 1.8138e+03, 1.8055e+03, 1.8019e+03, 1.7857e+03, 1.7615e+03,
        1.7550e+03, 1.7524e+03, 1.7437e+03, 1.7323e+03, 1.7133e+03, 1.7041e+03,
        1.7016e+03, 1.6952e+03, 1.6842e+03, 1.6705e+03, 1.6637e+03, 1.6538e+03,
        1.6439e+03, 1.6375e+03, 1.6200e+03, 1.6100e+03, 1.6002e+03, 1.5948e+03,
        1.5835e+03, 1.5788e+03, 1.5682e+03, 1.5637e+03, 1.5615e+03, 1.5529e+03,
        1.5345e+03, 1.5315e+03, 1.5255e+03, 1.5111e+03, 1.5000e+03, 1.4917e+03,
        1.4907e+03, 1.4828e+03, 1.4657e+03, 1.4600e+03, 1.4550e+03, 1.4429e+03,
        1.4334e+03, 1.4265e+03, 1.4160e+03, 1.4142e+03, 1.4032e+03, 1.3867e+03,
        1.3826e+03, 1.3722e+03, 1.3713e+03, 1.3615e+03, 1.3566e+03, 1.3505e+03,
        1.3497e+03, 1.3364e+03, 1.3336e+03, 1.3222e+03, 1.3128e+03, 1.2978e+03,
        1.2953e+03, 1.2826e+03, 1.2769e+03, 1.2736e+03, 1.2672e+03, 1.2562e+03,
        1.2468e+03, 1.2391e+03, 1.2269e+03, 1.2219e+03, 1.2131e+03, 1.2076e+03,
        1.2058e+03, 1.1950e+03, 1.1895e+03, 1.1824e+03, 1.1700e+03, 1.1627e+03,
        1.1598e+03, 1.1543e+03, 1.1483e+03, 1.1419e+03, 1.1318e+03, 1.1217e+03,
        1.1137e+03, 1.1095e+03, 1.1013e+03, 1.0930e+03, 1.0812e+03, 1.0715e+03,
        1.0596e+03, 1.0557e+03, 1.0508e+03, 1.0430e+03, 1.0386e+03, 1.0354e+03,
        1.0139e+03, 1.0100e+03, 1.0069e+03, 1.0023e+03, 9.9666e+02, 9.9364e+02,
        9.8874e+02, 9.7312e+02, 9.6959e+02, 9.6342e+02, 9.5534e+02, 9.4732e+02,
        9.3972e+02, 9.3318e+02, 9.1697e+02, 9.1513e+02, 9.0676e+02, 9.0258e+02,
        8.9618e+02, 8.7875e+02, 8.7344e+02, 8.7056e+02, 8.6049e+02, 8.5643e+02,
        8.5128e+02, 8.4111e+02, 8.3021e+02, 8.2225e+02, 8.1440e+02, 8.1083e+02,
        8.0162e+02, 7.9153e+02, 7.8630e+02, 7.8485e+02, 7.7793e+02, 7.6997e+02,
        7.6574e+02, 7.6025e+02, 7.4726e+02, 7.4294e+02, 7.4066e+02, 7.2693e+02,
        7.2023e+02, 7.1884e+02, 7.0986e+02, 7.0457e+02, 6.9434e+02, 6.9156e+02,
        6.8201e+02, 6.7975e+02, 6.6708e+02, 6.6530e+02, 6.4532e+02, 6.3524e+02,
        6.2896e+02, 6.2397e+02, 6.1605e+02, 6.0632e+02, 6.0419e+02, 5.9536e+02,
        5.8498e+02, 5.7394e+02, 5.6395e+02, 5.5238e+02, 5.3857e+02, 5.3632e+02,
        5.2857e+02, 5.1543e+02, 5.0280e+02, 4.9751e+02, 4.8249e+02, 4.7490e+02,
        4.6076e+02, 4.4715e+02, 4.3249e+02, 4.1955e+02, 3.9075e+02, 3.5083e+02],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 293]) 

NULL SPACE BASIS :  tensor([[ 0.0042, -0.0286, -0.0021,  ..., -0.0144,  0.0019,  0.0085],
        [-0.0040,  0.0331,  0.0219,  ...,  0.0162, -0.0168, -0.0143],
        [-0.0742,  0.0203, -0.0056,  ..., -0.0109,  0.0126,  0.0093],
        ...,
        [ 0.0526, -0.0214,  0.0816,  ...,  0.0180, -0.0312, -0.0059],
        [-0.0549, -0.0254,  0.0306,  ..., -0.0154,  0.0527, -0.0061],
        [ 0.0042,  0.0038,  0.0002,  ...,  0.0053, -0.0175,  0.0067]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0255, -0.0102, -0.0014,  ...,  0.0009, -0.0012, -0.0007],
        [-0.0102,  0.0309, -0.0094,  ..., -0.0010, -0.0008, -0.0006],
        [-0.0014, -0.0094,  0.0212,  ...,  0.0005, -0.0009, -0.0006],
        ...,
        [ 0.0009, -0.0010,  0.0005,  ...,  0.0314, -0.0091, -0.0041],
        [-0.0012, -0.0008, -0.0009,  ..., -0.0091,  0.0362, -0.0093],
        [-0.0007, -0.0006, -0.0006,  ..., -0.0041, -0.0093,  0.0292]],
       device='cuda:0') 

reserving basis 272/576; cond: 513765.0625, radio:0.0020915393251925707
PARAMETER       :  Parameter containing:
tensor([[[[-2.8070e-02, -1.2164e-02, -2.4715e-02],
          [ 3.2126e-02, -1.6207e-02, -2.4278e-02],
          [ 1.6361e-02, -6.1055e-03,  4.5395e-02]],

         [[ 3.5657e-02, -7.5420e-03, -2.6067e-02],
          [ 3.0995e-02,  6.6582e-03,  3.5303e-02],
          [ 8.6558e-03, -1.9343e-02,  2.8210e-03]],

         [[-7.4287e-03, -2.1956e-02, -1.8706e-02],
          [ 2.5437e-02, -4.5784e-02,  2.2427e-02],
          [-3.3434e-02,  2.9081e-03, -1.1094e-02]],

         ...,

         [[-1.7119e-02,  1.5173e-02,  2.5303e-02],
          [ 1.4588e-02, -1.3551e-02, -1.0158e-02],
          [ 1.4539e-02, -1.4445e-02, -3.2809e-02]],

         [[ 2.4616e-03,  3.1180e-02,  4.2411e-02],
          [ 3.1254e-03,  2.4872e-02,  2.4337e-02],
          [ 6.7601e-03,  1.0158e-02,  3.4042e-02]],

         [[-4.0069e-02, -2.2132e-02, -1.3792e-02],
          [-2.1703e-02, -2.2459e-02,  2.3515e-02],
          [-3.6239e-02, -2.4183e-03, -3.7708e-02]]],


        [[[ 5.7861e-03, -1.7450e-02, -5.0339e-02],
          [-1.1514e-02, -2.3734e-02, -4.4904e-02],
          [-5.0979e-03, -1.8705e-02,  1.2169e-02]],

         [[-1.4059e-02, -5.3395e-03,  9.0066e-03],
          [ 2.0928e-02,  3.9827e-02,  2.5542e-02],
          [-1.1986e-02, -4.7873e-03,  8.4439e-03]],

         [[-2.1313e-02,  1.1405e-02, -2.0212e-02],
          [ 2.8738e-02, -3.2715e-02,  3.8119e-02],
          [-1.2741e-02,  7.4530e-03, -1.0230e-03]],

         ...,

         [[ 4.3136e-03,  4.2081e-02, -1.4054e-02],
          [-1.4357e-03, -9.9960e-04, -2.0391e-02],
          [-2.5906e-02, -1.0433e-02, -1.1981e-02]],

         [[-5.1930e-02,  3.6797e-03,  1.4919e-02],
          [ 1.4034e-02, -1.5460e-03, -1.1097e-02],
          [ 4.8194e-02, -9.3474e-03,  1.7379e-04]],

         [[-4.2896e-02,  9.6201e-03,  1.4464e-02],
          [-1.4539e-02,  2.8150e-02, -9.7895e-03],
          [ 2.2640e-02,  3.6143e-02,  3.7087e-02]]],


        [[[-3.8503e-02,  2.3236e-02,  1.3190e-02],
          [ 2.9889e-02,  1.7225e-02,  3.1292e-02],
          [ 3.3443e-02,  1.5340e-02,  2.2784e-02]],

         [[-5.6622e-03,  5.0965e-03, -7.4518e-03],
          [ 2.3179e-02, -2.1417e-02,  2.0160e-02],
          [-8.5044e-03, -3.7032e-02, -2.6364e-02]],

         [[ 4.1648e-02, -1.5766e-02, -1.0212e-02],
          [ 1.1604e-02,  3.6966e-02, -6.2793e-03],
          [-2.0082e-02,  1.3168e-02,  3.8093e-02]],

         ...,

         [[ 1.8942e-02, -2.4451e-02,  2.3479e-02],
          [ 4.3275e-02,  1.2474e-02, -1.9822e-02],
          [ 3.4501e-02,  2.4796e-02, -1.3799e-02]],

         [[ 2.0973e-02, -4.0429e-02, -1.7365e-02],
          [-3.0043e-02,  2.8027e-02,  3.8309e-03],
          [ 2.8457e-03,  2.9603e-02,  2.2012e-02]],

         [[-2.3570e-02,  7.4488e-03,  7.9949e-03],
          [-2.4429e-02,  3.8977e-03, -3.4249e-02],
          [ 3.9042e-03,  1.6898e-02,  5.0872e-03]]],


        ...,


        [[[-1.5739e-02, -2.6792e-02, -3.5934e-02],
          [-9.7635e-04,  1.2334e-02, -4.9126e-02],
          [-1.0753e-02, -9.5498e-03, -3.8875e-02]],

         [[-2.1467e-02,  2.6791e-02,  3.8755e-02],
          [-1.5272e-02, -3.0133e-02, -3.3225e-02],
          [-1.1710e-02,  3.8309e-02,  4.2467e-02]],

         [[-2.5840e-03, -1.8196e-02,  3.9090e-02],
          [-5.3162e-03,  3.0905e-02,  3.9058e-02],
          [ 9.0783e-04,  4.2412e-02,  1.7289e-02]],

         ...,

         [[ 3.0634e-02, -4.3622e-02, -3.5981e-02],
          [ 1.5713e-02,  2.1227e-02, -9.2750e-03],
          [ 1.5676e-03, -4.7721e-03,  8.5109e-03]],

         [[ 2.2557e-02,  8.3174e-05,  8.2850e-03],
          [ 2.6718e-02,  1.0190e-02, -4.1248e-02],
          [ 4.5245e-02,  3.7010e-02, -3.8481e-02]],

         [[ 2.8842e-03,  4.2371e-02, -4.6963e-04],
          [ 3.0546e-02, -3.3947e-02, -6.5060e-03],
          [ 2.8481e-03, -3.3333e-02,  1.8159e-02]]],


        [[[ 3.6461e-02, -1.1447e-02,  4.5651e-02],
          [-4.8215e-02,  1.9895e-02, -1.4158e-02],
          [ 3.0855e-02, -1.3349e-02, -2.0500e-02]],

         [[ 2.6673e-02, -2.4306e-02, -1.0662e-02],
          [ 2.4519e-02,  8.9445e-03, -2.1978e-02],
          [ 5.1556e-02, -2.3675e-02,  4.4735e-02]],

         [[-3.0837e-02, -3.6544e-02,  2.7058e-02],
          [ 1.6309e-02, -3.7255e-02, -5.3742e-02],
          [-3.8705e-02,  4.5519e-04, -1.1388e-02]],

         ...,

         [[-1.9995e-02,  1.8847e-02,  3.2078e-02],
          [-3.5007e-02,  9.3530e-03, -9.0035e-03],
          [ 3.1469e-02, -3.3732e-02,  3.6291e-02]],

         [[-4.8219e-03, -1.6354e-02, -3.1988e-02],
          [ 2.2905e-02, -1.0556e-02,  1.2249e-02],
          [-5.0975e-04,  3.4756e-02,  1.3166e-02]],

         [[-3.0989e-02, -1.5236e-02, -4.1783e-02],
          [ 2.7531e-02, -4.1764e-02, -2.9660e-02],
          [-3.6273e-02,  4.3764e-02,  1.7192e-02]]],


        [[[-1.0957e-03, -5.4277e-03,  1.6308e-02],
          [-3.9015e-02, -5.8627e-03, -7.6871e-03],
          [ 2.4609e-02,  4.6194e-03,  3.5769e-02]],

         [[-2.3624e-02,  2.7795e-02,  1.5674e-02],
          [ 3.9198e-02,  2.5893e-02,  1.8314e-02],
          [-4.4484e-03,  2.3842e-02,  1.2384e-02]],

         [[-1.4457e-02, -2.6015e-02,  3.5094e-02],
          [-3.7363e-02,  1.9847e-02,  1.6783e-02],
          [ 2.1561e-03, -2.7280e-02, -3.3146e-02]],

         ...,

         [[ 5.1546e-02, -2.0021e-02,  2.7562e-02],
          [ 3.2911e-02, -3.9218e-02,  2.7237e-02],
          [-8.1254e-03, -4.1599e-02, -1.9941e-02]],

         [[-1.6298e-02, -8.0663e-04, -1.8703e-02],
          [-3.1220e-02, -4.8765e-02, -3.8836e-02],
          [ 2.2751e-02,  1.8482e-02, -1.4345e-02]],

         [[-8.9791e-03, -1.3620e-02,  4.7082e-02],
          [ 2.3789e-02,  1.8225e-02,  2.7948e-03],
          [ 1.1765e-02,  3.0862e-02, -1.6629e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([2.1252e+07, 1.0565e+06, 9.6449e+05, 6.2363e+05, 4.4474e+05, 2.5370e+05,
        2.3651e+05, 1.5856e+05, 1.4281e+05, 1.1127e+05, 7.1178e+04, 5.9215e+04,
        5.4690e+04, 4.8847e+04, 4.7417e+04, 4.4702e+04, 4.4232e+04, 4.1493e+04,
        3.7687e+04, 3.4436e+04, 3.2318e+04, 3.0484e+04, 2.9210e+04, 2.6249e+04,
        2.5501e+04, 2.4166e+04, 2.2888e+04, 2.2204e+04, 2.0015e+04, 1.9241e+04,
        1.8242e+04, 1.7860e+04, 1.6996e+04, 1.5869e+04, 1.4900e+04, 1.4293e+04,
        1.3184e+04, 1.2964e+04, 1.1899e+04, 1.1431e+04, 1.0473e+04, 9.9783e+03,
        9.9335e+03, 9.6177e+03, 9.3742e+03, 8.9984e+03, 8.7578e+03, 8.1014e+03,
        7.9386e+03, 7.8269e+03, 7.5064e+03, 6.9896e+03, 6.8023e+03, 6.7372e+03,
        6.5080e+03, 6.3892e+03, 6.2033e+03, 5.9704e+03, 5.8605e+03, 5.6970e+03,
        5.6078e+03, 5.5031e+03, 5.3722e+03, 5.2874e+03, 5.2132e+03, 4.9942e+03,
        4.9463e+03, 4.7758e+03, 4.7046e+03, 4.5536e+03, 4.3784e+03, 4.3272e+03,
        4.2752e+03, 4.2254e+03, 4.0562e+03, 3.9709e+03, 3.9309e+03, 3.9169e+03,
        3.7832e+03, 3.6883e+03, 3.6447e+03, 3.5342e+03, 3.4340e+03, 3.3674e+03,
        3.3342e+03, 3.2580e+03, 3.2153e+03, 3.1509e+03, 3.0998e+03, 3.0627e+03,
        2.9808e+03, 2.9662e+03, 2.8826e+03, 2.8686e+03, 2.8107e+03, 2.7455e+03,
        2.7067e+03, 2.6542e+03, 2.6124e+03, 2.5875e+03, 2.5531e+03, 2.4483e+03,
        2.3958e+03, 2.3701e+03, 2.3511e+03, 2.3433e+03, 2.2973e+03, 2.2817e+03,
        2.2330e+03, 2.2278e+03, 2.1763e+03, 2.1500e+03, 2.1434e+03, 2.1099e+03,
        2.0616e+03, 2.0033e+03, 1.9887e+03, 1.9547e+03, 1.9434e+03, 1.9097e+03,
        1.8956e+03, 1.8870e+03, 1.8701e+03, 1.8294e+03, 1.8242e+03, 1.7989e+03,
        1.7871e+03, 1.7550e+03, 1.7405e+03, 1.7248e+03, 1.6869e+03, 1.6603e+03,
        1.6534e+03, 1.6314e+03, 1.6042e+03, 1.6014e+03, 1.5752e+03, 1.5558e+03,
        1.5494e+03, 1.5297e+03, 1.5152e+03, 1.5043e+03, 1.4665e+03, 1.4551e+03,
        1.4522e+03, 1.4351e+03, 1.4085e+03, 1.3958e+03, 1.3888e+03, 1.3796e+03,
        1.3663e+03, 1.3345e+03, 1.3331e+03, 1.3179e+03, 1.2914e+03, 1.2871e+03,
        1.2713e+03, 1.2580e+03, 1.2464e+03, 1.2446e+03, 1.2384e+03, 1.2195e+03,
        1.1891e+03, 1.1837e+03, 1.1780e+03, 1.1665e+03, 1.1456e+03, 1.1388e+03,
        1.1255e+03, 1.1205e+03, 1.1151e+03, 1.0983e+03, 1.0962e+03, 1.0850e+03,
        1.0727e+03, 1.0582e+03, 1.0436e+03, 1.0311e+03, 1.0274e+03, 1.0176e+03,
        1.0091e+03, 1.0020e+03, 9.9598e+02, 9.8875e+02, 9.7828e+02, 9.7045e+02,
        9.5905e+02, 9.5315e+02, 9.4516e+02, 9.3849e+02, 9.2252e+02, 9.1059e+02,
        9.0811e+02, 8.9636e+02, 8.9134e+02, 8.8664e+02, 8.7074e+02, 8.6309e+02,
        8.5688e+02, 8.4709e+02, 8.4464e+02, 8.3615e+02, 8.3080e+02, 8.2300e+02,
        8.2241e+02, 8.1737e+02, 8.0783e+02, 8.0221e+02, 7.9793e+02, 7.8550e+02,
        7.8122e+02, 7.7509e+02, 7.7056e+02, 7.6422e+02, 7.5923e+02, 7.5456e+02,
        7.4721e+02, 7.3991e+02, 7.3801e+02, 7.3376e+02, 7.3095e+02, 7.2679e+02,
        7.2317e+02, 7.2076e+02, 7.1633e+02, 7.0445e+02, 7.0025e+02, 6.9702e+02,
        6.8948e+02, 6.8555e+02, 6.7489e+02, 6.7274e+02, 6.6682e+02, 6.6290e+02,
        6.6212e+02, 6.5087e+02, 6.4375e+02, 6.4208e+02, 6.3828e+02, 6.3599e+02,
        6.3457e+02, 6.2659e+02, 6.2403e+02, 6.1745e+02, 6.1584e+02, 6.0765e+02,
        6.0602e+02, 5.9924e+02, 5.9691e+02, 5.9239e+02, 5.8573e+02, 5.8205e+02,
        5.7713e+02, 5.6938e+02, 5.6855e+02, 5.6343e+02, 5.5987e+02, 5.5758e+02,
        5.5378e+02, 5.4634e+02, 5.4105e+02, 5.4011e+02, 5.3635e+02, 5.3399e+02,
        5.3205e+02, 5.2437e+02, 5.1960e+02, 5.1683e+02, 5.1643e+02, 5.1411e+02,
        5.1222e+02, 5.0695e+02, 5.0561e+02, 5.0339e+02, 4.9994e+02, 4.9802e+02,
        4.9155e+02, 4.8818e+02, 4.8731e+02, 4.8441e+02, 4.8107e+02, 4.7661e+02,
        4.7621e+02, 4.7275e+02, 4.6771e+02, 4.6520e+02, 4.6245e+02, 4.5762e+02,
        4.5494e+02, 4.5396e+02, 4.5094e+02, 4.4783e+02, 4.4586e+02, 4.4220e+02,
        4.3841e+02, 4.3538e+02, 4.3215e+02, 4.3165e+02, 4.2978e+02, 4.2636e+02,
        4.2407e+02, 4.2013e+02, 4.1720e+02, 4.1434e+02, 4.1279e+02, 4.0827e+02,
        4.0773e+02, 4.0650e+02, 4.0450e+02, 4.0141e+02, 3.9698e+02, 3.9547e+02,
        3.9335e+02, 3.9043e+02, 3.8817e+02, 3.8602e+02, 3.8410e+02, 3.8165e+02,
        3.8038e+02, 3.8008e+02, 3.7723e+02, 3.7511e+02, 3.7279e+02, 3.7172e+02,
        3.6896e+02, 3.6726e+02, 3.6325e+02, 3.6051e+02, 3.5625e+02, 3.5508e+02,
        3.5385e+02, 3.5303e+02, 3.5120e+02, 3.4937e+02, 3.4787e+02, 3.4435e+02,
        3.4065e+02, 3.3933e+02, 3.3774e+02, 3.3467e+02, 3.3409e+02, 3.3316e+02,
        3.3025e+02, 3.2850e+02, 3.2720e+02, 3.2449e+02, 3.2174e+02, 3.1979e+02,
        3.1807e+02, 3.1607e+02, 3.1505e+02, 3.1312e+02, 3.1043e+02, 3.0862e+02,
        3.0725e+02, 3.0384e+02, 3.0322e+02, 3.0228e+02, 2.9919e+02, 2.9833e+02,
        2.9661e+02, 2.9601e+02, 2.9464e+02, 2.9318e+02, 2.9151e+02, 2.9052e+02,
        2.8772e+02, 2.8658e+02, 2.8490e+02, 2.8344e+02, 2.8252e+02, 2.7985e+02,
        2.7880e+02, 2.7591e+02, 2.7276e+02, 2.7218e+02, 2.7092e+02, 2.6956e+02,
        2.6666e+02, 2.6589e+02, 2.6538e+02, 2.6387e+02, 2.6194e+02, 2.6078e+02,
        2.5956e+02, 2.5910e+02, 2.5808e+02, 2.5708e+02, 2.5578e+02, 2.5505e+02,
        2.5222e+02, 2.4975e+02, 2.4895e+02, 2.4724e+02, 2.4608e+02, 2.4576e+02,
        2.4399e+02, 2.4203e+02, 2.4167e+02, 2.4066e+02, 2.3871e+02, 2.3692e+02,
        2.3561e+02, 2.3494e+02, 2.3292e+02, 2.3132e+02, 2.2943e+02, 2.2794e+02,
        2.2681e+02, 2.2610e+02, 2.2482e+02, 2.2456e+02, 2.2345e+02, 2.2162e+02,
        2.2061e+02, 2.2012e+02, 2.1937e+02, 2.1731e+02, 2.1625e+02, 2.1504e+02,
        2.1398e+02, 2.1325e+02, 2.1132e+02, 2.1024e+02, 2.0920e+02, 2.0871e+02,
        2.0727e+02, 2.0591e+02, 2.0413e+02, 2.0346e+02, 2.0270e+02, 2.0098e+02,
        1.9995e+02, 1.9853e+02, 1.9816e+02, 1.9747e+02, 1.9709e+02, 1.9500e+02,
        1.9448e+02, 1.9403e+02, 1.9206e+02, 1.9167e+02, 1.8971e+02, 1.8950e+02,
        1.8880e+02, 1.8791e+02, 1.8708e+02, 1.8646e+02, 1.8475e+02, 1.8238e+02,
        1.8188e+02, 1.8006e+02, 1.7985e+02, 1.7892e+02, 1.7691e+02, 1.7667e+02,
        1.7511e+02, 1.7267e+02, 1.7247e+02, 1.7115e+02, 1.7045e+02, 1.6940e+02,
        1.6896e+02, 1.6811e+02, 1.6583e+02, 1.6444e+02, 1.6399e+02, 1.6313e+02,
        1.6284e+02, 1.6187e+02, 1.6135e+02, 1.6013e+02, 1.5866e+02, 1.5718e+02,
        1.5685e+02, 1.5502e+02, 1.5429e+02, 1.5273e+02, 1.5260e+02, 1.5198e+02,
        1.5095e+02, 1.4964e+02, 1.4869e+02, 1.4758e+02, 1.4688e+02, 1.4460e+02,
        1.4357e+02, 1.4255e+02, 1.4186e+02, 1.4090e+02, 1.4015e+02, 1.3892e+02,
        1.3737e+02, 1.3632e+02, 1.3553e+02, 1.3484e+02, 1.3350e+02, 1.3200e+02,
        1.3127e+02, 1.3033e+02, 1.3002e+02, 1.2878e+02, 1.2766e+02, 1.2709e+02,
        1.2571e+02, 1.2424e+02, 1.2405e+02, 1.2280e+02, 1.2192e+02, 1.2033e+02,
        1.1989e+02, 1.1907e+02, 1.1854e+02, 1.1793e+02, 1.1637e+02, 1.1606e+02,
        1.1544e+02, 1.1406e+02, 1.1317e+02, 1.1287e+02, 1.1125e+02, 1.1063e+02,
        1.0961e+02, 1.0924e+02, 1.0799e+02, 1.0672e+02, 1.0657e+02, 1.0572e+02,
        1.0517e+02, 1.0360e+02, 1.0279e+02, 1.0187e+02, 1.0102e+02, 1.0066e+02,
        9.9838e+01, 9.9062e+01, 9.8089e+01, 9.7357e+01, 9.5976e+01, 9.5318e+01,
        9.4595e+01, 9.2893e+01, 9.2577e+01, 9.1413e+01, 9.0450e+01, 8.9798e+01,
        8.9013e+01, 8.7191e+01, 8.6741e+01, 8.6327e+01, 8.4931e+01, 8.2889e+01,
        8.1608e+01, 8.0812e+01, 7.9752e+01, 7.7849e+01, 7.6619e+01, 7.5923e+01,
        7.4496e+01, 7.3092e+01, 7.1437e+01, 7.0802e+01, 6.8317e+01, 6.6800e+01,
        6.6506e+01, 6.4059e+01, 6.2645e+01, 6.2246e+01, 6.0532e+01, 5.8906e+01,
        5.7030e+01, 5.6411e+01, 5.5168e+01, 5.3580e+01, 4.4581e+01, 4.1365e+01],
       device='cuda:0') 

NULL SPACE DIM :  torch.Size([576, 272]) 

NULL SPACE BASIS :  tensor([[ 0.0177,  0.0271, -0.0270,  ..., -0.0029, -0.0001,  0.0110],
        [-0.0321, -0.0452,  0.0479,  ...,  0.0057, -0.0021, -0.0125],
        [ 0.0312,  0.0025, -0.0612,  ..., -0.0040,  0.0057,  0.0025],
        ...,
        [ 0.0312, -0.0028, -0.0086,  ..., -0.0084, -0.0046,  0.0019],
        [ 0.0353,  0.0304, -0.0687,  ...,  0.0055,  0.0006,  0.0147],
        [-0.0104,  0.0495,  0.0099,  ..., -0.0022, -0.0006, -0.0119]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.9114e-02, -1.6883e-02,  2.6543e-03,  ..., -1.3066e-03,
          1.4992e-03,  1.1073e-04],
        [-1.6883e-02,  3.0360e-02, -1.3929e-02,  ..., -7.9134e-05,
         -6.1927e-04, -6.4086e-04],
        [ 2.6543e-03, -1.3929e-02,  1.5508e-02,  ...,  1.2569e-03,
         -2.4575e-04,  1.4470e-04],
        ...,
        [-1.3066e-03, -7.9134e-05,  1.2569e-03,  ...,  1.2732e-02,
         -7.4586e-03, -5.2804e-04],
        [ 1.4992e-03, -6.1927e-04, -2.4575e-04,  ..., -7.4586e-03,
          1.7189e-02, -6.4635e-03],
        [ 1.1073e-04, -6.4086e-04,  1.4470e-04,  ..., -5.2804e-04,
         -6.4635e-03,  1.2114e-02]], device='cuda:0') 

reserving basis 698/1152; cond: 398062.0625, radio:0.007038325071334839
PARAMETER       :  Parameter containing:
tensor([[[[ 1.3027e-02,  2.9337e-02, -1.9381e-02],
          [-7.2753e-04, -1.6215e-02,  1.0499e-02],
          [ 1.7664e-02,  1.8100e-02, -3.5703e-03]],

         [[-1.9406e-02, -2.2899e-02, -2.5637e-02],
          [-2.2512e-02, -2.2957e-02,  7.1799e-03],
          [-1.5945e-02,  8.1825e-03, -2.8756e-02]],

         [[ 1.0964e-03, -1.2701e-02,  4.6770e-03],
          [ 7.7117e-03,  1.5260e-02, -1.5553e-02],
          [-1.5375e-02, -2.5790e-02,  2.6784e-02]],

         ...,

         [[-1.2743e-02,  1.9635e-03,  5.7693e-03],
          [-2.9032e-02, -1.9473e-02, -3.7141e-04],
          [ 8.2911e-03,  1.6718e-02,  2.8887e-02]],

         [[ 1.0215e-02, -3.2813e-02, -1.8145e-02],
          [ 2.3099e-02,  1.5803e-02,  8.9786e-04],
          [ 1.5720e-02, -1.4842e-02,  3.4872e-02]],

         [[ 1.8869e-02,  6.8926e-03,  1.5951e-02],
          [-2.2307e-02, -3.3267e-02, -2.5609e-02],
          [-2.8921e-02,  1.9767e-02, -1.5359e-02]]],


        [[[ 2.5749e-02,  1.5926e-03,  7.8680e-03],
          [-1.4807e-02, -1.1198e-02,  2.2479e-02],
          [-2.6320e-02, -2.8434e-02, -1.4794e-02]],

         [[ 8.9125e-03, -2.2750e-02, -2.1818e-02],
          [ 9.8615e-03, -1.1664e-02,  2.2845e-03],
          [-2.6336e-02,  7.1156e-03, -1.2107e-02]],

         [[ 5.6887e-04, -1.7753e-02, -1.9208e-02],
          [-4.3511e-03,  1.4084e-02, -2.9310e-02],
          [ 2.5640e-02,  1.7795e-02,  2.1714e-03]],

         ...,

         [[ 4.9271e-03,  1.3417e-02, -4.6390e-03],
          [-2.0775e-02,  2.5440e-02,  2.6418e-02],
          [ 1.5252e-02,  1.9988e-02,  2.0583e-02]],

         [[ 2.4648e-02,  1.9471e-03,  7.9744e-03],
          [-8.2136e-03, -2.1627e-02, -2.6271e-02],
          [ 4.2060e-02, -1.9582e-02, -2.8606e-02]],

         [[-1.2536e-02, -1.6401e-02, -3.0510e-02],
          [-2.1443e-02, -1.2281e-02, -7.1661e-03],
          [ 8.9311e-03, -1.0327e-02, -1.2163e-03]]],


        [[[-1.6425e-02,  1.8691e-02, -2.8603e-02],
          [-4.4238e-03,  1.8079e-02, -1.9211e-03],
          [-6.5763e-04,  2.7488e-02,  1.6423e-02]],

         [[ 4.5133e-03, -1.2090e-02,  2.4161e-02],
          [-4.6030e-03, -1.0041e-02,  4.6960e-03],
          [-2.0677e-02,  5.7529e-03,  9.6599e-03]],

         [[-1.7553e-02, -1.5048e-02, -1.2426e-02],
          [-2.7171e-02, -2.7710e-02, -3.1436e-02],
          [-1.4286e-02, -1.8352e-02,  1.6293e-02]],

         ...,

         [[ 2.4123e-02,  7.3775e-03, -1.7639e-02],
          [-3.1936e-02,  1.4777e-02,  3.7079e-03],
          [ 1.9591e-02, -1.6619e-02, -3.3508e-02]],

         [[-4.0703e-02,  1.4261e-03, -1.1484e-02],
          [-1.7016e-02,  1.6987e-02,  8.0940e-03],
          [-1.3864e-02,  8.3036e-03, -1.4147e-02]],

         [[-1.4156e-02, -1.9225e-02,  1.0795e-02],
          [-4.4987e-03, -9.9923e-03,  5.4701e-03],
          [ 1.0205e-02, -1.8180e-02,  3.0611e-02]]],


        ...,


        [[[ 2.8547e-02,  1.0465e-03,  3.2645e-02],
          [ 2.4723e-02,  1.6589e-02, -2.9508e-02],
          [ 3.3204e-03, -1.0555e-03,  2.7477e-02]],

         [[ 8.9467e-03, -7.7852e-03,  1.2412e-02],
          [ 4.7625e-03, -2.4390e-02,  4.6524e-03],
          [-1.4674e-02, -7.4176e-03,  1.6160e-02]],

         [[ 1.7196e-02,  1.2461e-02, -8.4947e-03],
          [-1.3186e-02,  2.7825e-02,  6.2499e-03],
          [ 6.4713e-03, -7.0734e-03, -1.3396e-02]],

         ...,

         [[-2.9780e-02,  1.3141e-02, -1.5973e-02],
          [ 7.1651e-03, -2.7736e-02, -1.6381e-02],
          [-1.9440e-02,  2.8590e-03,  7.6757e-03]],

         [[ 1.8783e-02,  1.8103e-02,  2.7607e-02],
          [-2.2844e-02,  5.5485e-05,  1.5365e-02],
          [-5.7125e-03,  9.7888e-03,  4.4419e-03]],

         [[-1.4029e-02, -2.1388e-03,  2.3898e-02],
          [ 1.5957e-02,  9.3598e-03,  3.1030e-02],
          [ 7.9262e-03,  1.0049e-02,  1.9932e-02]]],


        [[[-2.1974e-03, -1.0402e-02,  6.1819e-03],
          [-2.7599e-02, -7.7734e-03,  2.0435e-02],
          [ 1.0635e-02, -2.6565e-02, -2.1762e-02]],

         [[-1.3181e-02, -1.5086e-02,  2.4584e-02],
          [-1.9672e-02, -3.4273e-02, -4.5348e-03],
          [-2.5426e-02, -1.0335e-03, -2.2950e-02]],

         [[-1.6719e-02, -2.6976e-03, -2.1327e-02],
          [-1.1754e-02,  1.5533e-02,  2.3635e-02],
          [ 7.5621e-03, -2.0633e-02,  1.9777e-02]],

         ...,

         [[-1.4483e-02,  1.1495e-02,  5.3717e-03],
          [-3.0367e-02, -2.1841e-02, -1.4498e-02],
          [-2.4406e-02, -5.1268e-02, -1.8139e-02]],

         [[-2.3540e-03, -7.0550e-03, -1.2530e-02],
          [-3.0555e-02,  3.0617e-02,  2.9646e-02],
          [-3.7275e-02,  2.2757e-02, -8.0002e-03]],

         [[ 2.0244e-02, -2.4044e-02, -9.8051e-03],
          [ 2.8589e-02, -1.6737e-02,  3.1279e-02],
          [-5.4877e-03,  1.7591e-02,  6.1014e-04]]],


        [[[ 4.6317e-03, -2.6411e-02,  2.8573e-02],
          [-2.3471e-02,  5.5793e-03, -3.5539e-02],
          [-7.1018e-04, -2.6015e-02, -1.7466e-02]],

         [[ 1.6144e-02,  1.6100e-02, -1.4689e-02],
          [ 5.1908e-03,  1.4810e-02,  9.9275e-03],
          [-1.5775e-02, -2.5566e-02,  2.3698e-02]],

         [[-9.9291e-03, -8.9917e-03,  6.4553e-03],
          [-1.9714e-02,  8.5400e-05,  2.4796e-02],
          [-1.8185e-02,  2.4696e-02, -4.5502e-03]],

         ...,

         [[ 2.7053e-02,  9.7810e-03, -8.5966e-03],
          [-1.6562e-02,  2.9920e-02,  1.3803e-02],
          [ 2.7781e-02, -1.1585e-02, -2.5063e-03]],

         [[-1.5765e-02,  9.6864e-03, -4.5594e-03],
          [-2.2831e-02,  3.1622e-02,  2.7837e-02],
          [ 1.5682e-02,  1.2750e-02,  7.3306e-03]],

         [[ 1.0024e-02,  3.7975e-02, -1.8583e-02],
          [-1.4379e-02,  1.2773e-02,  3.2297e-02],
          [ 4.5862e-03, -2.5007e-02, -1.7990e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.1327e+07, 1.6091e+06, 1.5211e+06,  ..., 1.3954e+02, 1.2261e+02,
        1.0382e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 698]) 

NULL SPACE BASIS :  tensor([[-0.0177, -0.0059, -0.0305,  ..., -0.0166, -0.0114, -0.0086],
        [ 0.0457, -0.0345, -0.0052,  ...,  0.0043,  0.0129,  0.0026],
        [ 0.0300,  0.0286, -0.0037,  ...,  0.0144, -0.0028,  0.0021],
        ...,
        [ 0.0026, -0.0343,  0.0613,  ..., -0.0038,  0.0059,  0.0058],
        [ 0.0314, -0.0211,  0.0488,  ..., -0.0036, -0.0159,  0.0055],
        [ 0.0351, -0.0204, -0.0577,  ...,  0.0174,  0.0134, -0.0093]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.8887e-02, -1.3627e-03, -6.0099e-04,  ..., -2.8662e-05,
          5.7346e-04,  2.3917e-05],
        [-1.3627e-03,  3.0706e-02, -9.4793e-04,  ...,  7.0856e-05,
          1.5039e-04,  5.4057e-04],
        [-6.0099e-04, -9.4793e-04,  2.9961e-02,  ..., -1.6810e-04,
          2.8308e-04,  1.8290e-04],
        ...,
        [-2.8662e-05,  7.0856e-05, -1.6810e-04,  ...,  2.1978e-02,
         -3.3025e-03, -1.5070e-03],
        [ 5.7346e-04,  1.5039e-04,  2.8308e-04,  ..., -3.3025e-03,
          2.3326e-02, -2.9268e-03],
        [ 2.3917e-05,  5.4057e-04,  1.8290e-04,  ..., -1.5070e-03,
         -2.9268e-03,  2.4058e-02]], device='cuda:0') 

reserving basis 44/64; cond: 7196.97607421875, radio:0.017850395292043686
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0140]],

         [[ 0.1006]],

         [[ 0.0577]],

         ...,

         [[-0.0897]],

         [[-0.0852]],

         [[-0.0369]]],


        [[[ 0.0193]],

         [[-0.0233]],

         [[-0.0964]],

         ...,

         [[-0.0353]],

         [[ 0.0100]],

         [[ 0.0288]]],


        [[[ 0.0243]],

         [[-0.0726]],

         [[-0.0154]],

         ...,

         [[-0.1441]],

         [[ 0.0993]],

         [[ 0.0281]]],


        ...,


        [[[ 0.0203]],

         [[-0.0882]],

         [[-0.1059]],

         ...,

         [[-0.0390]],

         [[ 0.0609]],

         [[-0.0084]]],


        [[[ 0.0301]],

         [[-0.0489]],

         [[ 0.1360]],

         ...,

         [[ 0.0014]],

         [[ 0.0326]],

         [[ 0.0646]]],


        [[[-0.0031]],

         [[-0.0030]],

         [[ 0.0829]],

         ...,

         [[ 0.0062]],

         [[ 0.0550]],

         [[ 0.0915]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([2.6627e+06, 1.4894e+05, 1.2481e+05, 5.8325e+04, 3.8644e+04, 3.0800e+04,
        2.1894e+04, 1.8305e+04, 1.1696e+04, 1.0303e+04, 9.2953e+03, 8.1399e+03,
        7.5997e+03, 6.7251e+03, 6.2006e+03, 5.8104e+03, 5.2691e+03, 5.0042e+03,
        4.4273e+03, 4.1936e+03, 3.5731e+03, 3.3734e+03, 2.8941e+03, 2.7523e+03,
        2.6284e+03, 2.5329e+03, 2.4263e+03, 2.1195e+03, 2.0224e+03, 1.8876e+03,
        1.7560e+03, 1.6905e+03, 1.6572e+03, 1.5793e+03, 1.4570e+03, 1.3974e+03,
        1.3321e+03, 1.2918e+03, 1.2527e+03, 1.1924e+03, 1.1568e+03, 1.1213e+03,
        1.0467e+03, 9.9035e+02, 9.1350e+02, 9.0060e+02, 8.4065e+02, 7.9962e+02,
        7.6790e+02, 7.5970e+02, 7.3801e+02, 6.9814e+02, 6.8113e+02, 6.5714e+02,
        6.1858e+02, 5.9858e+02, 5.8826e+02, 5.4415e+02, 5.2281e+02, 5.0443e+02,
        4.6449e+02, 4.4135e+02, 4.1973e+02, 3.6997e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([64, 44]) 

NULL SPACE BASIS :  tensor([[ 0.1843,  0.1219,  0.1799,  ..., -0.0096, -0.1509,  0.0971],
        [ 0.0473,  0.0881, -0.0507,  ...,  0.0497,  0.0308,  0.0431],
        [ 0.0271,  0.1331,  0.0201,  ..., -0.2351,  0.3008, -0.0316],
        ...,
        [ 0.1313, -0.1108, -0.0247,  ..., -0.0186,  0.0136, -0.0146],
        [-0.0222, -0.0895,  0.0813,  ..., -0.0243, -0.0178,  0.0357],
        [-0.2486, -0.0177,  0.0087,  ..., -0.0092,  0.0080, -0.0358]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0934,  0.0046,  0.0202,  ...,  0.0012, -0.0021, -0.0128],
        [ 0.0046,  0.1238, -0.0097,  ...,  0.0103, -0.0087, -0.0043],
        [ 0.0202, -0.0097,  0.1096,  ..., -0.0072,  0.0030,  0.0061],
        ...,
        [ 0.0012,  0.0103, -0.0072,  ...,  0.0345, -0.0312, -0.0035],
        [-0.0021, -0.0087,  0.0030,  ..., -0.0312,  0.1160,  0.0024],
        [-0.0128, -0.0043,  0.0061,  ..., -0.0035,  0.0024,  0.0849]],
       device='cuda:0') 

reserving basis 777/1152; cond: 305267.5625, radio:0.009143942035734653
PARAMETER       :  Parameter containing:
tensor([[[[ 1.1815e-02, -4.1654e-03,  9.0154e-03],
          [-2.7467e-03,  1.6093e-02,  1.1362e-02],
          [-9.8179e-03, -1.1989e-02, -1.2173e-02]],

         [[ 5.5804e-03, -1.6110e-02, -2.1741e-02],
          [ 2.7213e-02, -1.2959e-03,  1.1020e-03],
          [ 6.3980e-03,  1.9928e-02, -1.6533e-02]],

         [[ 7.3699e-03, -3.9195e-05,  5.6997e-03],
          [ 1.4891e-02,  1.6087e-02,  1.5042e-02],
          [-2.4430e-02, -1.4773e-02,  1.2202e-02]],

         ...,

         [[ 1.6255e-02,  5.7007e-03, -1.2630e-03],
          [-1.0927e-02,  2.7203e-02,  6.5005e-03],
          [ 1.4824e-02, -1.4339e-03, -6.4657e-03]],

         [[ 8.6594e-03,  1.2573e-02,  7.2545e-03],
          [ 2.5812e-03,  2.3314e-02,  2.1456e-02],
          [ 2.3609e-02,  2.7701e-02, -6.2000e-03]],

         [[ 2.6065e-04,  2.6551e-02,  4.6896e-02],
          [-1.7871e-02,  1.4311e-02,  1.9706e-02],
          [ 6.2045e-03, -2.0559e-02,  1.6728e-02]]],


        [[[-1.0193e-02,  7.2008e-03, -2.1866e-02],
          [ 1.3268e-02, -7.9123e-03,  3.8721e-03],
          [ 3.1701e-03, -2.3259e-02,  2.0274e-02]],

         [[-3.7056e-03,  1.7584e-02,  2.3890e-02],
          [ 2.1447e-02,  1.7362e-02, -5.6191e-04],
          [-9.6034e-03,  1.7047e-02,  1.8539e-02]],

         [[ 1.7551e-02,  1.6501e-02,  8.8035e-03],
          [ 2.5816e-02, -6.1953e-03,  2.0562e-02],
          [-1.5923e-02,  1.1741e-03,  8.5385e-03]],

         ...,

         [[-3.0396e-03,  1.7534e-02, -1.6899e-02],
          [-2.8007e-02, -2.3152e-02, -3.1777e-02],
          [-2.3105e-02, -2.3676e-02, -2.4230e-02]],

         [[-2.2095e-02,  1.3169e-03,  2.9945e-02],
          [-1.5534e-02, -1.7423e-02, -2.9247e-03],
          [ 1.3328e-02, -1.1395e-02, -1.8513e-02]],

         [[ 2.3538e-02, -2.3621e-02,  1.4411e-02],
          [-7.9775e-03, -1.3514e-02, -2.7444e-02],
          [ 2.6167e-03,  1.5619e-02, -3.1979e-02]]],


        [[[ 1.8996e-02,  2.0353e-02,  1.4005e-02],
          [-5.6584e-03, -2.9378e-02,  2.7626e-02],
          [-1.1836e-02, -1.9491e-03, -2.5058e-02]],

         [[-9.6736e-03,  6.0635e-03, -1.4279e-02],
          [ 3.2164e-02,  6.0342e-03, -5.3127e-03],
          [-7.0968e-03, -1.9309e-02,  1.7567e-02]],

         [[-3.0734e-02, -1.5090e-02, -2.2980e-02],
          [-1.5854e-02, -1.4100e-02,  1.9092e-03],
          [ 2.3911e-02, -1.3039e-02, -8.1876e-03]],

         ...,

         [[-3.5635e-02, -7.6651e-03, -2.2422e-02],
          [-7.0065e-03, -3.4795e-02, -2.6563e-02],
          [-2.9855e-03, -2.7328e-02, -3.6288e-04]],

         [[ 2.4992e-02, -1.3542e-02,  5.7872e-03],
          [ 1.9706e-02, -1.7880e-02, -1.9701e-02],
          [-2.3813e-03, -2.9236e-02,  1.0763e-02]],

         [[ 1.0777e-02,  1.3944e-02, -3.0847e-04],
          [ 6.6274e-03,  1.0457e-02, -1.1350e-02],
          [ 4.2936e-03,  2.3393e-02, -1.2621e-02]]],


        ...,


        [[[-1.5429e-02, -2.3683e-02, -3.4666e-02],
          [-9.5907e-03, -1.5168e-02,  1.8985e-03],
          [ 1.4730e-02,  6.2213e-03,  2.2433e-02]],

         [[-3.1900e-02, -6.9355e-03, -2.4529e-02],
          [-1.3653e-02,  1.5566e-02,  1.1874e-02],
          [-1.2972e-02,  3.1079e-02,  2.7521e-02]],

         [[ 2.9416e-02,  6.5070e-03,  1.1414e-03],
          [-1.6925e-02,  1.9338e-03, -2.8588e-02],
          [ 3.1920e-02,  2.3749e-02,  9.3470e-03]],

         ...,

         [[-2.3466e-02, -2.7564e-02,  6.9525e-03],
          [ 1.6297e-02, -1.4208e-02,  1.6812e-02],
          [-1.0799e-02,  3.5203e-02, -4.7895e-03]],

         [[ 1.1564e-02,  3.5759e-02,  1.0360e-02],
          [-1.0119e-02, -1.8391e-02,  2.4826e-02],
          [ 1.7004e-02, -1.1797e-03, -4.5464e-03]],

         [[ 3.6514e-02,  7.3373e-03,  1.6873e-02],
          [ 2.1274e-02, -1.4978e-02,  1.6020e-02],
          [-1.1003e-02, -3.3445e-02, -3.7279e-02]]],


        [[[-2.8273e-02, -1.4806e-03,  1.3984e-02],
          [ 2.0571e-02,  1.4623e-02, -2.0575e-02],
          [ 3.0305e-02,  2.5818e-02, -1.4033e-03]],

         [[ 7.8080e-03, -3.2445e-03,  2.4619e-03],
          [ 2.1032e-02,  3.9741e-02,  2.0093e-02],
          [ 2.2603e-02,  3.7921e-02, -1.0862e-02]],

         [[-1.9503e-02, -1.6693e-02, -2.8121e-02],
          [-1.7048e-02, -1.2404e-03, -9.0745e-03],
          [-2.2411e-03,  5.2919e-03, -2.7800e-02]],

         ...,

         [[-2.5414e-02,  1.6003e-02, -2.5321e-02],
          [ 1.6860e-02, -1.1360e-02, -5.6331e-03],
          [-1.0337e-02,  2.7759e-02,  1.3697e-02]],

         [[-2.6902e-02,  2.5568e-03,  1.4689e-02],
          [-2.5164e-03, -4.8919e-03,  3.6591e-03],
          [ 2.1470e-02,  2.4309e-02,  3.4061e-03]],

         [[-2.8703e-02, -7.6792e-03,  9.5606e-03],
          [-2.5493e-02,  1.2979e-02,  2.7258e-02],
          [-3.0123e-02,  1.4439e-03,  1.4113e-02]]],


        [[[-1.5198e-02,  1.5720e-02, -3.0049e-03],
          [-3.7073e-03,  1.2444e-02,  4.2203e-03],
          [-5.0044e-03, -5.3109e-04,  1.9203e-02]],

         [[-2.9366e-04, -2.1123e-02, -1.9314e-02],
          [-2.4380e-02, -1.3075e-02, -3.8996e-02],
          [ 3.8465e-02,  1.5661e-02,  2.6767e-03]],

         [[ 2.9049e-02,  6.4220e-04, -9.3671e-03],
          [-1.8019e-02, -1.4249e-02, -1.6047e-02],
          [ 2.5967e-02, -7.6462e-03, -2.1651e-02]],

         ...,

         [[ 8.9201e-03, -7.0588e-03,  9.0274e-03],
          [-2.1534e-02, -1.2792e-02,  5.9740e-03],
          [ 1.3349e-02, -1.0387e-03, -1.2891e-02]],

         [[-1.5758e-02, -1.1052e-02,  8.8319e-04],
          [ 1.7439e-02,  1.0275e-02, -3.2043e-02],
          [-1.6489e-02, -7.1433e-03, -1.0274e-02]],

         [[-2.6740e-03, -3.0254e-02, -1.6332e-02],
          [ 3.7728e-03,  2.3820e-02, -2.6040e-02],
          [ 2.5864e-02,  1.1487e-02,  2.1862e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.9901e+07, 1.5592e+06, 1.4814e+06,  ..., 1.6003e+02, 1.5761e+02,
        1.3071e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 777]) 

NULL SPACE BASIS :  tensor([[ 1.4514e-02,  3.3891e-02,  4.1609e-02,  ..., -5.1916e-03,
         -3.2995e-03, -1.8308e-02],
        [ 1.7704e-02,  2.2550e-02,  3.2934e-02,  ...,  1.1677e-02,
          5.5222e-03,  1.7942e-02],
        [ 5.2813e-02,  1.3453e-02,  3.3708e-02,  ...,  9.0592e-04,
         -7.3712e-03, -8.3918e-03],
        ...,
        [-6.1360e-02,  5.9451e-03, -2.9335e-02,  ...,  5.4879e-04,
         -4.3237e-04,  3.4144e-03],
        [-8.9391e-03, -5.6822e-02, -6.5276e-03,  ..., -7.3247e-05,
          6.9712e-03, -4.5384e-03],
        [-2.2926e-02, -4.5320e-02,  4.6072e-02,  ..., -2.9836e-04,
         -5.8804e-03, -2.6457e-06]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.2136e-02, -5.3833e-03, -2.5151e-03,  ..., -4.0694e-04,
         -5.0340e-04,  3.0879e-04],
        [-5.3833e-03,  2.2949e-02, -5.0988e-03,  ..., -1.1412e-04,
          4.2973e-05, -1.7130e-04],
        [-2.5151e-03, -5.0988e-03,  2.1815e-02,  ..., -2.1626e-04,
         -1.2533e-04, -6.4393e-05],
        ...,
        [-4.0694e-04, -1.1412e-04, -2.1626e-04,  ...,  1.9584e-02,
         -6.6388e-03, -2.6759e-03],
        [-5.0340e-04,  4.2973e-05, -1.2533e-04,  ..., -6.6388e-03,
          2.0224e-02, -6.5857e-03],
        [ 3.0879e-04, -1.7130e-04, -6.4393e-05,  ..., -2.6759e-03,
         -6.5857e-03,  1.9747e-02]], device='cuda:0') 

reserving basis 270/1152; cond: 948255.0625, radio:0.0014779461780562997
PARAMETER       :  Parameter containing:
tensor([[[[ 2.3906e-03, -1.2957e-02, -7.1898e-03],
          [ 1.7122e-02,  1.7730e-02,  3.7300e-02],
          [-1.0900e-02, -5.0756e-03,  1.6300e-02]],

         [[-1.6369e-02,  1.9579e-02, -9.4078e-03],
          [-1.1372e-02, -2.3481e-02, -2.5460e-02],
          [-7.3101e-04, -1.7256e-02, -3.3997e-03]],

         [[-1.8836e-02, -1.2512e-02,  3.3163e-02],
          [ 3.6823e-02,  2.3939e-02,  8.2290e-05],
          [ 5.0604e-03,  6.3794e-03, -5.8559e-03]],

         ...,

         [[ 3.7765e-02,  1.2388e-02, -6.0032e-03],
          [ 4.0099e-02,  1.5706e-02,  1.5806e-02],
          [-1.6000e-02,  1.0329e-03, -1.5266e-02]],

         [[ 4.3747e-02,  5.3659e-03,  2.9670e-02],
          [ 3.0934e-02,  1.2568e-02,  1.9944e-02],
          [-2.2671e-02, -3.7917e-03,  1.8962e-02]],

         [[ 1.0640e-02, -1.3405e-02, -1.3836e-02],
          [ 2.0517e-02,  1.8098e-02,  3.7360e-02],
          [-1.3136e-02,  1.5673e-02,  5.3459e-04]]],


        [[[ 1.9988e-02,  3.6773e-02, -2.6036e-02],
          [ 2.4243e-02, -3.9567e-03, -2.7087e-02],
          [ 9.1920e-03, -2.5365e-02, -1.8691e-02]],

         [[-3.1208e-02, -3.4274e-02, -4.2514e-02],
          [-1.9225e-02, -2.3259e-02, -7.0595e-03],
          [ 1.7710e-02, -1.4133e-02,  5.7251e-03]],

         [[ 1.0760e-02,  1.8823e-02,  2.8332e-02],
          [-5.0546e-03,  1.3282e-02, -6.8865e-03],
          [ 1.4943e-02, -8.5439e-03, -2.9455e-02]],

         ...,

         [[-1.0830e-02,  3.0655e-02, -1.6939e-02],
          [ 3.1980e-02, -8.2635e-03,  1.1861e-02],
          [ 9.0187e-03,  1.8314e-02, -2.0755e-02]],

         [[-2.5581e-02,  1.9196e-02, -1.7794e-02],
          [-1.6854e-02, -3.1143e-03, -1.9449e-02],
          [ 2.8246e-02, -9.9628e-03,  2.1934e-02]],

         [[ 1.2799e-02, -2.5142e-02,  3.5545e-02],
          [-1.9083e-02,  2.2655e-02,  5.2350e-04],
          [ 1.6684e-02,  2.6305e-04, -8.5442e-03]]],


        [[[-1.7270e-03, -5.3421e-04,  9.5155e-03],
          [-2.0860e-02,  5.9071e-03, -1.7546e-02],
          [-1.6080e-02,  1.7144e-03, -5.3146e-03]],

         [[ 1.3972e-02, -1.6574e-02, -1.5541e-02],
          [ 2.5612e-02, -6.0316e-04,  3.0947e-02],
          [ 3.4022e-02,  1.9909e-02, -1.4132e-02]],

         [[-1.4534e-02,  1.0203e-02, -2.4925e-02],
          [ 6.3098e-03,  1.6914e-02,  1.6873e-03],
          [ 7.1777e-05, -6.6177e-03,  3.0076e-02]],

         ...,

         [[ 1.8946e-02,  2.3459e-02,  1.6102e-02],
          [-1.6539e-02, -2.4919e-02,  4.4237e-03],
          [ 3.0613e-03,  1.7128e-02, -1.0243e-02]],

         [[ 1.5442e-02,  1.6314e-02,  8.9877e-03],
          [ 8.1304e-03, -9.4716e-03, -2.0624e-02],
          [ 4.1711e-03,  1.1362e-02,  3.1579e-02]],

         [[-1.4214e-02, -1.5933e-02, -5.0791e-03],
          [-2.2273e-02,  2.0770e-03,  1.2612e-03],
          [-2.6592e-03,  7.6653e-03,  4.1285e-02]]],


        ...,


        [[[-9.9222e-03, -2.4609e-02,  1.1553e-02],
          [ 1.8913e-02, -7.3024e-03,  1.6767e-02],
          [ 3.2051e-03, -1.5747e-02,  2.4417e-02]],

         [[-2.2149e-02, -2.3974e-02, -7.8883e-03],
          [-1.8883e-02, -1.8097e-02,  1.5804e-03],
          [-3.3483e-02,  2.2636e-02, -5.9055e-03]],

         [[-7.0567e-03,  2.8588e-03, -8.1706e-03],
          [-5.7856e-03,  1.9578e-02,  2.9074e-03],
          [ 3.2659e-02,  1.2730e-02,  1.9433e-02]],

         ...,

         [[ 5.9429e-04, -1.2242e-02,  3.8997e-02],
          [-1.9046e-03,  2.0579e-02,  2.0808e-02],
          [ 1.4316e-02,  1.0422e-02,  9.3302e-03]],

         [[-2.0549e-02,  2.5161e-03, -5.3964e-03],
          [-1.7802e-02,  1.2762e-02,  5.4901e-03],
          [ 1.2180e-02, -1.3699e-03,  2.0839e-02]],

         [[-4.8877e-03, -5.8904e-03, -9.7356e-03],
          [ 5.3859e-03,  2.3343e-02, -1.8093e-02],
          [-1.3829e-03, -1.3543e-02, -6.7346e-03]]],


        [[[-8.4447e-03,  3.9038e-02,  1.1227e-02],
          [-2.3646e-03, -1.1854e-03, -8.2420e-04],
          [-2.3619e-02,  2.3659e-02,  2.2531e-02]],

         [[ 1.6181e-02, -9.0460e-03,  3.2068e-05],
          [-2.9500e-03,  3.2249e-02,  1.1699e-02],
          [ 5.4591e-03, -7.5622e-03, -2.4532e-02]],

         [[-8.4572e-03, -1.5884e-02,  4.2767e-03],
          [-1.3705e-02, -3.2622e-02, -5.6371e-03],
          [ 1.4085e-02, -1.3460e-02,  2.7574e-02]],

         ...,

         [[-1.9446e-03,  1.2048e-02, -2.0400e-02],
          [ 1.5967e-02,  5.8048e-04,  1.7747e-02],
          [ 2.9874e-02,  1.2572e-02,  2.3850e-02]],

         [[ 8.6573e-03, -2.7460e-02,  3.7451e-03],
          [-1.7704e-02,  1.2614e-03, -2.0220e-02],
          [ 1.8218e-02,  2.0722e-02,  2.4581e-02]],

         [[-3.7986e-03, -2.9404e-02, -2.0274e-02],
          [ 1.3228e-02,  1.1407e-02,  3.3244e-03],
          [ 1.7067e-02,  5.7301e-03, -2.7947e-02]]],


        [[[ 2.1965e-02, -2.5106e-02,  1.0863e-03],
          [ 1.1548e-02,  6.9751e-03, -2.8563e-02],
          [ 2.1168e-02,  2.4270e-02,  1.1554e-02]],

         [[-4.1338e-02, -1.6730e-02, -3.1265e-02],
          [ 1.5129e-04,  1.2194e-02, -1.8623e-03],
          [ 4.5347e-03, -2.6183e-02,  1.0981e-02]],

         [[-1.3556e-02, -1.2644e-03, -9.2772e-03],
          [-2.6099e-02, -1.5885e-02, -2.0495e-02],
          [ 2.7008e-03, -9.4920e-03,  1.4470e-02]],

         ...,

         [[-1.4403e-02, -2.7968e-02, -2.8318e-02],
          [ 1.1167e-02,  1.7135e-03,  1.1060e-03],
          [ 1.8774e-02, -1.5383e-02, -1.5610e-02]],

         [[-3.1159e-03, -1.7789e-02, -6.2682e-03],
          [ 2.2212e-03,  3.7397e-03,  1.8988e-02],
          [ 1.9739e-03,  2.2343e-03,  2.3147e-03]],

         [[ 2.1191e-02,  1.4237e-02,  2.1264e-03],
          [ 2.0564e-02, -6.4040e-03,  1.9498e-02],
          [-1.4247e-02, -1.4586e-02,  1.3844e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([4.4413e+07, 1.6743e+06, 1.4902e+06,  ..., 7.9327e+01, 5.1606e+01,
        4.6837e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 270]) 

NULL SPACE BASIS :  tensor([[ 8.3172e-03,  1.1427e-02,  4.8407e-04,  ...,  4.5717e-05,
          2.7887e-04, -7.0093e-04],
        [ 3.1769e-04, -2.3403e-02,  4.6980e-03,  ...,  3.8142e-04,
         -1.7450e-03, -9.3556e-04],
        [-2.1228e-04, -3.7489e-03,  4.5841e-03,  ...,  1.6865e-03,
          8.4743e-05,  2.0925e-03],
        ...,
        [-4.4302e-02,  4.8274e-02,  8.9477e-02,  ..., -4.9837e-03,
         -2.5937e-03, -1.1747e-04],
        [-4.9822e-02,  4.2888e-02,  5.9269e-02,  ..., -1.6337e-03,
         -5.0034e-04,  1.5967e-03],
        [-5.7987e-02,  1.6023e-02,  7.3877e-02,  ...,  3.7996e-03,
         -8.0718e-04, -2.7540e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.6155e-03, -5.9276e-04, -1.4670e-04,  ...,  3.9404e-05,
         -3.4895e-04, -2.6426e-05],
        [-5.9276e-04,  2.0019e-03, -8.2352e-04,  ..., -1.3923e-05,
          1.9386e-04, -2.4371e-04],
        [-1.4670e-04, -8.2352e-04,  1.5627e-03,  ...,  1.6095e-04,
         -3.1378e-04,  6.4743e-05],
        ...,
        [ 3.9404e-05, -1.3923e-05,  1.6095e-04,  ...,  3.1462e-02,
         -1.1407e-02, -3.2654e-03],
        [-3.4895e-04,  1.9386e-04, -3.1378e-04,  ..., -1.1407e-02,
          3.3799e-02, -1.0636e-02],
        [-2.6426e-05, -2.4371e-04,  6.4743e-05,  ..., -3.2654e-03,
         -1.0636e-02,  3.1594e-02]], device='cuda:0') 

reserving basis 737/1152; cond: 382315.625, radio:0.007463287096470594
PARAMETER       :  Parameter containing:
tensor([[[[-2.0240e-02, -1.3792e-02,  4.1720e-02],
          [ 2.1200e-02,  9.1961e-03, -1.7922e-03],
          [ 1.4986e-02,  2.1090e-02, -2.8199e-02]],

         [[ 8.0108e-03,  1.6463e-03,  3.3599e-02],
          [-2.4080e-03,  1.7777e-02,  2.9341e-02],
          [ 2.7239e-02, -2.3067e-02, -7.2586e-03]],

         [[ 1.8715e-02,  5.0702e-03,  7.7068e-03],
          [-5.8089e-03, -8.6491e-03,  2.6141e-02],
          [ 1.0057e-02,  1.9744e-02,  2.6813e-02]],

         ...,

         [[ 7.0052e-03,  1.5682e-02,  2.6003e-02],
          [ 3.2476e-02, -2.0165e-02,  2.3848e-02],
          [-1.1316e-03, -1.4208e-02, -1.3252e-02]],

         [[-7.5443e-03, -6.4456e-03, -1.8976e-02],
          [-2.2457e-03,  4.0944e-03,  5.0318e-03],
          [ 2.0047e-03, -4.1135e-04, -7.4379e-03]],

         [[-3.7198e-02,  8.9825e-03, -1.6854e-02],
          [ 1.1308e-02,  2.8073e-03,  1.8358e-02],
          [ 2.5864e-02, -5.3467e-03,  2.3651e-02]]],


        [[[ 5.1912e-03, -3.8373e-03,  1.5226e-02],
          [ 2.0627e-02,  1.2777e-02, -1.0357e-03],
          [-2.6731e-03, -9.9400e-03, -3.4979e-03]],

         [[ 4.0362e-03,  1.1785e-02,  1.7521e-02],
          [-2.5468e-02, -2.7972e-03, -2.4933e-02],
          [-1.4130e-02,  1.8522e-02, -2.6750e-03]],

         [[-1.8014e-02, -6.1849e-03,  2.7124e-02],
          [ 3.9556e-03, -1.1323e-02, -1.2183e-02],
          [ 6.6459e-04,  1.3441e-02, -1.3885e-02]],

         ...,

         [[-2.1198e-02, -1.6853e-02, -2.1819e-02],
          [-2.6997e-02,  7.3641e-03, -2.0032e-02],
          [ 3.2489e-02,  1.3623e-03, -3.5024e-02]],

         [[ 1.8210e-02,  2.6138e-02,  3.4375e-02],
          [-1.4268e-02,  1.8522e-02, -9.9991e-03],
          [ 2.1875e-02,  1.9252e-02,  4.0728e-02]],

         [[-2.5810e-05,  3.7029e-03,  4.1515e-05],
          [-3.7115e-03, -8.6436e-03,  2.1192e-02],
          [ 4.6721e-03,  4.6551e-02,  4.0662e-02]]],


        [[[-2.1607e-02, -2.1015e-02,  3.5427e-03],
          [-2.3405e-02,  4.5028e-03,  1.7037e-03],
          [ 5.9723e-03,  2.9876e-03,  4.0598e-03]],

         [[-2.6960e-03,  8.5484e-03,  9.4649e-03],
          [-7.4972e-04,  2.2826e-02,  6.3213e-03],
          [ 1.3686e-03,  3.4432e-02, -1.9427e-02]],

         [[-3.9752e-03, -1.8634e-02, -2.2599e-02],
          [-2.1862e-02, -2.9938e-02, -5.3890e-03],
          [-6.2614e-03, -2.7939e-02, -1.1202e-02]],

         ...,

         [[ 1.2994e-02,  1.0985e-02,  4.1717e-02],
          [-2.2972e-02,  7.7164e-03, -1.6078e-02],
          [-1.9329e-02,  2.7903e-02, -2.4046e-02]],

         [[-3.5774e-02, -1.0382e-02, -1.7107e-03],
          [ 6.0551e-03, -1.0672e-02,  7.9418e-03],
          [-9.0974e-03,  1.7834e-02, -2.5903e-02]],

         [[ 2.8306e-03, -7.7833e-03, -1.4564e-02],
          [-3.0876e-02,  3.6745e-03,  2.4406e-02],
          [-2.2298e-02,  1.0529e-02,  1.9319e-02]]],


        ...,


        [[[-9.9035e-04,  1.1274e-02, -1.9910e-02],
          [-1.9055e-02, -1.4192e-02, -2.3508e-02],
          [-4.4683e-03, -1.6387e-02, -2.3859e-02]],

         [[ 3.1118e-02,  3.4388e-03,  7.4817e-03],
          [-8.5541e-03,  6.1375e-03, -2.3915e-02],
          [-3.4702e-02, -3.7977e-02, -1.6071e-02]],

         [[ 8.6809e-03,  1.1480e-02, -2.1508e-03],
          [-5.3777e-03, -1.6644e-02,  3.8782e-03],
          [-3.3339e-03,  3.5833e-02,  2.3288e-02]],

         ...,

         [[ 2.0297e-02, -6.4753e-03,  8.8357e-03],
          [-1.2642e-02, -2.2512e-02,  2.0570e-02],
          [ 1.2443e-02, -2.5346e-02, -1.7261e-02]],

         [[-8.5033e-03, -1.2050e-02,  2.8791e-02],
          [-3.6094e-02,  2.6360e-02,  1.6501e-02],
          [-1.8519e-02,  5.0235e-03,  1.6234e-02]],

         [[-2.1002e-02, -1.9819e-02,  4.0940e-03],
          [-2.4873e-03, -1.8900e-02, -2.4345e-02],
          [-4.4379e-03,  1.4126e-02, -1.8748e-02]]],


        [[[-3.0759e-02,  8.4267e-03,  1.0568e-02],
          [-9.4022e-03, -1.6203e-02, -2.4733e-02],
          [-2.4602e-02, -9.6933e-03,  1.3253e-03]],

         [[ 9.8931e-03,  1.8961e-02,  1.2807e-02],
          [-1.8716e-02, -2.0893e-02, -2.3230e-02],
          [-9.4404e-03, -8.4689e-03, -1.8465e-02]],

         [[-2.8100e-02, -2.0549e-02, -2.0161e-02],
          [ 1.9370e-02,  2.2003e-02,  3.0452e-02],
          [ 1.6787e-02, -1.7204e-02, -1.8061e-02]],

         ...,

         [[-3.6238e-02,  1.4275e-02,  1.0840e-02],
          [-3.3142e-02,  5.4902e-04, -5.8018e-03],
          [-2.0946e-03,  1.5127e-03,  1.0540e-02]],

         [[ 1.7215e-03, -1.3629e-02, -1.2965e-03],
          [ 3.7999e-03, -3.0102e-02, -1.8832e-03],
          [ 5.2116e-03, -4.2822e-03,  9.9495e-03]],

         [[-4.3946e-03,  6.6670e-03,  4.1880e-03],
          [-1.1015e-02,  1.0496e-03, -2.1731e-02],
          [ 5.6060e-03,  9.7078e-03, -1.7712e-02]]],


        [[[ 6.2983e-03, -7.5284e-03,  1.7460e-02],
          [ 1.2894e-02, -1.9490e-02,  9.3357e-03],
          [ 6.8632e-03,  2.0866e-02,  4.6250e-03]],

         [[ 3.2111e-02,  6.2305e-03,  9.6970e-03],
          [-8.8830e-03,  3.1532e-02,  1.8254e-02],
          [ 8.4330e-03,  7.6704e-04,  1.4096e-02]],

         [[ 7.2394e-03, -1.2812e-02,  4.7507e-03],
          [-7.0412e-03, -3.5945e-02, -1.9454e-02],
          [-3.7060e-02, -1.2923e-02,  4.6455e-03]],

         ...,

         [[ 8.0440e-03, -3.3265e-02,  1.3636e-02],
          [-3.7154e-02,  1.6683e-02,  3.2940e-03],
          [-1.6581e-02,  4.4946e-03, -1.2247e-02]],

         [[ 1.6729e-02, -2.1585e-03,  1.3309e-03],
          [ 3.4347e-03,  2.7626e-02,  2.8181e-02],
          [ 8.9146e-03,  1.3934e-02,  2.3565e-02]],

         [[ 2.0584e-02,  3.0246e-02,  2.2438e-02],
          [-6.7375e-03,  6.7417e-03, -5.7682e-03],
          [-2.1873e-02,  8.7464e-03, -1.5438e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.0741e+07, 6.3488e+05, 6.0538e+05,  ..., 3.6497e+01, 3.3409e+01,
        2.8095e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([1152, 737]) 

NULL SPACE BASIS :  tensor([[-0.0631,  0.0140,  0.0242,  ..., -0.0020, -0.0124, -0.0055],
        [-0.0227,  0.0181, -0.0486,  ...,  0.0070,  0.0083,  0.0069],
        [-0.0615,  0.0196,  0.0256,  ..., -0.0051, -0.0014, -0.0011],
        ...,
        [-0.0155, -0.0123,  0.0067,  ..., -0.0130, -0.0096,  0.0034],
        [-0.0579,  0.0287,  0.0133,  ...,  0.0073,  0.0192,  0.0002],
        [-0.0444, -0.0195,  0.0264,  ..., -0.0029, -0.0033, -0.0045]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.3926e-02, -5.0283e-03, -1.7902e-03,  ..., -3.6633e-04,
         -1.6399e-04,  6.4187e-04],
        [-5.0283e-03,  2.3518e-02, -5.4815e-03,  ..., -3.0841e-04,
         -2.4349e-04,  3.9303e-05],
        [-1.7902e-03, -5.4815e-03,  2.3276e-02,  ..., -3.2626e-04,
          1.3536e-04, -2.1250e-04],
        ...,
        [-3.6633e-04, -3.0841e-04, -3.2626e-04,  ...,  2.3246e-02,
         -7.1963e-03, -3.0940e-03],
        [-1.6399e-04, -2.4349e-04,  1.3536e-04,  ..., -7.1963e-03,
          2.3332e-02, -6.9988e-03],
        [ 6.4187e-04,  3.9303e-05, -2.1250e-04,  ..., -3.0940e-03,
         -6.9988e-03,  2.1038e-02]], device='cuda:0') 

reserving basis 1346/2304; cond: 814802.5, radio:0.006305734161287546
PARAMETER       :  Parameter containing:
tensor([[[[ 3.8799e-03, -4.5209e-03, -4.0934e-04],
          [-2.4169e-02, -7.4384e-03,  1.3506e-02],
          [-2.6149e-02, -6.7945e-03, -1.7211e-02]],

         [[ 7.2400e-03,  2.2170e-02,  2.8126e-03],
          [-5.5161e-03, -1.8119e-02,  2.0695e-02],
          [-1.0390e-02, -7.7999e-04,  1.3654e-02]],

         [[-1.8190e-02, -1.7040e-02, -1.3559e-02],
          [-1.8175e-03, -1.4615e-02, -2.1514e-02],
          [ 6.7722e-03,  9.2746e-03, -1.1439e-02]],

         ...,

         [[ 1.5611e-02,  1.2952e-02,  1.7118e-02],
          [-1.6181e-02, -1.3170e-02,  1.1051e-02],
          [ 3.7494e-03,  3.8591e-04,  2.3389e-03]],

         [[ 2.1268e-02,  4.2031e-04,  1.4623e-02],
          [-1.3699e-02,  2.2864e-04,  2.3145e-02],
          [ 2.1329e-02, -1.9252e-03,  9.4441e-03]],

         [[ 7.7972e-03,  8.2154e-03, -2.1241e-02],
          [ 5.6803e-03, -3.4689e-03,  2.6812e-03],
          [ 2.3358e-02,  5.3227e-03,  1.1073e-02]]],


        [[[ 8.8341e-03, -5.3785e-03, -2.8540e-03],
          [ 4.5028e-03,  7.3273e-03, -9.3270e-03],
          [ 2.3511e-02, -9.3143e-03, -1.3683e-02]],

         [[-1.1223e-02,  1.1193e-02, -1.5056e-03],
          [-1.9499e-02,  2.1717e-02,  1.2288e-02],
          [-1.8456e-04, -6.2418e-03, -3.7569e-04]],

         [[-1.4953e-02, -1.3248e-03,  1.3341e-02],
          [ 2.5237e-02,  5.1707e-03,  6.8055e-03],
          [-6.8129e-03,  2.2646e-02, -1.2221e-02]],

         ...,

         [[-1.3453e-02,  2.1019e-02, -1.1216e-04],
          [ 1.9437e-02, -1.5527e-02,  1.3675e-02],
          [ 1.5666e-03, -2.0978e-02, -1.3967e-02]],

         [[-1.4921e-02, -2.6360e-02, -5.9150e-03],
          [-8.8231e-03,  3.9169e-03, -5.7737e-03],
          [-1.6076e-02,  1.5867e-02,  1.7255e-02]],

         [[ 7.6144e-03, -5.6982e-03, -2.0487e-02],
          [-7.0447e-03, -1.3324e-02,  7.2492e-03],
          [-1.8862e-02,  1.0178e-02, -5.9028e-03]]],


        [[[ 1.6240e-02, -2.2245e-02, -1.3295e-02],
          [ 1.6780e-02,  1.8766e-02, -2.6281e-03],
          [ 1.7588e-02,  2.1510e-02, -2.4714e-02]],

         [[-1.8252e-03,  1.2631e-02,  2.6598e-03],
          [-1.1280e-02, -1.3520e-02, -2.0019e-02],
          [-8.2727e-03,  2.3083e-02,  1.4160e-03]],

         [[-5.7612e-03, -1.2118e-02, -1.6222e-03],
          [-6.6708e-03, -6.2541e-03,  8.5859e-03],
          [-8.3924e-04, -1.3333e-02,  8.4711e-03]],

         ...,

         [[-1.5250e-02,  2.2297e-02, -7.4611e-03],
          [ 1.7048e-02, -1.6088e-02, -3.6601e-03],
          [-1.7013e-02, -1.3525e-02, -2.1873e-02]],

         [[-3.3219e-03,  1.8352e-02, -1.1534e-02],
          [-1.7315e-02, -2.1232e-02, -2.0422e-02],
          [ 1.8172e-02, -1.8989e-02, -1.0059e-02]],

         [[ 8.2322e-03,  3.1834e-03, -1.9359e-02],
          [ 2.3486e-03, -4.9377e-03,  1.0689e-02],
          [-9.7398e-03, -2.1798e-04, -9.1102e-03]]],


        ...,


        [[[ 7.7891e-03,  1.9152e-02,  7.5952e-03],
          [ 1.9584e-02, -1.5221e-02, -1.5597e-02],
          [-4.9596e-03, -1.3191e-02, -1.1117e-02]],

         [[-1.5435e-02, -1.0790e-02,  2.9489e-02],
          [ 3.5962e-04,  9.6314e-03, -3.6252e-03],
          [ 2.4075e-02, -1.9237e-02,  1.1613e-02]],

         [[-1.9248e-02, -1.8883e-03, -1.3516e-02],
          [ 3.0876e-03,  2.6373e-02, -2.3469e-02],
          [ 8.5630e-03,  3.1602e-02,  1.4433e-02]],

         ...,

         [[-5.0952e-03,  1.3170e-02,  3.4299e-03],
          [-1.2325e-02, -1.0497e-02,  6.6326e-03],
          [ 5.2257e-04,  6.7863e-04, -2.8352e-02]],

         [[-1.1615e-02,  2.7783e-03, -1.0723e-02],
          [-1.5797e-02, -1.0286e-02, -1.4506e-03],
          [-8.1980e-03,  3.8976e-03,  2.0676e-02]],

         [[-2.6474e-02, -4.2221e-02, -2.8296e-02],
          [ 9.3337e-03, -2.7331e-02, -2.0475e-02],
          [ 1.1603e-02, -1.0955e-02,  9.3087e-03]]],


        [[[-8.4527e-03,  1.6720e-02,  1.4417e-03],
          [-1.2474e-02, -1.9997e-02,  4.1352e-03],
          [ 9.6749e-03, -4.1841e-03, -1.3855e-02]],

         [[-2.7627e-03,  5.9619e-03, -4.6101e-03],
          [ 5.6100e-03, -1.5265e-02,  1.4114e-02],
          [ 9.2321e-03, -1.3330e-02, -1.5666e-02]],

         [[-2.1709e-02, -2.9959e-02, -2.1788e-02],
          [ 2.5042e-02,  1.6204e-02, -2.0348e-03],
          [-2.7005e-03,  9.4510e-03, -4.6856e-03]],

         ...,

         [[ 7.9750e-03, -1.3311e-02, -2.9180e-02],
          [ 1.6318e-02,  1.5250e-02, -2.2777e-02],
          [-1.4360e-02, -2.3185e-02,  1.3114e-02]],

         [[ 5.2154e-03,  1.2649e-03,  1.1274e-02],
          [-1.3585e-02, -7.5349e-03, -9.8388e-03],
          [-8.4908e-03,  8.6679e-03,  5.3745e-03]],

         [[-2.4467e-02,  1.5886e-02,  1.6654e-02],
          [-2.6756e-02, -9.7635e-03,  2.5573e-02],
          [-1.6322e-02,  3.6323e-02,  3.9740e-02]]],


        [[[-2.1655e-03, -3.0866e-02,  9.3453e-03],
          [-1.5824e-02, -8.6527e-03,  2.8218e-02],
          [-2.0743e-02, -9.2072e-03,  9.8734e-03]],

         [[-1.9442e-02, -1.6351e-02,  1.3512e-02],
          [-9.7964e-03,  8.2937e-03, -6.1381e-03],
          [ 5.2455e-03,  1.2447e-02,  1.5447e-03]],

         [[ 8.9944e-03, -6.4039e-03, -5.0625e-03],
          [-1.2693e-02,  2.2597e-02, -2.4719e-03],
          [-2.1880e-02, -1.9428e-04, -5.2700e-03]],

         ...,

         [[-4.4502e-03, -3.0190e-02, -3.4528e-03],
          [-2.3919e-02, -2.5612e-02,  7.2503e-03],
          [ 1.0323e-02, -1.4079e-02,  1.8712e-02]],

         [[ 9.7315e-03,  3.7209e-03,  1.8562e-02],
          [ 6.5603e-03, -1.1978e-02,  8.1703e-05],
          [-2.1716e-02,  1.5370e-02, -2.6266e-02]],

         [[-2.5071e-02,  8.8758e-03, -3.3971e-02],
          [ 1.0351e-02, -1.3074e-02, -5.0635e-03],
          [ 1.3331e-02, -2.5305e-02, -1.8891e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.5636e+07, 9.5059e+05, 8.9033e+05,  ..., 2.0766e+01, 1.9613e+01,
        1.9190e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1346]) 

NULL SPACE BASIS :  tensor([[-0.0037, -0.0023,  0.0054,  ..., -0.0027, -0.0019, -0.0031],
        [-0.0153, -0.0687, -0.0158,  ..., -0.0007, -0.0019, -0.0032],
        [-0.0085, -0.0226,  0.0137,  ...,  0.0035,  0.0010,  0.0026],
        ...,
        [-0.0060,  0.0010,  0.0093,  ..., -0.0028,  0.0003,  0.0074],
        [ 0.0232, -0.0017,  0.0290,  ..., -0.0052, -0.0012, -0.0065],
        [-0.0280, -0.0013,  0.0023,  ...,  0.0022,  0.0009,  0.0074]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.5483e-02, -5.8895e-04, -4.3274e-04,  ..., -1.8930e-05,
         -2.5135e-04, -1.2723e-04],
        [-5.8895e-04,  1.4917e-02, -7.8210e-04,  ...,  2.8021e-05,
          2.8091e-05, -2.0169e-04],
        [-4.3274e-04, -7.8210e-04,  1.5516e-02,  ..., -1.5129e-04,
         -8.1159e-05, -1.2103e-04],
        ...,
        [-1.8930e-05,  2.8021e-05, -1.5129e-04,  ...,  2.0901e-02,
         -2.8047e-04, -1.5210e-04],
        [-2.5135e-04,  2.8091e-05, -8.1159e-05,  ..., -2.8047e-04,
          2.0335e-02, -4.6978e-04],
        [-1.2723e-04, -2.0169e-04, -1.2103e-04,  ..., -1.5210e-04,
         -4.6978e-04,  2.1066e-02]], device='cuda:0') 

reserving basis 98/128; cond: 13761.546875, radio:0.02107335440814495
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0283]],

         [[-0.0364]],

         [[-0.0286]],

         ...,

         [[-0.0432]],

         [[-0.0693]],

         [[ 0.0823]]],


        [[[-0.0490]],

         [[ 0.0643]],

         [[ 0.0736]],

         ...,

         [[-0.0446]],

         [[-0.0013]],

         [[-0.0183]]],


        [[[ 0.0578]],

         [[-0.0204]],

         [[-0.0223]],

         ...,

         [[ 0.0539]],

         [[ 0.0060]],

         [[-0.0231]]],


        ...,


        [[[-0.0636]],

         [[ 0.0324]],

         [[ 0.0815]],

         ...,

         [[-0.0768]],

         [[-0.0465]],

         [[ 0.0342]]],


        [[[-0.0262]],

         [[-0.0706]],

         [[ 0.0123]],

         ...,

         [[ 0.0282]],

         [[ 0.0463]],

         [[ 0.0229]]],


        [[[ 0.0465]],

         [[-0.0408]],

         [[-0.0602]],

         ...,

         [[-0.0438]],

         [[ 0.0648]],

         [[-0.0306]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([1.5105e+06, 8.0989e+04, 6.6300e+04, 2.6254e+04, 1.6951e+04, 1.4114e+04,
        1.1005e+04, 7.6769e+03, 7.3841e+03, 6.5758e+03, 6.0939e+03, 4.9623e+03,
        4.4577e+03, 3.8684e+03, 3.2098e+03, 3.1137e+03, 2.6575e+03, 2.6267e+03,
        2.3588e+03, 2.1621e+03, 2.0647e+03, 1.9151e+03, 1.8165e+03, 1.7184e+03,
        1.5423e+03, 1.5270e+03, 1.3904e+03, 1.3117e+03, 1.2207e+03, 1.1791e+03,
        1.0960e+03, 1.0776e+03, 1.0319e+03, 9.6952e+02, 9.5154e+02, 9.1718e+02,
        8.6166e+02, 8.3176e+02, 7.8229e+02, 7.7493e+02, 7.5292e+02, 7.3381e+02,
        7.1408e+02, 6.8561e+02, 6.6841e+02, 6.3363e+02, 6.2425e+02, 6.0776e+02,
        5.9571e+02, 5.7129e+02, 5.6696e+02, 5.5421e+02, 5.3144e+02, 5.0868e+02,
        5.0598e+02, 5.0171e+02, 4.7654e+02, 4.7283e+02, 4.6652e+02, 4.5546e+02,
        4.3539e+02, 4.3180e+02, 4.2506e+02, 4.1499e+02, 4.0963e+02, 4.0567e+02,
        4.0367e+02, 3.8247e+02, 3.7990e+02, 3.6796e+02, 3.6315e+02, 3.5498e+02,
        3.5256e+02, 3.4584e+02, 3.3973e+02, 3.2989e+02, 3.2815e+02, 3.1844e+02,
        3.1225e+02, 3.0804e+02, 3.0333e+02, 2.9604e+02, 2.9163e+02, 2.9002e+02,
        2.8491e+02, 2.8453e+02, 2.8062e+02, 2.7340e+02, 2.6972e+02, 2.6562e+02,
        2.5835e+02, 2.5497e+02, 2.5378e+02, 2.5155e+02, 2.4874e+02, 2.4356e+02,
        2.4149e+02, 2.3868e+02, 2.3494e+02, 2.3168e+02, 2.2808e+02, 2.2447e+02,
        2.2125e+02, 2.2057e+02, 2.1647e+02, 2.1510e+02, 2.1118e+02, 2.0949e+02,
        2.0565e+02, 2.0228e+02, 1.9300e+02, 1.8973e+02, 1.8667e+02, 1.8397e+02,
        1.7972e+02, 1.7489e+02, 1.7189e+02, 1.6902e+02, 1.6838e+02, 1.6499e+02,
        1.5986e+02, 1.5625e+02, 1.5504e+02, 1.5257e+02, 1.5164e+02, 1.4522e+02,
        1.3002e+02, 1.0976e+02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([128, 98]) 

NULL SPACE BASIS :  tensor([[ 0.1677,  0.1323, -0.0469,  ...,  0.0655, -0.0609,  0.0481],
        [-0.0033, -0.1831, -0.0173,  ..., -0.1448,  0.0211, -0.0205],
        [-0.2053, -0.0189,  0.0051,  ..., -0.0836, -0.1686, -0.1186],
        ...,
        [-0.0478,  0.0189,  0.0848,  ..., -0.0068,  0.0807, -0.0073],
        [ 0.1272,  0.0055,  0.0021,  ...,  0.0768, -0.0287,  0.0468],
        [-0.0759, -0.0191,  0.0151,  ...,  0.0666, -0.0351,  0.0752]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0792,  0.0012,  0.0003,  ..., -0.0043,  0.0028, -0.0085],
        [ 0.0012,  0.0661,  0.0010,  ..., -0.0022, -0.0047,  0.0084],
        [ 0.0003,  0.0010,  0.0867,  ...,  0.0006, -0.0012, -0.0036],
        ...,
        [-0.0043, -0.0022,  0.0006,  ...,  0.0764,  0.0005, -0.0069],
        [ 0.0028, -0.0047, -0.0012,  ...,  0.0005,  0.0794, -0.0021],
        [-0.0085,  0.0084, -0.0036,  ..., -0.0069, -0.0021,  0.0711]],
       device='cuda:0') 

reserving basis 1063/2304; cond: 979303.25, radio:0.004592357203364372
PARAMETER       :  Parameter containing:
tensor([[[[ 4.8332e-03,  1.5053e-03, -1.0457e-02],
          [ 5.0479e-03, -1.9734e-02,  5.1196e-04],
          [-6.2154e-03,  2.1673e-02, -5.2838e-03]],

         [[-2.6599e-03,  1.2286e-02,  1.5784e-03],
          [ 2.6912e-02, -5.6219e-03, -2.0238e-02],
          [-2.1888e-03,  6.9612e-03,  1.2833e-02]],

         [[ 1.5795e-02,  2.7975e-02, -1.3399e-02],
          [ 1.4317e-02,  1.1874e-02,  1.2117e-02],
          [-3.3037e-03,  1.0405e-02,  5.8376e-03]],

         ...,

         [[-3.2131e-04,  2.0560e-02,  3.3884e-03],
          [-2.6773e-02, -5.5410e-03, -3.0349e-02],
          [-2.9985e-04, -1.5376e-02, -2.1320e-03]],

         [[-2.5081e-02,  1.3154e-02,  9.0878e-03],
          [ 4.0698e-03, -2.5779e-02, -1.9050e-02],
          [-2.0143e-02, -1.5433e-02, -1.5627e-02]],

         [[ 2.8591e-03, -7.6658e-04, -2.1090e-02],
          [ 8.4891e-03,  1.1574e-02,  5.0461e-03],
          [ 2.3157e-02,  2.0116e-02, -1.4987e-02]]],


        [[[-4.5377e-03,  2.2620e-02,  3.0712e-02],
          [ 2.1615e-03, -1.8871e-02,  1.5920e-02],
          [ 6.5060e-03,  1.7899e-02, -2.5325e-02]],

         [[-1.5042e-02, -1.9954e-03, -3.9278e-02],
          [-1.2057e-03, -5.8403e-03,  8.4572e-03],
          [ 1.4050e-03,  1.8346e-02,  2.4459e-02]],

         [[-7.1546e-03, -2.2858e-02,  1.5036e-02],
          [-1.0678e-02, -6.1306e-05, -1.5155e-02],
          [ 6.4164e-03,  6.6401e-03,  1.8942e-02]],

         ...,

         [[-1.1383e-02, -2.6491e-02, -1.0415e-02],
          [-8.9146e-03,  5.6408e-03,  1.6781e-02],
          [ 2.9704e-03,  1.3109e-02, -9.1425e-03]],

         [[ 2.1008e-03, -1.6789e-02, -7.3780e-04],
          [ 1.1529e-02,  1.7237e-02,  2.1702e-02],
          [-1.6810e-02, -1.9945e-02,  1.8885e-02]],

         [[ 5.5662e-03, -1.2617e-02, -2.9010e-02],
          [-7.2395e-03,  5.6474e-03,  3.5222e-03],
          [-1.3256e-02, -2.5357e-02, -1.2665e-02]]],


        [[[-1.3001e-02,  1.8301e-05,  3.1164e-03],
          [ 1.2086e-02, -2.1798e-02, -1.7435e-02],
          [-9.0315e-03,  2.1282e-02,  3.7675e-03]],

         [[-1.7252e-02,  8.2788e-03,  3.9794e-03],
          [-1.0793e-02, -4.4536e-03,  1.8518e-02],
          [-1.8986e-02,  1.4387e-02,  1.0454e-03]],

         [[ 2.1247e-02, -9.4915e-03,  1.8974e-02],
          [-1.5008e-02, -3.7715e-03,  2.5595e-02],
          [-1.8260e-02, -1.3542e-02,  2.1930e-02]],

         ...,

         [[ 1.6674e-02, -5.7781e-03, -1.6847e-02],
          [ 1.1804e-02, -2.8726e-02, -3.0765e-03],
          [ 9.2968e-03, -2.9123e-02, -3.3406e-02]],

         [[-9.6010e-03,  7.7977e-04, -1.4100e-02],
          [ 1.3048e-02,  6.4606e-03,  3.9640e-03],
          [ 2.4790e-02, -3.1674e-02, -9.0608e-03]],

         [[-3.7845e-03, -1.3421e-02, -1.3201e-02],
          [-9.5611e-03, -1.4300e-02,  2.9869e-03],
          [ 1.6730e-02,  3.1099e-02,  1.9106e-02]]],


        ...,


        [[[-1.9940e-02,  1.1590e-02, -2.7199e-02],
          [ 4.5985e-03, -1.1426e-02, -2.9478e-02],
          [ 9.9372e-03,  3.2362e-04, -5.6902e-03]],

         [[-9.6732e-03, -2.3437e-02, -1.7791e-03],
          [ 1.3687e-02,  1.6284e-02,  6.6534e-03],
          [-1.7464e-02, -2.5994e-02, -1.4870e-02]],

         [[ 3.1879e-03, -1.8221e-02,  1.2840e-02],
          [-1.9592e-02,  1.3390e-04,  2.2063e-02],
          [ 1.6150e-02,  4.6672e-03, -1.6944e-02]],

         ...,

         [[ 1.5253e-02, -3.7683e-02,  1.2905e-02],
          [-1.2592e-02, -1.7580e-03,  6.6367e-03],
          [-1.2414e-02, -2.3080e-02,  3.9999e-03]],

         [[ 1.6271e-02, -1.1786e-03,  3.7676e-03],
          [-1.5367e-02,  1.1387e-02,  1.5566e-03],
          [-2.1777e-02,  1.9420e-03,  4.6757e-03]],

         [[-2.0072e-02, -1.2387e-02, -2.3042e-02],
          [ 8.0218e-03,  1.1608e-02, -5.4279e-03],
          [-2.2213e-02,  2.5475e-02,  3.2131e-03]]],


        [[[ 7.7821e-03, -5.0200e-03,  3.6180e-02],
          [ 5.9159e-03,  1.9569e-02,  1.1957e-02],
          [ 2.5661e-02, -5.1913e-03,  3.4522e-02]],

         [[ 4.0024e-03,  2.0748e-02, -1.6520e-02],
          [-1.3768e-02,  3.1206e-03,  2.2143e-02],
          [ 3.7010e-03,  9.8354e-03,  2.7344e-02]],

         [[-1.1339e-02,  3.1605e-03,  1.4664e-02],
          [-1.2909e-02,  8.8625e-03, -2.1707e-02],
          [ 2.2177e-03,  1.7926e-02,  1.0381e-02]],

         ...,

         [[ 1.0781e-02,  1.2514e-02, -1.4438e-02],
          [ 1.3676e-02,  1.7769e-02, -1.8493e-02],
          [-3.3796e-05,  1.7720e-02,  3.0983e-02]],

         [[ 4.6430e-03, -9.4588e-03,  1.9104e-02],
          [ 9.9553e-03,  1.6173e-02,  2.0531e-02],
          [-1.3347e-02,  8.2955e-03,  1.4351e-02]],

         [[-1.3781e-02, -1.0143e-02,  4.7758e-03],
          [ 6.1542e-03, -4.7977e-03,  2.3997e-03],
          [-1.7578e-02, -4.0247e-03,  1.8107e-02]]],


        [[[ 2.8742e-02,  4.5060e-03, -6.0416e-03],
          [ 9.2970e-03, -4.7454e-03,  7.9966e-03],
          [-2.5640e-03, -1.3316e-02, -7.8095e-03]],

         [[-1.0258e-02, -6.7208e-03,  1.4530e-02],
          [ 1.7720e-02, -2.2297e-03,  2.4591e-02],
          [-2.7051e-03,  2.4502e-02,  9.6450e-03]],

         [[-1.1665e-02, -2.0963e-02, -2.0912e-03],
          [-1.3086e-02, -2.6770e-02, -1.3387e-02],
          [ 1.4950e-03,  1.1078e-03,  4.4960e-03]],

         ...,

         [[ 8.7912e-03,  7.3033e-03,  1.7284e-02],
          [ 1.6180e-02,  1.4399e-02, -1.2068e-02],
          [ 6.6671e-04, -1.2939e-02, -1.4576e-02]],

         [[ 1.2541e-02, -1.0589e-02, -1.3720e-02],
          [-9.1568e-04, -2.2969e-02, -1.5011e-02],
          [-1.5810e-05, -2.1355e-02, -1.9839e-02]],

         [[ 5.8967e-03, -2.1853e-03, -1.0557e-02],
          [-1.0701e-03, -8.5394e-03, -1.5382e-02],
          [ 2.0801e-02,  2.5550e-03,  1.4213e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.3545e+07, 9.1662e+05, 8.8773e+05,  ..., 1.8459e+01, 1.7090e+01,
        1.3831e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1063]) 

NULL SPACE BASIS :  tensor([[ 0.0220,  0.0051,  0.0160,  ..., -0.0062,  0.0019, -0.0019],
        [ 0.0086,  0.0017,  0.0124,  ...,  0.0018, -0.0041,  0.0040],
        [-0.0014,  0.0278, -0.0319,  ..., -0.0009,  0.0031, -0.0015],
        ...,
        [-0.0342,  0.0067, -0.0153,  ...,  0.0252, -0.0121, -0.0010],
        [ 0.0048, -0.0130, -0.0195,  ..., -0.0250,  0.0065,  0.0004],
        [ 0.0087,  0.0105, -0.0148,  ...,  0.0029,  0.0016, -0.0013]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 8.8196e-03, -4.2228e-03,  1.7379e-04,  ...,  9.1042e-05,
          2.4386e-05,  1.5509e-05],
        [-4.2228e-03,  1.0165e-02, -4.3892e-03,  ..., -1.4494e-04,
         -2.8047e-04, -2.4935e-04],
        [ 1.7379e-04, -4.3892e-03,  8.2273e-03,  ..., -9.5110e-05,
         -4.0661e-05, -4.6230e-05],
        ...,
        [ 9.1042e-05, -1.4494e-04, -9.5110e-05,  ...,  2.2686e-02,
         -8.4602e-04, -5.4230e-04],
        [ 2.4386e-05, -2.8047e-04, -4.0661e-05,  ..., -8.4602e-04,
          2.1545e-02, -6.9193e-04],
        [ 1.5509e-05, -2.4935e-04, -4.6230e-05,  ..., -5.4230e-04,
         -6.9193e-04,  2.1220e-02]], device='cuda:0') 

reserving basis 781/2304; cond: 1592715.625, radio:0.0019278714898973703
PARAMETER       :  Parameter containing:
tensor([[[[-1.4642e-02, -7.3547e-03, -1.5566e-02],
          [-5.0946e-03, -2.9994e-02,  6.9281e-03],
          [-1.5824e-02, -1.2810e-02, -4.1399e-03]],

         [[ 2.2689e-02,  2.5548e-02,  2.7372e-02],
          [-1.0464e-02,  1.1184e-02, -2.1817e-02],
          [-8.1418e-03,  2.4003e-02,  2.3885e-02]],

         [[-1.9318e-03, -9.1364e-03, -2.6637e-02],
          [-1.1793e-02, -1.5463e-02, -2.4321e-03],
          [ 1.6682e-02, -2.0092e-02, -1.4891e-02]],

         ...,

         [[-1.8129e-02,  2.2474e-02, -1.9400e-02],
          [ 5.1027e-03,  2.6924e-03, -2.2461e-02],
          [ 6.0188e-04,  1.6917e-03,  8.9648e-03]],

         [[ 2.0164e-02,  2.1882e-03, -2.0398e-02],
          [-1.2386e-02,  3.2365e-03, -8.7823e-03],
          [-2.6756e-02,  1.5317e-02,  9.8915e-03]],

         [[ 4.1193e-03,  2.5740e-02,  8.1322e-03],
          [-1.0393e-02,  1.4964e-02, -1.8098e-02],
          [ 8.8760e-03, -4.5574e-03,  1.6129e-02]]],


        [[[ 4.5449e-04,  2.0393e-02,  3.2850e-02],
          [-2.4411e-02,  1.9106e-02,  7.7615e-03],
          [ 3.1203e-03, -4.4567e-03,  7.6647e-03]],

         [[ 2.0380e-02,  1.6828e-02, -1.9074e-02],
          [ 1.0479e-02,  2.8341e-03, -2.5169e-03],
          [-2.7874e-02, -1.0320e-02, -1.5087e-02]],

         [[ 2.8200e-03, -3.0427e-03,  1.0554e-02],
          [ 4.9641e-03, -2.7004e-02, -1.6194e-02],
          [ 1.5855e-02, -9.6139e-03,  1.6453e-02]],

         ...,

         [[-1.8639e-02, -4.0648e-03, -1.4211e-02],
          [ 7.7015e-03,  1.2763e-02, -2.8388e-04],
          [-2.3471e-03,  1.4064e-02, -9.3047e-03]],

         [[ 2.6991e-03, -7.3880e-03,  2.0955e-02],
          [-7.4089e-04, -4.2512e-03,  2.0598e-02],
          [ 6.2771e-03,  9.9301e-03,  1.4280e-02]],

         [[-2.2595e-02, -1.6330e-02, -2.2726e-02],
          [-1.8778e-02, -1.5572e-02, -1.7421e-02],
          [-1.8936e-02, -1.6911e-02,  2.7701e-03]]],


        [[[-2.6863e-02, -1.9239e-02, -1.5690e-02],
          [ 1.4541e-03,  1.7252e-02,  9.2876e-03],
          [ 1.0499e-02, -9.6525e-03, -1.5784e-02]],

         [[-1.6187e-02,  1.5579e-02,  6.1688e-03],
          [-8.3877e-03,  4.6821e-03,  4.5228e-03],
          [-2.2119e-03, -1.2653e-02,  1.9871e-02]],

         [[-7.4435e-03,  7.9719e-03,  1.8051e-03],
          [-4.1648e-05, -1.5641e-02, -2.4191e-02],
          [ 9.6968e-03, -5.1745e-03,  1.4973e-03]],

         ...,

         [[-4.9890e-03,  2.7550e-02, -8.8365e-03],
          [-1.8008e-02,  1.7967e-03, -1.5749e-02],
          [-2.0940e-02, -2.3873e-02,  7.7330e-03]],

         [[-1.2832e-04,  2.3633e-03, -4.2949e-04],
          [-2.9687e-03,  1.2901e-02,  9.1714e-03],
          [-1.8350e-02, -1.6892e-02,  1.0691e-03]],

         [[-1.9937e-02, -1.0676e-02, -6.0303e-04],
          [ 1.0843e-02, -8.7049e-03,  1.9140e-02],
          [-6.3287e-03, -1.8365e-03, -1.2669e-02]]],


        ...,


        [[[-1.4190e-02, -1.6514e-02,  2.1047e-03],
          [-2.6968e-03,  1.0250e-02,  1.2325e-02],
          [-1.3072e-02, -1.2098e-02,  6.3709e-03]],

         [[-1.0086e-03,  2.0492e-02, -3.1337e-03],
          [ 1.3495e-02, -4.4206e-03,  2.7431e-02],
          [-1.2425e-03, -1.3042e-02, -1.0379e-02]],

         [[ 1.1237e-03, -1.6847e-02,  7.7856e-03],
          [ 1.8571e-03,  2.0596e-02, -2.3136e-03],
          [-3.5719e-02, -5.6416e-03,  2.3743e-02]],

         ...,

         [[ 2.3783e-02,  4.5092e-03,  1.3452e-02],
          [ 9.0153e-03,  1.7309e-02,  1.6145e-02],
          [-2.0586e-03, -1.0634e-02,  6.9187e-03]],

         [[ 3.5576e-03, -1.4142e-02, -2.3934e-02],
          [-4.7118e-03,  1.8958e-02, -5.8092e-03],
          [ 1.1208e-02, -1.2665e-02,  1.6629e-02]],

         [[ 1.1804e-02,  1.3896e-02, -6.5181e-03],
          [-1.1920e-02, -2.4909e-03,  1.8343e-02],
          [ 1.2875e-02, -1.5357e-03,  6.5266e-03]]],


        [[[ 8.0189e-03, -1.7948e-02,  7.2564e-03],
          [ 2.6081e-02,  4.3332e-03, -9.2766e-03],
          [-3.6435e-03,  1.7595e-03, -1.7934e-02]],

         [[ 4.8588e-03, -7.8558e-03,  1.3557e-02],
          [ 2.0518e-02,  2.1020e-02,  3.5129e-02],
          [ 3.2879e-02,  2.3492e-02,  3.4545e-02]],

         [[-1.2546e-02,  1.6361e-02, -8.2236e-03],
          [ 2.8595e-02,  2.9770e-02,  1.1702e-02],
          [-1.6852e-02,  2.6561e-04,  2.4516e-02]],

         ...,

         [[-1.4031e-02,  6.2859e-03,  2.0437e-02],
          [ 1.7434e-02,  9.5884e-03,  1.1483e-02],
          [-1.4119e-02,  9.1267e-03, -1.5406e-02]],

         [[-2.1306e-02, -2.0403e-02,  8.5535e-03],
          [-1.0952e-02,  9.1766e-03, -5.2238e-04],
          [ 1.6411e-02, -1.4884e-02, -8.6567e-03]],

         [[-1.8659e-02,  2.0699e-03,  2.2692e-02],
          [ 1.3000e-02,  2.0659e-02, -1.1235e-02],
          [ 1.2683e-02,  1.2178e-02, -7.1041e-03]]],


        [[[ 1.5596e-02, -1.1765e-02, -2.7163e-03],
          [-6.3486e-03, -1.0660e-02, -3.1603e-03],
          [-2.1810e-02, -1.8642e-02, -1.2463e-03]],

         [[ 5.1576e-03, -5.6302e-03, -1.2315e-02],
          [-7.2673e-03, -2.8805e-03, -1.7577e-02],
          [ 2.0628e-05, -2.1051e-02, -9.7302e-03]],

         [[ 3.2245e-02,  2.0984e-03,  1.9463e-03],
          [ 2.5655e-02,  5.5239e-03,  8.5782e-03],
          [-2.6482e-03, -6.1225e-03, -1.4287e-02]],

         ...,

         [[ 1.4772e-02,  1.2889e-02,  2.0647e-03],
          [ 1.9822e-02, -7.2379e-03,  5.6064e-03],
          [-6.2068e-03, -3.6277e-03,  1.8001e-04]],

         [[-6.7490e-03,  3.9763e-03, -1.9941e-02],
          [ 2.1322e-02,  1.8648e-02,  1.2621e-02],
          [ 7.5907e-04,  3.8297e-02,  3.3283e-02]],

         [[ 2.0746e-02, -1.0597e-03, -1.1857e-02],
          [ 7.7691e-03,  7.8732e-03, -1.2474e-02],
          [-3.1030e-02, -4.3690e-03, -1.7600e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.2281e+07, 8.4519e+05, 7.9774e+05,  ..., 9.1405e+00, 8.7419e+00,
        7.7110e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 781]) 

NULL SPACE BASIS :  tensor([[-0.0197, -0.0295,  0.0133,  ..., -0.0025,  0.0002,  0.0047],
        [ 0.0163,  0.0288, -0.0028,  ...,  0.0047, -0.0031, -0.0004],
        [-0.0140,  0.0114, -0.0175,  ...,  0.0008,  0.0002, -0.0035],
        ...,
        [-0.0039,  0.0062,  0.0116,  ...,  0.0011,  0.0041,  0.0006],
        [ 0.0146, -0.0128, -0.0087,  ...,  0.0029, -0.0026,  0.0003],
        [-0.0112,  0.0063,  0.0016,  ..., -0.0016,  0.0009, -0.0005]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 1.4596e-02, -6.3588e-03, -5.2040e-04,  ..., -1.0709e-04,
          3.1298e-05,  3.1526e-04],
        [-6.3588e-03,  1.4824e-02, -7.2216e-03,  ..., -1.1959e-04,
         -1.9195e-05, -2.1535e-04],
        [-5.2040e-04, -7.2216e-03,  1.7424e-02,  ...,  3.0119e-04,
         -9.0180e-05, -7.6682e-05],
        ...,
        [-1.0709e-04, -1.1959e-04,  3.0119e-04,  ...,  1.8362e-03,
         -7.6542e-04, -3.0867e-05],
        [ 3.1298e-05, -1.9195e-05, -9.0180e-05,  ..., -7.6542e-04,
          1.9271e-03, -6.5299e-04],
        [ 3.1526e-04, -2.1535e-04, -7.6682e-05,  ..., -3.0867e-05,
         -6.5299e-04,  1.8020e-03]], device='cuda:0') 

reserving basis 1132/2304; cond: 1205984.75, radio:0.0036404745187610388
PARAMETER       :  Parameter containing:
tensor([[[[-6.5829e-03, -8.8055e-03, -7.2322e-03],
          [-7.6395e-03,  1.5172e-02,  1.3396e-02],
          [-2.3171e-02, -4.7520e-03, -2.9054e-03]],

         [[ 9.4540e-04,  1.4416e-03, -1.2508e-02],
          [-6.7846e-03,  1.0922e-02, -5.2548e-03],
          [ 2.0731e-02, -2.5402e-02,  5.5535e-03]],

         [[ 1.3231e-02, -4.9479e-03,  7.9236e-03],
          [ 1.2042e-02, -1.0662e-02,  1.4104e-03],
          [-1.9250e-02, -1.2304e-02, -8.1205e-03]],

         ...,

         [[ 1.5197e-02, -1.5579e-02,  2.2076e-03],
          [ 1.0996e-02,  1.0718e-02,  1.3435e-02],
          [-1.0830e-03, -2.5925e-02,  1.3296e-02]],

         [[ 2.1226e-02,  1.1899e-02, -1.2615e-03],
          [-9.9338e-03,  1.8225e-03, -1.0559e-02],
          [-2.7726e-03,  1.8576e-02, -1.7264e-02]],

         [[-6.5662e-03, -4.7494e-03,  8.0065e-03],
          [-3.0979e-02,  3.1888e-03,  5.9152e-04],
          [ 6.6070e-03, -1.0845e-02,  1.9595e-02]]],


        [[[-1.2029e-02,  6.7482e-03,  7.4195e-03],
          [ 9.9084e-03, -7.0132e-03,  1.4953e-02],
          [ 1.5494e-02, -7.2375e-03, -8.6555e-04]],

         [[ 1.9269e-02, -1.2500e-03, -1.8713e-02],
          [ 3.6081e-03, -3.1918e-03,  9.4053e-03],
          [ 1.2830e-02, -2.1782e-02, -1.5213e-02]],

         [[ 1.7728e-02, -2.3611e-02,  4.1998e-03],
          [-2.1736e-02,  1.9168e-04,  9.5011e-03],
          [-9.9744e-03, -1.8384e-02,  4.2089e-03]],

         ...,

         [[-2.1301e-02, -1.7827e-02, -8.6336e-03],
          [ 2.9448e-04, -1.3006e-02, -2.6096e-02],
          [ 4.6482e-03, -5.1905e-03,  3.9209e-04]],

         [[-8.8605e-03,  6.2080e-03, -5.5991e-03],
          [ 7.5488e-03,  2.4340e-02, -1.0484e-02],
          [-1.5011e-02,  1.1718e-02, -8.9418e-03]],

         [[ 1.0790e-02,  2.0799e-02,  4.5746e-03],
          [ 9.5902e-04,  6.4541e-03, -9.1053e-03],
          [-6.9480e-03,  2.6981e-02, -2.0034e-03]]],


        [[[ 3.2490e-03, -2.4875e-02, -4.0674e-03],
          [-2.6312e-03,  1.0865e-02, -5.0902e-03],
          [ 6.7282e-03,  1.0673e-02,  3.5466e-03]],

         [[ 2.2473e-02,  1.7549e-02,  2.0632e-02],
          [ 1.9893e-02,  9.4857e-03,  4.7327e-03],
          [ 8.5568e-03, -4.7728e-03,  7.1718e-03]],

         [[-1.8872e-02, -1.4531e-02,  2.5155e-03],
          [ 5.8446e-03, -1.9500e-02, -6.2799e-03],
          [-1.4563e-02, -9.1449e-03, -3.1911e-02]],

         ...,

         [[ 2.0541e-03, -3.7380e-03, -3.0602e-02],
          [-5.3821e-03,  3.0776e-03, -8.4533e-03],
          [ 1.8100e-02,  1.2834e-03, -3.2007e-03]],

         [[ 1.2722e-02,  5.2051e-03, -5.2475e-03],
          [-5.7259e-03, -1.2399e-03,  7.2728e-03],
          [-2.1254e-02, -2.2173e-02, -1.5079e-02]],

         [[ 1.7476e-03,  5.5062e-03, -6.9990e-03],
          [ 2.4435e-03,  1.7071e-02,  6.3583e-05],
          [ 1.1549e-02,  1.0990e-02, -9.5986e-03]]],


        ...,


        [[[ 7.2194e-04,  1.0976e-03, -1.3847e-02],
          [-1.4093e-02,  2.6604e-03,  6.3762e-04],
          [ 8.2064e-03, -1.1102e-02, -8.0719e-03]],

         [[ 1.7000e-02, -4.2711e-03, -1.1998e-02],
          [ 2.1729e-02,  1.5581e-02, -1.5467e-02],
          [-6.8826e-03,  2.8732e-03, -1.5619e-02]],

         [[-2.0840e-02, -2.3662e-03,  8.6528e-03],
          [-7.3501e-03, -1.5428e-02,  6.3152e-03],
          [ 4.5379e-03,  3.6569e-03, -2.5000e-02]],

         ...,

         [[ 9.3147e-03, -1.3065e-02,  7.5886e-03],
          [ 1.0032e-03,  3.9850e-03, -6.3199e-03],
          [-6.4805e-03, -7.4719e-03, -2.9411e-02]],

         [[-1.5900e-02,  6.2993e-03, -3.5464e-03],
          [ 7.7485e-03, -2.3435e-02, -2.1052e-02],
          [-6.9908e-03, -1.3069e-02, -1.1705e-02]],

         [[ 9.9570e-03,  1.8595e-02,  6.7688e-03],
          [-1.0425e-02,  1.2016e-02, -1.0289e-02],
          [-1.4783e-03,  1.0567e-02, -5.6314e-03]]],


        [[[-1.8758e-02, -3.9310e-03, -8.2969e-04],
          [-9.7326e-03,  2.9386e-03,  7.3075e-03],
          [ 1.2179e-03,  1.1831e-02,  8.4790e-03]],

         [[-1.1125e-02,  2.1208e-02,  1.1219e-02],
          [-3.5103e-03,  3.0474e-03,  1.1533e-02],
          [ 7.8057e-03, -1.1507e-02,  1.0702e-02]],

         [[-9.6969e-03,  3.4973e-03,  1.7181e-02],
          [ 1.3688e-02, -4.5432e-03,  1.5571e-02],
          [-5.3049e-03, -1.0919e-02, -1.2027e-02]],

         ...,

         [[ 1.1387e-02, -1.9614e-02, -1.1175e-03],
          [-2.0400e-03, -1.5224e-02,  1.1193e-02],
          [-1.5428e-02, -1.2685e-02,  3.8311e-03]],

         [[-3.3259e-03,  6.2615e-03, -2.6347e-02],
          [-1.5762e-02, -5.6963e-03, -3.3741e-02],
          [-1.4991e-02, -2.0873e-02, -5.8120e-03]],

         [[ 1.1643e-02,  1.8089e-02,  2.7755e-02],
          [ 1.3732e-02,  2.6889e-03,  2.5479e-03],
          [ 7.2311e-03, -1.1014e-03,  1.2469e-02]]],


        [[[ 2.0748e-03,  7.3709e-03,  2.3437e-02],
          [-4.0322e-03, -2.9833e-03, -1.6969e-02],
          [ 6.2761e-03,  4.5582e-03, -8.1427e-03]],

         [[-1.0668e-03, -4.1547e-03, -1.1330e-02],
          [-9.7914e-03, -9.6424e-03, -1.6641e-02],
          [ 3.0339e-03, -2.6742e-02,  1.2313e-03]],

         [[ 2.1278e-03, -1.7579e-02,  8.6154e-03],
          [-1.3143e-02,  9.1562e-03,  4.7977e-03],
          [-2.0056e-02,  7.7381e-04, -9.7241e-03]],

         ...,

         [[-1.4377e-02, -1.6224e-02,  1.9571e-02],
          [-1.2326e-02, -1.2473e-02, -2.3356e-02],
          [ 8.5234e-03, -1.2788e-02,  1.8263e-02]],

         [[ 2.0920e-02,  8.7526e-03, -8.2182e-03],
          [ 1.3874e-02,  1.1422e-02,  1.5989e-02],
          [-9.9135e-03, -2.2429e-02, -1.0875e-03]],

         [[-9.4620e-03, -8.8080e-03,  2.5225e-03],
          [-1.7678e-02,  2.7915e-02,  1.5242e-02],
          [-1.9540e-02, -3.0253e-03, -1.6236e-02]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([3.8093e+06, 4.2670e+05, 4.1339e+05,  ..., 3.9237e+00, 3.5086e+00,
        3.1587e+00], device='cuda:0') 

NULL SPACE DIM :  torch.Size([2304, 1132]) 

NULL SPACE BASIS :  tensor([[-0.0034, -0.0107, -0.0209,  ...,  0.0040,  0.0030,  0.0091],
        [-0.0102,  0.0031, -0.0155,  ..., -0.0060, -0.0058, -0.0060],
        [-0.0005, -0.0084,  0.0184,  ...,  0.0050, -0.0007,  0.0040],
        ...,
        [-0.0056,  0.0011,  0.0072,  ...,  0.0059, -0.0045,  0.0004],
        [-0.0192,  0.0186, -0.0039,  ...,  0.0079,  0.0207,  0.0031],
        [ 0.0045, -0.0029,  0.0202,  ...,  0.0121, -0.0100, -0.0104]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 5.1124e-03, -2.6146e-03,  3.7050e-04,  ...,  1.6955e-04,
         -2.5862e-04, -1.1201e-04],
        [-2.6146e-03,  5.7736e-03, -2.3917e-03,  ..., -1.1142e-04,
          2.4695e-04,  1.1540e-04],
        [ 3.7050e-04, -2.3917e-03,  3.5352e-03,  ...,  2.1403e-04,
          1.7808e-04, -2.6801e-05],
        ...,
        [ 1.6955e-04, -1.1142e-04,  2.1403e-04,  ...,  2.3463e-02,
         -2.3782e-03, -9.8837e-04],
        [-2.5862e-04,  2.4695e-04,  1.7808e-04,  ..., -2.3782e-03,
          2.1921e-02, -3.2094e-03],
        [-1.1201e-04,  1.1540e-04, -2.6801e-05,  ..., -9.8837e-04,
         -3.2094e-03,  2.1170e-02]], device='cuda:0') 

reserving basis 1305/4608; cond: 5776614.5, radio:0.0007983635878190398
PARAMETER       :  Parameter containing:
tensor([[[[ 0.0048,  0.0042,  0.0131],
          [-0.0073, -0.0081,  0.0132],
          [-0.0035, -0.0046,  0.0104]],

         [[ 0.0042, -0.0068,  0.0014],
          [ 0.0058,  0.0099,  0.0023],
          [ 0.0173, -0.0072,  0.0087]],

         [[-0.0028, -0.0140, -0.0002],
          [-0.0053, -0.0068, -0.0234],
          [-0.0115,  0.0097, -0.0212]],

         ...,

         [[-0.0054,  0.0020,  0.0034],
          [-0.0019, -0.0014, -0.0170],
          [-0.0102, -0.0013, -0.0254]],

         [[-0.0049,  0.0109,  0.0140],
          [ 0.0025,  0.0159,  0.0027],
          [-0.0012,  0.0044,  0.0044]],

         [[-0.0141, -0.0311, -0.0279],
          [-0.0061,  0.0004, -0.0198],
          [ 0.0082,  0.0017, -0.0008]]],


        [[[ 0.0088, -0.0169,  0.0086],
          [-0.0030,  0.0055,  0.0065],
          [ 0.0051, -0.0129, -0.0114]],

         [[-0.0149,  0.0072, -0.0097],
          [-0.0031, -0.0026, -0.0036],
          [ 0.0072,  0.0046, -0.0109]],

         [[-0.0017, -0.0003,  0.0109],
          [ 0.0004,  0.0131,  0.0016],
          [ 0.0107,  0.0231,  0.0184]],

         ...,

         [[-0.0033,  0.0026, -0.0093],
          [ 0.0004, -0.0064, -0.0024],
          [-0.0019,  0.0075,  0.0135]],

         [[ 0.0047, -0.0038, -0.0118],
          [-0.0050, -0.0038,  0.0031],
          [-0.0057, -0.0002, -0.0125]],

         [[ 0.0011,  0.0020, -0.0044],
          [-0.0146, -0.0017,  0.0002],
          [ 0.0113, -0.0063, -0.0041]]],


        [[[ 0.0024,  0.0103,  0.0029],
          [ 0.0074,  0.0147,  0.0185],
          [-0.0073, -0.0025,  0.0037]],

         [[ 0.0005,  0.0132,  0.0091],
          [-0.0118, -0.0070,  0.0039],
          [-0.0098, -0.0074, -0.0060]],

         [[-0.0041, -0.0075,  0.0061],
          [-0.0059, -0.0116,  0.0098],
          [-0.0010, -0.0053, -0.0011]],

         ...,

         [[ 0.0081,  0.0018, -0.0102],
          [ 0.0069, -0.0024, -0.0120],
          [ 0.0084,  0.0036,  0.0013]],

         [[ 0.0090,  0.0043,  0.0194],
          [-0.0067, -0.0068,  0.0178],
          [-0.0052,  0.0026, -0.0056]],

         [[-0.0046,  0.0042, -0.0134],
          [-0.0128,  0.0026, -0.0072],
          [-0.0080,  0.0104,  0.0082]]],


        ...,


        [[[ 0.0063, -0.0043, -0.0167],
          [-0.0106,  0.0058, -0.0035],
          [ 0.0016,  0.0110, -0.0136]],

         [[-0.0061,  0.0002,  0.0034],
          [-0.0083,  0.0155, -0.0003],
          [-0.0073,  0.0093, -0.0078]],

         [[ 0.0008, -0.0093,  0.0009],
          [-0.0036, -0.0265, -0.0115],
          [ 0.0120, -0.0201, -0.0179]],

         ...,

         [[-0.0181,  0.0059, -0.0078],
          [-0.0073, -0.0049, -0.0116],
          [-0.0062,  0.0115,  0.0067]],

         [[-0.0013, -0.0034, -0.0031],
          [-0.0063,  0.0057,  0.0138],
          [ 0.0086, -0.0060, -0.0100]],

         [[ 0.0148,  0.0145,  0.0062],
          [-0.0007,  0.0053,  0.0020],
          [-0.0005,  0.0005, -0.0011]]],


        [[[-0.0136, -0.0042,  0.0035],
          [-0.0136,  0.0084, -0.0186],
          [-0.0069, -0.0025,  0.0027]],

         [[ 0.0013, -0.0137, -0.0223],
          [ 0.0097,  0.0043, -0.0162],
          [ 0.0084,  0.0042,  0.0049]],

         [[-0.0066,  0.0007,  0.0107],
          [ 0.0016, -0.0009, -0.0062],
          [-0.0073, -0.0050, -0.0074]],

         ...,

         [[ 0.0023,  0.0016, -0.0060],
          [-0.0245, -0.0180, -0.0173],
          [-0.0006, -0.0016,  0.0138]],

         [[ 0.0046, -0.0042,  0.0039],
          [-0.0027,  0.0015,  0.0003],
          [ 0.0116,  0.0107,  0.0058]],

         [[ 0.0087, -0.0139, -0.0131],
          [ 0.0020, -0.0032,  0.0140],
          [-0.0148,  0.0027, -0.0051]]],


        [[[ 0.0088, -0.0064, -0.0038],
          [-0.0140, -0.0005,  0.0048],
          [ 0.0013, -0.0098, -0.0120]],

         [[-0.0046,  0.0139,  0.0025],
          [ 0.0057, -0.0101,  0.0007],
          [ 0.0040,  0.0069, -0.0056]],

         [[-0.0129, -0.0038,  0.0168],
          [-0.0027,  0.0045,  0.0183],
          [-0.0086, -0.0017,  0.0025]],

         ...,

         [[ 0.0143,  0.0057, -0.0005],
          [ 0.0114, -0.0142,  0.0059],
          [ 0.0023, -0.0148,  0.0105]],

         [[ 0.0197,  0.0232,  0.0024],
          [ 0.0156,  0.0010, -0.0054],
          [-0.0102,  0.0112, -0.0130]],

         [[-0.0041, -0.0133,  0.0085],
          [-0.0117, -0.0063, -0.0053],
          [ 0.0118, -0.0020,  0.0029]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([4.4459e+06, 7.0179e+05, 6.8718e+05,  ..., 9.0931e-01, 8.9363e-01,
        7.6965e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1305]) 

NULL SPACE BASIS :  tensor([[-2.6077e-02, -1.9576e-02, -2.1694e-02,  ...,  5.5969e-03,
         -4.8485e-04,  2.6075e-03],
        [-2.7799e-03,  5.7802e-03, -5.3296e-03,  ..., -2.5205e-04,
          1.7975e-04,  1.1053e-04],
        [ 9.1927e-03,  1.7609e-02,  2.6868e-03,  ...,  1.3951e-03,
         -3.5924e-03, -8.5629e-05],
        ...,
        [ 1.3641e-02, -1.4238e-02,  2.5052e-03,  ..., -3.7288e-03,
         -3.4788e-04,  1.4376e-03],
        [-1.7435e-03,  7.4316e-03, -9.8359e-03,  ..., -5.0881e-03,
         -3.2153e-03, -1.5106e-03],
        [ 8.1249e-03,  2.8453e-02,  2.1792e-03,  ...,  9.8921e-04,
         -2.8753e-04, -6.6500e-03]], device='cuda:0') 

TRANSFORMS       :  tensor([[ 6.0889e-03, -4.4804e-04, -1.1605e-04,  ..., -9.1755e-05,
         -2.5493e-04, -1.7459e-04],
        [-4.4804e-04,  3.3946e-03, -1.9015e-04,  ...,  6.1172e-05,
          1.1009e-04, -7.4071e-05],
        [-1.1605e-04, -1.9015e-04,  4.9444e-03,  ...,  2.4427e-04,
         -1.1032e-04,  4.8133e-05],
        ...,
        [-9.1755e-05,  6.1172e-05,  2.4427e-04,  ...,  9.6645e-03,
         -6.1982e-04,  2.3389e-05],
        [-2.5493e-04,  1.1009e-04, -1.1032e-04,  ..., -6.1982e-04,
          5.6479e-03, -3.5661e-04],
        [-1.7459e-04, -7.4071e-05,  4.8133e-05,  ...,  2.3389e-05,
         -3.5661e-04,  6.6410e-03]], device='cuda:0') 

reserving basis 185/256; cond: 57859.9921875, radio:0.010912885889410973
PARAMETER       :  Parameter containing:
tensor([[[[-0.0151]],

         [[-0.0363]],

         [[-0.0352]],

         ...,

         [[ 0.0139]],

         [[-0.0481]],

         [[-0.0611]]],


        [[[ 0.0584]],

         [[-0.0578]],

         [[ 0.0127]],

         ...,

         [[ 0.0061]],

         [[ 0.0116]],

         [[ 0.0188]]],


        [[[ 0.0403]],

         [[ 0.0228]],

         [[-0.0108]],

         ...,

         [[ 0.0191]],

         [[ 0.0409]],

         [[-0.0298]]],


        ...,


        [[[-0.0256]],

         [[ 0.0132]],

         [[ 0.0384]],

         ...,

         [[ 0.0201]],

         [[-0.0195]],

         [[-0.0205]]],


        [[[ 0.0141]],

         [[ 0.0220]],

         [[-0.0289]],

         ...,

         [[ 0.0392]],

         [[ 0.0096]],

         [[ 0.0418]]],


        [[[-0.0328]],

         [[ 0.0249]],

         [[ 0.0376]],

         ...,

         [[ 0.0532]],

         [[-0.0585]],

         [[ 0.0424]]]], device='cuda:0', requires_grad=True) 

EIGEN VALUE     :  tensor([7.5478e+05, 4.5123e+04, 3.8109e+04, 1.3218e+04, 1.0514e+04, 7.0565e+03,
        6.5167e+03, 3.6542e+03, 2.5354e+03, 2.2657e+03, 1.8789e+03, 1.5529e+03,
        1.4714e+03, 1.3146e+03, 1.2670e+03, 1.1792e+03, 1.1428e+03, 1.0778e+03,
        9.8867e+02, 9.2584e+02, 8.5270e+02, 8.3275e+02, 7.6431e+02, 6.8989e+02,
        6.4086e+02, 5.9845e+02, 5.7019e+02, 5.1850e+02, 4.9463e+02, 4.7758e+02,
        4.5892e+02, 4.1803e+02, 4.1354e+02, 4.0831e+02, 3.7602e+02, 3.5449e+02,
        3.4049e+02, 3.1763e+02, 3.0967e+02, 2.9761e+02, 2.8408e+02, 2.7354e+02,
        2.6901e+02, 2.5870e+02, 2.4818e+02, 2.4226e+02, 2.3622e+02, 2.2876e+02,
        2.1735e+02, 2.1051e+02, 2.0184e+02, 1.9684e+02, 1.8992e+02, 1.8459e+02,
        1.8292e+02, 1.8116e+02, 1.7787e+02, 1.7192e+02, 1.6862e+02, 1.6644e+02,
        1.6157e+02, 1.5994e+02, 1.5641e+02, 1.5083e+02, 1.4788e+02, 1.4594e+02,
        1.4284e+02, 1.3925e+02, 1.3728e+02, 1.3524e+02, 1.3336e+02, 1.2957e+02,
        1.2829e+02, 1.2442e+02, 1.2172e+02, 1.2059e+02, 1.1877e+02, 1.1677e+02,
        1.1500e+02, 1.1206e+02, 1.1038e+02, 1.0888e+02, 1.0768e+02, 1.0652e+02,
        1.0502e+02, 1.0310e+02, 1.0187e+02, 1.0129e+02, 9.9774e+01, 9.8461e+01,
        9.6937e+01, 9.4988e+01, 9.4166e+01, 9.3987e+01, 9.2344e+01, 9.2036e+01,
        8.9084e+01, 8.8556e+01, 8.7757e+01, 8.7304e+01, 8.6167e+01, 8.5351e+01,
        8.3496e+01, 8.3065e+01, 8.2004e+01, 8.0859e+01, 8.0431e+01, 7.9348e+01,
        7.7971e+01, 7.7439e+01, 7.6641e+01, 7.6229e+01, 7.5424e+01, 7.4696e+01,
        7.3490e+01, 7.2553e+01, 7.1985e+01, 7.0983e+01, 7.0242e+01, 6.9790e+01,
        6.9365e+01, 6.8392e+01, 6.7819e+01, 6.7199e+01, 6.6356e+01, 6.5746e+01,
        6.5548e+01, 6.5159e+01, 6.4683e+01, 6.4359e+01, 6.3296e+01, 6.2165e+01,
        6.1712e+01, 6.1160e+01, 6.0159e+01, 5.9615e+01, 5.9400e+01, 5.9266e+01,
        5.8403e+01, 5.8256e+01, 5.7686e+01, 5.6938e+01, 5.6566e+01, 5.5743e+01,
        5.5438e+01, 5.5336e+01, 5.4979e+01, 5.4175e+01, 5.3807e+01, 5.3599e+01,
        5.2993e+01, 5.2669e+01, 5.1959e+01, 5.1259e+01, 5.1087e+01, 5.1059e+01,
        5.0386e+01, 4.9973e+01, 4.9849e+01, 4.9605e+01, 4.8892e+01, 4.8832e+01,
        4.8342e+01, 4.7755e+01, 4.7023e+01, 4.6641e+01, 4.5957e+01, 4.5892e+01,
        4.5711e+01, 4.5617e+01, 4.5038e+01, 4.4801e+01, 4.4489e+01, 4.4152e+01,
        4.3667e+01, 4.3514e+01, 4.3057e+01, 4.2765e+01, 4.2469e+01, 4.1790e+01,
        4.1570e+01, 4.1225e+01, 4.0970e+01, 4.0605e+01, 4.0317e+01, 4.0057e+01,
        3.9745e+01, 3.9229e+01, 3.9024e+01, 3.8379e+01, 3.8286e+01, 3.7620e+01,
        3.7564e+01, 3.7385e+01, 3.6769e+01, 3.6395e+01, 3.6094e+01, 3.5886e+01,
        3.5693e+01, 3.5574e+01, 3.5294e+01, 3.5124e+01, 3.4594e+01, 3.4262e+01,
        3.3826e+01, 3.3676e+01, 3.3273e+01, 3.2892e+01, 3.2755e+01, 3.2751e+01,
        3.2430e+01, 3.2217e+01, 3.1628e+01, 3.1335e+01, 3.1111e+01, 3.0804e+01,
        3.0495e+01, 3.0063e+01, 2.9967e+01, 2.9411e+01, 2.9345e+01, 2.8864e+01,
        2.8685e+01, 2.8096e+01, 2.7923e+01, 2.7729e+01, 2.6952e+01, 2.6919e+01,
        2.6546e+01, 2.6369e+01, 2.5997e+01, 2.5892e+01, 2.5573e+01, 2.5427e+01,
        2.5023e+01, 2.4354e+01, 2.4076e+01, 2.3503e+01, 2.3147e+01, 2.2791e+01,
        2.2661e+01, 2.1760e+01, 2.1615e+01, 2.1337e+01, 2.0269e+01, 1.9456e+01,
        1.9185e+01, 1.8671e+01, 1.8288e+01, 1.7874e+01, 1.7371e+01, 1.6711e+01,
        1.5923e+01, 1.5491e+01, 1.4860e+01, 1.3045e+01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([256, 185]) 

NULL SPACE BASIS :  tensor([[ 0.0087,  0.0649, -0.0068,  ...,  0.0034,  0.0063, -0.0066],
        [ 0.0340, -0.0249,  0.0204,  ..., -0.0104,  0.0266, -0.0388],
        [ 0.1074,  0.0039,  0.0014,  ...,  0.0467,  0.0327, -0.0135],
        ...,
        [ 0.0542,  0.0540, -0.0379,  ..., -0.0347, -0.0135, -0.0085],
        [-0.1064, -0.1071, -0.0526,  ...,  0.0126,  0.0005, -0.0004],
        [-0.0022,  0.0509,  0.0225,  ..., -0.0934, -0.3344, -0.0217]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 0.0203, -0.0005, -0.0012,  ...,  0.0027, -0.0023,  0.0011],
        [-0.0005,  0.0581, -0.0023,  ..., -0.0046,  0.0031,  0.0005],
        [-0.0012, -0.0023,  0.0613,  ..., -0.0028,  0.0037, -0.0006],
        ...,
        [ 0.0027, -0.0046, -0.0028,  ...,  0.0549, -0.0021,  0.0018],
        [-0.0023,  0.0031,  0.0037,  ..., -0.0021,  0.0440, -0.0010],
        [ 0.0011,  0.0005, -0.0006,  ...,  0.0018, -0.0010,  0.0683]],
       device='cuda:0') 

reserving basis 1430/4608; cond: 6780062.5, radio:0.0007434868603013456
PARAMETER       :  Parameter containing:
tensor([[[[-3.9510e-03,  4.4092e-03, -1.2418e-02],
          [-8.5102e-03,  5.3002e-03, -1.5857e-02],
          [ 9.5576e-03, -2.7462e-03,  6.5129e-03]],

         [[-7.0232e-03, -2.0790e-03,  4.0582e-03],
          [ 9.1351e-03,  4.6305e-03, -4.4165e-03],
          [ 5.4447e-03, -5.2085e-03,  6.5428e-03]],

         [[ 8.6371e-04, -1.3091e-02, -1.4193e-02],
          [-4.6832e-03, -1.7630e-03, -1.2224e-02],
          [ 1.8239e-03,  1.4045e-02, -4.4573e-03]],

         ...,

         [[-3.4167e-03,  9.5812e-03, -1.2180e-03],
          [ 9.7576e-03,  3.3431e-04,  6.9136e-03],
          [ 5.3336e-03,  1.0957e-02, -5.1604e-03]],

         [[-1.1006e-02,  2.2017e-04, -1.0536e-02],
          [ 1.1142e-03, -6.9941e-03, -1.8825e-03],
          [-3.6666e-05, -1.0086e-02,  1.0626e-02]],

         [[ 3.5316e-04,  1.0439e-02,  8.3454e-03],
          [ 6.8147e-03,  8.7316e-03,  1.2813e-02],
          [-1.2937e-02,  2.7330e-03, -9.0608e-03]]],


        [[[ 5.9735e-03, -3.7922e-03,  7.9989e-03],
          [-1.3408e-02, -9.0237e-03,  5.7507e-03],
          [-2.2960e-02,  8.2230e-04,  7.0741e-03]],

         [[-7.3756e-03,  1.7635e-03,  2.3547e-03],
          [-6.6496e-03,  1.6945e-02, -1.5505e-03],
          [ 1.0975e-02,  7.7007e-03,  1.5333e-02]],

         [[ 5.4264e-03, -6.8089e-03,  6.0948e-03],
          [ 1.2883e-03, -1.1313e-02,  4.8345e-03],
          [ 1.1496e-02, -6.3504e-06,  7.2259e-03]],

         ...,

         [[ 8.9597e-03,  3.5852e-03,  7.9398e-03],
          [ 1.1548e-02, -3.5811e-03, -4.4938e-03],
          [-6.2288e-03, -1.1766e-03,  2.6649e-03]],

         [[ 2.0484e-03, -1.7203e-03, -9.7642e-03],
          [ 8.0335e-04, -4.6914e-03, -1.9147e-03],
          [-3.1034e-03,  1.2050e-02,  2.6082e-03]],

         [[ 1.2103e-02, -7.3348e-03, -9.5218e-03],
          [ 1.2908e-02, -5.0659e-03,  1.7724e-03],
          [-1.7803e-03,  5.9504e-03, -1.0515e-02]]],


        [[[-6.1154e-03, -1.8186e-02, -8.4230e-03],
          [-2.3682e-03,  8.1187e-03, -1.4809e-02],
          [ 1.5392e-02,  1.8151e-02,  5.9163e-03]],

         [[ 7.2458e-03, -1.0418e-02,  5.8535e-04],
          [-5.2845e-03, -2.8140e-03, -2.1514e-02],
          [-4.3587e-03, -4.3694e-03, -2.1327e-02]],

         [[-1.2528e-03, -2.1443e-03,  5.4377e-03],
          [ 1.5147e-03,  1.9344e-03, -7.5846e-04],
          [-5.8071e-03,  1.0028e-02, -2.2705e-03]],

         ...,

         [[ 7.3564e-03,  1.4089e-02,  8.3409e-03],
          [-5.5874e-03,  9.5880e-03, -8.5929e-03],
          [ 3.7277e-04, -1.2573e-02, -2.2360e-03]],

         [[ 5.4494e-03, -1.0963e-02, -5.7508e-03],
          [ 3.4676e-03, -6.4677e-03, -1.9739e-02],
          [-4.2295e-03,  1.3645e-03, -1.4371e-02]],

         [[-1.2804e-02, -1.9083e-02, -3.9814e-03],
          [ 7.5506e-03,  5.9342e-03, -1.1727e-02],
          [-4.8502e-03, -1.0753e-02,  6.1166e-03]]],


        ...,


        [[[-1.1741e-02,  8.3385e-03, -8.4923e-03],
          [ 8.9414e-03,  7.0287e-03,  1.0381e-02],
          [-1.2788e-02, -1.7444e-02,  8.6542e-04]],

         [[-1.6189e-02, -1.9644e-03, -1.1732e-03],
          [ 4.1487e-03,  2.3298e-03, -1.9294e-03],
          [-9.8799e-03,  4.6200e-03, -3.4084e-03]],

         [[-1.5714e-02,  7.0086e-04,  1.1066e-03],
          [-1.2895e-02, -5.5019e-03, -1.5677e-02],
          [-8.2955e-03, -1.4358e-02, -3.0520e-03]],

         ...,

         [[ 9.8351e-03, -1.4781e-02,  5.7068e-03],
          [-3.0802e-03,  4.2489e-03, -1.0504e-02],
          [-7.7103e-03, -1.3154e-03,  5.6143e-03]],

         [[-1.8148e-03, -2.8809e-03, -1.6452e-03],
          [-1.2402e-02, -8.1179e-03, -1.9339e-02],
          [-1.4311e-02, -4.0987e-03, -3.9440e-03]],

         [[-3.3011e-03, -4.0499e-03, -1.0698e-02],
          [ 8.6932e-03,  9.6498e-03, -1.3433e-03],
          [ 1.0787e-02,  5.9177e-03,  1.0596e-02]]],


        [[[-1.3699e-02, -2.5556e-03,  1.0080e-02],
          [ 6.9036e-03, -7.3843e-04,  2.2615e-02],
          [-6.0790e-03, -7.4710e-03,  2.0968e-02]],

         [[ 1.0475e-02, -7.7690e-03, -1.7158e-03],
          [-3.6373e-03, -3.9492e-03,  9.1756e-03],
          [-1.9007e-02, -7.0620e-03, -1.9061e-02]],

         [[-1.5641e-02, -1.4368e-02, -1.3311e-02],
          [ 1.3396e-02,  1.3112e-03, -1.5389e-02],
          [ 8.5983e-03, -1.2169e-02, -3.0208e-03]],

         ...,

         [[ 1.2610e-02,  7.1803e-03, -2.3496e-03],
          [ 1.2735e-02,  9.8367e-03,  1.5683e-02],
          [-6.0691e-03,  9.6053e-03, -1.5124e-02]],

         [[-8.0067e-03,  2.3687e-03, -4.4596e-03],
          [ 5.0197e-03, -4.8158e-03,  1.2281e-03],
          [ 1.5290e-02,  5.4353e-03,  6.2714e-03]],

         [[ 3.3769e-03,  1.1519e-02,  1.0003e-02],
          [-8.2596e-04,  4.3792e-03,  3.1226e-03],
          [-1.8154e-03,  1.6693e-02,  2.1327e-03]]],


        [[[ 1.9647e-02,  5.1616e-03, -1.4958e-03],
          [ 2.9385e-03, -5.5499e-03, -7.1461e-03],
          [-1.0364e-02,  2.5148e-03,  3.8568e-03]],

         [[ 1.5576e-02,  7.4798e-04, -3.2039e-03],
          [-8.6909e-04,  1.7583e-02, -2.6463e-03],
          [ 6.4336e-03,  2.4318e-03,  1.3996e-02]],

         [[ 1.8263e-02,  2.7856e-03,  5.8964e-03],
          [-5.6559e-03, -9.8742e-03, -9.7709e-03],
          [-1.0805e-02, -8.2725e-04, -9.5406e-03]],

         ...,

         [[ 1.0586e-02, -1.4469e-02,  1.8367e-03],
          [ 3.8231e-03,  9.2408e-03,  4.4786e-03],
          [ 1.9237e-02,  2.3674e-02,  2.6584e-02]],

         [[-7.7834e-04,  1.1542e-02, -8.0174e-03],
          [-1.3041e-02,  1.6464e-03, -1.7131e-02],
          [ 7.9207e-03,  7.3292e-04, -1.5738e-03]],

         [[ 1.0888e-05,  3.7489e-03, -1.3518e-02],
          [-1.0326e-02,  6.4597e-03, -1.0788e-02],
          [-5.3095e-03, -1.1701e-02,  9.6716e-04]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([5.8585e+06, 1.1482e+06, 1.1171e+06,  ..., 9.5453e-01, 9.1016e-01,
        8.6408e-01], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 1430]) 

NULL SPACE BASIS :  tensor([[ 0.0066, -0.0056,  0.0399,  ..., -0.0011,  0.0003,  0.0034],
        [-0.0080, -0.0036,  0.0180,  ..., -0.0018,  0.0036, -0.0066],
        [ 0.0170,  0.0121, -0.0078,  ..., -0.0010, -0.0041,  0.0056],
        ...,
        [-0.0283, -0.0042,  0.0312,  ..., -0.0006,  0.0043,  0.0042],
        [ 0.0101,  0.0031,  0.0100,  ...,  0.0027, -0.0042, -0.0041],
        [ 0.0018, -0.0083, -0.0156,  ...,  0.0036,  0.0026,  0.0022]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 8.6953e-03, -2.6164e-03, -1.3922e-04,  ...,  2.7761e-04,
         -8.0170e-05,  8.9469e-05],
        [-2.6164e-03,  6.4162e-03, -2.4242e-03,  ...,  8.3941e-05,
          9.7730e-05,  5.4918e-05],
        [-1.3922e-04, -2.4242e-03,  7.3065e-03,  ...,  4.3995e-05,
         -3.7765e-05,  1.1342e-04],
        ...,
        [ 2.7761e-04,  8.3941e-05,  4.3995e-05,  ...,  3.7938e-03,
         -4.0721e-04, -1.3464e-04],
        [-8.0170e-05,  9.7730e-05, -3.7765e-05,  ..., -4.0721e-04,
          2.5075e-03, -2.8122e-04],
        [ 8.9469e-05,  5.4918e-05,  1.1342e-04,  ..., -1.3464e-04,
         -2.8122e-04,  2.9976e-03]], device='cuda:0') 

reserving basis 520/4608; cond: 47035676.0, radio:3.1356088584288955e-05
PARAMETER       :  Parameter containing:
tensor([[[[ 6.0314e-03,  4.5963e-03, -4.9170e-03],
          [-4.2469e-03, -9.3296e-03, -3.1987e-03],
          [ 8.1965e-04, -8.3748e-03,  4.8444e-03]],

         [[-4.0683e-03, -1.1551e-02,  7.9790e-04],
          [-3.9382e-03, -1.1784e-03, -2.5944e-03],
          [ 2.8251e-03,  9.3775e-03, -1.8245e-03]],

         [[-1.0998e-02,  6.4679e-03,  1.2027e-02],
          [ 1.5325e-02,  1.3360e-02,  3.2316e-03],
          [-4.5580e-04,  1.7767e-02,  5.2351e-03]],

         ...,

         [[ 1.1871e-02,  3.6231e-03,  8.6512e-04],
          [ 2.5139e-03, -8.3995e-03,  1.4224e-03],
          [ 5.9104e-03, -5.9439e-03,  7.7324e-03]],

         [[ 7.6232e-03, -1.4421e-03,  2.8887e-03],
          [ 3.7681e-03,  1.2313e-03,  7.1951e-03],
          [ 7.7996e-03,  8.5061e-03, -3.4456e-04]],

         [[-7.8633e-03,  1.3033e-02, -2.4347e-04],
          [-9.8837e-03, -1.2984e-02,  1.9659e-03],
          [-1.2284e-02,  5.2410e-03, -1.4225e-02]]],


        [[[-2.5961e-03,  7.3770e-03, -1.7801e-03],
          [ 3.3987e-04, -3.1890e-03, -9.6175e-03],
          [-4.6483e-03, -1.5656e-02,  5.5028e-03]],

         [[ 3.8747e-03,  7.0326e-03,  2.8994e-03],
          [ 9.6324e-03,  1.3994e-02, -1.2825e-02],
          [ 1.6576e-02,  7.9537e-03, -1.0106e-02]],

         [[ 7.8297e-03,  5.6029e-03,  2.7748e-03],
          [ 3.8466e-03, -4.3950e-03,  6.9217e-03],
          [-2.8292e-03, -7.4773e-03,  5.1745e-04]],

         ...,

         [[-2.0443e-03,  9.4199e-03,  4.7373e-03],
          [-7.6769e-03, -8.4783e-03,  7.2964e-03],
          [-7.4110e-03,  6.5821e-03, -3.7667e-03]],

         [[ 4.4821e-03, -2.7910e-03,  9.4597e-03],
          [-2.8936e-03, -1.4041e-02, -4.5483e-03],
          [ 4.9578e-03, -2.1921e-03, -1.4417e-02]],

         [[ 1.6223e-02,  1.9168e-02, -2.0322e-03],
          [ 2.7390e-04,  5.2581e-05,  5.9412e-03],
          [ 4.4475e-03, -2.8175e-03, -1.0309e-03]]],


        [[[-3.4300e-03, -5.5118e-03, -7.7549e-03],
          [-5.9509e-03, -1.2042e-02, -8.3504e-03],
          [-7.6299e-03, -4.5379e-03, -3.1040e-03]],

         [[-3.0546e-03,  5.3631e-03, -1.2380e-02],
          [ 2.1810e-03, -1.0308e-02, -1.3750e-02],
          [-5.3792e-03, -4.1897e-03, -8.7560e-03]],

         [[-2.1529e-03, -1.2513e-02,  4.2255e-03],
          [-5.5640e-03, -7.5063e-03, -1.3360e-02],
          [-4.8285e-03,  1.3631e-02, -6.2894e-03]],

         ...,

         [[ 3.2185e-03, -1.9033e-02, -3.6241e-04],
          [ 1.5995e-02,  8.6354e-04, -4.8278e-03],
          [-4.8267e-03,  8.4333e-03, -2.4844e-03]],

         [[ 3.9843e-03,  1.8544e-02,  4.6364e-03],
          [-2.5963e-03, -4.9943e-03,  8.8037e-03],
          [ 3.4233e-04, -4.4485e-03,  6.5853e-04]],

         [[-1.4015e-02, -1.5074e-02, -1.6449e-02],
          [-1.2121e-02, -6.2285e-04,  1.1730e-03],
          [-1.2722e-02, -1.0964e-02, -1.7020e-02]]],


        ...,


        [[[-1.0060e-04,  2.2075e-03,  8.4867e-03],
          [ 5.4826e-04,  7.3251e-03, -6.8590e-03],
          [ 5.2246e-03, -1.2177e-03,  4.0354e-03]],

         [[-8.3577e-03, -3.1578e-03,  9.3570e-03],
          [ 9.6896e-03,  6.0662e-03, -7.5407e-03],
          [ 1.2732e-02, -3.5720e-03, -3.1256e-03]],

         [[-1.4669e-02, -1.5852e-03, -1.1357e-02],
          [-1.2475e-02, -1.1487e-02, -3.7225e-03],
          [ 1.1644e-02,  1.6783e-02, -7.1673e-03]],

         ...,

         [[-3.9482e-03,  6.4248e-03,  5.4315e-03],
          [ 9.5977e-03,  2.3304e-03,  5.5850e-03],
          [-5.1430e-03, -3.6011e-03, -2.7584e-03]],

         [[-8.9410e-03,  1.1915e-02, -9.0079e-03],
          [ 1.0099e-02, -1.2391e-02, -8.1625e-03],
          [ 6.0644e-04,  1.2486e-03, -1.6686e-02]],

         [[ 1.6907e-02,  2.0634e-02,  2.3968e-02],
          [ 1.0872e-02, -2.5662e-03,  4.1217e-03],
          [ 1.0300e-02,  1.6103e-02,  6.6897e-03]]],


        [[[ 4.3826e-03, -6.6553e-04,  2.1710e-03],
          [-1.8770e-03, -1.1332e-02, -7.7728e-03],
          [-1.0180e-02,  1.0212e-03, -5.5229e-04]],

         [[-4.0084e-03,  9.6698e-03,  2.9826e-03],
          [-9.6461e-04,  5.7852e-03,  1.3149e-02],
          [-6.4243e-03, -9.8661e-03,  1.1189e-03]],

         [[ 4.7062e-03,  7.6819e-04,  4.5279e-03],
          [-1.2385e-02, -8.5241e-03,  9.8874e-03],
          [ 3.1006e-03,  1.0018e-02, -5.7790e-03]],

         ...,

         [[ 4.3209e-03,  3.7534e-03, -4.1446e-03],
          [ 8.5248e-03,  3.7744e-03,  3.3281e-03],
          [ 2.8125e-03,  2.0959e-03, -1.4405e-02]],

         [[-9.1362e-03, -3.1853e-03, -5.0433e-03],
          [ 2.4342e-03,  1.6575e-03, -1.4555e-02],
          [-1.2119e-04, -8.0013e-03,  2.2932e-03]],

         [[-1.3117e-02,  6.7394e-04,  2.7043e-04],
          [ 2.9575e-03, -4.8726e-03,  4.8640e-03],
          [-1.1501e-02, -1.1527e-02,  1.1859e-03]]],


        [[[-7.3397e-03, -1.0954e-02, -9.9429e-03],
          [ 3.0826e-03, -6.7641e-03, -1.5291e-03],
          [ 2.2962e-03,  4.4787e-04, -6.4159e-03]],

         [[-6.0410e-03,  1.0294e-02,  1.2890e-03],
          [ 2.0378e-03,  8.8384e-03,  2.3342e-03],
          [ 3.0096e-03,  1.1759e-02,  2.1499e-03]],

         [[-1.6938e-03,  5.8089e-03,  6.1972e-03],
          [-7.0956e-03,  1.0172e-02, -1.2518e-02],
          [ 6.0451e-04,  1.8427e-03,  2.7340e-03]],

         ...,

         [[ 5.1473e-03, -8.5692e-04,  1.3675e-02],
          [ 1.0625e-02,  3.8408e-03,  1.4336e-02],
          [ 7.7113e-03,  1.1113e-02,  4.1683e-03]],

         [[-8.0677e-03, -1.9345e-02, -6.6405e-04],
          [-1.1374e-02, -1.2161e-02,  4.4750e-03],
          [ 5.6403e-03, -3.5479e-04,  4.7949e-03]],

         [[-6.6156e-03,  4.8289e-03, -1.4964e-02],
          [-4.7875e-03,  1.0226e-02, -1.2308e-02],
          [ 6.6503e-03, -3.3166e-03, -6.0130e-03]]]], device='cuda:0',
       requires_grad=True) 

EIGEN VALUE     :  tensor([1.2921e+06, 2.8164e+05, 2.5288e+05,  ..., 4.2817e-02, 3.8201e-02,
        2.7470e-02], device='cuda:0') 

NULL SPACE DIM :  torch.Size([4608, 520]) 

NULL SPACE BASIS :  tensor([[-0.0214,  0.0459,  0.0260,  ...,  0.0128, -0.0020, -0.0139],
        [-0.0167, -0.0117,  0.0018,  ..., -0.0106, -0.0072,  0.0155],
        [ 0.0271,  0.0079,  0.0190,  ..., -0.0036,  0.0121,  0.0180],
        ...,
        [-0.0002,  0.0041,  0.0068,  ..., -0.0014, -0.0019, -0.0007],
        [-0.0004, -0.0048, -0.0038,  ...,  0.0036,  0.0022, -0.0004],
        [ 0.0005, -0.0017,  0.0032,  ..., -0.0015, -0.0037,  0.0008]],
       device='cuda:0') 

TRANSFORMS       :  tensor([[ 2.4719e-02, -5.3588e-03, -3.0277e-04,  ..., -3.6382e-05,
          6.2450e-05,  7.5096e-05],
        [-5.3588e-03,  2.0120e-02, -5.0969e-03,  ...,  2.0728e-04,
          1.7213e-05,  3.4291e-05],
        [-3.0277e-04, -5.0969e-03,  2.3117e-02,  ..., -8.5467e-05,
          2.5747e-04, -1.8152e-04],
        ...,
        [-3.6382e-05,  2.0728e-04, -8.5467e-05,  ...,  4.1340e-04,
         -2.3630e-04,  9.3838e-05],
        [ 6.2450e-05,  1.7213e-05,  2.5747e-04,  ..., -2.3630e-04,
          4.6956e-04, -2.5380e-04],
        [ 7.5096e-05,  3.4291e-05, -1.8152e-04,  ...,  9.3838e-05,
         -2.5380e-04,  4.1052e-04]], device='cuda:0') 

computing EWC
validation split name: 1
 * Val Acc 84.800, Total time 0.55
 * Val loss 0.849, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 98.080, Total time 3.15
 * Val loss 0.056, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 72.000, Total time 0.57
 * Val loss 0.828, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 72.840, Total time 3.16
 * Val loss 0.767, Total time 0.00
**************************************************
validation split name: 3
 * Val Acc 67.900, Total time 0.58
 * Val loss 0.949, Total time 0.00
**************************************************
training split name: 3
 * Val Acc 71.040, Total time 3.17
 * Val loss 0.843, Total time 0.00
**************************************************
validation split name: 4
 * Val Acc 65.000, Total time 0.57
 * Val loss 1.040, Total time 0.00
**************************************************
training split name: 4
 * Val Acc 66.440, Total time 3.19
 * Val loss 0.955, Total time 0.00
**************************************************
validation split name: 5
 * Val Acc 71.000, Total time 0.57
 * Val loss 0.819, Total time 0.00
**************************************************
training split name: 5
 * Val Acc 75.220, Total time 3.15
 * Val loss 0.736, Total time 0.00
**************************************************
validation split name: 6
 * Val Acc 72.900, Total time 0.58
 * Val loss 0.803, Total time 0.00
**************************************************
training split name: 6
 * Val Acc 74.360, Total time 3.16
 * Val loss 0.726, Total time 0.00
**************************************************
validation split name: 7
 * Val Acc 65.800, Total time 0.57
 * Val loss 0.950, Total time 0.00
**************************************************
training split name: 7
 * Val Acc 66.680, Total time 3.26
 * Val loss 0.919, Total time 0.00
**************************************************
validation split name: 8
 * Val Acc 69.300, Total time 0.56
 * Val loss 0.946, Total time 0.00
**************************************************
training split name: 8
 * Val Acc 68.040, Total time 3.22
 * Val loss 0.930, Total time 0.00
**************************************************
validation split name: 9
 * Val Acc 73.100, Total time 0.59
 * Val loss 0.781, Total time 0.00
**************************************************
training split name: 9
 * Val Acc 75.200, Total time 3.21
 * Val loss 0.737, Total time 0.00
**************************************************
====================== 10 =======================
Epoch:0
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0082 (0.0082)	2.348 (2.348)	6.25 (6.25)
[10/157]	0.0977 (0.0910)	0.0594 (0.0535)	1.928 (2.163)	34.38 (25.28)
[20/157]	0.0983 (0.0933)	0.0602 (0.0561)	1.733 (2.024)	53.12 (36.16)
[30/157]	0.0962 (0.0940)	0.0592 (0.0570)	1.618 (1.903)	50.00 (42.44)
[40/157]	0.0960 (0.0945)	0.0594 (0.0576)	1.386 (1.796)	59.38 (45.81)
[50/157]	0.0940 (0.0947)	0.0579 (0.0579)	1.369 (1.722)	59.38 (48.16)
[60/157]	0.0953 (0.0948)	0.0588 (0.0581)	1.386 (1.658)	50.00 (50.20)
[70/157]	0.0958 (0.0950)	0.0591 (0.0582)	1.150 (1.614)	71.88 (51.54)
[80/157]	0.0965 (0.0951)	0.0598 (0.0584)	1.653 (1.584)	50.00 (52.70)
[90/157]	0.0971 (0.0952)	0.0591 (0.0584)	1.379 (1.553)	53.12 (53.57)
[100/157]	0.0960 (0.0953)	0.0600 (0.0585)	1.415 (1.532)	50.00 (54.02)
[110/157]	0.0965 (0.0953)	0.0601 (0.0585)	1.163 (1.507)	71.88 (54.56)
[120/157]	0.0967 (0.0954)	0.0596 (0.0586)	0.960 (1.483)	75.00 (55.27)
[130/157]	0.0967 (0.0954)	0.0591 (0.0586)	1.291 (1.460)	56.25 (55.87)
[140/157]	0.0969 (0.0954)	0.0596 (0.0586)	1.005 (1.444)	81.25 (56.23)
[150/157]	0.0972 (0.0955)	0.0604 (0.0587)	1.001 (1.421)	71.88 (56.89)
[156/157]	0.0781 (0.0953)	0.0525 (0.0586)	1.217 (1.412)	50.00 (57.10)
 * Train Acc 57.100
 * Val Acc 65.700, Total time 0.57
 * Val loss 1.012, Total time 0.00
Epoch:1
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0084 (0.0084)	1.132 (1.132)	65.62 (65.62)
[10/157]	0.0971 (0.0907)	0.0600 (0.0534)	0.956 (1.117)	75.00 (65.34)
[20/157]	0.0972 (0.0932)	0.0600 (0.0562)	1.322 (1.126)	59.38 (65.33)
[30/157]	0.0977 (0.0941)	0.0596 (0.0572)	1.285 (1.166)	62.50 (64.42)
[40/157]	0.0962 (0.0945)	0.0591 (0.0575)	1.506 (1.142)	40.62 (65.70)
[50/157]	0.0952 (0.0947)	0.0587 (0.0578)	1.004 (1.123)	71.88 (65.69)
[60/157]	0.0938 (0.0949)	0.0576 (0.0580)	0.877 (1.126)	75.00 (65.32)
[70/157]	0.0957 (0.0951)	0.0597 (0.0582)	1.046 (1.118)	65.62 (65.32)
[80/157]	0.0969 (0.0952)	0.0597 (0.0583)	1.280 (1.124)	59.38 (64.85)
[90/157]	0.0964 (0.0953)	0.0585 (0.0584)	1.256 (1.122)	53.12 (64.90)
[100/157]	0.0957 (0.0955)	0.0575 (0.0584)	1.032 (1.124)	59.38 (64.60)
[110/157]	0.0966 (0.0956)	0.0585 (0.0584)	1.379 (1.122)	53.12 (64.81)
[120/157]	0.0975 (0.0957)	0.0596 (0.0585)	1.551 (1.123)	56.25 (64.90)
[130/157]	0.0968 (0.0957)	0.0593 (0.0585)	0.971 (1.119)	62.50 (64.98)
[140/157]	0.0962 (0.0957)	0.0592 (0.0585)	1.100 (1.120)	62.50 (65.03)
[150/157]	0.0961 (0.0958)	0.0584 (0.0585)	1.044 (1.115)	68.75 (65.13)
[156/157]	0.0800 (0.0956)	0.0542 (0.0585)	1.556 (1.114)	50.00 (65.18)
 * Train Acc 65.180
 * Val Acc 67.600, Total time 0.58
 * Val loss 0.929, Total time 0.00
Epoch:2
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0086 (0.0086)	1.467 (1.467)	43.75 (43.75)
[10/157]	0.0964 (0.0912)	0.0593 (0.0537)	0.948 (1.124)	75.00 (63.07)
[20/157]	0.0953 (0.0938)	0.0581 (0.0560)	1.378 (1.082)	59.38 (65.77)
[30/157]	0.0979 (0.0948)	0.0590 (0.0568)	1.270 (1.090)	62.50 (66.03)
[40/157]	0.0963 (0.0951)	0.0583 (0.0572)	0.920 (1.064)	68.75 (66.39)
[50/157]	0.0946 (0.0953)	0.0591 (0.0575)	1.358 (1.072)	59.38 (65.87)
[60/157]	0.0950 (0.0955)	0.0585 (0.0578)	0.995 (1.061)	75.00 (66.34)
[70/157]	0.0941 (0.0955)	0.0580 (0.0579)	1.076 (1.069)	65.62 (65.89)
[80/157]	0.0951 (0.0956)	0.0582 (0.0581)	1.066 (1.064)	68.75 (65.97)
[90/157]	0.0967 (0.0957)	0.0591 (0.0582)	0.955 (1.066)	65.62 (66.00)
[100/157]	0.0963 (0.0957)	0.0592 (0.0583)	0.838 (1.068)	71.88 (66.27)
[110/157]	0.0957 (0.0957)	0.0589 (0.0584)	1.012 (1.061)	65.62 (66.53)
[120/157]	0.0966 (0.0958)	0.0591 (0.0585)	0.806 (1.061)	71.88 (66.32)
[130/157]	0.0963 (0.0958)	0.0594 (0.0586)	1.085 (1.060)	59.38 (66.53)
[140/157]	0.0989 (0.0958)	0.0604 (0.0586)	1.086 (1.061)	65.62 (66.60)
[150/157]	0.0971 (0.0958)	0.0601 (0.0587)	1.113 (1.061)	59.38 (66.56)
[156/157]	0.0806 (0.0958)	0.0547 (0.0586)	0.784 (1.057)	87.50 (66.76)
 * Train Acc 66.760
 * Val Acc 67.000, Total time 0.59
 * Val loss 0.891, Total time 0.00
Epoch:3
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0096 (0.0096)	1.193 (1.193)	65.62 (65.62)
[10/157]	0.0978 (0.0912)	0.0596 (0.0541)	1.039 (1.054)	71.88 (67.05)
[20/157]	0.0967 (0.0937)	0.0588 (0.0567)	0.666 (1.022)	84.38 (68.75)
[30/157]	0.0969 (0.0944)	0.0596 (0.0576)	0.946 (1.023)	71.88 (67.94)
[40/157]	0.0988 (0.0951)	0.0604 (0.0580)	1.309 (1.032)	53.12 (66.54)
[50/157]	0.0991 (0.0954)	0.0615 (0.0584)	1.284 (1.036)	56.25 (66.61)
[60/157]	0.0957 (0.0956)	0.0584 (0.0585)	0.948 (1.016)	71.88 (67.78)
[70/157]	0.0963 (0.0958)	0.0582 (0.0585)	1.164 (1.019)	65.62 (67.87)
[80/157]	0.0952 (0.0959)	0.0590 (0.0586)	0.888 (1.015)	71.88 (68.09)
[90/157]	0.0955 (0.0959)	0.0586 (0.0587)	1.103 (1.021)	62.50 (67.58)
[100/157]	0.0948 (0.0959)	0.0586 (0.0588)	1.062 (1.021)	62.50 (67.57)
[110/157]	0.0953 (0.0960)	0.0588 (0.0588)	0.793 (1.022)	75.00 (67.57)
[120/157]	0.0956 (0.0960)	0.0588 (0.0588)	0.999 (1.020)	68.75 (67.51)
[130/157]	0.0980 (0.0960)	0.0603 (0.0589)	1.007 (1.022)	68.75 (67.44)
[140/157]	0.0975 (0.0960)	0.0596 (0.0589)	1.155 (1.025)	65.62 (67.53)
[150/157]	0.0951 (0.0961)	0.0588 (0.0589)	0.930 (1.028)	68.75 (67.53)
[156/157]	0.0785 (0.0960)	0.0538 (0.0589)	0.927 (1.025)	62.50 (67.50)
 * Train Acc 67.500
 * Val Acc 69.400, Total time 0.59
 * Val loss 0.864, Total time 0.00
Epoch:4
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0090 (0.0090)	1.048 (1.048)	68.75 (68.75)
[10/157]	0.0968 (0.0916)	0.0584 (0.0539)	1.179 (1.040)	62.50 (67.05)
[20/157]	0.0973 (0.0941)	0.0595 (0.0566)	1.178 (1.078)	50.00 (65.18)
[30/157]	0.0971 (0.0951)	0.0585 (0.0572)	1.154 (1.050)	59.38 (66.94)
[40/157]	0.0965 (0.0954)	0.0595 (0.0577)	1.001 (1.027)	78.12 (68.29)
[50/157]	0.0936 (0.0955)	0.0582 (0.0579)	0.892 (1.028)	75.00 (67.77)
[60/157]	0.0945 (0.0957)	0.0580 (0.0582)	0.860 (1.014)	78.12 (68.24)
[70/157]	0.0953 (0.0958)	0.0590 (0.0584)	0.716 (1.016)	84.38 (68.05)
[80/157]	0.0961 (0.0958)	0.0582 (0.0585)	1.057 (1.021)	75.00 (67.98)
[90/157]	0.1186 (0.0966)	0.0799 (0.0593)	0.818 (1.007)	75.00 (68.41)
[100/157]	0.0962 (0.0971)	0.0590 (0.0597)	1.127 (1.001)	59.38 (68.72)
[110/157]	0.0928 (0.0969)	0.0567 (0.0596)	0.839 (1.006)	71.88 (68.44)
[120/157]	0.0956 (0.0968)	0.0588 (0.0595)	1.102 (1.004)	62.50 (68.52)
[130/157]	0.0938 (0.0966)	0.0577 (0.0594)	1.166 (1.003)	59.38 (68.46)
[140/157]	0.0953 (0.0965)	0.0591 (0.0594)	0.963 (1.001)	68.75 (68.55)
[150/157]	0.0941 (0.0964)	0.0576 (0.0593)	0.968 (1.008)	65.62 (68.27)
[156/157]	0.0791 (0.0962)	0.0541 (0.0592)	0.673 (1.005)	75.00 (68.32)
 * Train Acc 68.320
 * Val Acc 69.200, Total time 0.57
 * Val loss 0.860, Total time 0.00
Epoch:5
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0467 (0.0467)	0.0086 (0.0086)	0.961 (0.961)	68.75 (68.75)
[10/157]	0.1009 (0.0942)	0.0626 (0.0560)	0.929 (0.953)	71.88 (68.47)
[20/157]	0.0982 (0.0968)	0.0607 (0.0587)	1.284 (0.985)	62.50 (68.01)
[30/157]	0.0992 (0.0978)	0.0619 (0.0600)	0.952 (0.974)	75.00 (68.55)
[40/157]	0.1000 (0.0982)	0.0623 (0.0605)	0.862 (0.977)	75.00 (69.05)
[50/157]	0.1002 (0.0986)	0.0610 (0.0607)	1.017 (0.985)	65.62 (68.87)
[60/157]	0.0992 (0.0988)	0.0621 (0.0609)	0.924 (0.994)	68.75 (68.75)
[70/157]	0.1005 (0.0990)	0.0621 (0.0611)	1.094 (0.983)	59.38 (69.23)
[80/157]	0.0991 (0.0992)	0.0603 (0.0610)	1.030 (0.987)	65.62 (69.17)
[90/157]	0.1005 (0.0993)	0.0608 (0.0610)	0.698 (0.978)	81.25 (69.16)
[100/157]	0.1001 (0.0994)	0.0616 (0.0610)	0.831 (0.972)	71.88 (69.59)
[110/157]	0.1019 (0.0995)	0.0624 (0.0610)	0.776 (0.967)	84.38 (69.74)
[120/157]	0.1014 (0.0995)	0.0619 (0.0610)	0.839 (0.962)	75.00 (70.07)
[130/157]	0.1003 (0.0995)	0.0617 (0.0610)	0.745 (0.961)	81.25 (70.28)
[140/157]	0.0999 (0.0995)	0.0621 (0.0610)	0.706 (0.963)	78.12 (70.17)
[150/157]	0.0992 (0.0995)	0.0620 (0.0611)	0.812 (0.965)	75.00 (70.20)
[156/157]	0.0831 (0.0994)	0.0550 (0.0611)	0.818 (0.967)	75.00 (70.20)
 * Train Acc 70.200
 * Val Acc 71.900, Total time 0.60
 * Val loss 0.826, Total time 0.00
Epoch:6
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0539 (0.0539)	0.0090 (0.0090)	0.859 (0.859)	65.62 (65.62)
[10/157]	0.0991 (0.0940)	0.0616 (0.0559)	1.061 (0.952)	65.62 (71.59)
[20/157]	0.0998 (0.0966)	0.0609 (0.0586)	0.927 (0.939)	65.62 (71.58)
[30/157]	0.1006 (0.0977)	0.0616 (0.0594)	0.796 (0.929)	75.00 (71.77)
[40/157]	0.0934 (0.0980)	0.0571 (0.0597)	1.129 (0.924)	59.38 (71.95)
[50/157]	0.0958 (0.0974)	0.0586 (0.0595)	1.075 (0.946)	65.62 (71.14)
[60/157]	0.0936 (0.0970)	0.0576 (0.0593)	0.939 (0.954)	75.00 (70.59)
[70/157]	0.0939 (0.0968)	0.0572 (0.0591)	0.854 (0.949)	68.75 (70.95)
[80/157]	0.0964 (0.0966)	0.0594 (0.0591)	1.144 (0.952)	68.75 (70.87)
[90/157]	0.0945 (0.0965)	0.0577 (0.0591)	0.849 (0.954)	71.88 (70.47)
[100/157]	0.0962 (0.0964)	0.0585 (0.0590)	0.676 (0.957)	75.00 (70.30)
[110/157]	0.0960 (0.0963)	0.0579 (0.0589)	1.049 (0.953)	75.00 (70.64)
[120/157]	0.0945 (0.0962)	0.0572 (0.0588)	1.056 (0.955)	65.62 (70.64)
[130/157]	0.0950 (0.0961)	0.0576 (0.0587)	0.991 (0.950)	71.88 (70.87)
[140/157]	0.0949 (0.0960)	0.0582 (0.0587)	0.983 (0.947)	65.62 (70.94)
[150/157]	0.0942 (0.0959)	0.0584 (0.0587)	0.854 (0.950)	71.88 (70.88)
[156/157]	0.0775 (0.0958)	0.0534 (0.0586)	1.317 (0.949)	62.50 (70.80)
 * Train Acc 70.800
 * Val Acc 71.700, Total time 0.56
 * Val loss 0.811, Total time 0.00
Epoch:7
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0086 (0.0086)	1.190 (1.190)	65.62 (65.62)
[10/157]	0.0959 (0.0920)	0.0580 (0.0544)	1.132 (0.923)	62.50 (72.44)
[20/157]	0.0957 (0.0934)	0.0591 (0.0566)	0.812 (0.907)	78.12 (71.88)
[30/157]	0.0941 (0.0939)	0.0580 (0.0572)	0.908 (0.890)	68.75 (71.98)
[40/157]	0.0962 (0.0941)	0.0597 (0.0576)	0.808 (0.898)	75.00 (71.88)
[50/157]	0.0935 (0.0943)	0.0567 (0.0578)	0.895 (0.898)	68.75 (72.06)
[60/157]	0.1012 (0.0950)	0.0627 (0.0583)	0.912 (0.889)	71.88 (72.34)
[70/157]	0.0998 (0.0957)	0.0620 (0.0588)	1.025 (0.908)	62.50 (71.39)
[80/157]	0.0990 (0.0963)	0.0611 (0.0592)	0.878 (0.924)	78.12 (70.87)
[90/157]	0.1007 (0.0967)	0.0629 (0.0595)	1.032 (0.933)	62.50 (70.67)
[100/157]	0.1005 (0.0970)	0.0628 (0.0598)	0.956 (0.933)	65.62 (70.67)
[110/157]	0.0942 (0.0972)	0.0571 (0.0600)	0.809 (0.934)	75.00 (70.61)
[120/157]	0.0944 (0.0970)	0.0582 (0.0598)	0.957 (0.932)	71.88 (70.58)
[130/157]	0.0938 (0.0968)	0.0576 (0.0598)	0.901 (0.929)	78.12 (70.75)
[140/157]	0.0956 (0.0967)	0.0593 (0.0597)	0.884 (0.930)	71.88 (70.61)
[150/157]	0.0948 (0.0966)	0.0582 (0.0596)	0.856 (0.927)	78.12 (70.72)
[156/157]	0.0783 (0.0964)	0.0539 (0.0595)	0.733 (0.930)	87.50 (70.90)
 * Train Acc 70.900
 * Val Acc 74.100, Total time 0.57
 * Val loss 0.790, Total time 0.00
Epoch:8
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0416 (0.0416)	0.0083 (0.0083)	0.796 (0.796)	75.00 (75.00)
[10/157]	0.0938 (0.0913)	0.0580 (0.0541)	1.036 (0.855)	71.88 (74.43)
[20/157]	0.0969 (0.0932)	0.0595 (0.0563)	0.872 (0.861)	81.25 (74.70)
[30/157]	0.0966 (0.0938)	0.0582 (0.0569)	0.834 (0.894)	75.00 (72.78)
[40/157]	0.0946 (0.0941)	0.0571 (0.0571)	1.308 (0.905)	65.62 (71.88)
[50/157]	0.0956 (0.0943)	0.0591 (0.0574)	0.875 (0.920)	68.75 (71.51)
[60/157]	0.0965 (0.0945)	0.0592 (0.0578)	0.814 (0.917)	78.12 (71.57)
[70/157]	0.0948 (0.0946)	0.0580 (0.0579)	1.088 (0.922)	65.62 (71.52)
[80/157]	0.0969 (0.0947)	0.0601 (0.0580)	0.731 (0.923)	78.12 (71.41)
[90/157]	0.0928 (0.0947)	0.0574 (0.0581)	0.705 (0.927)	78.12 (71.12)
[100/157]	0.0954 (0.0947)	0.0587 (0.0581)	0.862 (0.915)	68.75 (71.57)
[110/157]	0.0938 (0.0947)	0.0571 (0.0581)	0.833 (0.922)	65.62 (71.17)
[120/157]	0.0968 (0.0948)	0.0595 (0.0582)	0.721 (0.922)	71.88 (71.13)
[130/157]	0.0954 (0.0948)	0.0585 (0.0582)	0.818 (0.925)	78.12 (71.06)
[140/157]	0.0954 (0.0948)	0.0584 (0.0582)	1.025 (0.930)	68.75 (70.88)
[150/157]	0.1019 (0.0948)	0.0631 (0.0582)	0.956 (0.931)	71.88 (70.90)
[156/157]	0.0887 (0.0951)	0.0627 (0.0585)	1.123 (0.930)	62.50 (70.92)
 * Train Acc 70.920
 * Val Acc 74.000, Total time 0.56
 * Val loss 0.786, Total time 0.00
Epoch:9
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0422 (0.0422)	0.0085 (0.0085)	0.698 (0.698)	87.50 (87.50)
[10/157]	0.0963 (0.0918)	0.0593 (0.0545)	0.759 (0.908)	81.25 (72.16)
[20/157]	0.0950 (0.0935)	0.0575 (0.0563)	1.168 (0.929)	68.75 (72.77)
[30/157]	0.1073 (0.0962)	0.0672 (0.0584)	1.230 (0.946)	62.50 (71.67)
[40/157]	0.0957 (0.0965)	0.0575 (0.0587)	0.768 (0.925)	65.62 (71.27)
[50/157]	0.0950 (0.0961)	0.0583 (0.0586)	0.663 (0.928)	84.38 (71.57)
[60/157]	0.0945 (0.0959)	0.0584 (0.0586)	0.856 (0.927)	81.25 (71.31)
[70/157]	0.0964 (0.0959)	0.0580 (0.0585)	1.194 (0.924)	62.50 (70.86)
[80/157]	0.0945 (0.0958)	0.0579 (0.0585)	0.557 (0.912)	87.50 (71.22)
[90/157]	0.0959 (0.0957)	0.0588 (0.0585)	0.728 (0.911)	78.12 (71.26)
[100/157]	0.0971 (0.0957)	0.0596 (0.0585)	0.958 (0.910)	62.50 (71.50)
[110/157]	0.0956 (0.0956)	0.0589 (0.0586)	0.951 (0.903)	62.50 (71.76)
[120/157]	0.0960 (0.0956)	0.0596 (0.0586)	0.907 (0.908)	71.88 (71.49)
[130/157]	0.0950 (0.0955)	0.0586 (0.0586)	1.105 (0.902)	71.88 (71.90)
[140/157]	0.0960 (0.0955)	0.0578 (0.0586)	0.586 (0.905)	84.38 (71.70)
[150/157]	0.0963 (0.0955)	0.0584 (0.0585)	0.736 (0.899)	68.75 (71.83)
[156/157]	0.0771 (0.0954)	0.0507 (0.0585)	1.079 (0.902)	50.00 (71.60)
 * Train Acc 71.600
 * Val Acc 73.200, Total time 0.58
 * Val loss 0.784, Total time 0.00
Epoch:10
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0085 (0.0085)	0.514 (0.514)	90.62 (90.62)
[10/157]	0.0963 (0.0925)	0.0578 (0.0537)	0.909 (0.884)	71.88 (71.88)
[20/157]	0.0941 (0.0937)	0.0569 (0.0558)	0.580 (0.874)	87.50 (73.21)
[30/157]	0.0958 (0.0941)	0.0588 (0.0565)	0.944 (0.895)	68.75 (72.28)
[40/157]	0.0943 (0.0944)	0.0581 (0.0570)	0.899 (0.881)	68.75 (72.56)
[50/157]	0.0948 (0.0946)	0.0585 (0.0573)	1.031 (0.889)	78.12 (72.37)
[60/157]	0.0967 (0.0947)	0.0600 (0.0575)	0.722 (0.888)	75.00 (72.39)
[70/157]	0.0935 (0.0948)	0.0575 (0.0576)	0.850 (0.901)	62.50 (71.79)
[80/157]	0.0939 (0.0948)	0.0573 (0.0577)	0.906 (0.901)	75.00 (71.57)
[90/157]	0.0952 (0.0948)	0.0584 (0.0577)	0.827 (0.909)	75.00 (71.29)
[100/157]	0.0941 (0.0948)	0.0580 (0.0578)	0.584 (0.905)	84.38 (71.57)
[110/157]	0.0954 (0.0949)	0.0597 (0.0579)	0.719 (0.913)	84.38 (71.71)
[120/157]	0.0953 (0.0948)	0.0593 (0.0580)	1.019 (0.913)	65.62 (71.80)
[130/157]	0.0964 (0.0949)	0.0599 (0.0580)	1.455 (0.911)	53.12 (71.85)
[140/157]	0.0943 (0.0949)	0.0578 (0.0581)	0.653 (0.908)	78.12 (72.07)
[150/157]	0.0981 (0.0949)	0.0598 (0.0581)	1.012 (0.903)	75.00 (72.19)
[156/157]	0.0765 (0.0948)	0.0519 (0.0580)	1.045 (0.905)	62.50 (72.18)
 * Train Acc 72.180
 * Val Acc 72.800, Total time 0.57
 * Val loss 0.777, Total time 0.00
Epoch:11
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0086 (0.0086)	0.996 (0.996)	75.00 (75.00)
[10/157]	0.0928 (0.0919)	0.0569 (0.0547)	1.015 (0.919)	62.50 (71.88)
[20/157]	0.0959 (0.0933)	0.0600 (0.0566)	0.905 (0.917)	68.75 (71.58)
[30/157]	0.1034 (0.0963)	0.0657 (0.0593)	1.065 (0.919)	59.38 (70.36)
[40/157]	0.1014 (0.0982)	0.0660 (0.0611)	1.147 (0.936)	62.50 (70.12)
[50/157]	0.0949 (0.0976)	0.0593 (0.0605)	0.637 (0.928)	81.25 (70.34)
[60/157]	0.0904 (0.0971)	0.0547 (0.0602)	1.042 (0.915)	62.50 (71.06)
[70/157]	0.0965 (0.0968)	0.0593 (0.0599)	0.795 (0.901)	75.00 (71.79)
[80/157]	0.0954 (0.0966)	0.0585 (0.0597)	0.826 (0.898)	75.00 (71.95)
[90/157]	0.0957 (0.0964)	0.0586 (0.0596)	0.677 (0.898)	75.00 (72.01)
[100/157]	0.0944 (0.0963)	0.0580 (0.0594)	0.869 (0.896)	78.12 (72.06)
[110/157]	0.0940 (0.0962)	0.0579 (0.0593)	0.906 (0.897)	71.88 (72.24)
[120/157]	0.0939 (0.0961)	0.0573 (0.0593)	0.831 (0.886)	68.75 (72.55)
[130/157]	0.0942 (0.0960)	0.0590 (0.0592)	0.742 (0.883)	81.25 (72.69)
[140/157]	0.0964 (0.0960)	0.0593 (0.0592)	0.893 (0.881)	71.88 (72.72)
[150/157]	0.0947 (0.0959)	0.0574 (0.0592)	0.991 (0.883)	75.00 (72.68)
[156/157]	0.0794 (0.0958)	0.0543 (0.0591)	0.741 (0.884)	87.50 (72.72)
 * Train Acc 72.720
 * Val Acc 74.100, Total time 0.58
 * Val loss 0.780, Total time 0.00
Epoch:12
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0414 (0.0414)	0.0082 (0.0082)	0.941 (0.941)	71.88 (71.88)
[10/157]	0.0955 (0.0917)	0.0594 (0.0551)	0.690 (0.895)	84.38 (73.86)
[20/157]	0.0972 (0.0935)	0.0591 (0.0570)	0.787 (0.919)	81.25 (72.77)
[30/157]	0.0951 (0.0939)	0.0588 (0.0574)	0.752 (0.878)	71.88 (74.09)
[40/157]	0.0980 (0.0942)	0.0605 (0.0578)	0.865 (0.868)	68.75 (73.93)
[50/157]	0.0945 (0.0944)	0.0581 (0.0578)	1.064 (0.858)	59.38 (74.08)
[60/157]	0.0954 (0.0946)	0.0585 (0.0580)	0.805 (0.867)	75.00 (73.87)
[70/157]	0.0968 (0.0946)	0.0597 (0.0581)	0.725 (0.869)	71.88 (73.55)
[80/157]	0.0999 (0.0948)	0.0612 (0.0582)	0.577 (0.882)	84.38 (73.15)
[90/157]	0.0999 (0.0954)	0.0622 (0.0587)	0.872 (0.873)	68.75 (73.45)
[100/157]	0.1005 (0.0959)	0.0625 (0.0591)	1.186 (0.873)	71.88 (73.48)
[110/157]	0.0951 (0.0959)	0.0577 (0.0591)	0.988 (0.878)	75.00 (73.14)
[120/157]	0.0950 (0.0958)	0.0582 (0.0590)	0.781 (0.883)	75.00 (72.86)
[130/157]	0.0932 (0.0958)	0.0577 (0.0590)	0.827 (0.885)	71.88 (72.78)
[140/157]	0.1181 (0.0960)	0.0792 (0.0592)	0.798 (0.884)	68.75 (72.58)
[150/157]	0.0939 (0.0965)	0.0573 (0.0596)	1.034 (0.884)	75.00 (72.64)
[156/157]	0.0768 (0.0963)	0.0527 (0.0595)	1.433 (0.885)	62.50 (72.68)
 * Train Acc 72.680
 * Val Acc 73.800, Total time 0.57
 * Val loss 0.758, Total time 0.00
Epoch:13
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0084 (0.0084)	0.972 (0.972)	65.62 (65.62)
[10/157]	0.0941 (0.0918)	0.0578 (0.0545)	0.731 (0.813)	75.00 (73.58)
[20/157]	0.1027 (0.0943)	0.0644 (0.0573)	0.861 (0.812)	71.88 (75.30)
[30/157]	0.1014 (0.0969)	0.0631 (0.0596)	0.966 (0.820)	65.62 (74.80)
[40/157]	0.1035 (0.0983)	0.0654 (0.0606)	0.897 (0.810)	68.75 (74.70)
[50/157]	0.1035 (0.0993)	0.0642 (0.0614)	1.272 (0.823)	62.50 (74.26)
[60/157]	0.0940 (0.0991)	0.0574 (0.0613)	0.911 (0.838)	75.00 (73.92)
[70/157]	0.0959 (0.0986)	0.0591 (0.0609)	0.922 (0.848)	75.00 (73.46)
[80/157]	0.0942 (0.0981)	0.0582 (0.0606)	1.355 (0.855)	65.62 (73.46)
[90/157]	0.0952 (0.0977)	0.0592 (0.0604)	0.704 (0.854)	81.25 (73.66)
[100/157]	0.0957 (0.0974)	0.0595 (0.0602)	0.705 (0.851)	75.00 (73.70)
[110/157]	0.0937 (0.0972)	0.0577 (0.0601)	1.112 (0.854)	59.38 (73.48)
[120/157]	0.0966 (0.0970)	0.0604 (0.0600)	1.032 (0.864)	78.12 (73.17)
[130/157]	0.0938 (0.0969)	0.0583 (0.0599)	0.834 (0.865)	78.12 (73.23)
[140/157]	0.0956 (0.0967)	0.0590 (0.0598)	0.820 (0.864)	71.88 (73.23)
[150/157]	0.0942 (0.0966)	0.0578 (0.0597)	0.902 (0.866)	75.00 (73.24)
[156/157]	0.0784 (0.0964)	0.0539 (0.0596)	1.351 (0.870)	37.50 (73.04)
 * Train Acc 73.040
 * Val Acc 74.000, Total time 0.58
 * Val loss 0.753, Total time 0.00
Epoch:14
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0445 (0.0445)	0.0083 (0.0083)	0.839 (0.839)	75.00 (75.00)
[10/157]	0.0933 (0.0915)	0.0571 (0.0541)	1.036 (0.845)	71.88 (73.58)
[20/157]	0.0956 (0.0932)	0.0591 (0.0564)	0.978 (0.869)	71.88 (71.43)
[30/157]	0.0936 (0.0937)	0.0575 (0.0571)	0.896 (0.869)	68.75 (72.18)
[40/157]	0.1001 (0.0969)	0.0621 (0.0599)	0.972 (0.868)	68.75 (72.33)
[50/157]	0.0987 (0.0975)	0.0606 (0.0603)	0.705 (0.865)	81.25 (72.30)
[60/157]	0.0993 (0.0980)	0.0615 (0.0606)	1.069 (0.873)	75.00 (72.49)
[70/157]	0.1006 (0.0983)	0.0625 (0.0607)	1.030 (0.875)	78.12 (72.58)
[80/157]	0.0943 (0.0980)	0.0582 (0.0605)	0.748 (0.877)	78.12 (72.92)
[90/157]	0.0961 (0.0977)	0.0585 (0.0604)	0.652 (0.865)	75.00 (73.35)
[100/157]	0.0934 (0.0975)	0.0561 (0.0601)	0.769 (0.862)	71.88 (73.45)
[110/157]	0.0958 (0.0972)	0.0588 (0.0600)	0.954 (0.874)	75.00 (73.25)
[120/157]	0.0940 (0.0971)	0.0576 (0.0598)	1.207 (0.880)	68.75 (73.22)
[130/157]	0.0959 (0.0969)	0.0587 (0.0597)	0.774 (0.874)	81.25 (73.35)
[140/157]	0.0956 (0.0968)	0.0581 (0.0595)	0.805 (0.876)	75.00 (73.25)
[150/157]	0.0952 (0.0967)	0.0588 (0.0595)	0.943 (0.871)	65.62 (73.28)
[156/157]	0.0781 (0.0965)	0.0537 (0.0594)	0.754 (0.871)	75.00 (73.36)
 * Train Acc 73.360
 * Val Acc 75.900, Total time 0.57
 * Val loss 0.731, Total time 0.00
Epoch:15
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0397 (0.0397)	0.0082 (0.0082)	0.974 (0.974)	71.88 (71.88)
[10/157]	0.0950 (0.0919)	0.0583 (0.0544)	0.756 (0.823)	78.12 (74.15)
[20/157]	0.0963 (0.0934)	0.0585 (0.0562)	0.636 (0.845)	87.50 (72.92)
[30/157]	0.0968 (0.0940)	0.0592 (0.0568)	1.060 (0.870)	68.75 (72.08)
[40/157]	0.0935 (0.0943)	0.0568 (0.0571)	1.069 (0.847)	78.12 (73.48)
[50/157]	0.0968 (0.0945)	0.0585 (0.0573)	0.953 (0.852)	75.00 (73.10)
[60/157]	0.0973 (0.0947)	0.0587 (0.0574)	0.953 (0.853)	62.50 (73.31)
[70/157]	0.0960 (0.0957)	0.0581 (0.0583)	0.874 (0.847)	78.12 (73.68)
[80/157]	0.0948 (0.0956)	0.0571 (0.0582)	0.896 (0.848)	65.62 (73.92)
[90/157]	0.0961 (0.0955)	0.0594 (0.0583)	0.765 (0.858)	81.25 (73.73)
[100/157]	0.0978 (0.0955)	0.0604 (0.0583)	0.702 (0.854)	84.38 (73.89)
[110/157]	0.0969 (0.0954)	0.0592 (0.0583)	0.866 (0.855)	71.88 (73.82)
[120/157]	0.0937 (0.0954)	0.0572 (0.0583)	1.048 (0.859)	65.62 (73.68)
[130/157]	0.0953 (0.0953)	0.0589 (0.0584)	0.960 (0.860)	68.75 (73.52)
[140/157]	0.0946 (0.0953)	0.0583 (0.0584)	1.034 (0.860)	71.88 (73.71)
[150/157]	0.0935 (0.0953)	0.0574 (0.0584)	0.831 (0.856)	75.00 (73.80)
[156/157]	0.0790 (0.0951)	0.0545 (0.0584)	1.045 (0.858)	62.50 (73.78)
 * Train Acc 73.780
 * Val Acc 74.900, Total time 0.58
 * Val loss 0.735, Total time 0.00
Epoch:16
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0092 (0.0092)	0.915 (0.915)	71.88 (71.88)
[10/157]	0.0960 (0.0983)	0.0593 (0.0603)	0.798 (0.855)	71.88 (74.15)
[20/157]	0.0954 (0.0966)	0.0595 (0.0595)	0.736 (0.825)	81.25 (74.40)
[30/157]	0.0969 (0.0963)	0.0591 (0.0592)	0.836 (0.857)	78.12 (74.40)
[40/157]	0.0954 (0.0961)	0.0590 (0.0591)	0.574 (0.861)	84.38 (73.86)
[50/157]	0.0934 (0.0959)	0.0572 (0.0590)	0.739 (0.848)	81.25 (73.96)
[60/157]	0.0968 (0.0958)	0.0596 (0.0590)	0.743 (0.836)	71.88 (74.18)
[70/157]	0.0940 (0.0957)	0.0575 (0.0590)	1.057 (0.839)	56.25 (74.34)
[80/157]	0.0947 (0.0957)	0.0588 (0.0590)	1.155 (0.857)	65.62 (73.53)
[90/157]	0.0963 (0.0956)	0.0593 (0.0590)	0.647 (0.847)	81.25 (74.04)
[100/157]	0.0970 (0.0957)	0.0597 (0.0590)	0.694 (0.849)	78.12 (73.89)
[110/157]	0.0942 (0.0956)	0.0579 (0.0590)	0.826 (0.846)	71.88 (73.79)
[120/157]	0.0964 (0.0956)	0.0592 (0.0590)	0.806 (0.841)	78.12 (74.04)
[130/157]	0.0945 (0.0955)	0.0586 (0.0590)	0.787 (0.840)	78.12 (73.93)
[140/157]	0.0958 (0.0955)	0.0591 (0.0590)	0.756 (0.836)	78.12 (74.07)
[150/157]	0.0956 (0.0955)	0.0586 (0.0590)	1.128 (0.842)	75.00 (74.03)
[156/157]	0.0772 (0.0953)	0.0528 (0.0589)	0.763 (0.841)	75.00 (74.08)
 * Train Acc 74.080
 * Val Acc 76.300, Total time 0.57
 * Val loss 0.731, Total time 0.00
Epoch:17
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0083 (0.0083)	0.999 (0.999)	62.50 (62.50)
[10/157]	0.0969 (0.0907)	0.0603 (0.0539)	0.615 (0.883)	75.00 (71.88)
[20/157]	0.0956 (0.0925)	0.0593 (0.0561)	0.769 (0.871)	75.00 (71.13)
[30/157]	0.0941 (0.0984)	0.0579 (0.0614)	1.137 (0.852)	56.25 (72.28)
[40/157]	0.0961 (0.0977)	0.0596 (0.0608)	0.926 (0.863)	71.88 (72.03)
[50/157]	0.0970 (0.0972)	0.0599 (0.0605)	1.011 (0.856)	71.88 (72.86)
[60/157]	0.0958 (0.0970)	0.0586 (0.0602)	1.021 (0.866)	62.50 (73.05)
[70/157]	0.0947 (0.0968)	0.0585 (0.0600)	0.406 (0.857)	93.75 (73.37)
[80/157]	0.0942 (0.0966)	0.0575 (0.0599)	0.936 (0.864)	71.88 (73.50)
[90/157]	0.0966 (0.0965)	0.0594 (0.0597)	0.577 (0.848)	84.38 (74.14)
[100/157]	0.0948 (0.0964)	0.0587 (0.0596)	0.746 (0.843)	84.38 (74.57)
[110/157]	0.0950 (0.0963)	0.0580 (0.0595)	0.779 (0.835)	75.00 (74.89)
[120/157]	0.0990 (0.0962)	0.0602 (0.0594)	0.692 (0.832)	81.25 (74.79)
[130/157]	0.0966 (0.0962)	0.0580 (0.0594)	0.738 (0.836)	71.88 (74.55)
[140/157]	0.0972 (0.0962)	0.0595 (0.0593)	0.735 (0.835)	81.25 (74.42)
[150/157]	0.0964 (0.0961)	0.0594 (0.0592)	1.039 (0.838)	68.75 (74.30)
[156/157]	0.0790 (0.0960)	0.0543 (0.0592)	1.708 (0.840)	50.00 (74.10)
 * Train Acc 74.100
 * Val Acc 76.500, Total time 0.58
 * Val loss 0.731, Total time 0.00
Epoch:18
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0446 (0.0446)	0.0090 (0.0090)	0.907 (0.907)	62.50 (62.50)
[10/157]	0.0977 (0.0914)	0.0586 (0.0533)	0.626 (0.917)	81.25 (71.59)
[20/157]	0.0971 (0.0935)	0.0594 (0.0558)	0.634 (0.885)	87.50 (72.02)
[30/157]	0.0958 (0.0942)	0.0589 (0.0568)	1.099 (0.892)	65.62 (72.38)
[40/157]	0.0950 (0.0945)	0.0586 (0.0572)	0.857 (0.872)	71.88 (73.02)
[50/157]	0.0949 (0.0948)	0.0586 (0.0576)	1.035 (0.881)	68.75 (72.37)
[60/157]	0.0949 (0.0949)	0.0582 (0.0577)	0.956 (0.887)	65.62 (72.28)
[70/157]	0.0978 (0.0950)	0.0591 (0.0579)	0.689 (0.866)	87.50 (72.76)
[80/157]	0.0962 (0.0950)	0.0579 (0.0578)	0.594 (0.862)	87.50 (73.11)
[90/157]	0.0970 (0.0950)	0.0600 (0.0579)	0.668 (0.852)	84.38 (73.80)
[100/157]	0.0957 (0.0950)	0.0582 (0.0579)	0.869 (0.844)	71.88 (74.29)
[110/157]	0.0964 (0.0950)	0.0593 (0.0580)	0.629 (0.841)	81.25 (74.47)
[120/157]	0.0974 (0.0950)	0.0584 (0.0580)	1.106 (0.844)	62.50 (74.54)
[130/157]	0.0961 (0.0950)	0.0590 (0.0580)	0.726 (0.846)	81.25 (74.50)
[140/157]	0.0937 (0.0950)	0.0570 (0.0580)	0.708 (0.849)	71.88 (74.36)
[150/157]	0.0965 (0.0950)	0.0584 (0.0580)	1.011 (0.853)	59.38 (74.15)
[156/157]	0.0794 (0.0949)	0.0531 (0.0580)	1.087 (0.850)	62.50 (74.28)
 * Train Acc 74.280
 * Val Acc 76.500, Total time 0.59
 * Val loss 0.720, Total time 0.00
Epoch:19
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0087 (0.0087)	0.718 (0.718)	81.25 (81.25)
[10/157]	0.1048 (0.0979)	0.0657 (0.0592)	0.766 (0.849)	87.50 (72.73)
[20/157]	0.1096 (0.0972)	0.0708 (0.0595)	0.699 (0.842)	78.12 (72.92)
[30/157]	0.0938 (0.0985)	0.0576 (0.0611)	0.925 (0.838)	65.62 (73.19)
[40/157]	0.0965 (0.0977)	0.0590 (0.0604)	0.903 (0.843)	68.75 (73.40)
[50/157]	0.0950 (0.0972)	0.0583 (0.0599)	0.976 (0.835)	71.88 (74.02)
[60/157]	0.0959 (0.0967)	0.0597 (0.0597)	0.859 (0.834)	71.88 (74.18)
[70/157]	0.0953 (0.0965)	0.0583 (0.0595)	0.655 (0.839)	84.38 (74.25)
[80/157]	0.0954 (0.0962)	0.0593 (0.0594)	1.010 (0.836)	71.88 (74.50)
[90/157]	0.0958 (0.0960)	0.0595 (0.0593)	0.680 (0.830)	78.12 (74.48)
[100/157]	0.0938 (0.0959)	0.0570 (0.0592)	0.717 (0.821)	84.38 (75.19)
[110/157]	0.0969 (0.0958)	0.0595 (0.0591)	0.944 (0.817)	62.50 (75.48)
[120/157]	0.0944 (0.0957)	0.0583 (0.0591)	0.741 (0.825)	68.75 (75.13)
[130/157]	0.0956 (0.0957)	0.0585 (0.0591)	0.839 (0.829)	65.62 (74.90)
[140/157]	0.0954 (0.0956)	0.0591 (0.0590)	0.882 (0.826)	78.12 (75.07)
[150/157]	0.0938 (0.0955)	0.0579 (0.0590)	0.627 (0.826)	87.50 (74.98)
[156/157]	0.0790 (0.0954)	0.0546 (0.0589)	0.582 (0.828)	87.50 (74.92)
 * Train Acc 74.920
 * Val Acc 77.100, Total time 0.61
 * Val loss 0.706, Total time 0.00
Epoch:20
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0089 (0.0089)	0.868 (0.868)	75.00 (75.00)
[10/157]	0.0966 (0.0905)	0.0595 (0.0539)	1.046 (0.794)	65.62 (75.57)
[20/157]	0.0949 (0.0932)	0.0587 (0.0561)	0.801 (0.821)	81.25 (74.85)
[30/157]	0.0949 (0.0936)	0.0582 (0.0569)	0.805 (0.838)	78.12 (74.40)
[40/157]	0.0966 (0.0939)	0.0596 (0.0574)	0.579 (0.816)	81.25 (75.00)
[50/157]	0.1132 (0.0945)	0.0740 (0.0579)	0.832 (0.816)	75.00 (75.25)
[60/157]	0.1020 (0.0969)	0.0637 (0.0600)	0.792 (0.812)	81.25 (75.41)
[70/157]	0.1012 (0.0974)	0.0636 (0.0604)	0.641 (0.821)	78.12 (75.26)
[80/157]	0.0928 (0.0973)	0.0572 (0.0603)	0.727 (0.823)	78.12 (75.27)
[90/157]	0.0961 (0.0970)	0.0593 (0.0601)	0.909 (0.836)	68.75 (74.73)
[100/157]	0.0943 (0.0968)	0.0576 (0.0600)	0.710 (0.834)	75.00 (74.81)
[110/157]	0.0955 (0.0966)	0.0585 (0.0599)	0.762 (0.836)	78.12 (74.72)
[120/157]	0.0941 (0.0965)	0.0582 (0.0597)	0.813 (0.834)	75.00 (74.72)
[130/157]	0.0937 (0.0969)	0.0578 (0.0601)	0.643 (0.827)	81.25 (75.12)
[140/157]	0.0971 (0.0968)	0.0597 (0.0600)	1.095 (0.829)	71.88 (75.04)
[150/157]	0.0940 (0.0967)	0.0580 (0.0599)	0.851 (0.828)	71.88 (74.94)
[156/157]	0.0798 (0.0965)	0.0546 (0.0598)	0.833 (0.831)	75.00 (74.88)
 * Train Acc 74.880
 * Val Acc 76.000, Total time 0.57
 * Val loss 0.732, Total time 0.00
Epoch:21
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0416 (0.0416)	0.0084 (0.0084)	1.015 (1.015)	62.50 (62.50)
[10/157]	0.0976 (0.0926)	0.0611 (0.0549)	1.005 (0.897)	68.75 (70.45)
[20/157]	0.0968 (0.0952)	0.0603 (0.0577)	0.751 (0.904)	81.25 (71.73)
[30/157]	0.0978 (0.0962)	0.0610 (0.0588)	0.617 (0.876)	84.38 (72.68)
[40/157]	0.0983 (0.0966)	0.0606 (0.0591)	0.638 (0.825)	84.38 (75.00)
[50/157]	0.0978 (0.0970)	0.0597 (0.0594)	1.121 (0.824)	59.38 (74.82)
[60/157]	0.0970 (0.0971)	0.0593 (0.0596)	0.732 (0.830)	81.25 (74.80)
[70/157]	0.0991 (0.0973)	0.0610 (0.0597)	0.799 (0.816)	71.88 (75.31)
[80/157]	0.0987 (0.0974)	0.0610 (0.0598)	1.141 (0.823)	68.75 (75.08)
[90/157]	0.0988 (0.0975)	0.0609 (0.0599)	0.789 (0.820)	71.88 (75.17)
[100/157]	0.0995 (0.0975)	0.0614 (0.0600)	0.527 (0.822)	87.50 (75.12)
[110/157]	0.0995 (0.0975)	0.0618 (0.0600)	0.809 (0.823)	75.00 (75.39)
[120/157]	0.0986 (0.0976)	0.0608 (0.0601)	0.686 (0.822)	75.00 (75.21)
[130/157]	0.0981 (0.0976)	0.0608 (0.0601)	0.852 (0.828)	65.62 (74.76)
[140/157]	0.0986 (0.0976)	0.0610 (0.0602)	0.633 (0.825)	93.75 (75.09)
[150/157]	0.0992 (0.0977)	0.0616 (0.0602)	0.799 (0.823)	62.50 (75.12)
[156/157]	0.0795 (0.0975)	0.0544 (0.0602)	1.131 (0.823)	62.50 (75.10)
 * Train Acc 75.100
 * Val Acc 77.400, Total time 0.60
 * Val loss 0.705, Total time 0.00
Epoch:22
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0409 (0.0409)	0.0084 (0.0084)	0.746 (0.746)	71.88 (71.88)
[10/157]	0.0976 (0.0924)	0.0608 (0.0554)	1.164 (0.798)	62.50 (76.14)
[20/157]	0.0950 (0.0946)	0.0585 (0.0577)	0.692 (0.818)	84.38 (75.74)
[30/157]	0.0963 (0.0958)	0.0599 (0.0585)	0.823 (0.817)	81.25 (75.40)
[40/157]	0.0995 (0.0963)	0.0624 (0.0590)	0.686 (0.811)	78.12 (75.53)
[50/157]	0.0974 (0.0968)	0.0596 (0.0593)	0.957 (0.827)	62.50 (75.06)
[60/157]	0.0970 (0.0970)	0.0597 (0.0593)	0.908 (0.837)	71.88 (74.74)
[70/157]	0.0999 (0.0972)	0.0614 (0.0595)	0.786 (0.828)	68.75 (74.96)
[80/157]	0.0992 (0.0973)	0.0610 (0.0596)	1.029 (0.835)	59.38 (74.61)
[90/157]	0.0977 (0.0975)	0.0593 (0.0596)	0.971 (0.834)	71.88 (74.83)
[100/157]	0.0982 (0.0975)	0.0603 (0.0597)	0.466 (0.836)	90.62 (74.69)
[110/157]	0.0974 (0.0976)	0.0590 (0.0597)	0.633 (0.841)	81.25 (74.32)
[120/157]	0.0970 (0.0976)	0.0592 (0.0597)	0.899 (0.836)	75.00 (74.33)
[130/157]	0.0972 (0.0976)	0.0600 (0.0597)	0.671 (0.826)	84.38 (74.86)
[140/157]	0.0962 (0.0975)	0.0579 (0.0596)	0.518 (0.821)	90.62 (75.02)
[150/157]	0.0956 (0.0973)	0.0575 (0.0595)	0.938 (0.820)	65.62 (75.10)
[156/157]	0.0763 (0.0971)	0.0512 (0.0594)	1.202 (0.821)	62.50 (75.08)
 * Train Acc 75.080
 * Val Acc 76.200, Total time 0.58
 * Val loss 0.715, Total time 0.00
Epoch:23
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0436 (0.0436)	0.0087 (0.0087)	0.805 (0.805)	75.00 (75.00)
[10/157]	0.0937 (0.0922)	0.0562 (0.0538)	0.718 (0.835)	71.88 (74.72)
[20/157]	0.0956 (0.0935)	0.0587 (0.0559)	1.252 (0.844)	59.38 (73.96)
[30/157]	0.0934 (0.0939)	0.0568 (0.0566)	0.757 (0.839)	75.00 (74.60)
[40/157]	0.0966 (0.0942)	0.0587 (0.0570)	0.788 (0.844)	78.12 (74.31)
[50/157]	0.0926 (0.0943)	0.0575 (0.0571)	1.247 (0.828)	56.25 (74.82)
[60/157]	0.0959 (0.0944)	0.0597 (0.0574)	0.653 (0.826)	78.12 (74.85)
[70/157]	0.0938 (0.0945)	0.0575 (0.0576)	0.898 (0.816)	75.00 (75.13)
[80/157]	0.0940 (0.0945)	0.0578 (0.0577)	0.993 (0.822)	68.75 (74.69)
[90/157]	0.0950 (0.0945)	0.0582 (0.0578)	0.563 (0.815)	87.50 (75.10)
[100/157]	0.0943 (0.0946)	0.0580 (0.0579)	0.892 (0.820)	71.88 (74.88)
[110/157]	0.0956 (0.0946)	0.0589 (0.0579)	0.942 (0.817)	59.38 (74.97)
[120/157]	0.0933 (0.0946)	0.0574 (0.0580)	1.194 (0.816)	59.38 (75.05)
[130/157]	0.0963 (0.0947)	0.0582 (0.0580)	0.983 (0.817)	68.75 (74.98)
[140/157]	0.1074 (0.0953)	0.0692 (0.0585)	0.710 (0.813)	71.88 (75.18)
[150/157]	0.0961 (0.0955)	0.0588 (0.0586)	0.593 (0.809)	90.62 (75.35)
[156/157]	0.0776 (0.0954)	0.0513 (0.0585)	0.775 (0.810)	62.50 (75.18)
 * Train Acc 75.180
 * Val Acc 77.400, Total time 0.58
 * Val loss 0.699, Total time 0.00
Epoch:24
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0088 (0.0088)	0.944 (0.944)	71.88 (71.88)
[10/157]	0.0954 (0.0912)	0.0584 (0.0539)	0.455 (0.777)	90.62 (76.99)
[20/157]	0.0965 (0.0929)	0.0588 (0.0561)	0.877 (0.807)	71.88 (74.85)
[30/157]	0.0928 (0.0934)	0.0578 (0.0567)	0.942 (0.809)	65.62 (74.80)
[40/157]	0.0948 (0.0938)	0.0583 (0.0572)	1.054 (0.798)	65.62 (75.08)
[50/157]	0.1005 (0.0968)	0.0634 (0.0599)	0.654 (0.802)	84.38 (75.18)
[60/157]	0.1007 (0.0976)	0.0633 (0.0605)	0.695 (0.806)	78.12 (75.00)
[70/157]	0.0929 (0.0978)	0.0569 (0.0607)	1.007 (0.810)	71.88 (75.18)
[80/157]	0.0962 (0.0974)	0.0597 (0.0605)	0.794 (0.813)	78.12 (74.85)
[90/157]	0.0944 (0.0971)	0.0584 (0.0602)	0.810 (0.817)	75.00 (74.59)
[100/157]	0.0956 (0.0969)	0.0594 (0.0601)	0.784 (0.817)	75.00 (74.38)
[110/157]	0.0935 (0.0968)	0.0585 (0.0599)	0.941 (0.822)	75.00 (74.18)
[120/157]	0.0939 (0.0967)	0.0575 (0.0598)	0.752 (0.817)	81.25 (74.51)
[130/157]	0.0951 (0.0966)	0.0590 (0.0597)	0.830 (0.816)	75.00 (74.74)
[140/157]	0.0928 (0.0964)	0.0577 (0.0596)	1.117 (0.815)	62.50 (74.82)
[150/157]	0.0947 (0.0964)	0.0582 (0.0595)	0.531 (0.815)	93.75 (75.08)
[156/157]	0.0784 (0.0962)	0.0524 (0.0594)	0.551 (0.813)	87.50 (75.18)
 * Train Acc 75.180
 * Val Acc 76.900, Total time 0.60
 * Val loss 0.692, Total time 0.00
Epoch:25
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0086 (0.0086)	0.571 (0.571)	81.25 (81.25)
[10/157]	0.0968 (0.0918)	0.0603 (0.0552)	0.881 (0.752)	78.12 (78.12)
[20/157]	0.0953 (0.0932)	0.0592 (0.0569)	0.698 (0.778)	71.88 (76.79)
[30/157]	0.0940 (0.0937)	0.0583 (0.0575)	0.671 (0.788)	78.12 (76.61)
[40/157]	0.1187 (0.0946)	0.0799 (0.0583)	0.724 (0.790)	81.25 (76.45)
[50/157]	0.0956 (0.0967)	0.0590 (0.0602)	0.755 (0.799)	84.38 (76.47)
[60/157]	0.0955 (0.0964)	0.0592 (0.0599)	0.610 (0.788)	81.25 (76.79)
[70/157]	0.1187 (0.0968)	0.0799 (0.0604)	0.624 (0.785)	78.12 (76.63)
[80/157]	0.0972 (0.0974)	0.0602 (0.0609)	0.974 (0.789)	78.12 (76.35)
[90/157]	0.0936 (0.0971)	0.0577 (0.0607)	0.849 (0.790)	78.12 (76.13)
[100/157]	0.0963 (0.0969)	0.0592 (0.0605)	0.788 (0.794)	78.12 (75.93)
[110/157]	0.0939 (0.0967)	0.0577 (0.0603)	0.932 (0.793)	68.75 (76.04)
[120/157]	0.0966 (0.0966)	0.0593 (0.0602)	0.685 (0.794)	78.12 (75.98)
[130/157]	0.0945 (0.0965)	0.0587 (0.0601)	0.794 (0.796)	81.25 (75.93)
[140/157]	0.0957 (0.0964)	0.0585 (0.0599)	0.867 (0.800)	75.00 (75.78)
[150/157]	0.0915 (0.0963)	0.0571 (0.0599)	0.837 (0.797)	71.88 (75.77)
[156/157]	0.0793 (0.0961)	0.0550 (0.0598)	0.883 (0.798)	75.00 (75.62)
 * Train Acc 75.620
 * Val Acc 77.900, Total time 0.58
 * Val loss 0.698, Total time 0.00
Epoch:26
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0090 (0.0090)	0.851 (0.851)	78.12 (78.12)
[10/157]	0.0949 (0.0977)	0.0584 (0.0597)	0.646 (0.723)	78.12 (76.70)
[20/157]	0.0958 (0.0969)	0.0596 (0.0595)	0.533 (0.721)	87.50 (79.02)
[30/157]	0.0968 (0.0968)	0.0592 (0.0593)	0.788 (0.739)	71.88 (77.52)
[40/157]	0.0974 (0.0966)	0.0596 (0.0594)	0.775 (0.750)	71.88 (76.75)
[50/157]	0.0964 (0.0964)	0.0598 (0.0592)	0.808 (0.751)	81.25 (77.21)
[60/157]	0.0965 (0.0963)	0.0600 (0.0593)	0.815 (0.756)	71.88 (77.25)
[70/157]	0.0961 (0.0963)	0.0576 (0.0592)	0.543 (0.769)	87.50 (76.76)
[80/157]	0.0975 (0.0963)	0.0590 (0.0591)	0.892 (0.774)	65.62 (76.85)
[90/157]	0.0965 (0.0963)	0.0595 (0.0591)	0.744 (0.782)	78.12 (76.51)
[100/157]	0.0956 (0.0963)	0.0590 (0.0591)	1.054 (0.775)	65.62 (76.95)
[110/157]	0.0963 (0.0963)	0.0586 (0.0591)	1.033 (0.776)	62.50 (76.83)
[120/157]	0.0942 (0.0963)	0.0574 (0.0591)	0.858 (0.777)	81.25 (76.78)
[130/157]	0.0957 (0.0963)	0.0587 (0.0591)	0.761 (0.777)	68.75 (76.67)
[140/157]	0.0960 (0.0963)	0.0585 (0.0590)	1.160 (0.784)	65.62 (76.35)
[150/157]	0.0971 (0.0963)	0.0594 (0.0591)	0.568 (0.790)	87.50 (76.10)
[156/157]	0.0799 (0.0961)	0.0544 (0.0590)	0.933 (0.789)	75.00 (76.18)
 * Train Acc 76.180
 * Val Acc 77.700, Total time 0.60
 * Val loss 0.701, Total time 0.00
Epoch:27
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0089 (0.0089)	0.809 (0.809)	75.00 (75.00)
[10/157]	0.0962 (0.0908)	0.0596 (0.0540)	1.223 (0.768)	62.50 (78.98)
[20/157]	0.0959 (0.0934)	0.0590 (0.0565)	0.617 (0.763)	87.50 (77.08)
[30/157]	0.0956 (0.0944)	0.0596 (0.0575)	1.318 (0.780)	56.25 (76.71)
[40/157]	0.0921 (0.0948)	0.0571 (0.0579)	0.960 (0.784)	71.88 (75.91)
[50/157]	0.0957 (0.0951)	0.0587 (0.0582)	1.328 (0.792)	56.25 (75.98)
[60/157]	0.0989 (0.0953)	0.0587 (0.0583)	0.620 (0.778)	84.38 (76.59)
[70/157]	0.0980 (0.0954)	0.0598 (0.0585)	0.554 (0.764)	87.50 (77.20)
[80/157]	0.0967 (0.0955)	0.0598 (0.0586)	0.586 (0.758)	81.25 (77.51)
[90/157]	0.0953 (0.0956)	0.0587 (0.0587)	0.898 (0.764)	75.00 (77.40)
[100/157]	0.0969 (0.0956)	0.0597 (0.0587)	0.612 (0.763)	81.25 (77.29)
[110/157]	0.0957 (0.0957)	0.0582 (0.0587)	0.816 (0.770)	81.25 (77.03)
[120/157]	0.0969 (0.0957)	0.0592 (0.0587)	0.938 (0.768)	75.00 (77.01)
[130/157]	0.0963 (0.0957)	0.0599 (0.0588)	0.929 (0.768)	71.88 (77.03)
[140/157]	0.0954 (0.0957)	0.0593 (0.0588)	0.934 (0.775)	62.50 (76.68)
[150/157]	0.0936 (0.0957)	0.0575 (0.0588)	0.873 (0.780)	78.12 (76.47)
[156/157]	0.0799 (0.0956)	0.0552 (0.0588)	1.186 (0.781)	37.50 (76.36)
 * Train Acc 76.360
 * Val Acc 78.900, Total time 0.59
 * Val loss 0.678, Total time 0.00
Epoch:28
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0424 (0.0424)	0.0082 (0.0082)	1.067 (1.067)	65.62 (65.62)
[10/157]	0.0960 (0.0908)	0.0597 (0.0539)	0.913 (0.823)	71.88 (74.72)
[20/157]	0.0967 (0.0933)	0.0600 (0.0566)	1.294 (0.859)	50.00 (74.11)
[30/157]	0.1192 (0.0957)	0.0803 (0.0586)	0.697 (0.832)	84.38 (74.50)
[40/157]	0.0935 (0.0969)	0.0580 (0.0600)	0.570 (0.813)	81.25 (74.47)
[50/157]	0.0962 (0.0965)	0.0599 (0.0597)	1.018 (0.822)	65.62 (74.63)
[60/157]	0.0926 (0.0963)	0.0566 (0.0595)	0.580 (0.806)	81.25 (75.00)
[70/157]	0.1116 (0.0965)	0.0735 (0.0597)	0.756 (0.809)	71.88 (75.04)
[80/157]	0.0960 (0.0971)	0.0595 (0.0603)	0.729 (0.798)	81.25 (75.46)
[90/157]	0.0951 (0.0970)	0.0589 (0.0602)	0.824 (0.800)	71.88 (75.34)
[100/157]	0.0963 (0.0970)	0.0597 (0.0601)	0.859 (0.800)	71.88 (75.37)
[110/157]	0.0972 (0.0969)	0.0594 (0.0601)	0.593 (0.798)	84.38 (75.45)
[120/157]	0.0978 (0.0969)	0.0601 (0.0600)	0.826 (0.794)	84.38 (75.77)
[130/157]	0.0978 (0.0968)	0.0609 (0.0600)	0.707 (0.799)	84.38 (75.55)
[140/157]	0.0968 (0.0967)	0.0602 (0.0600)	0.792 (0.800)	75.00 (75.49)
[150/157]	0.0966 (0.0967)	0.0600 (0.0599)	0.611 (0.797)	84.38 (75.66)
[156/157]	0.0797 (0.0966)	0.0530 (0.0598)	1.509 (0.800)	62.50 (75.66)
 * Train Acc 75.660
 * Val Acc 78.800, Total time 0.58
 * Val loss 0.675, Total time 0.00
Epoch:29
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0085 (0.0085)	1.017 (1.017)	65.62 (65.62)
[10/157]	0.0969 (0.0915)	0.0595 (0.0542)	0.693 (0.835)	78.12 (72.73)
[20/157]	0.0959 (0.0940)	0.0586 (0.0564)	0.923 (0.815)	68.75 (73.96)
[30/157]	0.0919 (0.0947)	0.0574 (0.0573)	1.037 (0.798)	75.00 (75.10)
[40/157]	0.0970 (0.0953)	0.0595 (0.0576)	0.832 (0.792)	78.12 (76.14)
[50/157]	0.0981 (0.0955)	0.0606 (0.0579)	0.757 (0.789)	78.12 (76.53)
[60/157]	0.0966 (0.0956)	0.0596 (0.0581)	0.767 (0.793)	78.12 (76.23)
[70/157]	0.0954 (0.0957)	0.0586 (0.0582)	0.939 (0.790)	75.00 (76.10)
[80/157]	0.0953 (0.0958)	0.0590 (0.0583)	0.604 (0.785)	84.38 (76.43)
[90/157]	0.0952 (0.0958)	0.0585 (0.0584)	1.132 (0.794)	62.50 (76.13)
[100/157]	0.0960 (0.0959)	0.0585 (0.0585)	0.913 (0.788)	71.88 (76.05)
[110/157]	0.0968 (0.0959)	0.0589 (0.0585)	0.719 (0.779)	78.12 (76.27)
[120/157]	0.0973 (0.0959)	0.0591 (0.0586)	0.436 (0.779)	90.62 (76.39)
[130/157]	0.0964 (0.0959)	0.0599 (0.0586)	0.921 (0.784)	71.88 (76.31)
[140/157]	0.0950 (0.0960)	0.0577 (0.0586)	0.799 (0.787)	78.12 (76.00)
[150/157]	0.0952 (0.0960)	0.0580 (0.0586)	0.589 (0.787)	78.12 (76.16)
[156/157]	0.0797 (0.0959)	0.0541 (0.0586)	0.576 (0.782)	87.50 (76.46)
 * Train Acc 76.460
 * Val Acc 78.100, Total time 0.58
 * Val loss 0.681, Total time 0.00
Epoch:30
LR: 5e-05
LR: 0.001
LR: 0.0005
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0099 (0.0099)	0.730 (0.730)	75.00 (75.00)
[10/157]	0.0972 (0.0907)	0.0606 (0.0534)	0.561 (0.753)	81.25 (77.27)
[20/157]	0.0931 (0.0926)	0.0581 (0.0559)	0.872 (0.742)	71.88 (78.12)
[30/157]	0.0966 (0.0934)	0.0602 (0.0569)	0.867 (0.761)	71.88 (77.02)
[40/157]	0.0951 (0.0937)	0.0579 (0.0573)	0.749 (0.778)	81.25 (76.14)
[50/157]	0.0968 (0.0940)	0.0604 (0.0575)	0.777 (0.793)	84.38 (75.86)
[60/157]	0.0963 (0.0943)	0.0595 (0.0577)	0.990 (0.784)	62.50 (75.97)
[70/157]	0.0946 (0.0957)	0.0583 (0.0590)	0.862 (0.786)	78.12 (75.88)
[80/157]	0.0940 (0.0956)	0.0581 (0.0590)	0.733 (0.775)	84.38 (76.31)
[90/157]	0.0951 (0.0956)	0.0590 (0.0590)	0.889 (0.783)	65.62 (75.79)
[100/157]	0.0968 (0.0956)	0.0601 (0.0590)	0.614 (0.770)	84.38 (76.49)
[110/157]	0.0986 (0.0956)	0.0613 (0.0590)	0.650 (0.767)	84.38 (76.49)
[120/157]	0.0927 (0.0956)	0.0569 (0.0590)	0.721 (0.767)	81.25 (76.60)
[130/157]	0.0950 (0.0955)	0.0581 (0.0590)	0.665 (0.764)	75.00 (76.57)
[140/157]	0.0956 (0.0956)	0.0580 (0.0590)	0.750 (0.769)	75.00 (76.57)
[150/157]	0.0978 (0.0956)	0.0597 (0.0590)	0.925 (0.771)	65.62 (76.53)
[156/157]	0.0795 (0.0955)	0.0547 (0.0589)	0.869 (0.772)	75.00 (76.50)
 * Train Acc 76.500
 * Val Acc 78.400, Total time 0.57
 * Val loss 0.679, Total time 0.00
Epoch:31
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0085 (0.0085)	1.002 (1.002)	65.62 (65.62)
[10/157]	0.0942 (0.0913)	0.0579 (0.0543)	0.846 (0.791)	81.25 (75.85)
[20/157]	0.1093 (0.0985)	0.0704 (0.0611)	0.782 (0.754)	81.25 (77.38)
[30/157]	0.0952 (0.0978)	0.0583 (0.0606)	0.668 (0.738)	68.75 (77.82)
[40/157]	0.0948 (0.0972)	0.0583 (0.0601)	0.845 (0.746)	78.12 (77.36)
[50/157]	0.0950 (0.0970)	0.0580 (0.0599)	0.818 (0.757)	75.00 (76.72)
[60/157]	0.0992 (0.0967)	0.0624 (0.0597)	0.609 (0.761)	84.38 (76.74)
[70/157]	0.0939 (0.0964)	0.0582 (0.0596)	0.627 (0.765)	81.25 (76.67)
[80/157]	0.0960 (0.0963)	0.0606 (0.0595)	0.916 (0.781)	68.75 (76.08)
[90/157]	0.0999 (0.0962)	0.0613 (0.0595)	0.939 (0.784)	71.88 (75.79)
[100/157]	0.0961 (0.0963)	0.0602 (0.0595)	0.871 (0.782)	71.88 (75.90)
[110/157]	0.0998 (0.0964)	0.0612 (0.0596)	0.762 (0.778)	71.88 (76.07)
[120/157]	0.0962 (0.0964)	0.0592 (0.0596)	0.976 (0.776)	75.00 (76.08)
[130/157]	0.0972 (0.0964)	0.0602 (0.0596)	0.574 (0.778)	81.25 (76.03)
[140/157]	0.0963 (0.0965)	0.0590 (0.0596)	0.630 (0.778)	81.25 (76.04)
[150/157]	0.0979 (0.0965)	0.0601 (0.0596)	0.664 (0.782)	81.25 (75.85)
[156/157]	0.0810 (0.0964)	0.0544 (0.0596)	1.603 (0.782)	62.50 (75.92)
 * Train Acc 75.920
 * Val Acc 78.500, Total time 0.60
 * Val loss 0.668, Total time 0.00
Epoch:32
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0087 (0.0087)	0.496 (0.496)	93.75 (93.75)
[10/157]	0.0960 (0.0920)	0.0587 (0.0539)	0.611 (0.739)	87.50 (81.25)
[20/157]	0.0981 (0.0946)	0.0599 (0.0567)	0.765 (0.732)	71.88 (80.21)
[30/157]	0.0982 (0.0955)	0.0600 (0.0575)	0.972 (0.737)	71.88 (79.54)
[40/157]	0.0971 (0.0959)	0.0590 (0.0580)	0.599 (0.726)	75.00 (79.19)
[50/157]	0.0969 (0.0962)	0.0589 (0.0582)	0.560 (0.728)	90.62 (79.04)
[60/157]	0.0973 (0.0964)	0.0595 (0.0584)	0.766 (0.737)	78.12 (78.79)
[70/157]	0.0963 (0.0964)	0.0594 (0.0586)	0.790 (0.747)	78.12 (78.17)
[80/157]	0.0975 (0.0965)	0.0593 (0.0587)	0.786 (0.738)	81.25 (78.32)
[90/157]	0.0975 (0.0965)	0.0603 (0.0588)	0.425 (0.741)	90.62 (78.30)
[100/157]	0.0985 (0.0966)	0.0607 (0.0590)	0.981 (0.747)	68.75 (77.78)
[110/157]	0.0946 (0.0966)	0.0581 (0.0590)	0.780 (0.751)	75.00 (77.70)
[120/157]	0.0945 (0.0966)	0.0587 (0.0590)	0.671 (0.754)	75.00 (77.45)
[130/157]	0.0979 (0.0966)	0.0614 (0.0591)	0.961 (0.757)	68.75 (77.19)
[140/157]	0.0957 (0.0967)	0.0596 (0.0591)	0.644 (0.758)	87.50 (77.22)
[150/157]	0.0980 (0.0967)	0.0593 (0.0591)	0.804 (0.758)	75.00 (77.40)
[156/157]	0.0794 (0.0966)	0.0531 (0.0591)	0.614 (0.761)	87.50 (77.18)
 * Train Acc 77.180
 * Val Acc 78.700, Total time 0.60
 * Val loss 0.663, Total time 0.00
Epoch:33
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0086 (0.0086)	0.817 (0.817)	78.12 (78.12)
[10/157]	0.0969 (0.0913)	0.0598 (0.0544)	0.862 (0.789)	71.88 (75.85)
[20/157]	0.0985 (0.0942)	0.0602 (0.0569)	0.802 (0.745)	68.75 (77.08)
[30/157]	0.0961 (0.0950)	0.0592 (0.0578)	0.803 (0.725)	75.00 (77.12)
[40/157]	0.0972 (0.0955)	0.0592 (0.0583)	0.630 (0.727)	87.50 (77.06)
[50/157]	0.0972 (0.0958)	0.0599 (0.0586)	0.697 (0.754)	84.38 (76.84)
[60/157]	0.0956 (0.0959)	0.0599 (0.0587)	0.892 (0.769)	75.00 (76.23)
[70/157]	0.0975 (0.0960)	0.0601 (0.0589)	0.823 (0.770)	78.12 (76.54)
[80/157]	0.0976 (0.0962)	0.0602 (0.0590)	0.584 (0.772)	81.25 (76.58)
[90/157]	0.0960 (0.0962)	0.0592 (0.0591)	0.704 (0.776)	81.25 (76.55)
[100/157]	0.0981 (0.0962)	0.0617 (0.0592)	0.923 (0.774)	78.12 (76.70)
[110/157]	0.0987 (0.0964)	0.0606 (0.0592)	0.501 (0.776)	87.50 (76.55)
[120/157]	0.0969 (0.0964)	0.0606 (0.0592)	0.652 (0.771)	84.38 (76.73)
[130/157]	0.0959 (0.0964)	0.0590 (0.0593)	0.772 (0.772)	75.00 (76.65)
[140/157]	0.0981 (0.0964)	0.0606 (0.0593)	0.506 (0.770)	90.62 (76.66)
[150/157]	0.0971 (0.0964)	0.0602 (0.0594)	1.044 (0.776)	68.75 (76.51)
[156/157]	0.0806 (0.0963)	0.0556 (0.0594)	0.509 (0.772)	100.00 (76.64)
 * Train Acc 76.640
 * Val Acc 78.400, Total time 0.59
 * Val loss 0.671, Total time 0.00
Epoch:34
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0084 (0.0084)	0.557 (0.557)	81.25 (81.25)
[10/157]	0.0966 (0.0917)	0.0595 (0.0546)	0.477 (0.711)	87.50 (78.41)
[20/157]	0.0973 (0.0941)	0.0604 (0.0571)	0.882 (0.760)	68.75 (76.04)
[30/157]	0.0983 (0.0952)	0.0602 (0.0580)	0.888 (0.753)	71.88 (76.61)
[40/157]	0.0960 (0.0957)	0.0589 (0.0583)	0.693 (0.747)	78.12 (76.98)
[50/157]	0.0973 (0.0960)	0.0600 (0.0587)	1.055 (0.760)	68.75 (76.65)
[60/157]	0.0962 (0.0962)	0.0588 (0.0588)	0.710 (0.746)	75.00 (77.51)
[70/157]	0.0962 (0.0963)	0.0592 (0.0589)	0.851 (0.751)	65.62 (77.20)
[80/157]	0.0974 (0.0964)	0.0603 (0.0590)	0.744 (0.763)	75.00 (77.01)
[90/157]	0.0965 (0.0965)	0.0595 (0.0590)	0.674 (0.762)	78.12 (76.99)
[100/157]	0.0968 (0.0966)	0.0598 (0.0591)	0.576 (0.767)	81.25 (76.89)
[110/157]	0.0981 (0.0966)	0.0596 (0.0591)	0.699 (0.764)	84.38 (76.97)
[120/157]	0.0963 (0.0967)	0.0577 (0.0591)	0.582 (0.763)	81.25 (76.99)
[130/157]	0.0998 (0.0967)	0.0609 (0.0592)	0.815 (0.766)	78.12 (77.08)
[140/157]	0.0993 (0.0968)	0.0608 (0.0592)	0.844 (0.767)	75.00 (76.80)
[150/157]	0.0968 (0.0968)	0.0597 (0.0592)	0.353 (0.766)	96.88 (76.80)
[156/157]	0.0801 (0.0967)	0.0538 (0.0592)	1.308 (0.763)	50.00 (76.92)
 * Train Acc 76.920
 * Val Acc 79.000, Total time 0.58
 * Val loss 0.657, Total time 0.00
Epoch:35
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0095 (0.0095)	0.803 (0.803)	81.25 (81.25)
[10/157]	0.0958 (0.0916)	0.0588 (0.0546)	0.709 (0.793)	84.38 (78.12)
[20/157]	0.0993 (0.0942)	0.0607 (0.0571)	0.618 (0.748)	87.50 (79.32)
[30/157]	0.0969 (0.0950)	0.0601 (0.0579)	1.017 (0.745)	71.88 (79.23)
[40/157]	0.0984 (0.0956)	0.0606 (0.0585)	0.986 (0.749)	68.75 (78.89)
[50/157]	0.0962 (0.0958)	0.0594 (0.0587)	0.848 (0.750)	71.88 (78.12)
[60/157]	0.0983 (0.0961)	0.0605 (0.0589)	0.546 (0.751)	93.75 (77.92)
[70/157]	0.0964 (0.0962)	0.0597 (0.0590)	0.869 (0.754)	75.00 (77.51)
[80/157]	0.0976 (0.0963)	0.0606 (0.0591)	0.752 (0.759)	81.25 (77.08)
[90/157]	0.0963 (0.0964)	0.0596 (0.0591)	0.731 (0.760)	87.50 (77.30)
[100/157]	0.0975 (0.0964)	0.0598 (0.0591)	0.628 (0.757)	81.25 (77.51)
[110/157]	0.0981 (0.0965)	0.0598 (0.0592)	0.508 (0.757)	84.38 (77.34)
[120/157]	0.0964 (0.0966)	0.0595 (0.0592)	0.817 (0.764)	75.00 (77.04)
[130/157]	0.0980 (0.0966)	0.0591 (0.0592)	0.644 (0.775)	87.50 (76.57)
[140/157]	0.0957 (0.0966)	0.0581 (0.0592)	0.652 (0.778)	84.38 (76.51)
[150/157]	0.0987 (0.0967)	0.0603 (0.0592)	0.667 (0.775)	75.00 (76.57)
[156/157]	0.0803 (0.0965)	0.0550 (0.0591)	0.903 (0.773)	75.00 (76.78)
 * Train Acc 76.780
 * Val Acc 78.800, Total time 0.59
 * Val loss 0.660, Total time 0.00
Epoch:36
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0089 (0.0089)	0.516 (0.516)	81.25 (81.25)
[10/157]	0.0977 (0.0921)	0.0598 (0.0544)	0.575 (0.751)	90.62 (76.99)
[20/157]	0.0955 (0.0943)	0.0597 (0.0570)	0.639 (0.727)	81.25 (77.98)
[30/157]	0.0957 (0.0950)	0.0588 (0.0579)	0.702 (0.721)	75.00 (77.62)
[40/157]	0.0976 (0.0954)	0.0602 (0.0583)	0.805 (0.740)	71.88 (76.83)
[50/157]	0.0966 (0.0957)	0.0601 (0.0587)	0.690 (0.747)	78.12 (77.02)
[60/157]	0.0982 (0.0959)	0.0603 (0.0589)	0.850 (0.762)	87.50 (76.74)
[70/157]	0.0977 (0.0960)	0.0614 (0.0591)	0.841 (0.766)	68.75 (76.72)
[80/157]	0.0964 (0.0961)	0.0588 (0.0592)	0.742 (0.775)	75.00 (76.31)
[90/157]	0.0946 (0.0962)	0.0588 (0.0592)	0.902 (0.767)	65.62 (76.20)
[100/157]	0.0972 (0.0963)	0.0598 (0.0593)	0.759 (0.775)	81.25 (76.02)
[110/157]	0.0979 (0.0963)	0.0614 (0.0594)	0.636 (0.770)	75.00 (76.13)
[120/157]	0.0988 (0.0965)	0.0613 (0.0595)	0.753 (0.768)	71.88 (75.96)
[130/157]	0.0967 (0.0965)	0.0595 (0.0595)	0.556 (0.765)	84.38 (76.05)
[140/157]	0.0979 (0.0965)	0.0596 (0.0596)	0.830 (0.762)	78.12 (76.31)
[150/157]	0.0984 (0.0966)	0.0616 (0.0596)	0.751 (0.763)	71.88 (76.10)
[156/157]	0.0834 (0.0965)	0.0551 (0.0596)	0.631 (0.764)	75.00 (76.14)
 * Train Acc 76.140
 * Val Acc 79.000, Total time 0.58
 * Val loss 0.661, Total time 0.00
Epoch:37
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0419 (0.0419)	0.0087 (0.0087)	0.641 (0.641)	78.12 (78.12)
[10/157]	0.0967 (0.0920)	0.0595 (0.0540)	0.540 (0.746)	84.38 (75.28)
[20/157]	0.0957 (0.0935)	0.0572 (0.0560)	0.649 (0.727)	75.00 (77.83)
[30/157]	0.0951 (0.0966)	0.0585 (0.0592)	0.767 (0.707)	71.88 (79.23)
[40/157]	0.0957 (0.0963)	0.0579 (0.0590)	0.928 (0.729)	68.75 (78.81)
[50/157]	0.0947 (0.0961)	0.0583 (0.0588)	0.615 (0.734)	87.50 (78.43)
[60/157]	0.0965 (0.0959)	0.0589 (0.0588)	1.178 (0.758)	59.38 (77.46)
[70/157]	0.0949 (0.0959)	0.0584 (0.0587)	0.379 (0.757)	93.75 (77.90)
[80/157]	0.0953 (0.0957)	0.0586 (0.0587)	0.726 (0.756)	75.00 (77.97)
[90/157]	0.0964 (0.0957)	0.0589 (0.0587)	0.980 (0.761)	68.75 (77.61)
[100/157]	0.0946 (0.0956)	0.0591 (0.0587)	0.733 (0.761)	71.88 (77.38)
[110/157]	0.0962 (0.0956)	0.0598 (0.0587)	0.585 (0.760)	81.25 (77.39)
[120/157]	0.0948 (0.0956)	0.0585 (0.0587)	0.750 (0.757)	65.62 (77.45)
[130/157]	0.0940 (0.0955)	0.0581 (0.0587)	0.733 (0.752)	68.75 (77.48)
[140/157]	0.0954 (0.0955)	0.0587 (0.0587)	0.704 (0.746)	78.12 (77.62)
[150/157]	0.0947 (0.0955)	0.0578 (0.0586)	0.829 (0.750)	75.00 (77.61)
[156/157]	0.0788 (0.0953)	0.0536 (0.0586)	1.070 (0.750)	62.50 (77.48)
 * Train Acc 77.480
 * Val Acc 78.700, Total time 0.58
 * Val loss 0.656, Total time 0.00
Epoch:38
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0422 (0.0422)	0.0087 (0.0087)	0.755 (0.755)	71.88 (71.88)
[10/157]	0.0941 (0.0920)	0.0577 (0.0546)	0.879 (0.790)	78.12 (74.15)
[20/157]	0.0961 (0.0937)	0.0582 (0.0565)	0.693 (0.769)	78.12 (76.49)
[30/157]	0.0939 (0.0941)	0.0575 (0.0570)	1.130 (0.763)	59.38 (76.61)
[40/157]	0.0953 (0.0944)	0.0587 (0.0574)	0.703 (0.769)	81.25 (76.60)
[50/157]	0.0965 (0.0946)	0.0592 (0.0577)	0.562 (0.745)	84.38 (77.45)
[60/157]	0.0944 (0.0947)	0.0580 (0.0578)	0.831 (0.745)	71.88 (77.41)
[70/157]	0.0958 (0.0947)	0.0596 (0.0579)	0.735 (0.750)	84.38 (77.20)
[80/157]	0.0968 (0.0948)	0.0594 (0.0580)	0.822 (0.752)	75.00 (77.04)
[90/157]	0.0946 (0.0948)	0.0575 (0.0581)	0.838 (0.750)	62.50 (76.99)
[100/157]	0.0952 (0.0949)	0.0577 (0.0581)	0.767 (0.749)	65.62 (77.01)
[110/157]	0.0968 (0.0950)	0.0592 (0.0581)	0.711 (0.754)	78.12 (77.14)
[120/157]	0.0952 (0.0949)	0.0588 (0.0581)	0.924 (0.759)	68.75 (76.91)
[130/157]	0.0960 (0.0950)	0.0580 (0.0581)	0.782 (0.757)	71.88 (76.96)
[140/157]	0.0966 (0.0950)	0.0593 (0.0582)	0.853 (0.755)	78.12 (77.04)
[150/157]	0.0954 (0.0950)	0.0579 (0.0581)	1.044 (0.756)	75.00 (77.01)
[156/157]	0.0785 (0.0949)	0.0530 (0.0581)	1.080 (0.758)	62.50 (77.00)
 * Train Acc 77.000
 * Val Acc 79.600, Total time 0.58
 * Val loss 0.661, Total time 0.00
Epoch:39
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0087 (0.0087)	0.572 (0.572)	87.50 (87.50)
[10/157]	0.0956 (0.0915)	0.0582 (0.0540)	0.597 (0.749)	78.12 (75.85)
[20/157]	0.0927 (0.0932)	0.0568 (0.0559)	0.595 (0.768)	87.50 (75.89)
[30/157]	0.0965 (0.0938)	0.0592 (0.0568)	0.776 (0.781)	68.75 (74.70)
[40/157]	0.0950 (0.0941)	0.0587 (0.0572)	0.697 (0.784)	78.12 (74.85)
[50/157]	0.0951 (0.0944)	0.0585 (0.0575)	0.913 (0.792)	65.62 (74.63)
[60/157]	0.0959 (0.0945)	0.0593 (0.0578)	0.766 (0.787)	81.25 (75.00)
[70/157]	0.0930 (0.0946)	0.0569 (0.0579)	0.636 (0.779)	84.38 (75.35)
[80/157]	0.0946 (0.0947)	0.0580 (0.0580)	0.644 (0.784)	81.25 (75.50)
[90/157]	0.0959 (0.0948)	0.0591 (0.0581)	0.946 (0.787)	68.75 (75.34)
[100/157]	0.1043 (0.0956)	0.0667 (0.0588)	0.669 (0.781)	84.38 (75.74)
[110/157]	0.0957 (0.0963)	0.0586 (0.0594)	0.782 (0.777)	78.12 (75.90)
[120/157]	0.0955 (0.0961)	0.0592 (0.0593)	0.992 (0.778)	62.50 (75.88)
[130/157]	0.0966 (0.0961)	0.0598 (0.0593)	0.718 (0.774)	78.12 (76.03)
[140/157]	0.0951 (0.0960)	0.0585 (0.0592)	0.453 (0.767)	93.75 (76.48)
[150/157]	0.1028 (0.0969)	0.0654 (0.0601)	0.954 (0.768)	65.62 (76.35)
[156/157]	0.0798 (0.0967)	0.0533 (0.0600)	0.839 (0.767)	87.50 (76.38)
 * Train Acc 76.380
 * Val Acc 79.500, Total time 0.58
 * Val loss 0.662, Total time 0.00
Epoch:40
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0087 (0.0087)	0.625 (0.625)	81.25 (81.25)
[10/157]	0.0966 (0.0918)	0.0592 (0.0542)	0.902 (0.800)	75.00 (75.57)
[20/157]	0.0950 (0.0933)	0.0587 (0.0564)	0.988 (0.760)	71.88 (77.38)
[30/157]	0.1178 (0.0949)	0.0782 (0.0581)	0.890 (0.751)	75.00 (77.32)
[40/157]	0.0991 (0.0968)	0.0610 (0.0596)	0.669 (0.738)	84.38 (78.58)
[50/157]	0.0999 (0.0972)	0.0617 (0.0599)	1.121 (0.757)	65.62 (77.57)
[60/157]	0.1001 (0.0975)	0.0610 (0.0600)	0.643 (0.752)	81.25 (77.87)
[70/157]	0.0992 (0.0978)	0.0603 (0.0600)	0.764 (0.766)	71.88 (77.20)
[80/157]	0.0996 (0.0979)	0.0611 (0.0601)	0.626 (0.756)	81.25 (77.51)
[90/157]	0.0975 (0.0980)	0.0611 (0.0601)	0.709 (0.755)	81.25 (77.47)
[100/157]	0.0960 (0.0977)	0.0591 (0.0599)	0.551 (0.755)	78.12 (77.38)
[110/157]	0.0931 (0.0974)	0.0559 (0.0598)	0.619 (0.752)	87.50 (77.48)
[120/157]	0.0954 (0.0973)	0.0588 (0.0597)	0.829 (0.747)	81.25 (77.58)
[130/157]	0.0961 (0.0971)	0.0580 (0.0595)	0.883 (0.751)	75.00 (77.43)
[140/157]	0.0931 (0.0970)	0.0573 (0.0595)	0.859 (0.752)	68.75 (77.22)
[150/157]	0.0960 (0.0969)	0.0582 (0.0594)	0.732 (0.751)	75.00 (77.19)
[156/157]	0.0778 (0.0967)	0.0535 (0.0593)	1.072 (0.749)	75.00 (77.24)
 * Train Acc 77.240
 * Val Acc 78.800, Total time 0.57
 * Val loss 0.657, Total time 0.00
Epoch:41
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0443 (0.0443)	0.0089 (0.0089)	0.899 (0.899)	78.12 (78.12)
[10/157]	0.0938 (0.0916)	0.0576 (0.0541)	0.830 (0.802)	75.00 (76.70)
[20/157]	0.0947 (0.0934)	0.0582 (0.0563)	0.966 (0.787)	65.62 (76.34)
[30/157]	0.0962 (0.0940)	0.0590 (0.0570)	0.790 (0.756)	71.88 (77.12)
[40/157]	0.0948 (0.0944)	0.0569 (0.0572)	0.795 (0.769)	78.12 (76.83)
[50/157]	0.1040 (0.0947)	0.0652 (0.0575)	0.524 (0.759)	84.38 (77.33)
[60/157]	0.0956 (0.0961)	0.0592 (0.0587)	0.730 (0.755)	87.50 (77.77)
[70/157]	0.0951 (0.0961)	0.0586 (0.0587)	0.652 (0.760)	78.12 (77.42)
[80/157]	0.0951 (0.0961)	0.0583 (0.0588)	1.100 (0.763)	68.75 (77.31)
[90/157]	0.0958 (0.0960)	0.0598 (0.0588)	0.677 (0.767)	78.12 (76.82)
[100/157]	0.0950 (0.0960)	0.0587 (0.0589)	0.682 (0.765)	75.00 (76.89)
[110/157]	0.0953 (0.0960)	0.0588 (0.0589)	0.802 (0.767)	81.25 (76.89)
[120/157]	0.0961 (0.0960)	0.0583 (0.0589)	0.737 (0.762)	81.25 (76.86)
[130/157]	0.0950 (0.0960)	0.0586 (0.0589)	0.717 (0.759)	84.38 (76.86)
[140/157]	0.0969 (0.0960)	0.0597 (0.0589)	0.571 (0.754)	84.38 (77.06)
[150/157]	0.0965 (0.0959)	0.0591 (0.0590)	0.842 (0.750)	68.75 (77.11)
[156/157]	0.0780 (0.0958)	0.0536 (0.0589)	1.456 (0.756)	50.00 (76.94)
 * Train Acc 76.940
 * Val Acc 78.900, Total time 0.57
 * Val loss 0.651, Total time 0.00
Epoch:42
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0499 (0.0499)	0.0112 (0.0112)	0.680 (0.680)	84.38 (84.38)
[10/157]	0.0965 (0.0980)	0.0593 (0.0593)	0.854 (0.683)	75.00 (80.40)
[20/157]	0.0973 (0.0970)	0.0598 (0.0591)	0.832 (0.701)	75.00 (79.76)
[30/157]	0.0980 (0.0967)	0.0595 (0.0590)	0.815 (0.681)	71.88 (80.44)
[40/157]	0.0980 (0.0965)	0.0600 (0.0589)	0.744 (0.720)	84.38 (79.19)
[50/157]	0.0989 (0.0964)	0.0604 (0.0589)	0.735 (0.723)	78.12 (79.35)
[60/157]	0.0953 (0.0962)	0.0580 (0.0589)	0.666 (0.716)	84.38 (79.56)
[70/157]	0.0942 (0.0961)	0.0574 (0.0589)	0.535 (0.721)	90.62 (79.23)
[80/157]	0.0956 (0.0961)	0.0585 (0.0588)	0.523 (0.725)	90.62 (79.28)
[90/157]	0.0952 (0.0960)	0.0584 (0.0588)	0.881 (0.717)	84.38 (79.46)
[100/157]	0.0959 (0.0960)	0.0583 (0.0588)	0.663 (0.718)	78.12 (79.42)
[110/157]	0.0948 (0.0960)	0.0583 (0.0588)	1.146 (0.723)	62.50 (79.19)
[120/157]	0.0954 (0.0960)	0.0585 (0.0588)	0.688 (0.730)	71.88 (78.59)
[130/157]	0.0974 (0.0960)	0.0601 (0.0588)	0.698 (0.727)	78.12 (78.65)
[140/157]	0.0991 (0.0960)	0.0604 (0.0589)	0.587 (0.729)	87.50 (78.59)
[150/157]	0.0971 (0.0960)	0.0589 (0.0589)	0.817 (0.739)	78.12 (78.12)
[156/157]	0.0776 (0.0958)	0.0534 (0.0588)	0.672 (0.742)	75.00 (77.94)
 * Train Acc 77.940
 * Val Acc 79.000, Total time 0.58
 * Val loss 0.651, Total time 0.00
Epoch:43
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0482 (0.0482)	0.0115 (0.0115)	0.796 (0.796)	78.12 (78.12)
[10/157]	0.0943 (0.0908)	0.0582 (0.0539)	0.652 (0.729)	87.50 (79.55)
[20/157]	0.0958 (0.0932)	0.0588 (0.0565)	0.820 (0.762)	75.00 (77.08)
[30/157]	0.0955 (0.0939)	0.0592 (0.0573)	0.883 (0.755)	65.62 (76.61)
[40/157]	0.0974 (0.0944)	0.0599 (0.0578)	0.668 (0.762)	81.25 (76.83)
[50/157]	0.0953 (0.0946)	0.0589 (0.0580)	0.643 (0.774)	84.38 (76.47)
[60/157]	0.1037 (0.0971)	0.0654 (0.0603)	0.912 (0.771)	75.00 (76.59)
[70/157]	0.0961 (0.0968)	0.0599 (0.0600)	0.569 (0.758)	84.38 (77.02)
[80/157]	0.0941 (0.0966)	0.0577 (0.0598)	0.686 (0.760)	78.12 (77.04)
[90/157]	0.0961 (0.0964)	0.0592 (0.0597)	0.689 (0.750)	84.38 (77.27)
[100/157]	0.0959 (0.0963)	0.0594 (0.0595)	0.922 (0.749)	71.88 (77.07)
[110/157]	0.0955 (0.0974)	0.0590 (0.0606)	0.792 (0.744)	75.00 (77.11)
[120/157]	0.0957 (0.0972)	0.0587 (0.0604)	0.851 (0.747)	75.00 (77.22)
[130/157]	0.0984 (0.0970)	0.0611 (0.0603)	1.135 (0.747)	62.50 (77.22)
[140/157]	0.0936 (0.0969)	0.0576 (0.0602)	0.872 (0.749)	75.00 (77.26)
[150/157]	0.0960 (0.0968)	0.0595 (0.0601)	0.808 (0.745)	78.12 (77.46)
[156/157]	0.0780 (0.0966)	0.0528 (0.0600)	1.057 (0.743)	62.50 (77.52)
 * Train Acc 77.520
 * Val Acc 78.900, Total time 0.57
 * Val loss 0.661, Total time 0.00
Epoch:44
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0412 (0.0412)	0.0086 (0.0086)	0.901 (0.901)	78.12 (78.12)
[10/157]	0.0930 (0.0912)	0.0573 (0.0543)	0.622 (0.794)	87.50 (79.26)
[20/157]	0.0991 (0.0997)	0.0617 (0.0624)	0.749 (0.777)	81.25 (79.32)
[30/157]	0.0999 (0.0997)	0.0619 (0.0622)	0.904 (0.766)	71.88 (78.93)
[40/157]	0.1001 (0.0998)	0.0622 (0.0621)	0.542 (0.765)	81.25 (78.35)
[50/157]	0.0991 (0.0998)	0.0610 (0.0620)	1.172 (0.752)	65.62 (78.80)
[60/157]	0.0994 (0.0998)	0.0611 (0.0620)	0.808 (0.769)	75.00 (77.77)
[70/157]	0.1007 (0.0998)	0.0631 (0.0620)	0.754 (0.773)	81.25 (77.16)
[80/157]	0.0990 (0.0999)	0.0613 (0.0620)	0.696 (0.769)	81.25 (77.20)
[90/157]	0.0998 (0.0999)	0.0621 (0.0620)	0.718 (0.759)	71.88 (77.61)
[100/157]	0.0987 (0.0999)	0.0611 (0.0620)	0.779 (0.749)	75.00 (77.82)
[110/157]	0.0943 (0.0995)	0.0577 (0.0616)	0.760 (0.750)	78.12 (77.62)
[120/157]	0.0964 (0.0991)	0.0593 (0.0613)	1.113 (0.757)	56.25 (77.30)
[130/157]	0.0913 (0.0987)	0.0554 (0.0611)	0.809 (0.749)	81.25 (77.43)
[140/157]	0.0959 (0.0985)	0.0589 (0.0609)	0.671 (0.747)	84.38 (77.62)
[150/157]	0.0933 (0.0983)	0.0574 (0.0607)	0.327 (0.748)	90.62 (77.57)
[156/157]	0.0811 (0.0980)	0.0552 (0.0606)	1.544 (0.748)	12.50 (77.42)
 * Train Acc 77.420
 * Val Acc 78.700, Total time 0.57
 * Val loss 0.654, Total time 0.00
Epoch:45
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0453 (0.0453)	0.0086 (0.0086)	0.520 (0.520)	90.62 (90.62)
[10/157]	0.0945 (0.0920)	0.0584 (0.0538)	0.870 (0.712)	78.12 (77.84)
[20/157]	0.1193 (0.0979)	0.0805 (0.0597)	0.928 (0.740)	75.00 (76.49)
[30/157]	0.0955 (0.0974)	0.0590 (0.0596)	0.818 (0.764)	68.75 (76.71)
[40/157]	0.0942 (0.0968)	0.0585 (0.0594)	0.870 (0.784)	71.88 (75.76)
[50/157]	0.0927 (0.0965)	0.0571 (0.0592)	0.642 (0.758)	84.38 (76.10)
[60/157]	0.0969 (0.0964)	0.0587 (0.0590)	0.749 (0.756)	75.00 (76.08)
[70/157]	0.0945 (0.0962)	0.0564 (0.0588)	0.397 (0.753)	96.88 (76.50)
[80/157]	0.0950 (0.0960)	0.0585 (0.0586)	0.612 (0.746)	84.38 (76.97)
[90/157]	0.1013 (0.0962)	0.0621 (0.0588)	0.923 (0.747)	78.12 (77.03)
[100/157]	0.1004 (0.0966)	0.0615 (0.0591)	0.662 (0.737)	81.25 (77.63)
[110/157]	0.0965 (0.0967)	0.0589 (0.0592)	0.844 (0.738)	75.00 (77.67)
[120/157]	0.0941 (0.0966)	0.0571 (0.0591)	0.914 (0.739)	68.75 (77.69)
[130/157]	0.0958 (0.0964)	0.0595 (0.0591)	0.998 (0.744)	68.75 (77.43)
[140/157]	0.0935 (0.0963)	0.0571 (0.0590)	0.586 (0.744)	81.25 (77.30)
[150/157]	0.0957 (0.0962)	0.0586 (0.0590)	0.687 (0.746)	75.00 (77.13)
[156/157]	0.0771 (0.0960)	0.0522 (0.0589)	1.158 (0.744)	62.50 (77.26)
 * Train Acc 77.260
 * Val Acc 78.700, Total time 0.57
 * Val loss 0.659, Total time 0.00
Epoch:46
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0086 (0.0086)	0.489 (0.489)	81.25 (81.25)
[10/157]	0.0961 (0.0913)	0.0592 (0.0544)	0.735 (0.666)	84.38 (80.97)
[20/157]	0.0942 (0.0929)	0.0576 (0.0564)	1.175 (0.727)	65.62 (78.42)
[30/157]	0.0931 (0.0934)	0.0580 (0.0571)	0.766 (0.727)	78.12 (78.33)
[40/157]	0.0964 (0.0938)	0.0590 (0.0573)	0.600 (0.710)	84.38 (79.04)
[50/157]	0.0938 (0.0939)	0.0571 (0.0574)	0.668 (0.726)	81.25 (78.80)
[60/157]	0.0963 (0.0941)	0.0597 (0.0577)	0.732 (0.732)	81.25 (78.79)
[70/157]	0.0944 (0.0942)	0.0578 (0.0578)	0.813 (0.736)	71.88 (78.65)
[80/157]	0.1014 (0.0949)	0.0643 (0.0583)	0.685 (0.733)	78.12 (78.43)
[90/157]	0.1015 (0.0957)	0.0640 (0.0589)	0.630 (0.730)	84.38 (78.43)
[100/157]	0.1019 (0.0963)	0.0644 (0.0595)	0.625 (0.736)	87.50 (78.16)
[110/157]	0.1033 (0.0968)	0.0654 (0.0599)	0.925 (0.739)	68.75 (77.79)
[120/157]	0.1038 (0.0973)	0.0654 (0.0602)	0.726 (0.737)	81.25 (77.71)
[130/157]	0.0939 (0.0972)	0.0574 (0.0602)	0.645 (0.732)	81.25 (77.96)
[140/157]	0.0954 (0.0970)	0.0583 (0.0600)	0.796 (0.734)	75.00 (77.90)
[150/157]	0.0941 (0.0969)	0.0578 (0.0600)	0.585 (0.734)	84.38 (77.77)
[156/157]	0.0786 (0.0967)	0.0540 (0.0599)	0.732 (0.738)	87.50 (77.66)
 * Train Acc 77.660
 * Val Acc 79.300, Total time 0.57
 * Val loss 0.653, Total time 0.00
Epoch:47
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0086 (0.0086)	0.770 (0.770)	71.88 (71.88)
[10/157]	0.0960 (0.0911)	0.0578 (0.0539)	1.016 (0.771)	65.62 (75.00)
[20/157]	0.0970 (0.0931)	0.0594 (0.0559)	1.078 (0.794)	68.75 (74.85)
[30/157]	0.0955 (0.0937)	0.0584 (0.0567)	0.739 (0.773)	75.00 (75.81)
[40/157]	0.1128 (0.0947)	0.0743 (0.0579)	1.205 (0.776)	56.25 (75.69)
[50/157]	0.0979 (0.0964)	0.0608 (0.0594)	0.667 (0.770)	75.00 (75.55)
[60/157]	0.0987 (0.0966)	0.0602 (0.0595)	0.560 (0.765)	81.25 (75.77)
[70/157]	0.0948 (0.0967)	0.0576 (0.0595)	0.739 (0.752)	78.12 (76.45)
[80/157]	0.0976 (0.0967)	0.0605 (0.0595)	0.608 (0.759)	78.12 (76.16)
[90/157]	0.0984 (0.0968)	0.0602 (0.0595)	0.784 (0.755)	68.75 (76.06)
[100/157]	0.0945 (0.0969)	0.0582 (0.0595)	0.777 (0.752)	78.12 (76.30)
[110/157]	0.1002 (0.0969)	0.0622 (0.0596)	0.781 (0.743)	68.75 (76.63)
[120/157]	0.0971 (0.0969)	0.0594 (0.0596)	1.021 (0.743)	65.62 (76.68)
[130/157]	0.0959 (0.0969)	0.0594 (0.0596)	0.911 (0.747)	75.00 (76.67)
[140/157]	0.0975 (0.0969)	0.0602 (0.0596)	0.678 (0.742)	78.12 (76.84)
[150/157]	0.0967 (0.0969)	0.0605 (0.0596)	0.542 (0.736)	87.50 (77.17)
[156/157]	0.0790 (0.0968)	0.0540 (0.0596)	0.583 (0.737)	87.50 (77.16)
 * Train Acc 77.160
 * Val Acc 78.200, Total time 0.59
 * Val loss 0.659, Total time 0.00
Epoch:48
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0421 (0.0421)	0.0087 (0.0087)	0.790 (0.790)	75.00 (75.00)
[10/157]	0.0961 (0.0909)	0.0602 (0.0539)	0.875 (0.711)	78.12 (79.55)
[20/157]	0.0958 (0.0929)	0.0590 (0.0562)	0.892 (0.695)	71.88 (80.80)
[30/157]	0.0928 (0.0936)	0.0571 (0.0569)	0.545 (0.699)	87.50 (79.64)
[40/157]	0.0957 (0.0940)	0.0580 (0.0574)	0.784 (0.705)	75.00 (79.73)
[50/157]	0.0936 (0.0942)	0.0573 (0.0576)	0.614 (0.716)	78.12 (78.74)
[60/157]	0.0935 (0.0943)	0.0568 (0.0576)	0.624 (0.716)	75.00 (78.79)
[70/157]	0.0964 (0.0945)	0.0591 (0.0577)	1.065 (0.736)	75.00 (78.26)
[80/157]	0.0942 (0.0946)	0.0566 (0.0578)	0.841 (0.738)	65.62 (78.05)
[90/157]	0.0926 (0.0947)	0.0571 (0.0578)	0.877 (0.751)	75.00 (77.51)
[100/157]	0.0964 (0.0947)	0.0590 (0.0579)	0.806 (0.749)	78.12 (77.75)
[110/157]	0.1183 (0.0956)	0.0803 (0.0587)	0.776 (0.748)	81.25 (77.76)
[120/157]	0.0947 (0.0956)	0.0582 (0.0588)	0.799 (0.749)	78.12 (77.74)
[130/157]	0.0971 (0.0956)	0.0598 (0.0588)	0.567 (0.753)	84.38 (77.72)
[140/157]	0.0953 (0.0956)	0.0574 (0.0588)	0.879 (0.756)	78.12 (77.62)
[150/157]	0.0986 (0.0956)	0.0595 (0.0588)	0.669 (0.751)	81.25 (77.81)
[156/157]	0.0788 (0.0954)	0.0536 (0.0587)	0.593 (0.749)	75.00 (77.96)
 * Train Acc 77.960
 * Val Acc 79.700, Total time 0.59
 * Val loss 0.650, Total time 0.00
Epoch:49
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0088 (0.0088)	1.091 (1.091)	71.88 (71.88)
[10/157]	0.0988 (0.0937)	0.0598 (0.0553)	0.752 (0.814)	75.00 (77.56)
[20/157]	0.0993 (0.0962)	0.0613 (0.0580)	0.665 (0.758)	81.25 (79.61)
[30/157]	0.0976 (0.0970)	0.0610 (0.0590)	0.559 (0.752)	81.25 (79.03)
[40/157]	0.0995 (0.0975)	0.0602 (0.0593)	0.815 (0.755)	75.00 (78.66)
[50/157]	0.0991 (0.0977)	0.0617 (0.0595)	0.864 (0.746)	68.75 (78.86)
[60/157]	0.0977 (0.0979)	0.0606 (0.0597)	0.584 (0.750)	75.00 (78.64)
[70/157]	0.0999 (0.0980)	0.0609 (0.0600)	0.730 (0.746)	75.00 (78.48)
[80/157]	0.0986 (0.0981)	0.0618 (0.0600)	0.582 (0.735)	90.62 (78.74)
[90/157]	0.0990 (0.0982)	0.0611 (0.0602)	0.889 (0.744)	75.00 (78.37)
[100/157]	0.0970 (0.0982)	0.0597 (0.0602)	0.876 (0.750)	75.00 (77.94)
[110/157]	0.0946 (0.0979)	0.0566 (0.0600)	0.451 (0.740)	87.50 (78.24)
[120/157]	0.0951 (0.0977)	0.0586 (0.0599)	1.033 (0.743)	68.75 (78.12)
[130/157]	0.0954 (0.0975)	0.0592 (0.0598)	0.604 (0.739)	87.50 (78.15)
[140/157]	0.0949 (0.0973)	0.0582 (0.0597)	0.713 (0.743)	71.88 (77.99)
[150/157]	0.0958 (0.0972)	0.0593 (0.0596)	0.680 (0.742)	90.62 (77.98)
[156/157]	0.0798 (0.0970)	0.0536 (0.0595)	0.797 (0.741)	75.00 (78.02)
 * Train Acc 78.020
 * Val Acc 79.700, Total time 0.58
 * Val loss 0.638, Total time 0.00
Epoch:50
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0085 (0.0085)	0.674 (0.674)	87.50 (87.50)
[10/157]	0.0965 (0.0918)	0.0590 (0.0544)	0.514 (0.741)	90.62 (78.69)
[20/157]	0.0938 (0.0934)	0.0563 (0.0563)	0.753 (0.723)	68.75 (78.57)
[30/157]	0.0947 (0.0940)	0.0580 (0.0569)	0.851 (0.723)	84.38 (79.74)
[40/157]	0.0962 (0.0943)	0.0593 (0.0573)	0.490 (0.722)	90.62 (79.19)
[50/157]	0.0965 (0.0947)	0.0582 (0.0575)	0.981 (0.724)	75.00 (79.47)
[60/157]	0.0954 (0.0948)	0.0569 (0.0575)	0.499 (0.733)	90.62 (78.89)
[70/157]	0.0956 (0.0951)	0.0593 (0.0578)	0.903 (0.739)	68.75 (78.21)
[80/157]	0.0976 (0.0954)	0.0598 (0.0581)	0.796 (0.752)	68.75 (77.51)
[90/157]	0.0977 (0.0957)	0.0596 (0.0583)	0.683 (0.748)	84.38 (77.64)
[100/157]	0.0965 (0.0958)	0.0606 (0.0584)	0.638 (0.745)	81.25 (77.69)
[110/157]	0.0978 (0.0960)	0.0606 (0.0586)	0.580 (0.739)	78.12 (77.82)
[120/157]	0.0979 (0.0961)	0.0601 (0.0587)	0.749 (0.748)	81.25 (77.61)
[130/157]	0.0976 (0.0962)	0.0598 (0.0588)	0.825 (0.746)	62.50 (77.65)
[140/157]	0.0963 (0.0963)	0.0588 (0.0589)	0.695 (0.741)	78.12 (77.95)
[150/157]	0.0983 (0.0964)	0.0608 (0.0589)	0.789 (0.741)	75.00 (77.98)
[156/157]	0.0795 (0.0963)	0.0540 (0.0589)	0.949 (0.740)	50.00 (77.92)
 * Train Acc 77.920
 * Val Acc 80.100, Total time 0.58
 * Val loss 0.637, Total time 0.00
Epoch:51
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0431 (0.0431)	0.0087 (0.0087)	0.791 (0.791)	75.00 (75.00)
[10/157]	0.0962 (0.0919)	0.0597 (0.0544)	0.777 (0.830)	78.12 (75.28)
[20/157]	0.0977 (0.0945)	0.0598 (0.0569)	0.682 (0.765)	75.00 (76.34)
[30/157]	0.0985 (0.0956)	0.0598 (0.0578)	0.848 (0.725)	78.12 (78.73)
[40/157]	0.0974 (0.0961)	0.0595 (0.0583)	0.507 (0.730)	81.25 (78.43)
[50/157]	0.0974 (0.0964)	0.0594 (0.0586)	0.750 (0.740)	75.00 (77.45)
[60/157]	0.0965 (0.0966)	0.0597 (0.0587)	0.537 (0.742)	84.38 (77.51)
[70/157]	0.0969 (0.0967)	0.0594 (0.0589)	0.810 (0.754)	78.12 (77.07)
[80/157]	0.0983 (0.0967)	0.0590 (0.0590)	0.789 (0.750)	75.00 (77.55)
[90/157]	0.0939 (0.0968)	0.0575 (0.0589)	0.751 (0.748)	78.12 (77.71)
[100/157]	0.0963 (0.0969)	0.0596 (0.0591)	0.766 (0.747)	78.12 (77.60)
[110/157]	0.0977 (0.0969)	0.0593 (0.0591)	0.650 (0.749)	78.12 (77.48)
[120/157]	0.0991 (0.0970)	0.0603 (0.0592)	0.937 (0.749)	62.50 (77.53)
[130/157]	0.0966 (0.0970)	0.0597 (0.0592)	0.601 (0.743)	81.25 (77.70)
[140/157]	0.0975 (0.0970)	0.0595 (0.0592)	0.875 (0.743)	68.75 (77.48)
[150/157]	0.0959 (0.0970)	0.0601 (0.0593)	0.528 (0.745)	81.25 (77.46)
[156/157]	0.0786 (0.0969)	0.0535 (0.0593)	0.664 (0.743)	75.00 (77.42)
 * Train Acc 77.420
 * Val Acc 79.500, Total time 0.57
 * Val loss 0.648, Total time 0.00
Epoch:52
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0432 (0.0432)	0.0088 (0.0088)	0.696 (0.696)	75.00 (75.00)
[10/157]	0.0955 (0.0919)	0.0592 (0.0545)	0.758 (0.798)	75.00 (75.85)
[20/157]	0.0942 (0.0934)	0.0578 (0.0566)	0.402 (0.753)	93.75 (75.74)
[30/157]	0.1152 (0.0981)	0.0775 (0.0611)	0.658 (0.743)	81.25 (76.71)
[40/157]	0.0940 (0.0974)	0.0572 (0.0605)	0.544 (0.727)	84.38 (77.52)
[50/157]	0.0936 (0.0969)	0.0582 (0.0602)	0.496 (0.709)	84.38 (78.55)
[60/157]	0.0960 (0.0966)	0.0591 (0.0599)	0.778 (0.718)	71.88 (78.23)
[70/157]	0.0959 (0.0964)	0.0580 (0.0597)	1.211 (0.730)	65.62 (77.99)
[80/157]	0.0946 (0.0962)	0.0583 (0.0595)	0.721 (0.723)	75.00 (78.12)
[90/157]	0.0943 (0.0960)	0.0581 (0.0594)	0.687 (0.722)	71.88 (78.06)
[100/157]	0.1195 (0.0969)	0.0797 (0.0601)	0.738 (0.724)	78.12 (78.12)
[110/157]	0.0968 (0.0970)	0.0595 (0.0601)	0.856 (0.721)	75.00 (78.12)
[120/157]	0.0968 (0.0969)	0.0593 (0.0600)	0.723 (0.726)	75.00 (77.82)
[130/157]	0.0959 (0.0968)	0.0592 (0.0599)	0.570 (0.728)	87.50 (77.84)
[140/157]	0.0957 (0.0968)	0.0586 (0.0598)	0.596 (0.725)	81.25 (77.86)
[150/157]	0.0966 (0.0967)	0.0589 (0.0598)	0.818 (0.730)	71.88 (77.59)
[156/157]	0.0781 (0.0966)	0.0532 (0.0597)	0.343 (0.730)	100.00 (77.62)
 * Train Acc 77.620
 * Val Acc 79.900, Total time 0.58
 * Val loss 0.643, Total time 0.00
Epoch:53
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0425 (0.0425)	0.0084 (0.0084)	0.750 (0.750)	71.88 (71.88)
[10/157]	0.0950 (0.0909)	0.0584 (0.0539)	0.831 (0.668)	71.88 (77.84)
[20/157]	0.0951 (0.0932)	0.0585 (0.0563)	0.782 (0.710)	81.25 (77.98)
[30/157]	0.0960 (0.0939)	0.0593 (0.0571)	0.590 (0.739)	84.38 (77.32)
[40/157]	0.0971 (0.0943)	0.0600 (0.0576)	0.706 (0.729)	75.00 (77.52)
[50/157]	0.0937 (0.0945)	0.0583 (0.0579)	0.762 (0.737)	68.75 (77.14)
[60/157]	0.1194 (0.0963)	0.0807 (0.0594)	0.776 (0.733)	78.12 (77.36)
[70/157]	0.0949 (0.0966)	0.0579 (0.0596)	0.918 (0.734)	78.12 (77.33)
[80/157]	0.0961 (0.0966)	0.0592 (0.0596)	0.774 (0.739)	75.00 (77.20)
[90/157]	0.0977 (0.0966)	0.0599 (0.0595)	1.042 (0.734)	68.75 (77.30)
[100/157]	0.0969 (0.0965)	0.0597 (0.0595)	0.915 (0.725)	62.50 (77.48)
[110/157]	0.0974 (0.0965)	0.0598 (0.0594)	0.726 (0.729)	81.25 (77.56)
[120/157]	0.0966 (0.0964)	0.0587 (0.0594)	0.823 (0.728)	78.12 (77.45)
[130/157]	0.0982 (0.0964)	0.0603 (0.0593)	0.524 (0.729)	84.38 (77.53)
[140/157]	0.0980 (0.0963)	0.0601 (0.0593)	0.987 (0.727)	65.62 (77.68)
[150/157]	0.0971 (0.0963)	0.0601 (0.0593)	0.683 (0.726)	81.25 (77.84)
[156/157]	0.0788 (0.0962)	0.0538 (0.0593)	1.362 (0.730)	62.50 (77.68)
 * Train Acc 77.680
 * Val Acc 79.500, Total time 0.59
 * Val loss 0.650, Total time 0.00
Epoch:54
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0085 (0.0085)	0.714 (0.714)	71.88 (71.88)
[10/157]	0.0950 (0.0913)	0.0572 (0.0532)	0.688 (0.770)	78.12 (76.14)
[20/157]	0.0951 (0.0932)	0.0577 (0.0556)	0.594 (0.750)	81.25 (77.08)
[30/157]	0.0956 (0.0938)	0.0600 (0.0564)	0.834 (0.741)	78.12 (77.42)
[40/157]	0.0940 (0.0940)	0.0586 (0.0569)	0.493 (0.735)	81.25 (77.82)
[50/157]	0.0968 (0.0942)	0.0596 (0.0574)	0.692 (0.724)	71.88 (77.51)
[60/157]	0.0937 (0.0944)	0.0571 (0.0575)	0.688 (0.727)	78.12 (77.31)
[70/157]	0.0943 (0.0945)	0.0592 (0.0577)	0.713 (0.737)	78.12 (76.89)
[80/157]	0.1195 (0.0952)	0.0800 (0.0584)	0.566 (0.730)	81.25 (77.35)
[90/157]	0.0986 (0.0961)	0.0604 (0.0592)	0.577 (0.732)	84.38 (77.34)
[100/157]	0.0975 (0.0961)	0.0589 (0.0592)	0.774 (0.727)	75.00 (77.44)
[110/157]	0.0980 (0.0961)	0.0598 (0.0592)	0.668 (0.728)	84.38 (77.53)
[120/157]	0.0951 (0.0961)	0.0590 (0.0592)	0.649 (0.732)	84.38 (77.61)
[130/157]	0.0965 (0.0962)	0.0593 (0.0592)	0.653 (0.728)	78.12 (77.67)
[140/157]	0.0960 (0.0962)	0.0597 (0.0592)	0.671 (0.736)	81.25 (77.50)
[150/157]	0.0973 (0.0962)	0.0601 (0.0593)	0.858 (0.736)	78.12 (77.44)
[156/157]	0.0801 (0.0961)	0.0554 (0.0593)	1.117 (0.738)	75.00 (77.44)
 * Train Acc 77.440
 * Val Acc 79.600, Total time 0.58
 * Val loss 0.641, Total time 0.00
Epoch:55
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0084 (0.0084)	0.859 (0.859)	75.00 (75.00)
[10/157]	0.0970 (0.0914)	0.0594 (0.0542)	0.655 (0.739)	81.25 (79.83)
[20/157]	0.0963 (0.0936)	0.0595 (0.0567)	0.736 (0.747)	81.25 (79.17)
[30/157]	0.0949 (0.0946)	0.0581 (0.0575)	0.881 (0.754)	75.00 (77.72)
[40/157]	0.0981 (0.0952)	0.0594 (0.0580)	0.459 (0.746)	93.75 (78.12)
[50/157]	0.0980 (0.0954)	0.0599 (0.0582)	0.551 (0.734)	84.38 (78.19)
[60/157]	0.0966 (0.0955)	0.0595 (0.0583)	0.670 (0.722)	78.12 (78.28)
[70/157]	0.0970 (0.0956)	0.0594 (0.0584)	0.900 (0.720)	75.00 (78.39)
[80/157]	0.0961 (0.0957)	0.0590 (0.0585)	0.907 (0.730)	71.88 (78.12)
[90/157]	0.0949 (0.0958)	0.0583 (0.0586)	0.650 (0.722)	75.00 (78.43)
[100/157]	0.0979 (0.0959)	0.0603 (0.0586)	0.890 (0.725)	68.75 (78.16)
[110/157]	0.0972 (0.0959)	0.0610 (0.0587)	0.598 (0.728)	90.62 (78.12)
[120/157]	0.0963 (0.0959)	0.0594 (0.0587)	0.730 (0.728)	75.00 (78.12)
[130/157]	0.0996 (0.0960)	0.0613 (0.0587)	0.928 (0.729)	65.62 (78.05)
[140/157]	0.0946 (0.0961)	0.0594 (0.0588)	1.050 (0.732)	68.75 (77.90)
[150/157]	0.0969 (0.0961)	0.0596 (0.0588)	0.613 (0.728)	81.25 (77.96)
[156/157]	0.0791 (0.0960)	0.0537 (0.0588)	0.449 (0.727)	87.50 (78.00)
 * Train Acc 78.000
 * Val Acc 79.000, Total time 0.59
 * Val loss 0.648, Total time 0.00
Epoch:56
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0088 (0.0088)	0.909 (0.909)	68.75 (68.75)
[10/157]	0.0984 (0.0914)	0.0603 (0.0541)	0.678 (0.746)	78.12 (78.69)
[20/157]	0.0968 (0.0937)	0.0595 (0.0566)	0.875 (0.776)	75.00 (76.79)
[30/157]	0.0968 (0.0945)	0.0596 (0.0574)	0.847 (0.778)	75.00 (76.11)
[40/157]	0.0969 (0.0951)	0.0580 (0.0578)	0.604 (0.771)	90.62 (76.98)
[50/157]	0.0981 (0.0954)	0.0603 (0.0581)	0.791 (0.759)	71.88 (77.21)
[60/157]	0.0965 (0.0956)	0.0596 (0.0583)	0.553 (0.755)	87.50 (77.56)
[70/157]	0.0958 (0.0957)	0.0586 (0.0584)	0.750 (0.748)	84.38 (77.86)
[80/157]	0.0985 (0.0958)	0.0610 (0.0585)	0.792 (0.739)	68.75 (77.97)
[90/157]	0.0976 (0.0959)	0.0605 (0.0586)	0.839 (0.734)	78.12 (78.23)
[100/157]	0.0952 (0.0959)	0.0587 (0.0586)	0.680 (0.728)	81.25 (78.50)
[110/157]	0.0965 (0.0960)	0.0591 (0.0586)	0.936 (0.737)	71.88 (78.21)
[120/157]	0.0971 (0.0961)	0.0593 (0.0587)	0.741 (0.737)	78.12 (78.10)
[130/157]	0.0973 (0.0961)	0.0603 (0.0588)	0.618 (0.738)	81.25 (77.98)
[140/157]	0.0976 (0.0962)	0.0585 (0.0588)	0.661 (0.737)	84.38 (78.12)
[150/157]	0.0979 (0.0962)	0.0587 (0.0587)	0.693 (0.740)	78.12 (77.90)
[156/157]	0.0796 (0.0961)	0.0542 (0.0587)	0.640 (0.741)	75.00 (77.70)
 * Train Acc 77.700
 * Val Acc 79.600, Total time 0.60
 * Val loss 0.629, Total time 0.00
Epoch:57
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0428 (0.0428)	0.0090 (0.0090)	0.723 (0.723)	62.50 (62.50)
[10/157]	0.0939 (0.0906)	0.0581 (0.0539)	0.545 (0.678)	84.38 (78.12)
[20/157]	0.1086 (0.0981)	0.0697 (0.0607)	0.710 (0.687)	81.25 (78.57)
[30/157]	0.0933 (0.0976)	0.0565 (0.0602)	0.750 (0.711)	71.88 (78.02)
[40/157]	0.0946 (0.0970)	0.0570 (0.0598)	1.148 (0.743)	62.50 (76.60)
[50/157]	0.0945 (0.0968)	0.0571 (0.0595)	0.635 (0.737)	81.25 (76.90)
[60/157]	0.0966 (0.0966)	0.0594 (0.0593)	0.693 (0.711)	81.25 (78.18)
[70/157]	0.0963 (0.0965)	0.0591 (0.0592)	0.762 (0.722)	71.88 (77.95)
[80/157]	0.0953 (0.0963)	0.0585 (0.0592)	0.474 (0.725)	90.62 (77.97)
[90/157]	0.0944 (0.0963)	0.0576 (0.0591)	0.834 (0.731)	75.00 (77.78)
[100/157]	0.0944 (0.0962)	0.0578 (0.0590)	0.496 (0.719)	87.50 (78.37)
[110/157]	0.0979 (0.0961)	0.0600 (0.0590)	0.608 (0.717)	84.38 (78.46)
[120/157]	0.0969 (0.0961)	0.0591 (0.0590)	0.549 (0.711)	90.62 (78.64)
[130/157]	0.0956 (0.0960)	0.0588 (0.0590)	0.980 (0.714)	71.88 (78.55)
[140/157]	0.0940 (0.0960)	0.0578 (0.0589)	0.922 (0.715)	68.75 (78.59)
[150/157]	0.0942 (0.0960)	0.0574 (0.0589)	0.950 (0.720)	75.00 (78.25)
[156/157]	0.0798 (0.0958)	0.0552 (0.0589)	0.684 (0.724)	75.00 (78.16)
 * Train Acc 78.160
 * Val Acc 80.700, Total time 0.58
 * Val loss 0.631, Total time 0.00
Epoch:58
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0420 (0.0420)	0.0084 (0.0084)	0.466 (0.466)	87.50 (87.50)
[10/157]	0.0959 (0.1032)	0.0589 (0.0646)	0.838 (0.642)	71.88 (80.97)
[20/157]	0.0922 (0.0991)	0.0564 (0.0615)	0.758 (0.631)	81.25 (81.10)
[30/157]	0.0965 (0.0978)	0.0591 (0.0606)	0.453 (0.651)	87.50 (80.65)
[40/157]	0.0950 (0.0972)	0.0586 (0.0600)	1.100 (0.671)	68.75 (80.26)
[50/157]	0.0954 (0.0967)	0.0593 (0.0598)	0.667 (0.680)	78.12 (79.84)
[60/157]	0.0934 (0.0964)	0.0576 (0.0596)	0.776 (0.688)	71.88 (79.35)
[70/157]	0.1050 (0.0973)	0.0656 (0.0602)	0.939 (0.700)	65.62 (78.83)
[80/157]	0.1019 (0.0979)	0.0636 (0.0608)	0.629 (0.698)	71.88 (78.43)
[90/157]	0.1031 (0.0984)	0.0648 (0.0611)	0.491 (0.702)	84.38 (78.16)
[100/157]	0.1030 (0.0989)	0.0651 (0.0615)	0.820 (0.713)	65.62 (77.48)
[110/157]	0.1029 (0.0992)	0.0653 (0.0617)	0.907 (0.711)	78.12 (77.59)
[120/157]	0.1036 (0.0995)	0.0644 (0.0620)	0.974 (0.721)	75.00 (77.35)
[130/157]	0.1029 (0.0997)	0.0643 (0.0622)	0.953 (0.725)	78.12 (77.22)
[140/157]	0.1029 (0.0999)	0.0655 (0.0623)	0.658 (0.723)	81.25 (77.46)
[150/157]	0.1017 (0.1001)	0.0639 (0.0624)	1.160 (0.721)	68.75 (77.69)
[156/157]	0.0774 (0.0998)	0.0525 (0.0623)	1.301 (0.731)	75.00 (77.28)
 * Train Acc 77.280
 * Val Acc 80.100, Total time 0.57
 * Val loss 0.644, Total time 0.00
Epoch:59
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0416 (0.0416)	0.0084 (0.0084)	0.857 (0.857)	68.75 (68.75)
[10/157]	0.0944 (0.0913)	0.0578 (0.0541)	0.813 (0.707)	75.00 (78.69)
[20/157]	0.0957 (0.0930)	0.0589 (0.0562)	0.752 (0.736)	81.25 (78.72)
[30/157]	0.0947 (0.0937)	0.0590 (0.0568)	0.496 (0.743)	90.62 (78.73)
[40/157]	0.0960 (0.0940)	0.0596 (0.0572)	0.735 (0.741)	84.38 (78.12)
[50/157]	0.0937 (0.0942)	0.0574 (0.0575)	0.455 (0.733)	87.50 (78.19)
[60/157]	0.0949 (0.0943)	0.0589 (0.0576)	0.552 (0.732)	84.38 (78.28)
[70/157]	0.0958 (0.0945)	0.0586 (0.0577)	0.705 (0.737)	78.12 (78.12)
[80/157]	0.0944 (0.0956)	0.0586 (0.0586)	0.363 (0.736)	93.75 (78.20)
[90/157]	0.0974 (0.0957)	0.0605 (0.0587)	0.564 (0.730)	75.00 (78.37)
[100/157]	0.0986 (0.0957)	0.0608 (0.0588)	0.803 (0.732)	78.12 (78.25)
[110/157]	0.0968 (0.0958)	0.0608 (0.0589)	0.618 (0.732)	75.00 (78.10)
[120/157]	0.0964 (0.0959)	0.0594 (0.0589)	0.586 (0.725)	78.12 (78.43)
[130/157]	0.0978 (0.0959)	0.0598 (0.0589)	0.755 (0.723)	81.25 (78.65)
[140/157]	0.0977 (0.0960)	0.0608 (0.0590)	0.700 (0.727)	71.88 (78.35)
[150/157]	0.0958 (0.0960)	0.0584 (0.0590)	1.013 (0.730)	65.62 (78.15)
[156/157]	0.0788 (0.0959)	0.0527 (0.0590)	0.901 (0.726)	75.00 (78.30)
 * Train Acc 78.300
 * Val Acc 79.800, Total time 0.59
 * Val loss 0.639, Total time 0.00
Epoch:60
LR: 2.5e-05
LR: 0.0005
LR: 0.00025
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0087 (0.0087)	0.575 (0.575)	81.25 (81.25)
[10/157]	0.0950 (0.0910)	0.0585 (0.0538)	0.643 (0.643)	78.12 (80.11)
[20/157]	0.0952 (0.0934)	0.0589 (0.0565)	0.975 (0.698)	68.75 (78.72)
[30/157]	0.0959 (0.0944)	0.0591 (0.0575)	0.826 (0.738)	78.12 (77.62)
[40/157]	0.0968 (0.0949)	0.0599 (0.0581)	0.507 (0.722)	87.50 (78.51)
[50/157]	0.0966 (0.0951)	0.0594 (0.0584)	0.610 (0.721)	87.50 (78.49)
[60/157]	0.0971 (0.0953)	0.0603 (0.0586)	1.020 (0.730)	68.75 (77.92)
[70/157]	0.0959 (0.0955)	0.0596 (0.0587)	0.812 (0.725)	78.12 (78.26)
[80/157]	0.0972 (0.0956)	0.0603 (0.0588)	0.746 (0.723)	84.38 (78.51)
[90/157]	0.0962 (0.0957)	0.0596 (0.0589)	0.875 (0.717)	71.88 (78.74)
[100/157]	0.0969 (0.0958)	0.0605 (0.0590)	1.008 (0.719)	75.00 (78.93)
[110/157]	0.0959 (0.0959)	0.0589 (0.0590)	0.648 (0.709)	78.12 (79.14)
[120/157]	0.0980 (0.0959)	0.0603 (0.0591)	0.578 (0.712)	84.38 (79.11)
[130/157]	0.0966 (0.0960)	0.0603 (0.0591)	0.438 (0.717)	93.75 (78.84)
[140/157]	0.0952 (0.0960)	0.0589 (0.0591)	0.982 (0.722)	81.25 (78.61)
[150/157]	0.0947 (0.0960)	0.0590 (0.0591)	0.607 (0.716)	87.50 (78.64)
[156/157]	0.0796 (0.0959)	0.0541 (0.0591)	0.742 (0.717)	62.50 (78.66)
 * Train Acc 78.660
 * Val Acc 80.300, Total time 0.58
 * Val loss 0.636, Total time 0.00
Epoch:61
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0423 (0.0423)	0.0086 (0.0086)	0.622 (0.622)	84.38 (84.38)
[10/157]	0.0965 (0.0913)	0.0594 (0.0544)	0.712 (0.713)	75.00 (77.27)
[20/157]	0.0968 (0.0977)	0.0576 (0.0602)	0.852 (0.716)	78.12 (77.98)
[30/157]	0.0997 (0.0981)	0.0613 (0.0603)	0.630 (0.720)	84.38 (77.92)
[40/157]	0.0993 (0.0984)	0.0605 (0.0603)	0.757 (0.719)	68.75 (77.90)
[50/157]	0.0983 (0.0985)	0.0604 (0.0603)	0.766 (0.716)	71.88 (78.00)
[60/157]	0.0992 (0.0986)	0.0612 (0.0605)	0.449 (0.708)	87.50 (77.97)
[70/157]	0.0991 (0.0987)	0.0609 (0.0606)	0.616 (0.705)	75.00 (78.17)
[80/157]	0.0992 (0.0988)	0.0610 (0.0606)	0.768 (0.707)	75.00 (78.24)
[90/157]	0.0988 (0.0988)	0.0605 (0.0606)	0.842 (0.716)	78.12 (78.09)
[100/157]	0.0995 (0.0989)	0.0607 (0.0606)	0.613 (0.708)	87.50 (78.50)
[110/157]	0.0941 (0.0988)	0.0572 (0.0605)	0.522 (0.707)	90.62 (78.72)
[120/157]	0.0944 (0.0985)	0.0582 (0.0603)	0.898 (0.720)	56.25 (77.97)
[130/157]	0.0981 (0.0982)	0.0591 (0.0602)	0.648 (0.715)	87.50 (78.20)
[140/157]	0.0968 (0.0980)	0.0605 (0.0601)	0.574 (0.713)	84.38 (78.28)
[150/157]	0.0961 (0.0978)	0.0580 (0.0599)	0.606 (0.717)	81.25 (78.15)
[156/157]	0.0779 (0.0976)	0.0523 (0.0598)	0.892 (0.719)	75.00 (78.16)
 * Train Acc 78.160
 * Val Acc 80.400, Total time 0.58
 * Val loss 0.636, Total time 0.00
Epoch:62
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0087 (0.0087)	0.669 (0.669)	78.12 (78.12)
[10/157]	0.0961 (0.0919)	0.0585 (0.0541)	0.569 (0.656)	81.25 (80.40)
[20/157]	0.0937 (0.0934)	0.0572 (0.0562)	0.609 (0.702)	81.25 (79.91)
[30/157]	0.0961 (0.0939)	0.0586 (0.0570)	0.673 (0.710)	81.25 (79.03)
[40/157]	0.0940 (0.0942)	0.0573 (0.0573)	0.746 (0.724)	75.00 (77.97)
[50/157]	0.0953 (0.0944)	0.0588 (0.0576)	0.692 (0.705)	78.12 (78.74)
[60/157]	0.0946 (0.0945)	0.0581 (0.0578)	0.827 (0.717)	75.00 (78.69)
[70/157]	0.1171 (0.0959)	0.0786 (0.0590)	0.834 (0.725)	81.25 (78.30)
[80/157]	0.0942 (0.0961)	0.0577 (0.0591)	0.584 (0.728)	84.38 (78.59)
[90/157]	0.0945 (0.0960)	0.0579 (0.0590)	0.579 (0.717)	87.50 (78.98)
[100/157]	0.0952 (0.0959)	0.0579 (0.0590)	0.947 (0.717)	71.88 (79.02)
[110/157]	0.0942 (0.0959)	0.0575 (0.0589)	0.881 (0.716)	71.88 (79.08)
[120/157]	0.0961 (0.0958)	0.0596 (0.0589)	0.762 (0.715)	75.00 (78.93)
[130/157]	0.0976 (0.0964)	0.0597 (0.0594)	0.490 (0.712)	87.50 (79.03)
[140/157]	0.0980 (0.0964)	0.0592 (0.0593)	0.720 (0.714)	75.00 (79.01)
[150/157]	0.0963 (0.0964)	0.0591 (0.0592)	0.777 (0.713)	78.12 (78.93)
[156/157]	0.0803 (0.0962)	0.0549 (0.0592)	1.030 (0.716)	50.00 (78.82)
 * Train Acc 78.820
 * Val Acc 80.100, Total time 0.58
 * Val loss 0.637, Total time 0.00
Epoch:63
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0427 (0.0427)	0.0087 (0.0087)	1.203 (1.203)	65.62 (65.62)
[10/157]	0.0975 (0.0915)	0.0592 (0.0535)	0.618 (0.736)	81.25 (77.84)
[20/157]	0.0966 (0.0936)	0.0590 (0.0561)	0.817 (0.693)	75.00 (79.76)
[30/157]	0.0969 (0.0943)	0.0588 (0.0571)	0.544 (0.704)	78.12 (77.92)
[40/157]	0.0945 (0.0948)	0.0584 (0.0574)	0.715 (0.690)	75.00 (78.35)
[50/157]	0.0968 (0.0950)	0.0605 (0.0578)	0.681 (0.685)	78.12 (78.74)
[60/157]	0.0943 (0.0952)	0.0586 (0.0581)	0.896 (0.691)	65.62 (78.43)
[70/157]	0.0964 (0.0953)	0.0595 (0.0582)	0.593 (0.698)	84.38 (78.30)
[80/157]	0.0966 (0.0953)	0.0594 (0.0583)	0.710 (0.703)	78.12 (78.36)
[90/157]	0.0963 (0.0953)	0.0591 (0.0584)	0.578 (0.699)	84.38 (78.47)
[100/157]	0.0942 (0.0954)	0.0586 (0.0585)	0.585 (0.707)	78.12 (78.25)
[110/157]	0.0957 (0.0954)	0.0589 (0.0585)	0.792 (0.710)	78.12 (78.27)
[120/157]	0.0952 (0.0954)	0.0598 (0.0586)	0.768 (0.715)	87.50 (78.23)
[130/157]	0.0960 (0.0954)	0.0591 (0.0586)	0.849 (0.715)	71.88 (78.27)
[140/157]	0.0971 (0.0954)	0.0599 (0.0586)	0.572 (0.710)	90.62 (78.41)
[150/157]	0.0944 (0.0954)	0.0579 (0.0586)	0.958 (0.712)	68.75 (78.33)
[156/157]	0.0821 (0.0954)	0.0539 (0.0586)	1.437 (0.716)	62.50 (78.20)
 * Train Acc 78.200
 * Val Acc 80.500, Total time 0.58
 * Val loss 0.631, Total time 0.00
Epoch:64
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0447 (0.0447)	0.0087 (0.0087)	0.414 (0.414)	93.75 (93.75)
[10/157]	0.0946 (0.0937)	0.0582 (0.0564)	0.861 (0.627)	75.00 (81.82)
[20/157]	0.1053 (0.0998)	0.0663 (0.0623)	0.973 (0.677)	71.88 (79.91)
[30/157]	0.0967 (0.0995)	0.0596 (0.0619)	0.827 (0.723)	75.00 (77.92)
[40/157]	0.0948 (0.0983)	0.0586 (0.0611)	0.748 (0.734)	81.25 (78.20)
[50/157]	0.0934 (0.0976)	0.0573 (0.0606)	0.869 (0.744)	78.12 (77.82)
[60/157]	0.0938 (0.0972)	0.0564 (0.0602)	0.726 (0.735)	84.38 (78.07)
[70/157]	0.0952 (0.0969)	0.0587 (0.0600)	0.827 (0.732)	65.62 (78.04)
[80/157]	0.0988 (0.0976)	0.0611 (0.0606)	0.629 (0.723)	84.38 (78.28)
[90/157]	0.0976 (0.0977)	0.0615 (0.0605)	0.626 (0.730)	78.12 (77.85)
[100/157]	0.0986 (0.0978)	0.0602 (0.0605)	0.664 (0.731)	75.00 (77.82)
[110/157]	0.0988 (0.0979)	0.0596 (0.0605)	0.722 (0.734)	71.88 (77.62)
[120/157]	0.0991 (0.0979)	0.0622 (0.0605)	0.817 (0.735)	78.12 (77.69)
[130/157]	0.0990 (0.0980)	0.0603 (0.0605)	0.830 (0.730)	75.00 (77.89)
[140/157]	0.0974 (0.0980)	0.0589 (0.0605)	0.510 (0.730)	84.38 (77.79)
[150/157]	0.0992 (0.0980)	0.0605 (0.0605)	0.693 (0.727)	71.88 (77.98)
[156/157]	0.0816 (0.0980)	0.0543 (0.0604)	1.077 (0.728)	50.00 (77.86)
 * Train Acc 77.860
 * Val Acc 79.900, Total time 0.60
 * Val loss 0.635, Total time 0.00
Epoch:65
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0088 (0.0088)	1.174 (1.174)	56.25 (56.25)
[10/157]	0.0995 (0.0929)	0.0622 (0.0553)	0.480 (0.745)	90.62 (79.26)
[20/157]	0.0972 (0.0954)	0.0608 (0.0580)	1.172 (0.735)	62.50 (78.87)
[30/157]	0.0975 (0.0964)	0.0597 (0.0589)	0.607 (0.719)	84.38 (78.93)
[40/157]	0.0984 (0.0968)	0.0609 (0.0594)	0.940 (0.710)	71.88 (79.12)
[50/157]	0.0997 (0.0971)	0.0608 (0.0596)	0.810 (0.702)	71.88 (78.92)
[60/157]	0.1000 (0.0973)	0.0623 (0.0597)	0.800 (0.716)	78.12 (78.28)
[70/157]	0.0986 (0.0975)	0.0611 (0.0598)	0.616 (0.709)	78.12 (78.39)
[80/157]	0.0988 (0.0976)	0.0607 (0.0600)	0.649 (0.705)	78.12 (78.59)
[90/157]	0.0966 (0.0978)	0.0598 (0.0601)	0.779 (0.708)	78.12 (78.57)
[100/157]	0.0993 (0.0979)	0.0606 (0.0602)	0.906 (0.718)	75.00 (78.43)
[110/157]	0.0987 (0.0979)	0.0600 (0.0602)	0.689 (0.718)	84.38 (78.32)
[120/157]	0.0982 (0.0979)	0.0600 (0.0602)	0.566 (0.713)	81.25 (78.41)
[130/157]	0.0990 (0.0980)	0.0616 (0.0602)	0.560 (0.713)	90.62 (78.53)
[140/157]	0.0981 (0.0980)	0.0604 (0.0602)	0.685 (0.718)	84.38 (78.39)
[150/157]	0.0961 (0.0978)	0.0596 (0.0601)	0.873 (0.718)	71.88 (78.21)
[156/157]	0.0782 (0.0976)	0.0525 (0.0600)	0.609 (0.716)	87.50 (78.30)
 * Train Acc 78.300
 * Val Acc 77.900, Total time 0.58
 * Val loss 0.655, Total time 0.00
Epoch:66
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0426 (0.0426)	0.0083 (0.0083)	0.655 (0.655)	75.00 (75.00)
[10/157]	0.0946 (0.0918)	0.0578 (0.0537)	0.850 (0.761)	65.62 (77.27)
[20/157]	0.0952 (0.0933)	0.0576 (0.0558)	0.549 (0.734)	84.38 (77.38)
[30/157]	0.0958 (0.0938)	0.0593 (0.0567)	0.696 (0.731)	81.25 (77.02)
[40/157]	0.0953 (0.0942)	0.0582 (0.0568)	0.575 (0.747)	84.38 (76.91)
[50/157]	0.0959 (0.0943)	0.0586 (0.0571)	0.429 (0.740)	90.62 (77.08)
[60/157]	0.0938 (0.0944)	0.0566 (0.0573)	0.645 (0.741)	78.12 (77.05)
[70/157]	0.0972 (0.0946)	0.0599 (0.0574)	0.597 (0.724)	87.50 (77.73)
[80/157]	0.0929 (0.0946)	0.0576 (0.0576)	0.718 (0.719)	78.12 (78.05)
[90/157]	0.0967 (0.0946)	0.0593 (0.0577)	0.753 (0.721)	75.00 (78.16)
[100/157]	0.0945 (0.0947)	0.0571 (0.0577)	0.447 (0.722)	90.62 (78.16)
[110/157]	0.0942 (0.0947)	0.0567 (0.0577)	0.993 (0.723)	62.50 (78.01)
[120/157]	0.0963 (0.0948)	0.0578 (0.0577)	0.615 (0.721)	87.50 (78.23)
[130/157]	0.0949 (0.0948)	0.0577 (0.0577)	0.500 (0.718)	87.50 (78.24)
[140/157]	0.0941 (0.0949)	0.0578 (0.0577)	0.477 (0.715)	90.62 (78.55)
[150/157]	0.1048 (0.0953)	0.0666 (0.0581)	0.699 (0.711)	87.50 (78.75)
[156/157]	0.0812 (0.0954)	0.0536 (0.0582)	1.147 (0.711)	75.00 (78.78)
 * Train Acc 78.780
 * Val Acc 79.600, Total time 0.58
 * Val loss 0.632, Total time 0.00
Epoch:67
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0435 (0.0435)	0.0089 (0.0089)	0.545 (0.545)	90.62 (90.62)
[10/157]	0.0969 (0.0917)	0.0596 (0.0542)	0.726 (0.723)	78.12 (78.98)
[20/157]	0.0937 (0.0934)	0.0572 (0.0562)	0.808 (0.687)	75.00 (80.51)
[30/157]	0.0957 (0.0940)	0.0578 (0.0567)	0.743 (0.696)	75.00 (79.74)
[40/157]	0.0928 (0.0942)	0.0580 (0.0570)	0.499 (0.690)	84.38 (79.19)
[50/157]	0.0962 (0.0943)	0.0599 (0.0573)	0.886 (0.687)	75.00 (79.41)
[60/157]	0.0950 (0.0945)	0.0580 (0.0575)	0.763 (0.718)	75.00 (78.02)
[70/157]	0.0949 (0.0946)	0.0583 (0.0576)	0.671 (0.724)	75.00 (77.68)
[80/157]	0.1199 (0.0952)	0.0807 (0.0583)	1.061 (0.731)	75.00 (77.58)
[90/157]	0.0951 (0.0960)	0.0591 (0.0591)	0.590 (0.718)	87.50 (78.26)
[100/157]	0.0965 (0.0959)	0.0599 (0.0591)	0.938 (0.715)	62.50 (78.37)
[110/157]	0.0970 (0.0959)	0.0599 (0.0591)	0.597 (0.709)	84.38 (78.58)
[120/157]	0.0941 (0.0958)	0.0581 (0.0591)	0.575 (0.702)	84.38 (78.82)
[130/157]	0.0953 (0.0958)	0.0591 (0.0591)	0.595 (0.705)	81.25 (78.82)
[140/157]	0.0970 (0.0958)	0.0599 (0.0591)	1.102 (0.706)	62.50 (78.79)
[150/157]	0.0972 (0.0958)	0.0599 (0.0591)	0.599 (0.708)	81.25 (78.64)
[156/157]	0.0793 (0.0957)	0.0532 (0.0590)	1.129 (0.714)	62.50 (78.32)
 * Train Acc 78.320
 * Val Acc 79.900, Total time 0.59
 * Val loss 0.625, Total time 0.00
Epoch:68
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0430 (0.0430)	0.0086 (0.0086)	0.732 (0.732)	71.88 (71.88)
[10/157]	0.0966 (0.0915)	0.0596 (0.0540)	0.580 (0.733)	84.38 (77.84)
[20/157]	0.0962 (0.0934)	0.0585 (0.0561)	0.664 (0.709)	78.12 (78.42)
[30/157]	0.0950 (0.0940)	0.0583 (0.0568)	0.508 (0.732)	87.50 (78.02)
[40/157]	0.0970 (0.0943)	0.0588 (0.0572)	0.706 (0.726)	81.25 (78.12)
[50/157]	0.0938 (0.0944)	0.0581 (0.0575)	0.565 (0.737)	84.38 (77.63)
[60/157]	0.0961 (0.0945)	0.0592 (0.0577)	0.421 (0.732)	90.62 (77.82)
[70/157]	0.0967 (0.0958)	0.0593 (0.0589)	0.635 (0.731)	81.25 (77.95)
[80/157]	0.0971 (0.0959)	0.0597 (0.0589)	0.926 (0.740)	78.12 (77.58)
[90/157]	0.0967 (0.0959)	0.0595 (0.0590)	0.615 (0.729)	78.12 (77.95)
[100/157]	0.0955 (0.0959)	0.0582 (0.0590)	0.843 (0.730)	68.75 (77.88)
[110/157]	0.0956 (0.0960)	0.0595 (0.0590)	0.829 (0.722)	75.00 (78.29)
[120/157]	0.0957 (0.0960)	0.0589 (0.0591)	0.631 (0.713)	87.50 (78.62)
[130/157]	0.0968 (0.0959)	0.0600 (0.0590)	0.977 (0.711)	71.88 (78.60)
[140/157]	0.0944 (0.0958)	0.0582 (0.0589)	0.595 (0.712)	84.38 (78.48)
[150/157]	0.0951 (0.0958)	0.0586 (0.0589)	0.530 (0.713)	81.25 (78.41)
[156/157]	0.0777 (0.0957)	0.0530 (0.0589)	0.658 (0.713)	75.00 (78.50)
 * Train Acc 78.500
 * Val Acc 80.200, Total time 0.60
 * Val loss 0.628, Total time 0.00
Epoch:69
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0429 (0.0429)	0.0089 (0.0089)	0.446 (0.446)	87.50 (87.50)
[10/157]	0.0970 (0.0906)	0.0597 (0.0541)	0.654 (0.773)	75.00 (75.85)
[20/157]	0.1113 (0.0959)	0.0726 (0.0590)	0.628 (0.786)	84.38 (76.04)
[30/157]	0.0977 (0.0974)	0.0605 (0.0603)	0.486 (0.759)	93.75 (77.32)
[40/157]	0.0982 (0.0975)	0.0613 (0.0604)	0.744 (0.756)	75.00 (76.83)
[50/157]	0.0983 (0.0975)	0.0607 (0.0604)	0.724 (0.731)	78.12 (77.94)
[60/157]	0.0985 (0.0976)	0.0607 (0.0604)	0.386 (0.720)	90.62 (78.23)
[70/157]	0.0972 (0.0976)	0.0599 (0.0604)	0.718 (0.724)	84.38 (78.35)
[80/157]	0.0980 (0.0977)	0.0603 (0.0603)	0.691 (0.724)	81.25 (78.28)
[90/157]	0.0975 (0.0977)	0.0597 (0.0603)	0.975 (0.721)	65.62 (78.23)
[100/157]	0.0970 (0.0976)	0.0602 (0.0603)	0.758 (0.723)	65.62 (78.06)
[110/157]	0.0989 (0.0976)	0.0618 (0.0604)	0.830 (0.722)	78.12 (78.15)
[120/157]	0.0977 (0.0976)	0.0609 (0.0603)	0.744 (0.720)	78.12 (78.31)
[130/157]	0.0986 (0.0976)	0.0605 (0.0603)	0.599 (0.718)	81.25 (78.24)
[140/157]	0.0973 (0.0976)	0.0604 (0.0603)	0.910 (0.720)	71.88 (78.15)
[150/157]	0.0980 (0.0976)	0.0602 (0.0603)	0.903 (0.722)	65.62 (78.04)
[156/157]	0.0806 (0.0975)	0.0547 (0.0602)	1.441 (0.722)	62.50 (78.00)
 * Train Acc 78.000
 * Val Acc 80.100, Total time 0.60
 * Val loss 0.629, Total time 0.00
Epoch:70
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0409 (0.0409)	0.0084 (0.0084)	0.725 (0.725)	78.12 (78.12)
[10/157]	0.0996 (0.0927)	0.0603 (0.0550)	0.725 (0.752)	78.12 (77.27)
[20/157]	0.0970 (0.0952)	0.0599 (0.0570)	1.187 (0.759)	62.50 (78.57)
[30/157]	0.1007 (0.0961)	0.0618 (0.0582)	0.760 (0.741)	81.25 (77.82)
[40/157]	0.1000 (0.0966)	0.0606 (0.0587)	0.504 (0.733)	90.62 (78.12)
[50/157]	0.1002 (0.0968)	0.0607 (0.0590)	0.684 (0.730)	81.25 (78.31)
[60/157]	0.0990 (0.0970)	0.0597 (0.0591)	0.821 (0.723)	68.75 (78.59)
[70/157]	0.0991 (0.0971)	0.0599 (0.0592)	0.798 (0.727)	78.12 (78.21)
[80/157]	0.0987 (0.0972)	0.0611 (0.0592)	0.916 (0.719)	65.62 (78.43)
[90/157]	0.0990 (0.0973)	0.0610 (0.0593)	0.759 (0.726)	81.25 (78.09)
[100/157]	0.0989 (0.0974)	0.0603 (0.0594)	0.736 (0.716)	68.75 (78.53)
[110/157]	0.0973 (0.0974)	0.0597 (0.0594)	0.517 (0.713)	84.38 (78.58)
[120/157]	0.0958 (0.0974)	0.0598 (0.0595)	0.549 (0.713)	90.62 (78.41)
[130/157]	0.0955 (0.0974)	0.0588 (0.0596)	0.648 (0.717)	78.12 (78.15)
[140/157]	0.0991 (0.0975)	0.0606 (0.0597)	0.732 (0.713)	81.25 (78.30)
[150/157]	0.0979 (0.0975)	0.0602 (0.0597)	0.511 (0.715)	87.50 (78.29)
[156/157]	0.0799 (0.0974)	0.0534 (0.0596)	0.697 (0.712)	75.00 (78.42)
 * Train Acc 78.420
 * Val Acc 80.400, Total time 0.60
 * Val loss 0.630, Total time 0.00
Epoch:71
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0437 (0.0437)	0.0090 (0.0090)	0.506 (0.506)	81.25 (81.25)
[10/157]	0.0999 (0.0925)	0.0610 (0.0548)	1.007 (0.666)	62.50 (80.11)
[20/157]	0.0989 (0.0950)	0.0610 (0.0572)	0.995 (0.694)	71.88 (79.32)
[30/157]	0.0967 (0.0957)	0.0600 (0.0581)	1.160 (0.702)	62.50 (78.93)
[40/157]	0.0983 (0.0961)	0.0601 (0.0586)	0.758 (0.706)	68.75 (78.28)
[50/157]	0.0965 (0.0964)	0.0610 (0.0590)	0.853 (0.704)	68.75 (78.31)
[60/157]	0.0986 (0.0966)	0.0607 (0.0592)	0.992 (0.708)	68.75 (78.18)
[70/157]	0.0960 (0.0967)	0.0593 (0.0593)	0.514 (0.705)	84.38 (78.12)
[80/157]	0.0983 (0.0968)	0.0613 (0.0595)	0.656 (0.711)	84.38 (78.24)
[90/157]	0.0971 (0.0969)	0.0597 (0.0596)	0.853 (0.724)	75.00 (78.06)
[100/157]	0.0977 (0.0969)	0.0601 (0.0597)	0.430 (0.710)	87.50 (78.71)
[110/157]	0.0964 (0.0969)	0.0598 (0.0597)	0.585 (0.712)	78.12 (78.52)
[120/157]	0.0975 (0.0970)	0.0604 (0.0598)	0.548 (0.704)	81.25 (78.95)
[130/157]	0.0966 (0.0970)	0.0600 (0.0598)	0.547 (0.704)	87.50 (78.96)
[140/157]	0.0985 (0.0971)	0.0614 (0.0599)	0.854 (0.716)	75.00 (78.57)
[150/157]	0.0988 (0.0971)	0.0616 (0.0599)	0.680 (0.715)	81.25 (78.64)
[156/157]	0.0813 (0.0970)	0.0559 (0.0598)	0.467 (0.711)	87.50 (78.74)
 * Train Acc 78.740
 * Val Acc 81.000, Total time 0.60
 * Val loss 0.629, Total time 0.00
Epoch:72
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0441 (0.0441)	0.0087 (0.0087)	0.533 (0.533)	81.25 (81.25)
[10/157]	0.0980 (0.0920)	0.0612 (0.0548)	0.621 (0.659)	81.25 (79.83)
[20/157]	0.0981 (0.0946)	0.0604 (0.0575)	0.574 (0.640)	87.50 (81.40)
[30/157]	0.0973 (0.0955)	0.0599 (0.0583)	0.635 (0.681)	78.12 (79.94)
[40/157]	0.0972 (0.0960)	0.0598 (0.0588)	0.560 (0.706)	81.25 (78.35)
[50/157]	0.0977 (0.0964)	0.0608 (0.0591)	0.700 (0.710)	78.12 (77.63)
[60/157]	0.0967 (0.0967)	0.0604 (0.0593)	0.618 (0.710)	84.38 (77.61)
[70/157]	0.0958 (0.0968)	0.0589 (0.0594)	0.583 (0.699)	84.38 (78.12)
[80/157]	0.0992 (0.0969)	0.0609 (0.0595)	0.694 (0.708)	84.38 (77.97)
[90/157]	0.0977 (0.0970)	0.0595 (0.0596)	0.637 (0.705)	81.25 (78.09)
[100/157]	0.0981 (0.0971)	0.0611 (0.0597)	0.568 (0.707)	78.12 (77.91)
[110/157]	0.0972 (0.0971)	0.0597 (0.0596)	0.625 (0.711)	84.38 (77.93)
[120/157]	0.0975 (0.0971)	0.0609 (0.0597)	0.775 (0.708)	81.25 (78.02)
[130/157]	0.0967 (0.0972)	0.0603 (0.0598)	0.826 (0.710)	68.75 (77.98)
[140/157]	0.0990 (0.0972)	0.0612 (0.0598)	0.617 (0.712)	81.25 (78.10)
[150/157]	0.0986 (0.0972)	0.0602 (0.0598)	0.665 (0.716)	84.38 (78.00)
[156/157]	0.0841 (0.0971)	0.0563 (0.0597)	0.268 (0.712)	100.00 (78.08)
 * Train Acc 78.080
 * Val Acc 81.000, Total time 0.59
 * Val loss 0.627, Total time 0.00
Epoch:73
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0433 (0.0433)	0.0082 (0.0082)	0.852 (0.852)	68.75 (68.75)
[10/157]	0.0985 (0.0924)	0.0615 (0.0547)	0.837 (0.712)	65.62 (78.98)
[20/157]	0.0980 (0.0949)	0.0611 (0.0574)	0.676 (0.680)	84.38 (79.61)
[30/157]	0.0988 (0.0959)	0.0614 (0.0583)	1.101 (0.721)	62.50 (77.12)
[40/157]	0.0980 (0.0964)	0.0595 (0.0588)	0.560 (0.693)	87.50 (78.43)
[50/157]	0.0971 (0.0966)	0.0585 (0.0590)	0.716 (0.702)	81.25 (78.19)
[60/157]	0.0969 (0.0968)	0.0605 (0.0592)	0.733 (0.687)	84.38 (78.84)
[70/157]	0.0968 (0.0969)	0.0597 (0.0593)	0.615 (0.693)	84.38 (78.48)
[80/157]	0.0969 (0.0969)	0.0606 (0.0594)	0.677 (0.690)	75.00 (78.51)
[90/157]	0.0991 (0.0971)	0.0608 (0.0595)	0.888 (0.692)	65.62 (78.23)
[100/157]	0.0974 (0.0971)	0.0608 (0.0596)	0.626 (0.696)	84.38 (78.19)
[110/157]	0.0982 (0.0972)	0.0604 (0.0597)	0.865 (0.700)	78.12 (78.41)
[120/157]	0.0988 (0.0973)	0.0605 (0.0598)	0.968 (0.705)	68.75 (78.20)
[130/157]	0.0964 (0.0973)	0.0594 (0.0598)	0.650 (0.708)	81.25 (77.93)
[140/157]	0.0969 (0.0973)	0.0597 (0.0598)	0.789 (0.706)	75.00 (78.08)
[150/157]	0.0972 (0.0974)	0.0604 (0.0598)	0.714 (0.706)	78.12 (78.10)
[156/157]	0.0835 (0.0973)	0.0568 (0.0598)	0.970 (0.708)	75.00 (77.96)
 * Train Acc 77.960
 * Val Acc 80.300, Total time 0.59
 * Val loss 0.626, Total time 0.00
Epoch:74
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0418 (0.0418)	0.0086 (0.0086)	0.754 (0.754)	75.00 (75.00)
[10/157]	0.0984 (0.0924)	0.0605 (0.0552)	0.618 (0.719)	87.50 (79.26)
[20/157]	0.0978 (0.0953)	0.0601 (0.0575)	0.748 (0.701)	71.88 (79.46)
[30/157]	0.0986 (0.0960)	0.0610 (0.0586)	0.657 (0.685)	87.50 (79.64)
[40/157]	0.0976 (0.0964)	0.0606 (0.0591)	0.743 (0.716)	68.75 (78.51)
[50/157]	0.0981 (0.0967)	0.0609 (0.0594)	0.898 (0.728)	71.88 (78.25)
[60/157]	0.0978 (0.0969)	0.0602 (0.0595)	0.615 (0.732)	81.25 (77.77)
[70/157]	0.0966 (0.0970)	0.0590 (0.0596)	0.662 (0.724)	81.25 (78.12)
[80/157]	0.0968 (0.0970)	0.0602 (0.0597)	0.880 (0.720)	68.75 (78.40)
[90/157]	0.0966 (0.0971)	0.0599 (0.0598)	0.770 (0.727)	81.25 (78.09)
[100/157]	0.0983 (0.0971)	0.0607 (0.0598)	1.130 (0.732)	62.50 (78.09)
[110/157]	0.0966 (0.0971)	0.0602 (0.0599)	0.712 (0.721)	84.38 (78.49)
[120/157]	0.0992 (0.0972)	0.0606 (0.0598)	0.654 (0.714)	78.12 (78.77)
[130/157]	0.0972 (0.0972)	0.0595 (0.0598)	0.634 (0.716)	84.38 (78.72)
[140/157]	0.0970 (0.0973)	0.0602 (0.0599)	0.701 (0.710)	87.50 (79.06)
[150/157]	0.0963 (0.0973)	0.0607 (0.0599)	0.763 (0.712)	65.62 (78.85)
[156/157]	0.0798 (0.0972)	0.0548 (0.0599)	0.507 (0.716)	87.50 (78.74)
 * Train Acc 78.740
 * Val Acc 80.500, Total time 0.57
 * Val loss 0.623, Total time 0.00
Epoch:75
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0411 (0.0411)	0.0084 (0.0084)	0.578 (0.578)	84.38 (84.38)
[10/157]	0.0951 (0.0920)	0.0588 (0.0549)	0.641 (0.700)	75.00 (75.85)
[20/157]	0.0952 (0.0935)	0.0589 (0.0562)	0.509 (0.677)	84.38 (78.72)
[30/157]	0.0964 (0.0941)	0.0598 (0.0571)	0.826 (0.727)	71.88 (77.32)
[40/157]	0.0946 (0.0942)	0.0591 (0.0575)	0.777 (0.720)	75.00 (77.59)
[50/157]	0.0965 (0.0944)	0.0594 (0.0577)	0.753 (0.720)	81.25 (77.57)
[60/157]	0.0933 (0.0962)	0.0577 (0.0595)	0.795 (0.724)	75.00 (77.82)
[70/157]	0.0960 (0.0960)	0.0595 (0.0594)	0.618 (0.725)	84.38 (77.68)
[80/157]	0.0955 (0.0959)	0.0593 (0.0593)	0.715 (0.729)	87.50 (77.66)
[90/157]	0.0969 (0.0958)	0.0594 (0.0592)	0.720 (0.723)	78.12 (77.88)
[100/157]	0.0941 (0.0957)	0.0577 (0.0591)	0.830 (0.722)	75.00 (77.85)
[110/157]	0.0957 (0.0957)	0.0582 (0.0591)	0.856 (0.716)	68.75 (77.90)
[120/157]	0.0970 (0.0956)	0.0591 (0.0590)	0.661 (0.718)	75.00 (77.94)
[130/157]	0.0954 (0.0956)	0.0590 (0.0589)	0.504 (0.718)	90.62 (77.93)
[140/157]	0.0966 (0.0956)	0.0590 (0.0589)	0.744 (0.716)	78.12 (78.08)
[150/157]	0.0943 (0.0955)	0.0580 (0.0589)	0.609 (0.712)	84.38 (78.31)
[156/157]	0.0878 (0.0961)	0.0618 (0.0595)	0.407 (0.710)	87.50 (78.40)
 * Train Acc 78.400
 * Val Acc 80.300, Total time 0.59
 * Val loss 0.623, Total time 0.00
Epoch:76
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0415 (0.0415)	0.0085 (0.0085)	0.667 (0.667)	81.25 (81.25)
[10/157]	0.0945 (0.0992)	0.0589 (0.0614)	0.816 (0.722)	75.00 (76.70)
[20/157]	0.0964 (0.0974)	0.0597 (0.0602)	0.610 (0.701)	81.25 (77.38)
[30/157]	0.0947 (0.0965)	0.0576 (0.0595)	0.594 (0.689)	81.25 (77.92)
[40/157]	0.0961 (0.0961)	0.0583 (0.0592)	0.528 (0.698)	78.12 (77.52)
[50/157]	0.1006 (0.0969)	0.0628 (0.0598)	0.603 (0.697)	81.25 (77.88)
[60/157]	0.1023 (0.0976)	0.0643 (0.0604)	0.776 (0.702)	71.88 (77.97)
[70/157]	0.1001 (0.0981)	0.0629 (0.0608)	0.730 (0.696)	78.12 (78.39)
[80/157]	0.0965 (0.0978)	0.0592 (0.0606)	0.703 (0.699)	84.38 (78.32)
[90/157]	0.0942 (0.0974)	0.0581 (0.0604)	0.777 (0.696)	78.12 (78.57)
[100/157]	0.0969 (0.0972)	0.0596 (0.0602)	0.954 (0.699)	71.88 (78.47)
[110/157]	0.0951 (0.0970)	0.0586 (0.0601)	0.559 (0.697)	87.50 (78.72)
[120/157]	0.0970 (0.0968)	0.0598 (0.0600)	0.725 (0.695)	75.00 (78.82)
[130/157]	0.0941 (0.0967)	0.0577 (0.0599)	0.781 (0.697)	78.12 (78.65)
[140/157]	0.0956 (0.0966)	0.0595 (0.0598)	0.698 (0.703)	84.38 (78.52)
[150/157]	0.0938 (0.0965)	0.0582 (0.0598)	0.550 (0.704)	84.38 (78.44)
[156/157]	0.0778 (0.0963)	0.0531 (0.0596)	0.521 (0.703)	87.50 (78.42)
 * Train Acc 78.420
 * Val Acc 80.000, Total time 0.58
 * Val loss 0.620, Total time 0.00
Epoch:77
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0408 (0.0408)	0.0084 (0.0084)	0.755 (0.755)	81.25 (81.25)
[10/157]	0.0963 (0.0915)	0.0592 (0.0545)	0.716 (0.657)	78.12 (80.68)
[20/157]	0.1188 (0.0976)	0.0804 (0.0606)	0.603 (0.630)	78.12 (81.85)
[30/157]	0.0944 (0.0972)	0.0588 (0.0604)	0.705 (0.670)	84.38 (80.65)
[40/157]	0.0940 (0.0995)	0.0570 (0.0625)	0.920 (0.694)	71.88 (79.57)
[50/157]	0.0941 (0.0987)	0.0573 (0.0616)	0.690 (0.722)	81.25 (78.19)
[60/157]	0.0965 (0.0981)	0.0593 (0.0611)	0.654 (0.718)	84.38 (78.43)
[70/157]	0.0973 (0.0978)	0.0600 (0.0608)	0.465 (0.712)	87.50 (78.30)
[80/157]	0.1191 (0.0986)	0.0801 (0.0615)	0.856 (0.721)	71.88 (77.82)
[90/157]	0.0993 (0.0987)	0.0609 (0.0615)	0.524 (0.715)	84.38 (78.16)
[100/157]	0.0984 (0.0988)	0.0610 (0.0615)	0.727 (0.714)	71.88 (78.16)
[110/157]	0.1012 (0.0990)	0.0620 (0.0616)	0.587 (0.715)	81.25 (78.29)
[120/157]	0.0934 (0.0990)	0.0576 (0.0615)	0.849 (0.712)	71.88 (78.49)
[130/157]	0.0955 (0.0986)	0.0593 (0.0613)	0.887 (0.716)	71.88 (78.29)
[140/157]	0.0963 (0.0984)	0.0584 (0.0611)	0.708 (0.716)	81.25 (78.41)
[150/157]	0.0959 (0.0982)	0.0592 (0.0609)	0.462 (0.712)	90.62 (78.41)
[156/157]	0.0774 (0.0979)	0.0531 (0.0608)	0.624 (0.710)	75.00 (78.50)
 * Train Acc 78.500
 * Val Acc 80.300, Total time 0.58
 * Val loss 0.637, Total time 0.00
Epoch:78
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0412 (0.0412)	0.0085 (0.0085)	0.516 (0.516)	90.62 (90.62)
[10/157]	0.0989 (0.0989)	0.0620 (0.0611)	0.564 (0.654)	84.38 (79.26)
[20/157]	0.0997 (0.0990)	0.0615 (0.0612)	0.695 (0.670)	81.25 (79.76)
[30/157]	0.0987 (0.0990)	0.0624 (0.0614)	0.673 (0.663)	81.25 (80.24)
[40/157]	0.0987 (0.0990)	0.0621 (0.0616)	0.581 (0.674)	81.25 (79.80)
[50/157]	0.0980 (0.0990)	0.0613 (0.0615)	0.634 (0.683)	81.25 (79.72)
[60/157]	0.0991 (0.0990)	0.0613 (0.0615)	0.671 (0.698)	78.12 (79.00)
[70/157]	0.0988 (0.0990)	0.0618 (0.0615)	0.841 (0.694)	71.88 (79.09)
[80/157]	0.0989 (0.0990)	0.0615 (0.0616)	0.880 (0.701)	75.00 (79.01)
[90/157]	0.1000 (0.0991)	0.0613 (0.0615)	0.684 (0.705)	78.12 (78.54)
[100/157]	0.0954 (0.0990)	0.0589 (0.0614)	0.717 (0.707)	84.38 (78.62)
[110/157]	0.0933 (0.0986)	0.0566 (0.0611)	0.833 (0.710)	71.88 (78.66)
[120/157]	0.0963 (0.0983)	0.0595 (0.0609)	0.671 (0.708)	81.25 (78.69)
[130/157]	0.0939 (0.0981)	0.0567 (0.0607)	0.424 (0.705)	90.62 (78.86)
[140/157]	0.0954 (0.0979)	0.0590 (0.0605)	0.496 (0.707)	87.50 (78.77)
[150/157]	0.0965 (0.0977)	0.0586 (0.0603)	0.870 (0.707)	68.75 (78.64)
[156/157]	0.0780 (0.0975)	0.0530 (0.0602)	0.942 (0.706)	75.00 (78.60)
 * Train Acc 78.600
 * Val Acc 80.400, Total time 0.58
 * Val loss 0.622, Total time 0.00
Epoch:79
LR: 1.25e-05
LR: 0.00025
LR: 0.000125
Itr		Time		  Data		  Loss		Acc
[0/157]	0.0434 (0.0434)	0.0090 (0.0090)	0.626 (0.626)	81.25 (81.25)
[10/157]	0.0922 (0.0919)	0.0562 (0.0541)	0.544 (0.726)	84.38 (75.85)
[20/157]	0.1044 (0.0989)	0.0673 (0.0612)	0.438 (0.709)	93.75 (78.42)
[30/157]	0.0972 (0.0977)	0.0605 (0.0605)	0.851 (0.708)	78.12 (78.93)
[40/157]	0.0967 (0.0971)	0.0604 (0.0602)	0.958 (0.709)	68.75 (78.43)
[50/157]	0.0947 (0.0967)	0.0582 (0.0598)	0.630 (0.714)	81.25 (78.49)
[60/157]	0.0969 (0.0965)	0.0601 (0.0597)	0.838 (0.725)	68.75 (78.07)
[70/157]	0.0951 (0.0964)	0.0581 (0.0596)	0.653 (0.712)	87.50 (78.70)
[80/157]	0.0946 (0.0963)	0.0587 (0.0594)	0.630 (0.714)	84.38 (78.63)
[90/157]	0.0961 (0.0962)	0.0593 (0.0594)	0.651 (0.707)	78.12 (78.78)
[100/157]	0.0933 (0.0961)	0.0587 (0.0594)	0.564 (0.716)	84.38 (78.43)
[110/157]	0.0959 (0.0961)	0.0582 (0.0593)	0.330 (0.712)	96.88 (78.43)
[120/157]	0.0951 (0.0961)	0.0579 (0.0592)	0.459 (0.710)	87.50 (78.56)
[130/157]	0.0960 (0.0960)	0.0596 (0.0592)	0.637 (0.711)	84.38 (78.51)
[140/157]	0.0968 (0.0961)	0.0588 (0.0592)	0.653 (0.711)	84.38 (78.63)
[150/157]	0.0954 (0.0960)	0.0588 (0.0591)	0.508 (0.708)	84.38 (78.73)
[156/157]	0.0778 (0.0959)	0.0525 (0.0591)	0.593 (0.706)	75.00 (78.70)
 * Train Acc 78.700
 * Val Acc 80.700, Total time 0.58
 * Val loss 0.623, Total time 0.00
validation split name: 1
 * Val Acc 85.600, Total time 0.60
 * Val loss 0.758, Total time 0.00
**************************************************
training split name: 1
 * Val Acc 98.520, Total time 3.25
 * Val loss 0.046, Total time 0.00
**************************************************
validation split name: 2
 * Val Acc 72.800, Total time 0.60
 * Val loss 0.808, Total time 0.00
**************************************************
training split name: 2
 * Val Acc 73.620, Total time 3.25
 * Val loss 0.759, Total time 0.00
**************************************************
validation split name: 3
 * Val Acc 69.800, Total time 0.59
 * Val loss 0.878, Total time 0.00
**************************************************
training split name: 3
 * Val Acc 74.700, Total time 3.24
 * Val loss 0.770, Total time 0.00
**************************************************
validation split name: 4
 * Val Acc 69.200, Total time 0.59
 * Val loss 0.901, Total time 0.00
**************************************************
training split name: 4
 * Val Acc 72.360, Total time 3.23
 * Val loss 0.808, Total time 0.00
**************************************************
validation split name: 5
 * Val Acc 72.700, Total time 0.58
 * Val loss 0.855, Total time 0.00
**************************************************
training split name: 5
 * Val Acc 74.120, Total time 3.22
 * Val loss 0.740, Total time 0.00
**************************************************
validation split name: 6
 * Val Acc 72.300, Total time 0.58
 * Val loss 0.776, Total time 0.00
**************************************************
training split name: 6
 * Val Acc 75.060, Total time 3.25
 * Val loss 0.702, Total time 0.00
**************************************************
validation split name: 7
 * Val Acc 72.800, Total time 0.58
 * Val loss 0.795, Total time 0.00
**************************************************
training split name: 7
 * Val Acc 74.860, Total time 3.24
 * Val loss 0.738, Total time 0.00
**************************************************
validation split name: 8
 * Val Acc 72.500, Total time 0.58
 * Val loss 0.818, Total time 0.00
**************************************************
training split name: 8
 * Val Acc 72.200, Total time 3.21
 * Val loss 0.797, Total time 0.00
**************************************************
validation split name: 9
 * Val Acc 67.600, Total time 0.58
 * Val loss 0.948, Total time 0.00
**************************************************
training split name: 9
 * Val Acc 70.300, Total time 3.22
 * Val loss 0.850, Total time 0.00
**************************************************
validation split name: 10
 * Val Acc 80.700, Total time 0.58
 * Val loss 0.623, Total time 0.00
**************************************************
training split name: 10
 * Val Acc 81.060, Total time 3.20
 * Val loss 0.574, Total time 0.00
**************************************************
OrderedDict([('1', OrderedDict([('1', 88.2), ('2', 83.9), ('3', 84.1), ('4', 82.4), ('5', 85.1), ('6', 85.1), ('7', 83.9), ('8', 84.1), ('9', 84.8), ('10', 85.6)])), ('2', OrderedDict([('2', 72.9), ('3', 69.9), ('4', 71.6), ('5', 71.9), ('6', 71.9), ('7', 70.1), ('8', 69.5), ('9', 72.0), ('10', 72.8)])), ('3', OrderedDict([('3', 75.9), ('4', 66.8), ('5', 69.1), ('6', 66.0), ('7', 64.0), ('8', 66.0), ('9', 67.9), ('10', 69.8)])), ('4', OrderedDict([('4', 73.2), ('5', 69.7), ('6', 69.3), ('7', 68.3), ('8', 68.5), ('9', 65.0), ('10', 69.2)])), ('5', OrderedDict([('5', 74.4), ('6', 71.5), ('7', 72.8), ('8', 73.0), ('9', 71.0), ('10', 72.7)])), ('6', OrderedDict([('6', 73.4), ('7', 70.6), ('8', 69.9), ('9', 72.9), ('10', 72.3)])), ('7', OrderedDict([('7', 72.1), ('8', 71.9), ('9', 65.8), ('10', 72.8)])), ('8', OrderedDict([('8', 73.0), ('9', 69.3), ('10', 72.5)])), ('9', OrderedDict([('9', 73.1), ('10', 67.6)])), ('10', OrderedDict([('10', 80.7)]))])
Task 1 average acc: 88.2
Task 1 backward transfer: 0
Task 2 average acc: 78.4
Task 2 backward transfer: -4.299999999999997
Task 3 average acc: 76.63333333333334
Task 3 backward transfer: -3.5500000000000043
Task 4 average acc: 73.5
Task 4 backward transfer: -5.400000000000006
Task 5 average acc: 74.04
Task 5 backward transfer: -3.600000000000005
Task 6 average acc: 72.86666666666667
Task 6 backward transfer: -4.1600000000000055
Task 7 average acc: 71.6857142857143
Task 7 backward transfer: -4.716666666666673
Task 8 average acc: 71.9875
Task 8 backward transfer: -3.8857142857142883
Task 9 average acc: 71.31111111111112
Task 9 backward transfer: -4.3000000000000025
Task 10 average acc: 73.6
Task 10 backward transfer: -2.322222222222226
===Summary of experiment repeats: 1 / 1 ===
The last avg acc of all repeats: [73.6]
The last bwt of all repeats: [-2.32222222]
acc mean: 73.6 acc std: 0.0
bwt mean: -2.322222222222226 bwt std: 0.0
